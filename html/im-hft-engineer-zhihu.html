<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>我是高频交易工程师</title>
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="dirtysalt" />
<link rel="shortcut icon" href="/themes/favicon.ico" /><link rel="stylesheet" type="text/css" href="/themes/simple.css"/></head>
<body>
<div id="content">
<h1 class="title">我是高频交易工程师</h1>
<p>
我是高频交易工程师：知乎董可人自选集 (知乎「盐」系列)
</p>

<p>
作者：董可人、 知乎
</p>

<p>
第一，交易本身和其他人类社会的行为一样，受到表面信息的影响很大。在商业社会，任何企业都需要做广告来吸引潜在客户。而对交易来说，交易量本身即是重要信息，交易量越大，越说明对应的商品容易买卖（所谓流动性好），人们就更愿意进行交易。因此，即使是单一的交易所，也有动机通过雇佣做市商来增加市场上的交易量，从而吸引更多客户。
</p>

<p>
高频做市策略的动机和驱动力：在市场碎片化的大背景下，各家交易所为提高自身竞争力从而吸引更多交易量，必须聘请高水平做市商入驻，帮助改进流动性质量。 ·  高频做市商所做交易的行为本质：这种交易只求参与其中，作为买卖双方的中转即可，并不需要追求买卖价差的绝对数值。极端情况下，参与程度（Participation Rate）本身即为所出售的产品，而具体的某笔交易并不必以盈利为目的。
</p>

<p>
目前（2014 年）的情况是，这个延迟如果平均控制在个位数字微秒级就是顶级了。因为网络传输才是延迟的大头，如果网络上的平均延迟是 1 毫秒（1000 微秒）以上，你的单机延迟是 2 微秒还是 20 微秒其实是没有区别的。一般单机比网络低一个数量级就可以了，比如网络上需要 100 微秒（很现实的数字），单机控制在 10 微秒足以保证速度上没有劣势。至于公众报道，有时是为博人眼球，难免有夸大的成分，不必太当真。
</p>

<p>
首先，网络架设上光纤肯定是最差的方案。国外几个主要的交易所（同一洲内）之间基本上都有微波（microwave/milliwave）线路，比光纤的延迟要低很多，延迟敏感的应用一定要选择这种线路。这个差距首先受制于光在光纤中的传播速度只有在空气中的 2/3 左右。另外，在大城市建筑密集地区（也正是一般交易所的所在地），光纤的复杂布线会进一步增大延迟，差距可能增至 2 到 3 倍。
</p>

<p>
但微波技术有两个主要的缺点：第一是微波在空气里传播受天气影响很大，刮风下雨都会导致通信受损，有时直接故障，所以需要备用的光纤线路，以及监控天气…… 这方面进一步的发展可能是激光技术；第二是带宽太小，如果是跨交易所的业务，不可能通过微波来转移大流量的市场数据，只能用来收发下单指令，这方面有一些潜在发展空间，比如可以做一点有损压缩，传一个缩减版的市场数据，也能起到加快信息传递作用。这块网络服务本身就是一个独立的业务了，一般所说的 colocation 也是由服务商负责的，HFT 主要需要的是选择适合自己的服务商。
</p>

<p>
网络线路确定以后，数据就送到了 HFT 主机。这时候需要决定网卡的方案。专用的网卡除了自身硬件的设计外，一定需要的是切换掉系统自带的 kernel space TCP/IP stack，避免昂贵的 context switching。网络栈上的 I/O 延迟，收包、发包加起来做到 2～3 微秒是可以的。这个层面上 FPGA 是很有应用价值的，因为可以做一些额外的逻辑处理，进一步解放 CPU。
</p>

<p>
网络部分的问题解决以后，最后就是核心的业务逻辑的处理。这部分也许会用到一些数学建模，但是没有什么神话，并非什么菲尔兹奖得主才能搞的东西（那些人的用武之地更多是去投行那边做衍生品，那才是真正需要高等数学的东西）。很多时候核心的还是延迟，这个在计算机内部分两个部分：一是 Core 的使用率，比如 IRQ Balance，cpuisol，Affinity 等等，主要是要尽可能地独占 Core；另一个是 Cache Invalidation，从 L1/L2/L3 cache 到 TLB，Page Fault，Memory Locality 之类的，都要仔细考虑——这个更多考验的是对体系结构的理解和程序设计的功力，跟语言的关系不大。
</p>

<p>
操作系统同样是一个不需要神话的东西，普通的 Linux 已经有足够的空间用来做性能优化。简单地说，一个企业级的 Linux（如 Redhat）加上通用的架构（Intel 主流处理器）足以做到市面上已知的最低延迟，不必幻想有什么奇妙的软硬件可以做出超出想象的事情。
</p>

<p>
因为是自动化交易系统，人工干预的部分肯定比较小，所以图形界面不是重点。而为了性能考虑，图形界面需要和后台分开部署在不同的机器上，通过网络交互，以免任何图形界面上的问题导致后台系统故障或者被抢占资源。这样又要在后台增加新的 TCP/IP 连接。
</p>

<p>
高频交易系统对延迟异常敏感，目前（2014 年）市面上的主流系统（可以直接买到的大众系统）延迟至少在 100 微秒级别，顶尖的系统（HFT 专有）可以达到 10 微秒以下。有人说低延迟不难 C++随便写写，延迟轻松做到几百微秒，但这是肯定不行的，这样的性能对于高频交易来说会是一场灾难。
</p>

<p>
有人认为要压低平时的资源使用率，这是完全错误的设计思路。问题同样出在上下文切换上，一旦系统进入 IDLE 状态，再重新切换回处理模式是要付出时间代价的。正确的做法是在线程同步代码中保持对共享变量/内存区的疯狂轮询，一旦有消息就立刻处理，之后继续轮询，这样是最快的处理方式。（顺便提一下，现在的 CPU 一般会带有环保功能，使用率低了会导致 CPU 进入低功耗模式，同样对性能有严重影响。真正的低延迟系统一定是永远发烫的！）
</p>

<p>
对于策略工程师来说，拿到一份更新频率达到秒级以下的高频数据， 首先要面对的问题是该怎样建立时序模型。常见的时间序列（Time Series）方法，一般要求数据的时间点是等间距分布的，但是交易数据有时并不能满足这个条件，我们可能会发现在一段时间内出现频繁交易，在另一段时间内市场又趋于平静。针对这个问题，对交易数据建模就需要进行一些特别的处理，这里提供两种思路。
</p>

<p>
而如果从实际的角度说，我觉得策略的思路是比模型更重要的事情。毫秒级甚至以下这个尺度上，基本上就是统计套利和高频做市的天下，若是做别的，我个人认为难度都会太大而不划算了。如果你的交易思路明确了，再选择什么模型其实就是按需决定而已。不要本末倒置。
</p>

<p>
如果关心高频，那么处理的就是直连交易所收到的原始数据，可以完全重构 Order Book，数据量也远超其他形式。这个时候还是根据具体问题来分析。比如你关心交易数据，那么其他的委托修改、撤销之类的消息都可以忽略了，一下子就剪掉很多数据；如果关心的是 Order Book 动态变化过程，所有的消息就都应该考虑，这样当然计算量也会变得非常大。
</p>

<p>
需要强调的是，虽然真正的 Order Book 只存在于交易所内部，所有交易都在交易所内完成，但是交易所会把每笔报价和市价单都转发给所有人，因此所有的买家和卖家都可以自己维护一个同样的数据结构，相当于交易所 Order Book 的镜像。通过跟踪分析自己手里这份的镜像变化，来制定交易策略，是高频交易算法的核心思想。
</p>

<p>
冰山订单的存在，一定程度上反映了挂单人对市场情况的解读，是其认为有必要使用冰山订单而做出的判断。需要强调的是，使用冰山订单并不是没有代价的，因为你隐藏了真实的需求，在屏蔽掉潜在的攻击者的同时，也屏蔽掉了真正的交易者！而且会使得成交时间显著增加－－因为没人知道你想买／卖这么多，你只能慢慢等待对手盘的出现。所以当有人下决定发出冰山订单的时候，也会有对市场情况的考虑，只有等到合适的时机才会做这种选择。
</p>

<p>
高频世界里，有一条永恒的建模准则值得铭记：先看数据再建模。如果你看了上面的介绍就开始天马行空地思考数学模型，那基本上是死路一条。我见过很多年轻人，特别有热情，一上来就开始做数学定义，然后推导偏微分方程，数学公式写满一摞纸，最后一接触数据才发现模型根本行不通。这是非常遗憾的。
</p>

<p>
任何行业的发展都是由简单粗暴的个人手工作坊向着精细分工的工业化生产前进，量化交易也不例外。在这个行业越久，就越会发现所谓策略只是众多环节中的一环。因此，自己闭关面壁苦练出一条绝世策略，从而走上人生巅峰，这样的事情还是不要妄想的好。
</p>

<p>
Q 是指风险中性测度。风险中性的意思主要是说历史数据不能帮助你预测未来的走势，所以你的决策是没有风险补偿的。这当然是一个非常虚幻的假设，但是由此而得的模型可以给出漂亮的数学性质，而且可以在缺乏数据的情况下得到一些结论，所以有一定的实际意义。涉及的数学技术主要是随机过程、偏微分方程之类。在数学派系里，这些显得相对高端，一般人概念里都是那些思维不同于常人的人捣鼓的玩意。
</p>

<p>
P 是指真实概率测度。所谓真实，主要是说模型依赖的概率分布是从历史数据上估算出来的。严格来讲，我个人不认为这种东西叫做「真实」，最多只能说是从真实数据上估算出来的，显然没有什么东西保证历史一定会重演（比如黑天鹅）。但是这个是目前大家公认的说法，所以咱们不较真。从定义可以看出这套方法主要依赖数据，数据量越大估算的效果越好。涉及的技术主要是时间序列（ARIMA，GARCH 之类），Bayesian，以及现在流行的机器学习等方法。不难看出，为了倒腾数据，这套方法练到上层就要开始刷装备。在电子化时代这最终演化为拼机房的「军备竞赛」。
</p>

<p>
两者对比可以看出：Q 重模型而轻数据，P 则重数据而轻模型。当然两者也都要即有模型也有数据，但从应用上来讲，Q 者是模型固定，用数据来精化模型的参数（calibration）；P 者则可以有若干备选模型，由数据的计算结果来选择最佳的模型（estimation）。
</p>

<p>
我们知道，长期经受理工科思维训练的人，很容易沉迷于甚至迷信技术。但是要记住，那些位居高位的权臣们，绝非按照数学定理或是电脑程序来出牌。他们甚至有时还会做出让你匪夷所思的行为。你要理解，这并非他们的错，因为这就是人性，是人性啊！
</p>

<p>
谨记始皇遗训：模型未稳市场变，土豪原来不读书。这个世界的残酷之处正在于，财富和权力往往掌握在你无法理解的人手中。而他们对于这些资源的使用方式很可能超出你的想象。
</p>

<p>
专家要控制自己的情感，并靠理性而行动。他们不仅具备较强的专业知识和技能以及较强的伦理道德，而且无一例外地将顾客放在第一位，具备永不衰竭的好奇心和进取心，严格遵守纪律。以上条件全部具备的人，我们称之为专家。 无论前提条件发生多大的变化，都能够认清深层变化的本质，比别人发挥出更大的能力，这样的人才是专家。
</p>
</div>
</body>
</html>
