<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Building Software Systems at Google and Lessons Learned</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="dirtysalt" />
<link rel="shortcut icon" href="/themes/favicon.ico" /><link rel="stylesheet" type="text/css" href="/themes/simple.css"/></head>
<body>
<div id="content">
<h1 class="title">Building Software Systems at Google and Lessons Learned</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orged2fad3">1. Outlines</a></li>
<li><a href="#org7ddc588">2. Google Web Search: 1999 vs. 2010</a>
<ul>
<li><a href="#org9c32bda">2.1. Caching in Web Search</a></li>
<li><a href="#org8939801">2.2. Indexing (circa 1998-1999)</a></li>
<li><a href="#orga379e13">2.3. Early 2001: In-Memory Index</a></li>
<li><a href="#org64dcf53">2.4. Canary Requests</a></li>
<li><a href="#orge0e0132">2.5. Query Serving System, 2004 edition</a></li>
<li><a href="#org89b3563">2.6. Encodings</a></li>
<li><a href="#org3a2d81d">2.7. 2007: Universal Search</a></li>
</ul>
</li>
<li><a href="#orgefd70d7">3. System Software Evolution</a></li>
<li><a href="#org2498f74">4. System Building Experiences and Patterns</a>
<ul>
<li><a href="#org33f01ca">4.1. Many Internal Services</a></li>
<li><a href="#org379226e">4.2. Designing Efficient Systems</a></li>
<li><a href="#orge9e998d">4.3. Designing &amp; Building Infrastructure</a></li>
<li><a href="#org5648f7d">4.4. Design for Growth</a></li>
<li><a href="#org34f40fa">4.5. Pattern: Single Master, 1000s of Workers</a></li>
<li><a href="#org859e6a7">4.6. Pattern: Tree Distribution of Requests</a></li>
<li><a href="#org5aefb7f">4.7. Pattern: Backup Requests to Minimize Latency</a></li>
<li><a href="#org1585436">4.8. Pattern: Multiple Smaller Units per Machine</a></li>
<li><a href="#org26de1cf">4.9. Pattern: Elastic Systems</a></li>
<li><a href="#org7e643de">4.10. Pattern: Combine Multiple Implementations</a></li>
</ul>
</li>
<li><a href="#org5b12b17">5. Final Thoughts</a></li>
</ul>
</div>
</div>
<p>
Jeff Dean @ 2010
</p>

<div id="outline-container-orged2fad3" class="outline-2">
<h2 id="orged2fad3"><span class="section-number-2">1</span> Outlines</h2>
<div class="outline-text-2" id="text-1">
<ul class="org-ul">
<li>Evolution of various systems at Google
<ul class="org-ul">
<li>computing hardware</li>
<li>core search systems</li>
<li>infrastructure software</li>
</ul></li>
<li>Techniques for building large-scale systems
<ul class="org-ul">
<li>decomposition into services</li>
<li>design patterns for performance &amp; reliability</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org7ddc588" class="outline-2">
<h2 id="org7ddc588"><span class="section-number-2">2</span> Google Web Search: 1999 vs. 2010</h2>
<div class="outline-text-2" id="text-2">
<ul class="org-ul">
<li># docs: tens of millions to tens of billions <b>~1000X</b></li>
<li>queries processed/day: <b>~1000X</b></li>
<li>per doc info in index: <b>~3X</b></li>
<li>update latency: months to tens of secs <b>~50000X</b> <b>这个似乎加快了不少，搜索实时性是一个很重要的问题</b></li>
<li>avg. query latency: &lt;1s to &lt;0.2s <b>~5X</b></li>
<li>More machines * faster machines: <b>~1000X</b></li>
<li>Continuous evolution:
<ul class="org-ul">
<li>7 significant revisions in last 11 years</li>
<li>often rolled out without users realizing we’ve made major changes</li>
</ul></li>
</ul>
</div>

<div id="outline-container-org9c32bda" class="outline-3">
<h3 id="org9c32bda"><span class="section-number-3">2.1</span> Caching in Web Search</h3>
<div class="outline-text-3" id="text-2-1">
<ul class="org-ul">
<li>Cache servers:
<ul class="org-ul">
<li>cache both index results and doc snippets</li>
<li>hit rates typically 30-60%
<ul class="org-ul">
<li>depends on frequency of index updates, mix of query traffic, level of personalization, etc</li>
</ul></li>
</ul></li>
<li>Main benefits:
<ul class="org-ul">
<li>performance! a few machines do work of 100s or 1000s</li>
<li>much lower query latency on hits
<ul class="org-ul">
<li>queries that hit in cache tend to be both popular and expensive (common words, lots of documents to score, etc.)</li>
</ul></li>
</ul></li>
<li>Beware: big latency spike/capacity drop when index updated or cache flushed <b>使用cache系统就必须注意cache挂掉时候带来的latency spike</b></li>
</ul>


<div class="figure">
<p><img src="images/google-serving-system-at-1999.png" alt="google-serving-system-at-1999.png" />
</p>
</div>
</div>
</div>


<div id="outline-container-org8939801" class="outline-3">
<h3 id="org8939801"><span class="section-number-3">2.2</span> Indexing (circa 1998-1999)</h3>
<div class="outline-text-3" id="text-2-2">
<ul class="org-ul">
<li>Simple batch indexing system
<ul class="org-ul">
<li>No real checkpointing, so machine failures painful 在做索引的时候没有checkpoint,所以机器故障会非常麻烦</li>
<li>No checksumming of raw data, so hardware bit errors caused problems 对于大规模数据来说，checksum还是非常必须的。</li>
<li>Exacerbated by early machines having no ECC, no parity</li>
<li>Sort 1 TB of data without parity: ends up "mostly sorted"</li>
<li>Sort it again: "mostly sorted" another way</li>
</ul></li>
<li>“Programming with adversarial memory”
<ul class="org-ul">
<li>Developed file abstraction that stores checksums of small records and can skip and resynchronize after corrupted records</li>
</ul></li>
</ul>


<div class="figure">
<p><img src="images/google-serving-system-at-1999-dealing-with-growth.png" alt="google-serving-system-at-1999-dealing-with-growth.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-orga379e13" class="outline-3">
<h3 id="orga379e13"><span class="section-number-3">2.3</span> Early 2001: In-Memory Index</h3>
<div class="outline-text-3" id="text-2-3">
<ul class="org-ul">
<li>解决index server压力的办法是通过添加更多的index replicas,但是突然有一天发现 <b>Eventually have enough replicas so that total memory across all index machines can hold ONE entire copy of index in memory</b>  replicas机器已经足够多，在内存完全可以存放下index. 然后变成如下结构</li>
<li>Many positives:
<ul class="org-ul">
<li>big increase in throughput</li>
<li>big decrease in query latency</li>
</ul></li>
<li>Some issues:
<ul class="org-ul">
<li>Variance: query touches 1000s of machines, not dozens
<ul class="org-ul">
<li><b>因为需要查询更多的机器，因此查询的变化也会非常大，不太容易控制差异</b></li>
<li>e.g. randomized cron jobs caused us trouble for a while</li>
</ul></li>
<li>Availability: 1 or few replicas of each doc’s index data
<ul class="org-ul">
<li>Availability of index data when machine failed (esp for important docs): replicate important docs</li>
<li>Queries of death that kill all the backends at once: very bad</li>
<li><b>存在一个问题就是，一个query可能会因为system bug造成crash. 如何解决这个问题，就是下面的canary request</b></li>
</ul></li>
</ul></li>
</ul>


<div class="figure">
<p><img src="images/google-serving-system-at-2001.png" alt="google-serving-system-at-2001.png" />
</p>
</div>
</div>
</div>


<div id="outline-container-org64dcf53" class="outline-3">
<h3 id="org64dcf53"><span class="section-number-3">2.4</span> Canary Requests</h3>
<div class="outline-text-3" id="text-2-4">
<ul class="org-ul">
<li>主要就是针对特定query会造成system crash.但是我们不希望所有的nodes都crash.</li>
<li>Problem: requests sometimes cause server process to crash
<ul class="org-ul">
<li>testing can help reduce probability, but can’t eliminate</li>
</ul></li>
<li>If sending same or similar request to 1000s of machines:
<ul class="org-ul">
<li>they all might crash!</li>
<li>recovery time for 1000s of processes pretty slow</li>
</ul></li>
<li>Solution: send canary request first to one machine 可以首先将request发送到一个节点上，如果正常的话那么发送到其他节点，否则重试几次失败就放弃。
<ul class="org-ul">
<li>if RPC finishes successfully, go ahead and send to all the rest</li>
<li>if RPC fails unexpectedly, try another machine (might have just been coincidence)</li>
<li>if fails K times, reject request</li>
</ul></li>
<li>Crash only a few servers, not 1000s</li>
</ul>
</div>
</div>

<div id="outline-container-orge0e0132" class="outline-3">
<h3 id="orge0e0132"><span class="section-number-3">2.5</span> Query Serving System, 2004 edition</h3>
<div class="outline-text-3" id="text-2-5">
<p>
我觉得这个应该是google query serving system的最终模型了。leaf所有的数据都是全内存的，但是在GFS上面有on-disk数据做持久化，或者说leaf是一个简单的query system + bigtable吧。之所有使用这种multi-level tree结构在这篇文章后面的pattern部分会说到。
</p>


<div class="figure">
<p><img src="images/google-serving-system-at-2004.png" alt="google-serving-system-at-2004.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-org89b3563" class="outline-3">
<h3 id="org89b3563"><span class="section-number-3">2.6</span> Encodings</h3>
<div class="outline-text-3" id="text-2-6">
<ul class="org-ul">
<li>Byte-Aligned Variable-length Encodings
<ul class="org-ul">
<li>7 bits per byte with continuation bit
<ul class="org-ul">
<li>Con: Decoding requires lots of branches/shifts/masks</li>
</ul></li>
<li>Encode byte length using 2 bits
<ul class="org-ul">
<li>Better: fewer branches, shifts, and masks</li>
<li>Con: Limited to 30-bit values, still some shifting to decode</li>
</ul></li>
</ul></li>
<li>Group Varint Encoding
<ul class="org-ul">
<li>encode groups of 4 32-bit values in 5-17 bytes</li>
<li>Pull out 4 2-bit binary lengths into single byte prefix</li>
<li>Much faster than alternatives:
<ul class="org-ul">
<li>7-bit-per-byte varint: decode ~180M numbers/second</li>
<li>30-bit Varint w/ 2-bit length: decode ~240M numbers/second</li>
<li>Group varint: decode ~400M numbers/second</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org3a2d81d" class="outline-3">
<h3 id="org3a2d81d"><span class="section-number-3">2.7</span> 2007: Universal Search</h3>
<div class="outline-text-3" id="text-2-7">
<p>
从多个产品整合搜索结果，但是有下面这些问题：
</p>
<ul class="org-ul">
<li>Performance: most of the corpora weren’t designed to deal with high QPS level of web search 性能匹配</li>
<li>Mixing: Which corpora are relevant to query? 相关性</li>
<li>UI: How to organize results from different corpora? UI布局</li>
</ul>


<div class="figure">
<p><img src="images/google-universal-search-at-2007.png" alt="google-universal-search-at-2007.png" />
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgefd70d7" class="outline-2">
<h2 id="orgefd70d7"><span class="section-number-2">3</span> System Software Evolution</h2>
<div class="outline-text-2" id="text-3">
<ul class="org-ul">
<li>The Joys of Real Hardware (Typical first year for a new cluster):
<ul class="org-ul">
<li>~1 network rewiring (rolling ~5% of machines down over 2-day span)</li>
<li>~20 rack failures (40-80 machines instantly disappear, 1-6 hours to get back)</li>
<li>~5 racks go wonky (40-80 machines see 50% packetloss)</li>
<li>~8 network maintenances (4 might cause ~30-minute random connectivity losses)</li>
<li>~12 router reloads (takes out DNS and external vips for a couple minutes)</li>
<li>~3 router failures (have to immediately pull traffic for an hour)</li>
<li>~dozens of minor 30-second blips for dns</li>
<li>~1000 individual machine failures</li>
<li>~thousands of hard drive failures</li>
<li>slow disks, bad memory, misconfigured machines, flaky machines, etc.</li>
<li>Long distance links: wild dogs, sharks, dead horses, drunken hunters, etc.</li>
<li><b>Reliability/availability must come from software!</b></li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org2498f74" class="outline-2">
<h2 id="org2498f74"><span class="section-number-2">4</span> System Building Experiences and Patterns</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-org33f01ca" class="outline-3">
<h3 id="org33f01ca"><span class="section-number-3">4.1</span> Many Internal Services</h3>
<div class="outline-text-3" id="text-4-1">
<ul class="org-ul">
<li>Break large complex systems down into many services!</li>
<li>Simpler from a software engineering standpoint
<ul class="org-ul">
<li>few dependencies, clearly specified</li>
<li>easy to test and deploy new versions of individual services</li>
<li>ability to run lots of experiments</li>
<li>easy to reimplement service without affecting clients</li>
</ul></li>
<li>Development cycles largely decoupled
<ul class="org-ul">
<li>lots of benefits: small teams can work independently</li>
<li>easier to have many engineering offices around the world</li>
</ul></li>
<li>e.g. google.com search touches 200+ services
<ul class="org-ul">
<li>ads, web search, books, news, spelling correction, &#x2026;</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org379226e" class="outline-3">
<h3 id="org379226e"><span class="section-number-3">4.2</span> Designing Efficient Systems</h3>
<div class="outline-text-3" id="text-4-2">
<ul class="org-ul">
<li>Given a basic problem definition, how do you choose "best" solution?
<ul class="org-ul">
<li>Best might be simplest, highest performance, easiest to extend, etc.</li>
</ul></li>
<li>Back of the Envelope Calculations</li>
<li>Know Your Basic Building Blocks
<ul class="org-ul">
<li>Core language libraries, basic data structures, protocol buffers, GFS, BigTable, indexing systems, MapReduce, &#x2026;</li>
<li>Not just their interfaces, but understand their implementations (at least at a high level)</li>
<li>If you don’t know what’s going on, you can’t do decent back-of-the-envelope calculations!</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orge9e998d" class="outline-3">
<h3 id="orge9e998d"><span class="section-number-3">4.3</span> Designing &amp; Building Infrastructure</h3>
<div class="outline-text-3" id="text-4-3">
<ul class="org-ul">
<li>Identify common problems, and build software systems to address them in a general way <b>尝试从general角度解决问题，这样才能够做出infrastructure</b></li>
<li>Important to not try to be all things to all people <b>但是对不同需求需要不同对待，不一定需要将解决方案放在一个实现里面</b>
<ul class="org-ul">
<li>Clients might be demanding 8 different things</li>
<li>Doing 6 of them is easy</li>
<li>&#x2026;handling 7 of them requires real thought</li>
<li>&#x2026;dealing with all 8 usually results in a worse system</li>
<li>more complex, compromises other clients in trying to satisfy everyone</li>
</ul></li>
<li>Don't build infrastructure just for its own sake: <b>设计通用组件的话，还需要去排除那些潜在的不需要的需求，抑制复杂性</b>
<ul class="org-ul">
<li>Identify common needs and address them</li>
<li>Don't imagine unlikely potential needs that aren't really there</li>
</ul></li>
<li>Best approach: use your own infrastructure (especially at first!)
<ul class="org-ul">
<li>(much more rapid feedback about what works, what doesn't)</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org5648f7d" class="outline-3">
<h3 id="org5648f7d"><span class="section-number-3">4.4</span> Design for Growth</h3>
<div class="outline-text-3" id="text-4-4">
<ul class="org-ul">
<li>Try to anticipate how requirements will evolve keep likely features in mind as you design base system</li>
<li>Don’t design to scale infinitely: <b>扩展性只需要考虑5x-50x左右的扩展即可</b>
<ul class="org-ul">
<li>~5X - 50X growth good to consider</li>
<li>&gt;100X probably requires rethink and rewrite</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org34f40fa" class="outline-3">
<h3 id="org34f40fa"><span class="section-number-3">4.5</span> Pattern: Single Master, 1000s of Workers</h3>
<div class="outline-text-3" id="text-4-5">
<p>
master主要完成全局性质的工作，其余工作交给worker完成。通常存在hot standby来做failover. 优点是可以很容易地进行全局控制，但是实现上必须小心，而缺点非常明显就是支撑worker不会很多，在1k级别上。如果涉及到更大规模集群的话，那么worker需要和master有更加频繁的交互，这对于master压力会非常大。
</p>

<ul class="org-ul">
<li>Master orchestrates global operation of system
<ul class="org-ul">
<li>load balancing, assignment of work, reassignment when machines fail, etc.</li>
<li>&#x2026; but client interaction with master is fairly minimal</li>
<li>Often: hot standby of master waiting to take over</li>
<li>Always: bulk of data transfer directly between clients and workers</li>
</ul></li>
<li>Examples:
<ul class="org-ul">
<li>GFS, BigTable, MapReduce, file transfer service, cluster scheduling system, &#x2026;</li>
</ul></li>
<li>Pro:
<ul class="org-ul">
<li>simpler to reason about state of system with centralized master</li>
</ul></li>
<li>Caveats:
<ul class="org-ul">
<li>careful design required to keep master out of common case ops</li>
<li>scales to 1000s of workers, but not 100,000s of workers</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org859e6a7" class="outline-3">
<h3 id="org859e6a7"><span class="section-number-3">4.6</span> Pattern: Tree Distribution of Requests</h3>
<div class="outline-text-3" id="text-4-6">
<p>
这个模型本质上是从single master模型发展过来的，是multi master实现。随着master管理worker数目增加，CPU以及network IO都会bounded. 以single master为例，如果每个master最多管理1k worker的话，那么1k master可以由另外一个master管理，这样就可以支持1k * 1k worker级别了。
</p>

<ul class="org-ul">
<li>Problem: Single machine sending 1000s of RPCs overloads NIC on machine when handling replies
<ul class="org-ul">
<li>wide fan in causes TCP drops/retransmits, significant latency</li>
<li>CPU becomes bottleneck on single machine</li>
</ul></li>
<li>Solution: Use tree distribution of requests/responses
<ul class="org-ul">
<li>fan in at root is smaller</li>
<li>cost of processing leaf responses spread across many parents</li>
</ul></li>
<li>Most effective when parent processing can trim/combine leaf data
<ul class="org-ul">
<li>can also co-locate parents on same rack as leaves</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org5aefb7f" class="outline-3">
<h3 id="org5aefb7f"><span class="section-number-3">4.7</span> Pattern: Backup Requests to Minimize Latency</h3>
<div class="outline-text-3" id="text-4-7">
<p>
通过backup request来降低延迟，因为部分请求可能会成为straggler，这点在mapreduce里面的speculative非常经典。 #note: jeff dean在另外一篇文章tail at scale里面也提到即便如何也存在一些bad case
</p>

<ul class="org-ul">
<li>Problem: variance high when requests go to 1000s of machines
<ul class="org-ul">
<li>last few machines to respond stretch out latency tail substantially</li>
</ul></li>
<li>Often, multiple replicas can handle same kind of request</li>
<li>When few tasks remaining, send backup requests to other replicas</li>
<li>Whichever duplicate request finishes first wins
<ul class="org-ul">
<li>useful when variance is unrelated to specifics of request</li>
<li>increases overall load by a tiny percentage</li>
<li>decreases latency tail significantly</li>
</ul></li>
<li>Examples:
<ul class="org-ul">
<li>MapReduce backup tasks (granularity: many seconds)</li>
<li>various query serving systems (granularity: milliseconds)</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org1585436" class="outline-3">
<h3 id="org1585436"><span class="section-number-3">4.8</span> Pattern: Multiple Smaller Units per Machine</h3>
<div class="outline-text-3" id="text-4-8">
<p>
每个机器上部署更小的单元，可以使得调度更加容易，集群资源利用率更高。
</p>

<ul class="org-ul">
<li>Problems:
<ul class="org-ul">
<li>want to minimize recovery time when machine crashes</li>
<li>want to do fine-grained load balancing</li>
</ul></li>
<li>Having each machine manage 1 unit of work is inflexible
<ul class="org-ul">
<li>slow recovery: new replica must recover data that is O(machine state) in size</li>
<li>load balancing much harder</li>
</ul></li>
<li>Have each machine manage many smaller units of work/data
<ul class="org-ul">
<li>typical: ~10-100 units/machine</li>
<li>allows fine grained load balancing (shed or add one unit)</li>
<li>fast recovery from failure (N machines each pick up 1 unit)</li>
</ul></li>
<li>Examples:
<ul class="org-ul">
<li>map and reduce tasks, GFS chunks, Bigtable tablets, query serving system index shards</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org26de1cf" class="outline-3">
<h3 id="org26de1cf"><span class="section-number-3">4.9</span> Pattern: Elastic Systems</h3>
<div class="outline-text-3" id="text-4-9">
<p>
可伸缩的系统，自动调节整个集群资源利用率。这个东西可以打个比方，如果整个集群资源空闲的话，那么可以减少线程数目，释放一些内存让其他程序可以有效运行。而当压力比较大的时候，可以保持在一个水平不至于崩溃。
</p>

<ul class="org-ul">
<li>Problem: Planning for exact peak load is hard
<ul class="org-ul">
<li>overcapacity: wasted resources</li>
<li>undercapacity: meltdown</li>
</ul></li>
<li>Design system to adapt:
<ul class="org-ul">
<li>automatically shrink capacity during idle period</li>
<li>automatically grow capacity as load grows</li>
</ul></li>
<li>Make system resilient to overload:
<ul class="org-ul">
<li>do something reasonable even up to 2X planned capacity
<ul class="org-ul">
<li>e.g. shrink size of index searched, back off to less CPU intensive algorithms, drop spelling correction tips, etc.</li>
</ul></li>
<li>more aggressive load balancing when imbalance more severe</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org7e643de" class="outline-3">
<h3 id="org7e643de"><span class="section-number-3">4.10</span> Pattern: Combine Multiple Implementations</h3>
<div class="outline-text-3" id="text-4-10">
<p>
多种实现的结合，这点以realtime + batch说明非常直观。
</p>

<ul class="org-ul">
<li>Example: Google web search system wants all of these:
<ul class="org-ul">
<li>freshness (update documents in ~1 second)</li>
<li>massive capacity (10000s of requests per second)</li>
<li>high quality retrieval (lots of information about each document)</li>
<li>massive size (billions of documents)</li>
</ul></li>
<li>Very difficult to accomplish in single implementation</li>
<li>Partition problem into several subproblems with different engineering tradeoffs. E.g.
<ul class="org-ul">
<li>realtime system: few docs, ok to pay lots of $$$/doc</li>
<li>base system: high # of docs, optimized for low $/doc</li>
<li>realtime+base: high # of docs, fresh, low $/doc</li>
</ul></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org5b12b17" class="outline-2">
<h2 id="org5b12b17"><span class="section-number-2">5</span> Final Thoughts</h2>
<div class="outline-text-2" id="text-5">
<ul class="org-ul">
<li>Today: exciting collection of trends: <b>未来趋势的一些思考</b>
<ul class="org-ul">
<li>large-scale datacenters + 大规模数据中心建设</li>
<li>increasing scale and diversity of available data sets +  大量数据需要分析和挖掘</li>
<li>proliferation of more powerful client devices 各种设备接入</li>
</ul></li>

<li>Many interesting opportunities: <b>值得去做的事情</b>
<ul class="org-ul">
<li>planetary scale distributed systems 宇宙级别分布式系统</li>
<li>development of new CPU and data intensive services 新的CPU和数据密集服务</li>
<li>new tools and techniques for constructing such systems 以及构建这些服务的工具</li>
</ul></li>
</ul>
</div>
</div>
</div>
</body>
</html>
