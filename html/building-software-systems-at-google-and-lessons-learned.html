<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Building Software Systems at Google and Lessons Learned</title>
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="dirtysalt" />
<link rel="shortcut icon" href="/themes/favicon.ico" /><link rel="stylesheet" type="text/css" href="/themes/simple.css"/></head>
<body>
<div id="content">
<h1 class="title">Building Software Systems at Google and Lessons Learned</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1. Outlines</a></li>
<li><a href="#sec-2">2. Google Web Search: 1999 vs. 2010</a>
<ul>
<li><a href="#sec-2-1">2.1. Caching in Web Search</a></li>
<li><a href="#sec-2-2">2.2. Indexing (circa 1998-1999)</a></li>
<li><a href="#sec-2-3">2.3. Early 2001: In-Memory Index</a></li>
<li><a href="#sec-2-4">2.4. Canary Requests</a></li>
<li><a href="#sec-2-5">2.5. Query Serving System, 2004 edition</a></li>
<li><a href="#sec-2-6">2.6. Encodings</a></li>
<li><a href="#sec-2-7">2.7. 2007: Universal Search</a></li>
</ul>
</li>
<li><a href="#sec-3">3. System Software Evolution</a></li>
<li><a href="#sec-4">4. System Building Experiences and Patterns</a>
<ul>
<li><a href="#sec-4-1">4.1. Many Internal Services</a></li>
<li><a href="#sec-4-2">4.2. Designing Efficient Systems</a></li>
<li><a href="#sec-4-3">4.3. Designing &amp; Building Infrastructure</a></li>
<li><a href="#sec-4-4">4.4. Design for Growth</a></li>
<li><a href="#sec-4-5">4.5. Pattern: Single Master, 1000s of Workers</a></li>
<li><a href="#sec-4-6">4.6. Pattern: Tree Distribution of Requests</a></li>
<li><a href="#sec-4-7">4.7. Pattern: Backup Requests to Minimize Latency</a></li>
<li><a href="#sec-4-8">4.8. Pattern: Multiple Smaller Units per Machine</a></li>
<li><a href="#sec-4-9">4.9. Pattern: Elastic Systems</a></li>
<li><a href="#sec-4-10">4.10. Pattern: Combine Multiple Implementations</a></li>
</ul>
</li>
<li><a href="#sec-5">5. Final Thoughts</a></li>
</ul>
</div>
</div>
<p>
Jeff Dean @ 2010
</p>

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> Outlines</h2>
<div class="outline-text-2" id="text-1">
<ul class="org-ul">
<li>Evolution of various systems at Google
<ul class="org-ul">
<li>computing hardware
</li>
<li>core search systems
</li>
<li>infrastructure software
</li>
</ul>
</li>
<li>Techniques for building large-scale systems
<ul class="org-ul">
<li>decomposition into services
</li>
<li>design patterns for performance &amp; reliability
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2"><span class="section-number-2">2</span> Google Web Search: 1999 vs. 2010</h2>
<div class="outline-text-2" id="text-2">
<ul class="org-ul">
<li># docs: tens of millions to tens of billions <b>~1000X</b>
</li>
<li>queries processed/day: <b>~1000X</b>
</li>
<li>per doc info in index: <b>~3X</b>
</li>
<li>update latency: months to tens of secs <b>~50000X</b> <b>这个似乎加快了不少，搜索实时性是一个很重要的问题</b>
</li>
<li>avg. query latency: &lt;1s to &lt;0.2s <b>~5X</b>
</li>
<li>More machines * faster machines: <b>~1000X</b>
</li>
<li>Continuous evolution:
<ul class="org-ul">
<li>7 significant revisions in last 11 years
</li>
<li>often rolled out without users realizing we’ve made major changes
</li>
</ul>
</li>
</ul>
</div>

<div id="outline-container-sec-2-1" class="outline-3">
<h3 id="sec-2-1"><span class="section-number-3">2.1</span> Caching in Web Search</h3>
<div class="outline-text-3" id="text-2-1">
<ul class="org-ul">
<li>Cache servers:
<ul class="org-ul">
<li>cache both index results and doc snippets
</li>
<li>hit rates typically 30-60%
<ul class="org-ul">
<li>depends on frequency of index updates, mix of query traffic, level of personalization, etc
</li>
</ul>
</li>
</ul>
</li>
<li>Main benefits:
<ul class="org-ul">
<li>performance! a few machines do work of 100s or 1000s
</li>
<li>much lower query latency on hits
<ul class="org-ul">
<li>queries that hit in cache tend to be both popular and expensive (common words, lots of documents to score, etc.)
</li>
</ul>
</li>
</ul>
</li>
<li>Beware: big latency spike/capacity drop when index updated or cache flushed <b>使用cache系统就必须注意cache挂掉时候带来的latency spike</b>
</li>
</ul>


<div class="figure">
<p><img src="images/google-serving-system-at-1999.png" alt="google-serving-system-at-1999.png" />
</p>
</div>
</div>
</div>


<div id="outline-container-sec-2-2" class="outline-3">
<h3 id="sec-2-2"><span class="section-number-3">2.2</span> Indexing (circa 1998-1999)</h3>
<div class="outline-text-3" id="text-2-2">
<ul class="org-ul">
<li>Simple batch indexing system
<ul class="org-ul">
<li>No real checkpointing, so machine failures painful 在做索引的时候没有checkpoint,所以机器故障会非常麻烦
</li>
<li>No checksumming of raw data, so hardware bit errors caused problems 对于大规模数据来说，checksum还是非常必须的。
</li>
<li>Exacerbated by early machines having no ECC, no parity
</li>
<li>Sort 1 TB of data without parity: ends up "mostly sorted"
</li>
<li>Sort it again: "mostly sorted" another way
</li>
</ul>
</li>
<li>“Programming with adversarial memory”
<ul class="org-ul">
<li>Developed file abstraction that stores checksums of small records and can skip and resynchronize after corrupted records
</li>
</ul>
</li>
</ul>


<div class="figure">
<p><img src="images/google-serving-system-at-1999-dealing-with-growth.png" alt="google-serving-system-at-1999-dealing-with-growth.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-sec-2-3" class="outline-3">
<h3 id="sec-2-3"><span class="section-number-3">2.3</span> Early 2001: In-Memory Index</h3>
<div class="outline-text-3" id="text-2-3">
<ul class="org-ul">
<li>解决index server压力的办法是通过添加更多的index replicas,但是突然有一天发现 <b>Eventually have enough replicas so that total memory across all index machines can hold ONE entire copy of index in memory</b>  replicas机器已经足够多，在内存完全可以存放下index. 然后变成如下结构
</li>
<li>Many positives:
<ul class="org-ul">
<li>big increase in throughput
</li>
<li>big decrease in query latency
</li>
</ul>
</li>
<li>Some issues:
<ul class="org-ul">
<li>Variance: query touches 1000s of machines, not dozens
<ul class="org-ul">
<li><b>因为需要查询更多的机器，因此查询的变化也会非常大，不太容易控制差异</b>
</li>
<li>e.g. randomized cron jobs caused us trouble for a while
</li>
</ul>
</li>
<li>Availability: 1 or few replicas of each doc’s index data
<ul class="org-ul">
<li>Availability of index data when machine failed (esp for important docs): replicate important docs
</li>
<li>Queries of death that kill all the backends at once: very bad
</li>
<li><b>存在一个问题就是，一个query可能会因为system bug造成crash. 如何解决这个问题，就是下面的canary request</b>
</li>
</ul>
</li>
</ul>
</li>
</ul>


<div class="figure">
<p><img src="images/google-serving-system-at-2001.png" alt="google-serving-system-at-2001.png" />
</p>
</div>
</div>
</div>


<div id="outline-container-sec-2-4" class="outline-3">
<h3 id="sec-2-4"><span class="section-number-3">2.4</span> Canary Requests</h3>
<div class="outline-text-3" id="text-2-4">
<ul class="org-ul">
<li>主要就是针对特定query会造成system crash.但是我们不希望所有的nodes都crash.
</li>
<li>Problem: requests sometimes cause server process to crash
<ul class="org-ul">
<li>testing can help reduce probability, but can’t eliminate
</li>
</ul>
</li>
<li>If sending same or similar request to 1000s of machines:
<ul class="org-ul">
<li>they all might crash!
</li>
<li>recovery time for 1000s of processes pretty slow
</li>
</ul>
</li>
<li>Solution: send canary request first to one machine 可以首先将request发送到一个节点上，如果正常的话那么发送到其他节点，否则重试几次失败就放弃。
<ul class="org-ul">
<li>if RPC finishes successfully, go ahead and send to all the rest
</li>
<li>if RPC fails unexpectedly, try another machine (might have just been coincidence)
</li>
<li>if fails K times, reject request
</li>
</ul>
</li>
<li>Crash only a few servers, not 1000s
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-2-5" class="outline-3">
<h3 id="sec-2-5"><span class="section-number-3">2.5</span> Query Serving System, 2004 edition</h3>
<div class="outline-text-3" id="text-2-5">
<p>
我觉得这个应该是google query serving system的最终模型了。leaf所有的数据都是全内存的，但是在GFS上面有on-disk数据做持久化，或者说leaf是一个简单的query system + bigtable吧。之所有使用这种multi-level tree结构在这篇文章后面的pattern部分会说到。
</p>


<div class="figure">
<p><img src="images/google-serving-system-at-2004.png" alt="google-serving-system-at-2004.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-sec-2-6" class="outline-3">
<h3 id="sec-2-6"><span class="section-number-3">2.6</span> Encodings</h3>
<div class="outline-text-3" id="text-2-6">
<ul class="org-ul">
<li>Byte-Aligned Variable-length Encodings
<ul class="org-ul">
<li>7 bits per byte with continuation bit
<ul class="org-ul">
<li>Con: Decoding requires lots of branches/shifts/masks
</li>
</ul>
</li>
<li>Encode byte length using 2 bits
<ul class="org-ul">
<li>Better: fewer branches, shifts, and masks
</li>
<li>Con: Limited to 30-bit values, still some shifting to decode
</li>
</ul>
</li>
</ul>
</li>
<li>Group Varint Encoding
<ul class="org-ul">
<li>encode groups of 4 32-bit values in 5-17 bytes
</li>
<li>Pull out 4 2-bit binary lengths into single byte prefix
</li>
<li>Much faster than alternatives:
<ul class="org-ul">
<li>7-bit-per-byte varint: decode ~180M numbers/second
</li>
<li>30-bit Varint w/ 2-bit length: decode ~240M numbers/second
</li>
<li>Group varint: decode ~400M numbers/second
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-2-7" class="outline-3">
<h3 id="sec-2-7"><span class="section-number-3">2.7</span> 2007: Universal Search</h3>
<div class="outline-text-3" id="text-2-7">
<p>
从多个产品整合搜索结果，但是有下面这些问题：
</p>
<ul class="org-ul">
<li>Performance: most of the corpora weren’t designed to deal with high QPS level of web search 性能匹配
</li>
<li>Mixing: Which corpora are relevant to query? 相关性
</li>
<li>UI: How to organize results from different corpora? UI布局
</li>
</ul>


<div class="figure">
<p><img src="images/google-universal-search-at-2007.png" alt="google-universal-search-at-2007.png" />
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3"><span class="section-number-2">3</span> System Software Evolution</h2>
<div class="outline-text-2" id="text-3">
<ul class="org-ul">
<li>The Joys of Real Hardware (Typical first year for a new cluster):
<ul class="org-ul">
<li>~1 network rewiring (rolling ~5% of machines down over 2-day span)
</li>
<li>~20 rack failures (40-80 machines instantly disappear, 1-6 hours to get back)
</li>
<li>~5 racks go wonky (40-80 machines see 50% packetloss)
</li>
<li>~8 network maintenances (4 might cause ~30-minute random connectivity losses)
</li>
<li>~12 router reloads (takes out DNS and external vips for a couple minutes)
</li>
<li>~3 router failures (have to immediately pull traffic for an hour)
</li>
<li>~dozens of minor 30-second blips for dns
</li>
<li>~1000 individual machine failures
</li>
<li>~thousands of hard drive failures
</li>
<li>slow disks, bad memory, misconfigured machines, flaky machines, etc.
</li>
<li>Long distance links: wild dogs, sharks, dead horses, drunken hunters, etc.
</li>
<li><b>Reliability/availability must come from software!</b>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-4" class="outline-2">
<h2 id="sec-4"><span class="section-number-2">4</span> System Building Experiences and Patterns</h2>
<div class="outline-text-2" id="text-4">
</div><div id="outline-container-sec-4-1" class="outline-3">
<h3 id="sec-4-1"><span class="section-number-3">4.1</span> Many Internal Services</h3>
<div class="outline-text-3" id="text-4-1">
<ul class="org-ul">
<li>Break large complex systems down into many services!
</li>
<li>Simpler from a software engineering standpoint
<ul class="org-ul">
<li>few dependencies, clearly specified
</li>
<li>easy to test and deploy new versions of individual services
</li>
<li>ability to run lots of experiments
</li>
<li>easy to reimplement service without affecting clients
</li>
</ul>
</li>
<li>Development cycles largely decoupled
<ul class="org-ul">
<li>lots of benefits: small teams can work independently
</li>
<li>easier to have many engineering offices around the world
</li>
</ul>
</li>
<li>e.g. google.com search touches 200+ services
<ul class="org-ul">
<li>ads, web search, books, news, spelling correction, &#x2026;
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-4-2" class="outline-3">
<h3 id="sec-4-2"><span class="section-number-3">4.2</span> Designing Efficient Systems</h3>
<div class="outline-text-3" id="text-4-2">
<ul class="org-ul">
<li>Given a basic problem definition, how do you choose "best" solution?
<ul class="org-ul">
<li>Best might be simplest, highest performance, easiest to extend, etc.
</li>
</ul>
</li>
<li>Back of the Envelope Calculations
</li>
<li>Know Your Basic Building Blocks
<ul class="org-ul">
<li>Core language libraries, basic data structures, protocol buffers, GFS, BigTable, indexing systems, MapReduce, &#x2026;
</li>
<li>Not just their interfaces, but understand their implementations (at least at a high level)
</li>
<li>If you don’t know what’s going on, you can’t do decent back-of-the-envelope calculations!
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-4-3" class="outline-3">
<h3 id="sec-4-3"><span class="section-number-3">4.3</span> Designing &amp; Building Infrastructure</h3>
<div class="outline-text-3" id="text-4-3">
<ul class="org-ul">
<li>Identify common problems, and build software systems to address them in a general way <b>尝试从general角度解决问题，这样才能够做出infrastructure</b>
</li>
<li>Important to not try to be all things to all people <b>但是对不同需求需要不同对待，不一定需要将解决方案放在一个实现里面</b>
<ul class="org-ul">
<li>Clients might be demanding 8 different things
</li>
<li>Doing 6 of them is easy
</li>
<li>&#x2026;handling 7 of them requires real thought
</li>
<li>&#x2026;dealing with all 8 usually results in a worse system
</li>
<li>more complex, compromises other clients in trying to satisfy everyone
</li>
</ul>
</li>
<li>Don't build infrastructure just for its own sake: <b>设计通用组件的话，还需要去排除那些潜在的不需要的需求，抑制复杂性</b>
<ul class="org-ul">
<li>Identify common needs and address them
</li>
<li>Don't imagine unlikely potential needs that aren't really there
</li>
</ul>
</li>
<li>Best approach: use your own infrastructure (especially at first!)
<ul class="org-ul">
<li>(much more rapid feedback about what works, what doesn't)
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-4-4" class="outline-3">
<h3 id="sec-4-4"><span class="section-number-3">4.4</span> Design for Growth</h3>
<div class="outline-text-3" id="text-4-4">
<ul class="org-ul">
<li>Try to anticipate how requirements will evolve keep likely features in mind as you design base system
</li>
<li>Don’t design to scale infinitely: <b>扩展性只需要考虑5x-50x左右的扩展即可</b>
<ul class="org-ul">
<li>~5X - 50X growth good to consider
</li>
<li>&gt;100X probably requires rethink and rewrite
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-4-5" class="outline-3">
<h3 id="sec-4-5"><span class="section-number-3">4.5</span> Pattern: Single Master, 1000s of Workers</h3>
<div class="outline-text-3" id="text-4-5">
<p>
master主要完成全局性质的工作，其余工作交给worker完成。通常存在hot standby来做failover. 优点是可以很容易地进行全局控制，但是实现上必须小心，而缺点非常明显就是支撑worker不会很多，在1k级别上。如果涉及到更大规模集群的话，那么worker需要和master有更加频繁的交互，这对于master压力会非常大。
</p>

<ul class="org-ul">
<li>Master orchestrates global operation of system
<ul class="org-ul">
<li>load balancing, assignment of work, reassignment when machines fail, etc.
</li>
<li>&#x2026; but client interaction with master is fairly minimal
</li>
<li>Often: hot standby of master waiting to take over
</li>
<li>Always: bulk of data transfer directly between clients and workers
</li>
</ul>
</li>
<li>Examples:
<ul class="org-ul">
<li>GFS, BigTable, MapReduce, file transfer service, cluster scheduling system, &#x2026;
</li>
</ul>
</li>
<li>Pro:
<ul class="org-ul">
<li>simpler to reason about state of system with centralized master
</li>
</ul>
</li>
<li>Caveats:
<ul class="org-ul">
<li>careful design required to keep master out of common case ops
</li>
<li>scales to 1000s of workers, but not 100,000s of workers
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-4-6" class="outline-3">
<h3 id="sec-4-6"><span class="section-number-3">4.6</span> Pattern: Tree Distribution of Requests</h3>
<div class="outline-text-3" id="text-4-6">
<p>
这个模型本质上是从single master模型发展过来的，是multi master实现。随着master管理worker数目增加，CPU以及network IO都会bounded. 以single master为例，如果每个master最多管理1k worker的话，那么1k master可以由另外一个master管理，这样就可以支持1k * 1k worker级别了。
</p>

<ul class="org-ul">
<li>Problem: Single machine sending 1000s of RPCs overloads NIC on machine when handling replies
<ul class="org-ul">
<li>wide fan in causes TCP drops/retransmits, significant latency
</li>
<li>CPU becomes bottleneck on single machine
</li>
</ul>
</li>
<li>Solution: Use tree distribution of requests/responses
<ul class="org-ul">
<li>fan in at root is smaller
</li>
<li>cost of processing leaf responses spread across many parents
</li>
</ul>
</li>
<li>Most effective when parent processing can trim/combine leaf data
<ul class="org-ul">
<li>can also co-locate parents on same rack as leaves
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-4-7" class="outline-3">
<h3 id="sec-4-7"><span class="section-number-3">4.7</span> Pattern: Backup Requests to Minimize Latency</h3>
<div class="outline-text-3" id="text-4-7">
<p>
通过backup request来降低延迟，因为部分请求可能会成为straggler，这点在mapreduce里面的speculative非常经典。 #note: jeff dean在另外一篇文章tail at scale里面也提到即便如何也存在一些bad case
</p>

<ul class="org-ul">
<li>Problem: variance high when requests go to 1000s of machines
<ul class="org-ul">
<li>last few machines to respond stretch out latency tail substantially
</li>
</ul>
</li>
<li>Often, multiple replicas can handle same kind of request
</li>
<li>When few tasks remaining, send backup requests to other replicas
</li>
<li>Whichever duplicate request finishes first wins
<ul class="org-ul">
<li>useful when variance is unrelated to specifics of request
</li>
<li>increases overall load by a tiny percentage
</li>
<li>decreases latency tail significantly
</li>
</ul>
</li>
<li>Examples:
<ul class="org-ul">
<li>MapReduce backup tasks (granularity: many seconds)
</li>
<li>various query serving systems (granularity: milliseconds)
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-4-8" class="outline-3">
<h3 id="sec-4-8"><span class="section-number-3">4.8</span> Pattern: Multiple Smaller Units per Machine</h3>
<div class="outline-text-3" id="text-4-8">
<p>
每个机器上部署更小的单元，可以使得调度更加容易，集群资源利用率更高。
</p>

<ul class="org-ul">
<li>Problems:
<ul class="org-ul">
<li>want to minimize recovery time when machine crashes
</li>
<li>want to do fine-grained load balancing
</li>
</ul>
</li>
<li>Having each machine manage 1 unit of work is inflexible
<ul class="org-ul">
<li>slow recovery: new replica must recover data that is O(machine state) in size
</li>
<li>load balancing much harder
</li>
</ul>
</li>
<li>Have each machine manage many smaller units of work/data
<ul class="org-ul">
<li>typical: ~10-100 units/machine
</li>
<li>allows fine grained load balancing (shed or add one unit)
</li>
<li>fast recovery from failure (N machines each pick up 1 unit)
</li>
</ul>
</li>
<li>Examples:
<ul class="org-ul">
<li>map and reduce tasks, GFS chunks, Bigtable tablets, query serving system index shards
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-4-9" class="outline-3">
<h3 id="sec-4-9"><span class="section-number-3">4.9</span> Pattern: Elastic Systems</h3>
<div class="outline-text-3" id="text-4-9">
<p>
可伸缩的系统，自动调节整个集群资源利用率。这个东西可以打个比方，如果整个集群资源空闲的话，那么可以减少线程数目，释放一些内存让其他程序可以有效运行。而当压力比较大的时候，可以保持在一个水平不至于崩溃。
</p>

<ul class="org-ul">
<li>Problem: Planning for exact peak load is hard
<ul class="org-ul">
<li>overcapacity: wasted resources
</li>
<li>undercapacity: meltdown
</li>
</ul>
</li>
<li>Design system to adapt:
<ul class="org-ul">
<li>automatically shrink capacity during idle period
</li>
<li>automatically grow capacity as load grows
</li>
</ul>
</li>
<li>Make system resilient to overload:
<ul class="org-ul">
<li>do something reasonable even up to 2X planned capacity
<ul class="org-ul">
<li>e.g. shrink size of index searched, back off to less CPU intensive algorithms, drop spelling correction tips, etc.
</li>
</ul>
</li>
<li>more aggressive load balancing when imbalance more severe
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-4-10" class="outline-3">
<h3 id="sec-4-10"><span class="section-number-3">4.10</span> Pattern: Combine Multiple Implementations</h3>
<div class="outline-text-3" id="text-4-10">
<p>
多种实现的结合，这点以realtime + batch说明非常直观。
</p>

<ul class="org-ul">
<li>Example: Google web search system wants all of these:
<ul class="org-ul">
<li>freshness (update documents in ~1 second)
</li>
<li>massive capacity (10000s of requests per second)
</li>
<li>high quality retrieval (lots of information about each document)
</li>
<li>massive size (billions of documents)
</li>
</ul>
</li>
<li>Very difficult to accomplish in single implementation
</li>
<li>Partition problem into several subproblems with different engineering tradeoffs. E.g.
<ul class="org-ul">
<li>realtime system: few docs, ok to pay lots of $$$/doc
</li>
<li>base system: high # of docs, optimized for low $/doc
</li>
<li>realtime+base: high # of docs, fresh, low $/doc
</li>
</ul>
</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-sec-5" class="outline-2">
<h2 id="sec-5"><span class="section-number-2">5</span> Final Thoughts</h2>
<div class="outline-text-2" id="text-5">
<ul class="org-ul">
<li>Today: exciting collection of trends: <b>未来趋势的一些思考</b>
<ul class="org-ul">
<li>large-scale datacenters + 大规模数据中心建设
</li>
<li>increasing scale and diversity of available data sets +  大量数据需要分析和挖掘
</li>
<li>proliferation of more powerful client devices 各种设备接入
</li>
</ul>
</li>

<li>Many interesting opportunities: <b>值得去做的事情</b>
<ul class="org-ul">
<li>planetary scale distributed systems 宇宙级别分布式系统
</li>
<li>development of new CPU and data intensive services 新的CPU和数据密集服务
</li>
<li>new tools and techniques for constructing such systems 以及构建这些服务的工具
</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</body>
</html>
