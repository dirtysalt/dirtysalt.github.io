<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>SSDs and Distributed Data Systems</title>
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="dirtysalt" />
<link rel="shortcut icon" href="/themes/favicon.ico" /><link rel="stylesheet" type="text/css" href="/themes/simple.css"/></head>
<body>
<div id="content">
<h1 class="title">SSDs and Distributed Data Systems</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1. How SSDs may impact data system design</a></li>
<li><a href="#sec-2">2. Making SSDs Cheap Without Losing All Your Data</a></li>
</ul>
</div>
</div>
<p>
<a href="http://blog.empathybox.com/post/24415262152/ssds-and-distributed-data-systems">http://blog.empathybox.com/post/24415262152/ssds-and-distributed-data-systems</a>
</p>

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> How SSDs may impact data system design</h2>
<div class="outline-text-2" id="text-1">
<p>
For distributed data systems the big change SSDs introduce is the relative latency of a random disk access versus a remote network hop. In the case of a traditional hard drive a single seek may have a latency cost easily 10 or 20x that of TCP request on a local network, which means a remote cache hit is much cheaper than a local cache miss. SSDs essentially erase this difference making them fairly close in terms of latency. The consequence should be favoring designs that store more data per machine and do fewer network requests（磁盘访问和网络访问延迟差异变化）
</p>

<p>
Less radically SSDs will likely change how caching is done. Many web companies have large memcached installations. Memcached is very good at serving high throughput at low latency on a small dataset, but since everything is in RAM it is actually rather expensive if you are space rather than CPU bound. If you place 32GB of cache per server, then 5TB of total cache space requires 160 servers. Having 5 servers each with 1TB of SSD space may be a huge win. Furthermore caching in RAM has a practical problem: restarts dump a full server worth of cache. This is an annoyance if you need to restart your cache servers frequently or if you need to bring up a new stack with completely cold cache as you may not actually be able to run your application without any caching (if you can, then why have caching, after all).（SSD用来作为更大的cache使用）
</p>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2"><span class="section-number-2">2</span> Making SSDs Cheap Without Losing All Your Data</h2>
<div class="outline-text-2" id="text-2">
<p>
Here is a table that compares prices and write-capacity per block for MLC, SLC, RAM, and SAS drives. These are taken at random off the internet, your milage would certainly vary, but this gives some idea of the pricing as of May 2012.
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="left" />

<col  class="left" />

<col  class="left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="left">&#xa0;</th>
<th scope="col" class="left">Cost/GB</th>
<th scope="col" class="left">Program-Erase Cycles</th>
</tr>
</thead>
<tbody>
<tr>
<td class="left">RAM</td>
<td class="left">$5-6</td>
<td class="left">Unlimited</td>
</tr>

<tr>
<td class="left">15k RPM SAS Hardrive</td>
<td class="left">$0.75</td>
<td class="left">Unlimited</td>
</tr>

<tr>
<td class="left">MLC SSD</td>
<td class="left">$1</td>
<td class="left">5,000-10,000</td>
</tr>

<tr>
<td class="left">SLC SSD</td>
<td class="left">$4-6</td>
<td class="left">~100,000</td>
</tr>
</tbody>
</table>
<p>
A few obvious conclusions are that SLC SSDs are priced roughly the same as memory. In a sense SLC SSDs are better than memory since they are persistent, but if keeping all your data in memory sounds expensive then so will SLCs. And in any case you can’t eliminate memory caching entirely as at least some part of the index likely needs to reside in memory even with the faster access times SSDs provide.
</p>

<p>
So the question is, is there a way to live with the low write-endurance of the MLC devices and still get the great performance and cost? Here is where the complex dependency between the storage format and the SSD comes in. （SSD写入次数分析）
</p>
<ul class="org-ul">
<li>If your storage engine does large linear writes (say a full 512KB block or more) then calculating the number of writes you can do on one drive before it is all used up is easy. If the drive has 300GB and each block can be rewritten 5,000 times then each drive will allow writing 5000 x 300GB (about 1.4 petabytes). Let’s say you have 8 of these in a box with no RAID and that box takes 50MB/sec of writes 24 hours a day evenly balanced over the drives, then the drives will last around 7.8 years. This should be plenty of time, for most systems. But this lifetime is only realistic for large linear writes—the best possible case for SSD write-endurance.（顺序大块写入的话，那么可以写很久）
</li>
<li>The other case is that you are doing small random writes immediately sync’d to disk. If you are doing 100 byte random writes and the SSD’s internal firmware can’t manage to somehow coalesce these into larger physical writes then each 100 byte write will turn into a full program-erase cycle of a full 512KB block. In this case you would expect to be able to do only 5000*100 bytes = 500KB of writes per block before it died; so a 300GB drive with 300GB/512KB = 614,400 blocks would only take around 286GB of writes total before crapping out. Assuming, again, 8 of these drives with no RAID and 50MB/sec, you get a lifetime of only about half a day. This is the worst case for SSDs and is obviously totally unworkable.（但是如果随机小块写入的话，那么写的时间就不会很长）
</li>
<li>A particularly important factor in making this work is whether the storage engine requires an immediate fsync to disk with each write. Many systems do require this for data integrity. An immediate fsync will, of course, require a small write unless the record being written is itself large.（fsync操作会影响实际的写模式，因为如果不fsync的话那么底层实际上可以将随机小块写做合并的）
</li>
</ul>

<p>
An interesting question is whether cloud hosting providers will rent instances with SSDs any time soon. The write-endurance problem makes SSDs somewhat problematic for a shared hosting environment, so they may need to add a way to bill on a per block-erase basis. I have heard rumors that Amazon will offer them, but I have no idea in what form or how much they will cost (which is the critical detail). # 对于cloud来说，最好是提供service而不是直接暴露hardware，这样可以有效控制ssd write endurance
</p>
</div>
</div>
</div>
</body>
</html>
