<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>HDFS</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="dirtysalt" />
<link rel="shortcut icon" href="/themes/favicon.ico" /><link rel="stylesheet" type="text/css" href="/themes/simple.css"/></head>
<body>
<div id="content">
<h1 class="title">HDFS</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org5de022c">1. HDFS Shell</a></li>
<li><a href="#orge45c532">2. Filesystem Corruption and Missing Blocks</a></li>
<li><a href="#orgbd86566">3. 文件系统API</a></li>
<li><a href="#orgb150f5b">4. 一致性问题</a></li>
<li><a href="#orgfe37e0d">5. 读写进度</a></li>
<li><a href="#orgfc4215b">6. 获取集群运行状况</a></li>
<li><a href="#org26c0956">7. All datanodes are bad. Aborting</a></li>
</ul>
</div>
</div>

<div id="outline-container-org5de022c" class="outline-2">
<h2 id="org5de022c"><span class="section-number-2">1</span> HDFS Shell</h2>
<div class="outline-text-2" id="text-1">
<ul class="org-ul">
<li>balancer 可以限制比例阈值和传输带宽</li>
<li>fsck 检查hdfs文件系统并作修复</li>
</ul>
</div>
</div>

<div id="outline-container-orge45c532" class="outline-2">
<h2 id="orge45c532"><span class="section-number-2">2</span> Filesystem Corruption and Missing Blocks</h2>
<div class="outline-text-2" id="text-2">
<ul class="org-ul">
<li>HadoopRecovery &lt; Storage &lt; TWiki <a href="https://www.opensciencegrid.org/bin/view/Storage/HadoopRecovery">https://www.opensciencegrid.org/bin/view/Storage/HadoopRecovery</a></li>
<li>HadoopOperations &lt; Storage &lt; TWiki <a href="https://www.opensciencegrid.org/bin/view/Storage/HadoopOperations">https://www.opensciencegrid.org/bin/view/Storage/HadoopOperations</a></li>
</ul>
<p>
如果hdfs文件系统出现损坏的话，可以在webpage上面看到报警提示
</p>


<div class="figure">
<p><img src="images/hdfs-filesystem-corruption-and-missing-blocks.png" alt="hdfs-filesystem-corruption-and-missing-blocks.png" />
</p>
</div>

<p>
或者可以通过运行命令hadoop dfsadmin -report看到系统状况
</p>
<pre class="example">
dp@dp1:~$ hadoop dfsadmin -report
Configured Capacity: 487173353816064 (443.08 TB)
Present Capacity: 466468596971008 (424.25 TB)
DFS Remaining: 288401443913728 (262.3 TB)
DFS Used: 178067153057280 (161.95 TB)
DFS Used%: 38.17%
Under replicated blocks: 1
Blocks with corrupt replicas: 1
Missing blocks: 1
</pre>

<p>
按照提示可以运行hadoop fsck来检查整个文件系统。首先使用hadoop fsck /察看整个文件系统的状态如何。如果某个文件出现问题的话那么会报告
</p>
<pre class="example">
/hbase/.corrupt/dp18.umeng.com%3A60020.1349065853563: CORRUPT block blk_6229461233186357508
/hbase/.corrupt/dp18.umeng.com%3A60020.1349065853563:  Under replicated blk_6229461233186357508_18529950. Target Replicas is 3 but found 1 replica(s).
</pre>
<p>
说明文件/hbase/.corrupt/dp18.umeng.com%3A60020.1349065853563存在问题。
</p>

<p>
我们可以进一步察看这个文件的状态。使用下面的命令 hadoop fsck <i>hbase</i>.corrupt/dp18.umeng.com%3A60020.1349065853563 -files -locations -blocks -racks
</p>
<pre class="example">
dp@dp2:~$ hadoop fsck /hbase/.corrupt/dp18.umeng.com%3A60020.1349065853563 -files -locations -blocks -racks
FSCK started by dp (auth:SIMPLE) from /10.18.10.55 for path /hbase/.corrupt/dp18.umeng.com%3A60020.1349065853563 at Mon Oct 08 15:17:07 CST 2012
/hbase/.corrupt/dp18.umeng.com%3A60020.1349065853563 66050 bytes, 1 block(s):
/hbase/.corrupt/dp18.umeng.com%3A60020.1349065853563: CORRUPT block blk_6229461233186357508
 Under replicated blk_6229461233186357508_18529950. Target Replicas is 3 but found 1 replica(s).
0. blk_6229461233186357508_18529950 len=66050 repl=1 [/default-rack/10.18.10.71:50010]

Status: CORRUPT
 Total size:	66050 B
 Total dirs:	0
 Total files:	1
 Total blocks (validated):	1 (avg. block size 66050 B)
  ********************************
  CORRUPT FILES:	1
  CORRUPT BLOCKS: 	1
  ********************************
 Minimally replicated blocks:	1 (100.0 %)
 Over-replicated blocks:	0 (0.0 %)
 Under-replicated blocks:	1 (100.0 %)
 Mis-replicated blocks:		0 (0.0 %)
 Default replication factor:	3
 Average block replication:	1.0
 Corrupt blocks:		1
 Missing replicas:		2 (200.0 %)
 Number of data-nodes:		29
 Number of racks:		1
FSCK ended at Mon Oct 08 15:17:07 CST 2012 in 1 milliseconds


The filesystem under path '/hbase/.corrupt/dp18.umeng.com%3A60020.1349065853563' is CORRUPT

</pre>

<hr />

<p>
默认情况下面如果hdfs发现某个block under replicated的话，会自动对这个block做replication的，直到replicaion factor达到需求。但是有时候hdfs也会stuck住。除了重启的话，也可以试试上面链接提到的方法。
</p>
<ul class="org-ul">
<li>首先将这个文件的rep factor设置为1，hadoop fs -setrep 1 &lt;file&gt;</li>
<li>然后将这个文件的rep factor修改回3，hadoop fs -setrep 3 &lt;file&gt;</li>
<li>#note: 不过很悲剧的是，即使我按照这个方法，这个block似乎也没有回复到指定的factor上面</li>
<li>#note: 不是所有的hdfs file都是使用replication=3的方案的，对于mapreduce提交的jar以及libjars（在/user/&lt;user&gt;/.staging/&lt;jobid&gt;/下面）的，考虑到需要被多个tasktracker同时取到，replication的数目会偏高，通常是10</li>
</ul>
</div>
</div>

<div id="outline-container-orgbd86566" class="outline-2">
<h2 id="orgbd86566"><span class="section-number-2">3</span> 文件系统API</h2>
<div class="outline-text-2" id="text-3">
<p>
HDFS文件系统的操作步骤主要如下：
</p>
<ul class="org-ul">
<li>首先通过configuration获得FileSystem实例</li>
<li>然后通过FileSystem这个实例操作文件系统上的文件</li>
<li>代码可以参考 <a href="https://github.com/dirtysalt/codes/tree/master/java/hdfs/src/main/java/com/dirlt/java/hdfs/GetFS.java">code on github</a></li>
</ul>

<p>
影响获取到的具体文件系统是fs.default.name这个值，hdfs文件系统API支持下面几个文件系统(不仅限于，只是常用的）
</p>
<ul class="org-ul">
<li>Local file fs.LocalFileSystem</li>
<li>HDFS hdfs hdfs.DistributedFileSystem
<ul class="org-ul">
<li>No file update options(record append, etc). all files are write-once.</li>
<li>Designed for streaming. Random seeks devastate performance.</li>
</ul></li>
<li>HAR(Hadoop Archive) har fs.HarFileSystem</li>
</ul>

<p>
以 com.dirlt.java.hdfs.GetFS 为例，如果使用java -cp方式运行的话，那么结果如下
</p>
<pre class="example">
fs.default.name = file:///
uri = file:///
uri = file:///
</pre>

<p>
而如果以hadoop来运行的话，因为configuration首先会加载conf/core-site.xml里面存在fs.default.name，因此运行结果如下
</p>
<pre class="example">
➜  hdfs git:(master) ✗ export HADOOP_CLASSPATH=./target/classes
➜  hdfs git:(master) ✗ hadoop com.dirlt.java.hdfs.GetFS
fs.default.name = hdfs://localhost:9000
uri = hdfs://localhost:9000
uri = file:///
</pre>

<p>
如果指定的URI schema在configuration里面找不到对应实现的话，那么就会使用fs.default.name作为默认的文件系统。
</p>
</div>
</div>

<div id="outline-container-orgb150f5b" class="outline-2">
<h2 id="orgb150f5b"><span class="section-number-2">4</span> 一致性问题</h2>
<div class="outline-text-2" id="text-4">
<ul class="org-ul">
<li>hdfs一致性模型是reader不能够读取到当前被write的block，除非writer调用sync强制进行同步
<ul class="org-ul">
<li>FileSystem有下面几个方法需要稍微说明一下 flush,sync,hflush,hsync</li>
<li>flush是DataOutputStream的virtual method，调用flush会调用底层stream的flush，或许我们可以简单地认为这个实现就是将缓冲区的数据刷到device上面</li>
<li>sync是FSDataOutputStream特有的，老版本相当是将datanode数据同步到namenode，这样reader就可以读取到当前的block，但是在高版本deprecated</li>
<li>hflush则是高版本推荐的sync用法</li>
<li>hsync不仅仅有hflush功能，还能够调用对应的datanode将数据刷到local fs上面。</li>
<li>#note: 但是似乎不太work. 参考代码 <a href="https://github.com/dirtysalt/codes/tree/master/java/hdfs/src/main/java/com/dirlt/java/hdfs/TestConsistency.java">code on github</a></li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgfe37e0d" class="outline-2">
<h2 id="orgfe37e0d"><span class="section-number-2">5</span> 读写进度</h2>
<div class="outline-text-2" id="text-5">
<ul class="org-ul">
<li>hdfs每次将64KB数据写入datanode pipeline的时候都会调用progress.</li>
<li>对于本地文件系统的话，可以跟进到RawLocalFileSystem.create发现progress这个方法并没有使用。</li>
<li>对于分布式文件系统的话，可以跟进到DFSClient.DFSOutputStream.DataStreamer在run里面调用progress
<ul class="org-ul">
<li>但是过程似乎有点复杂，所以也不确实是否真的写入64KB才会调用progress</li>
</ul></li>
<li>代码可以参考 <a href="https://github.com/dirtysalt/codes/tree/master/java/hdfs/src/main/java/com/dirlt/java/hdfs/TestProgress.java">code on github</a></li>
</ul>
</div>
</div>

<div id="outline-container-orgfc4215b" class="outline-2">
<h2 id="orgfc4215b"><span class="section-number-2">6</span> 获取集群运行状况</h2>
<div class="outline-text-2" id="text-6">
<ul class="org-ul">
<li>参考代码 <a href="https://github.com/dirtysalt/codes/tree/master/java/hdfs/src/main/java/com/dirlt/java/hdfs/ClusterSummary.java">code on github</a></li>
<li>通过DFSClient可以获取集群运行状况</li>
</ul>
</div>
</div>

<div id="outline-container-org26c0956" class="outline-2">
<h2 id="org26c0956"><span class="section-number-2">7</span> All datanodes are bad. Aborting</h2>
<div class="outline-text-2" id="text-7">
<p>
当时的情况是增加了datanode的处理线程数目但是没有重启regionserver.怀疑原因可能是文件句柄数量不够，重启regionserver之后恢复正常。
</p>

<pre class="example">
2013-06-05 03:45:16,866 FATAL org.apache.hadoop.hbase.regionserver.wal.HLog: Could not append. Requesting close of hlog
java.io.IOException: All datanodes 10.11.0.41:50010 are bad. Aborting...
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3088)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$1900(DFSClient.java:2627)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2799)
</pre>
</div>
</div>
</div>
</body>
</html>
