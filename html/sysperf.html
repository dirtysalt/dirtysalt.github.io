<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>sysperf</title>
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="dirtysalt" />
<link rel="shortcut icon" href="/themes/favicon.ico" /><link rel="stylesheet" type="text/css" href="/themes/simple.css"/></head>
<body>
<div id="content">
<h1 class="title">sysperf</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1. What Your Computer Does While You Wait</a></li>
<li><a href="#sec-2">2. Numbers Everyone Should Know</a></li>
<li><a href="#sec-3">3. The Joys of Real Hardware</a></li>
<li><a href="#sec-4">4. 延迟和带宽</a></li>
<li><a href="#sec-5">5. 存储系统IOPS</a></li>
<li><a href="#sec-6">6. 存储系统性能</a></li>
</ul>
</div>
</div>
<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> What Your Computer Does While You Wait</h2>
<div class="outline-text-2" id="text-1">
<p>
<a href="http://duartes.org/gustavo/blog/post/what-your-computer-does-while-you-wait">http://duartes.org/gustavo/blog/post/what-your-computer-does-while-you-wait</a>
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="left" />

<col  class="left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="left">指标</th>
<th scope="col" class="left">数据</th>
</tr>
</thead>
<tbody>
<tr>
<td class="left">CPU主频</td>
<td class="left">3.0GHz</td>
</tr>

<tr>
<td class="left">指令周期</td>
<td class="left">1 insn/cycle</td>
</tr>

<tr>
<td class="left">时间周期</td>
<td class="left">0.33ns/cycle</td>
</tr>

<tr>
<td class="left">L1数据缓存</td>
<td class="left">32KB</td>
</tr>

<tr>
<td class="left">L1指令缓存</td>
<td class="left">32KB</td>
</tr>

<tr>
<td class="left">L1存取延迟</td>
<td class="left">3cycles(1ns)</td>
</tr>

<tr>
<td class="left">L2缓存</td>
<td class="left">6MB</td>
</tr>

<tr>
<td class="left">L2存取延迟</td>
<td class="left">14cycles(4.7ns)</td>
</tr>

<tr>
<td class="left">分支预测失败</td>
<td class="left">5ns</td>
</tr>

<tr>
<td class="left">锁操作开销</td>
<td class="left">25ns</td>
</tr>

<tr>
<td class="left">内存延迟</td>
<td class="left">250cycles(83ns)</td>
</tr>

<tr>
<td class="left">内存带宽</td>
<td class="left">10GB/s</td>
</tr>

<tr>
<td class="left">内存顺序读取1MB</td>
<td class="left">0.25ms</td>
</tr>

<tr>
<td class="left">Snappy压缩1M数据</td>
<td class="left">3ms</td>
</tr>

<tr>
<td class="left">SATA(Serial ATA)端口带宽</td>
<td class="left">300MB/s</td>
</tr>

<tr>
<td class="left">磁盘转速</td>
<td class="left">7200RPM</td>
</tr>

<tr>
<td class="left">磁盘读寻道延迟</td>
<td class="left">41Mcycles(13.7ms)</td>
</tr>

<tr>
<td class="left">磁盘写寻道延迟</td>
<td class="left">45Mcycles(15ms)</td>
</tr>

<tr>
<td class="left">磁盘读写带宽</td>
<td class="left">60MB/s</td>
</tr>

<tr>
<td class="left">磁盘顺序读取1MB</td>
<td class="left">20ms</td>
</tr>

<tr>
<td class="left">千兆网卡带宽</td>
<td class="left">100MB/s</td>
</tr>

<tr>
<td class="left">同机房RTT</td>
<td class="left">0.5ms</td>
</tr>

<tr>
<td class="left">gettimeofday系统调用</td>
<td class="left">3000cycles(1us)</td>
</tr>

<tr>
<td class="left">单CPU软中断次数</td>
<td class="left">10w/s</td>
</tr>
</tbody>
</table>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2"><span class="section-number-2">2</span> Numbers Everyone Should Know</h2>
<div class="outline-text-2" id="text-2">
<p>
<a href="designs-lessons-and-advice-from-building-large-distributed-systems.html">Designs, Lessons and Advice from Building Large Distributed Systems</a> @ladis-2009 jeff dean
</p>

<ul class="org-ul">
<li>L1 cache reference 0.5 ns
</li>
<li>Branch mispredict 5 ns
</li>
<li>L2 cache reference 7 ns
</li>
<li>Mutex lock/unlock 100 ns
</li>
<li>Main memory reference 100 ns
</li>
<li>Compress 1K bytes with Zippy 10,000 ns
</li>
<li>Send 2K bytes over 1 Gbps network 20,000 ns
</li>
<li>Read 1 MB sequentially from memory 250,000 ns
</li>
<li>Round trip within same datacenter 500,000 ns
</li>
<li>Disk seek 10,000,000 ns
</li>
<li>Read 1 MB sequentially from network 10,000,000 ns
</li>
<li>Read 1 MB sequentially from disk 30,000,000 ns
</li>
<li>Send packet CA-&gt;Netherlands-&gt;CA 150,000,000 ns
</li>
</ul>


<div class="figure">
<p><img src="images/numbers-everyone-should-know.jpg" alt="numbers-everyone-should-know.jpg" />
</p>
</div>
</div>
</div>

<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3"><span class="section-number-2">3</span> The Joys of Real Hardware</h2>
<div class="outline-text-2" id="text-3">
<p>
<a href="designs-lessons-and-advice-from-building-large-distributed-systems.html">Designs, Lessons and Advice from Building Large Distributed Systems</a> @ladis-2009 jeff dean
</p>

<p>
Typical first year for a new cluster:
</p>
<ul class="org-ul">
<li>~0.5 overheating (power down most machines in &lt;5 mins, ~1-2 days to recover)
</li>
<li>~1 PDU failure (~500-1000 machines suddenly disappear, ~6 hours to come back)
</li>
<li>~1 rack-move (plenty of warning, ~500-1000 machines powered down, ~6 hours)
</li>
<li>~1 network rewiring (rolling ~5% of machines down over 2-day span)
</li>
<li>~20 rack failures (40-80 machines instantly disappear, 1-6 hours to get back)
</li>
<li>~5 racks go wonky (40-80 machines see 50% packetloss)
</li>
<li>~8 network maintenances (4 might cause ~30-minute random connectivity losses)
</li>
<li>~12 router reloads (takes out DNS and external vips for a couple minutes)
</li>
<li>~3 router failures (have to immediately pull traffic for an hour)
</li>
<li>~dozens of minor 30-second blips for dns
</li>
<li>~1000 individual machine failures
</li>
<li>~thousands of hard drive failures
</li>
<li>slow disks, bad memory, misconfigured machines, flaky machines, etc.
</li>
<li>Long distance links: wild dogs, sharks, dead horses, drunken hunters, etc.
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-4" class="outline-2">
<h2 id="sec-4"><span class="section-number-2">4</span> 延迟和带宽</h2>
<div class="outline-text-2" id="text-4">
<p>
延迟指原子信息通过介质所需要的时间，带宽指信息在介质中传播的速度。如果我们以浏览Web页面为例的话，如果等待长时间才开始显示一个页面，但立刻就全部出现了，这说明网络的延迟比较大，带宽还不错。如果页面立刻开始出现，但是花了很长时间才全部显示出来，这说明网络的延迟还可以，但带宽较小。
</p>

<p>
对于网络传输来说，网卡，传输线路以及交换机路由器本身都是有延迟和带宽指标的。我没有办法获得所有这些指标的具体数据，只能够就我所知道的稍微说说。对于网卡来说，延迟取决于系统负载和所处网络拥塞程度。更细程度考量的话可能就是从user buffer-&gt;kernel buffer-&gt;device buffer同时考虑系统完成这件事情调度时间比如TCP_NODELAY和TCP_CORK带来的影响，我猜想的:（。网卡带宽就是所谓的千兆网卡(1000Mbps)和万兆网卡(10000Mbps).对于传输线路来说，如果是光纤的话那么传播速度是光速，在光纤中传播距离可能是实际距离的1.1-1.2倍(估计，因为不可能走直线).假设天津机房和北京机房距离150km,那么延迟为150km / 光速(3*10^5km/s) * 1.2(实际距离比率) = 0.6ms.不过考虑同事告诉我说这样计算可能是不太准确的，撇开铺线的实际距离不谈，光纤每段上会加一个中继器来增强光信号，这样计算实际上是不准确的。他友好地给我说了一下北京机房&lt;-&gt;天津机房RTT大概10ms.最后就是交换机路由器。很少有人关注这个单项指标，因为这个取决于内部机房机架是怎么部署的，大家更关心的是从同机房两个机架上服务器延迟多少，带宽多少。而事实证明(实际上是同事告诉我的)延迟基本没有，带宽取决于服务器网卡带宽。
</p>

<ul class="org-ul">
<li>ping本机0.01ms
</li>
<li>ping同机房机器0.1ms
</li>
<li>ping同城机器1ms
</li>
<li>ping不同城机器20ms
</li>
<li>北（南）方ping南（北）方机器50ms
</li>
<li>ping外国机器200ms
</li>
<li>ping不通是因为GFW
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-5" class="outline-2">
<h2 id="sec-5"><span class="section-number-2">5</span> 存储系统IOPS</h2>
<div class="outline-text-2" id="text-5">
<p>
<a href="http://rickardnobel.se/storage-performance-iops-latency-throughput/">http://rickardnobel.se/storage-performance-iops-latency-throughput/</a>
</p>

<p>
iops(io per second)是我们在分析存储介质时抽象出的概念，表示可以发起多少个io操作/s.  <b>因为每种存储介质工作方式是不同的，抽象出的iops则可以更好地让我们分析。</b>  好比磁盘的话我们使用转速来衡量，但是放在磁带或者是SSD上的话，那么转速这个指标就没有意义。iops可以比较好定量地分析某个存储介质的操作速度。
</p>

<p>
<del>如果我们考虑磁盘的话，那么iops基本上和磁盘转速相关。</del> （和磁盘带宽和读写负载也有关系） 比如转速是7200RPM的话，那么应该是120RPS.如果一个操作磁头需要转一圈的话，那么延迟大概在8ms左右。另外考虑向某个磁道移动时间的话，我们可以大概可以认为延迟在15ms左右。 <del>这样折合计算iops大概在66-67</del>. (这个数值没有意义，因为不可能每次读取都需要转动磁头以及移动磁道）
</p>

<p>
<del>存储系统一方便受限于iops，一方便受限于磁盘带宽 。通常磁盘带宽大约在80MB/s一下，50~60MB/s是比较典型的值。</del> （磁盘带宽取决于读写负载）
</p>

<p>
note@2015-05-22: 更正一下，即使对于HDD来说iops也不仅仅和磁盘转速相关，还和磁盘带宽以及读写负载有关，所以事实上iops就是衡量存储介质和存储系统的一个独立综合指标。这也同时意味着，当我们宣称某个存储介质或者是存储系统iops是多少时，我们一定要把读写负载情况这个context也说明情况。
</p>

<hr  />

<p>
note@ 015-05-21: iops的测算非常复杂，主要是涉及到的环境配置参数非常多，比如read/write buffer size, 多少个线程来做读写，以及随机和顺序读写等。前段时间想要测量一下磁盘的iops，所以就在网上搜索了一下这方面的工具和文章：
</p>
<ul class="org-ul">
<li><a href="http://code.google.com/p/ioping/">http://code.google.com/p/ioping/</a> # ioping. C, 使用方便，可测读写，但是不支持多线程
</li>
<li>Measuring Disk Usage In Linux (%iowait vs IOPS) - Everything is a Ghetto : <a href="http://www.thattommyhall.com/2011/02/18/iops-linux-iostat/">http://www.thattommyhall.com/2011/02/18/iops-linux-iostat/</a> # 使用iostat -dx 1来查看iops
</li>
<li>Measuring Disk IO Performance « Benjamin Schweizer. : <a href="http://benjamin-schweizer.de/measuring-disk-io-performance.html">http://benjamin-schweizer.de/measuring-disk-io-performance.html</a> # iops. Python, 使用方便，只能测读，支持多线程
</li>
<li><a href="http://www.ee.pw.edu.pl/~pileckip/aix/iowait.htm">http://www.ee.pw.edu.pl/~pileckip/aix/iowait.htm</a> # 只看iowait比例是没有意义的
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-6" class="outline-2">
<h2 id="sec-6"><span class="section-number-2">6</span> 存储系统性能</h2>
<div class="outline-text-2" id="text-6">
<p>
存储系统的性能主要包括两个维度：吞吐量和访问延迟。设计系统时要求能够在保证访问延迟的基础上，通过最低的成本实现尽可能高的吞吐量。磁盘和SSD的访问延迟差别很大，但是带宽差别不大。因此磁盘适合大块顺序访问的存储系统，SSD适合随机访问较多或者对延迟比较敏感的关键系统。二者也常常组合在一起进行混合存储，热数据（访问频繁）存储到SSD，冷数据（访问不频繁）存储到磁盘中。
</p>
</div>
</div>
</div>
</body>
</html>
