<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Neural Networks and Deep Learning</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="dirtysalt" />
<link rel="shortcut icon" href="/themes/favicon.ico" /><link rel="stylesheet" type="text/css" href="/themes/simple.css"/></head>
<body>
<div id="content">
<h1 class="title">Neural Networks and Deep Learning</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgc5e3a8f">1. 规范符号</a></li>
<li><a href="#org353d29a">2. FP和BP的公式推导</a></li>
<li><a href="#orgd92fe20">3. Chain Rule.</a></li>
</ul>
</div>
</div>
<p>
连续花费了几天突击了这门 <a href="https://www.coursera.org/learn/neural-networks-deep-learning">课程</a>. 这门课程并不难学，但是里面提到了一些BP的推导，并且给出了DNN的完整实现，为NN/DL的学习提供了基础知识。如果你已经对NN/DL有一定的了解，而且也可以比较熟练地使用python numpy这套东西的话，基本上没有什么难度。
</p>

<p>
学习这门课程的前面几天，正好我在看 <a href="https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/">这本书</a> 。看上去这本书没有什么名气，我也不知道从什么地方找到的。粗略地看了里面一些内容，觉得它对于BP的分析让我有点启发。正好这门课程也是关于NN/DL的基础知识，所以就干脆写在一起吧。
</p>

<p>
几个比较小的tricks:
</p>
<ul class="org-ul">
<li>初始化weight的时候，可以 *0.001。让初始A尽可能地小，这样学习起来更快。</li>
<li>tanh比sigmoid好。如果z=0的话，sigmoid(z)=0.5, 还会触发后层，而tanh(z)=0更加如何直觉吧。</li>
<li>relu(rectified linear unit) &amp; leaky relu. 在x&gt;0的时候导数保持常数而不会消失。</li>
</ul>

<div id="outline-container-orgc5e3a8f" class="outline-2">
<h2 id="orgc5e3a8f"><span class="section-number-2">1</span> 规范符号</h2>
<div class="outline-text-2" id="text-1">
<p>
我觉得这门课程对我最大的帮助就是规范符号。作者总结了个 <a href="images/dl-notation-standard.pdf">pdf</a> 来总结和规范这些符号。
</p>

<p>
下面是我个人的笔记：
</p>
<ul class="org-ul">
<li>小写和大写字符。小写表示一个训练case, 大小表示多个训练cases.
<ul class="org-ul">
<li>x, X</li>
<li>z, Z</li>
<li>a(activation), A</li>
</ul></li>
<li>nx表示# of features, m表示# of training examples</li>
<li>输入X的维度是(nx, m). 这样表示最方便计算
<ul class="org-ul">
<li>多个examples垂直地堆积起来</li>
<li>W(l)的维度就是(L(l+1), L(l)), 这样可以直接W(l) * A(l-1)</li>
</ul></li>
</ul>


<div class="figure">
<p><img src="images/nn-dl-vector-impl.png" alt="nn-dl-vector-impl.png" />
</p>
</div>

<p>
上图可以看到Z是在column(axis=1)轴上堆积起来的。这样规定维度推导起来就不用老惦记着转置了。
</p>
</div>
</div>

<div id="outline-container-org353d29a" class="outline-2">
<h2 id="org353d29a"><span class="section-number-2">2</span> FP和BP的公式推导</h2>
<div class="outline-text-2" id="text-2">
<p>
<img src="images/nn-dl-fp-layer.png" alt="nn-dl-fp-layer.png" /> <img src="images/nn-dl-bp-layer.png" alt="nn-dl-bp-layer.png" /> <img src="images/nn-dl-fp-bp-layers.png" alt="nn-dl-fp-bp-layers.png" /> <img src="images/nn-dl-fp-bp-cache.png" alt="nn-dl-fp-bp-cache.png" />
</p>

<p>
只要花点时间，还是可以自己推导出来的。在实现bp的时候，需要拿到A(l-1)以及A(l)这些数据，所以在做fp的时候，需要把每层得到的数据都缓存(caches)起来。
</p>

<p>
iteration flow如下图：
</p>
<ol class="org-ol">
<li>初始化parameters</li>
<li>迭代下面几步
<ol class="org-ol">
<li>fp并且保存每层的cache</li>
<li>计算cost, 并且计算dAL</li>
<li>根据dAL，结合每层的cache，做bp</li>
<li>在bp的过程中得到每层的parameters的delta</li>
<li>更新parameters</li>
</ol></li>
</ol>


<div class="figure">
<p><img src="images/nn-dl-flow.png" alt="nn-dl-flow.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-orgd92fe20" class="outline-2">
<h2 id="orgd92fe20"><span class="section-number-2">3</span> Chain Rule.</h2>
<div class="outline-text-2" id="text-3">
<p>
微积分的连锁律，很符合直觉。比如z = g(f(x))的话，那么dz/dx = dg/df * df/dx.
</p>

<p>
很长一段时间内，我对下图没有办法通过公式推导出来. 如果out=A*B的话，那么dout/da = B, 那么da = dout / B才对
</p>


<div class="figure">
<p><img src="images/nn-dl-bp-basic.png" alt="nn-dl-bp-basic.png" />
</p>
</div>

<p>
上来这门课程才知道，原来dout是(dL/dout，这里的d可以认为是偏微分)的缩写，相当于是损失函数在out这个变量上的导数。所以dout开头这个d并不是偏微分，而应该理解为delta.
</p>

<p>
如果这样的话就好理解了。da = dL/da = dL/dout * dout/da = dout * B. （这里大家注意区分d是delta还是偏微分符号）.
</p>

<p>
按照上面的原理，我们就可以得到很多函数的bp公式了：
</p>
<ul class="org-ul">
<li>ReLU(x) = max(x, 0). 那么dx = dout if x &gt;=0 else 0</li>
<li>Dropout(x) = x * p. 其中p可以认为是开关。那么dx = dout * p</li>
</ul>
</div>
</div>
</div>
</body>
</html>
