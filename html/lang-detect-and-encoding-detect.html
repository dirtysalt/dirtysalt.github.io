<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>语言检测和编码检测</title>
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="dirtysalt" />
<link rel="shortcut icon" href="/themes/favicon.ico" /><link rel="stylesheet" type="text/css" href="/themes/simple.css"/></head>
<body>
<div id="content">
<h1 class="title">语言检测和编码检测</h1>
<p>
今天想到一个问题，网络抓取的时候经常会抓到很多乱码数据。通常来说这些乱码数据能只是因为错误的编码方式才呈现乱码，如果使用另外的编码方式或许就会得到有意义的文本。是否可以自动地进行编码检测呢？
</p>

<p>
我觉得可以从语言检测这个问题入手。对于一段乱码或者是无意义的文本，语言检测可能没有办法得到某种显著的结果，比如概率分布或许是(en=0.3, pt=0.3, zh=0.3)这样的。而如果是有意义的文本的话，通常可以很容易地被识别出某种语言，比如概率分布或许是(zh=0.95, en=0.03).
</p>

<p>
阅读了一下python langdetect <a href="https://github.com/Mimino666/langdetect">https://github.com/Mimino666/langdetect</a> 的代码，大致思路是这样的：
</p>
<ol class="org-ol">
<li>首先做各种语言的原始文本做1,2,3-gram 可以计算出各种words的出现次数
</li>
<li>将各种语言的words出现次数汇总，形成一个map&lt;string, vector&gt;. 其中string是word, vector是每种语言对应的概率。这个概率只需要在语言内部归一化即可，不用跨语言的归一化。
</li>
<li>对输入文本进行清洗，比如针对url, mail过滤掉，将空格合并等操作。
</li>
<li>对输入文件切分按照1,2,3-gram进行切分，这样同样可以得到许多words。
</li>
<li>将这些words多次地随机地在map中去查找，将prob相乘，这样可以得到最终每种语言的最终概率。
</li>
</ol>

<p>
如果按照这种思路来做乱码检测和纠正的话，可以这样完成。
</p>
<ol class="org-ol">
<li>枚举所有可能的编码方式。网络上常用的编码方式不多，UTF-8, GBK, ISO-8859-1, latin-1 这些是常用的
</li>
<li>针对每种编码方式解码，将解码后的文本应用langdetect. 如果某个语言的概率显著的高比如&gt;90%的话，那么基本可以认为是这种编码方式。
</li>
</ol>
</div>
</body>
</html>
