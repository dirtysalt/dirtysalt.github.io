<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Java Class 加载乱象一例</title>
<meta name="author" content="dirtysalt" />
<meta name="generator" content="Org Mode" />
<link rel="shortcut icon" href="/themes/favicon.ico" /><link rel="stylesheet" type="text/css" href="/themes/simple.css"/></head>
<body>
<div id="content" class="content">
<h1 class="title">Java Class 加载乱象一例</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org5c70613">1. 问题</a></li>
<li><a href="#org067151b">2. Hudi MOR Reader</a></li>
<li><a href="#org5864d86">3. 引入hadoop-aws的问题</a></li>
<li><a href="#org5d8b3c3">4. NoSuchFieldError</a></li>
<li><a href="#orgda63e48">5. NoClassDefFoundError</a></li>
<li><a href="#orga466005">6. 总结</a></li>
</ul>
</div>
</div>

<div id="outline-container-org5c70613" class="outline-2">
<h2 id="org5c70613"><span class="section-number-2">1.</span> 问题</h2>
<div class="outline-text-2" id="text-1">
<p>
这个问题是从github issue上来的.
</p>

<p>
<a href="https://github.com/StarRocks/starrocks/issues/27556">Hudi fails to read Merge On Read table after adding logs files · Issue #27556 · StarRocks/starrocks</a>
</p>

<p>
目前我们读取hudi table有两个代码分支：
</p>
<ol class="org-ol">
<li>对于COW表(_ro)，因为里面只有parquet文件，那么使用我们的native C++ code.</li>
<li>对于MOR表(_rt)，因为可能涉及到多个文件的merge, 逻辑比较复杂，所以使用的是Java Code, C++中使用JNI来调用。</li>
</ol>

<p>
这个issue看上去是，在访问Hudi Table的时候，没有办法读取 `s3://` 文件系统，走的是MOR表这个代码分支。
</p>
</div>
</div>

<div id="outline-container-org067151b" class="outline-2">
<h2 id="org067151b"><span class="section-number-2">2.</span> Hudi MOR Reader</h2>
<div class="outline-text-2" id="text-2">
<p>
目前Hudi MOR Reader代码在我们的 java-extensions 这个目录下面。
</p>

<p>
<a href="https://github.com/StarRocks/starrocks/blob/main/java-extensions/hudi-reader/src/main/java/com/starrocks/hudi/reader/HudiSliceScannerFactory.java">https://github.com/StarRocks/starrocks/blob/main/java-extensions/hudi-reader/src/main/java/com/starrocks/hudi/reader/HudiSliceScannerFactory.java</a>
</p>

<p>
这个factory类使用了定制的class loader. 因为如果不定制class loader的话，那么BE启动的时候其实会使用 `hadoop_env.sh` 里面定义的classpath
</p>

<div class="org-src-container">
<pre class="src src-Bash">export HADOOP_CLASSPATH=${STARROCKS_HOME}/lib/hadoop/common/*:${STARROCKS_HOME}/lib/hadoop/common/lib/*:${STARROCKS_HOME}/lib/hadoop/hdfs/*:${STARROCKS_HOME}/lib/hadoop/hdfs/lib/*
</pre>
</div>

<p>
这个classpath加载的是 `be/lib/hadoop` 下面的jar.  <b><b>这些jars可能和目前hudi reader的jar是不兼容的，这就是要做定制class loader的原因。</b></b>
</p>

<div class="org-src-container">
<pre class="src src-Java">public class HudiSliceScannerFactory implements ScannerFactory {
    static ChildFirstClassLoader classLoader;

    static {
        String basePath = System.getenv("STARROCKS_HOME");
        File dir = new File(basePath + "/lib/hudi-reader-lib");
        URL[] jars = Arrays.stream(Objects.requireNonNull(dir.listFiles()))
                .map(f -&gt; {
                    try {
                        return f.toURI().toURL();
                    } catch (MalformedURLException e) {
                        e.printStackTrace();
                        throw new RuntimeException("Cannot init hudi slice classloader.", e);
                    }
                }).toArray(URL[]::new);
        classLoader = new ChildFirstClassLoader(jars, ClassLoader.getSystemClassLoader());
    }

    /**
     * Hudi scanner uses own independent classloader to find all classes
     * due to hadoop version (hadoop-2.x) conflicts with JNI launcher of libhdfs (hadoop-3.x).
     */
    @Override
    public Class getScannerClass() throws ClassNotFoundException {
        try {
            return classLoader.loadClass("com.starrocks.hudi.reader.HudiSliceScanner");
        } catch (ClassNotFoundException e) {
            e.printStackTrace();
            throw e;
        }
    }
}
</pre>
</div>
</div>
</div>

<div id="outline-container-org5864d86" class="outline-2">
<h2 id="org5864d86"><span class="section-number-2">3.</span> 引入hadoop-aws的问题</h2>
<div class="outline-text-2" id="text-3">
<p>
所以解决办法就是让hadoop也支持s3这个协议，我们需要引入 hadoop-aws 这个包。 <a href="https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html">Apache Hadoop Amazon Web Services support – Hadoop-AWS module: Integration with Amazon Web Services</a>.
</p>

<p>
这个比较简单，修改pom.xml就行。并且为了和presto自带的hadoop兼容，我们需要使用相同的版本 2.7.4.
</p>

<div class="org-src-container">
<pre class="src src-Xml">&lt;presto.hadoop.version&gt;2.7.4-11&lt;/presto.hadoop.version&gt;
&lt;presto.hive.version&gt;3.0.0-8&lt;/presto.hive.version&gt;
&lt;hadoop.version&gt;2.7.4&lt;/hadoop.version&gt;

&lt;dependency&gt;
    &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
    &lt;artifactId&gt;hadoop-aws&lt;/artifactId&gt;
    &lt;version&gt;${hadoop.version}&lt;/version&gt;
&lt;/dependency&gt;
</pre>
</div>

<p>
完了之后还需要修改一下 be/conf/core-site.xml, 告诉hadoop jar：如果遇到 s3://这样的协议，那么使用其中某个类来处理。
</p>

<div class="org-src-container">
<pre class="src src-Xml">&lt;property&gt;
    &lt;name&gt;fs.s3.impl&lt;/name&gt;
    &lt;value&gt;org.apache.hadoop.fs.s3a.S3AFileSystem&lt;/value&gt;
 &lt;/property&gt;
 &lt;property&gt;
    &lt;name&gt;fs.AbstractFileSystem.s3.impl&lt;/name&gt;
    &lt;value&gt;org.apache.hadoop.fs.s3a.S3A&lt;/value&gt;
 &lt;/property&gt;
 &lt;property&gt;
    &lt;name&gt;fs.s3.access.key&lt;/name&gt;
    &lt;value&gt;*****&lt;/value&gt;
 &lt;/property&gt;
 &lt;property&gt;
    &lt;name&gt;fs.s3.secret.key&lt;/name&gt;
    &lt;value&gt;*****&lt;/value&gt;
 &lt;/property&gt;
 &lt;property&gt;
    &lt;name&gt;fs.s3.endpoint&lt;/name&gt;
    &lt;value&gt;*****&lt;/value&gt;
 &lt;/property&gt;
</pre>
</div>

<p>
测试下来访问s3上的文件没有问题
</p>
</div>
</div>

<div id="outline-container-org5d8b3c3" class="outline-2">
<h2 id="org5d8b3c3"><span class="section-number-2">4.</span> NoSuchFieldError</h2>
<div class="outline-text-2" id="text-4">
<p>
但是今天早上daily出错了，而且是在访问hdfs文件系统出错的，错误信息如下
</p>

<pre class="example" id="org06a9b00">
Exception in thread "main" java.lang.NoSuchFieldError: LOG
        at org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.&lt;init&gt;(DomainSocketFactory.java:110)
        at org.apache.hadoop.hdfs.ClientContext.&lt;init&gt;(ClientContext.java:117)
        at org.apache.hadoop.hdfs.ClientContext.get(ClientContext.java:159)
        at org.apache.hadoop.hdfs.DFSClient.&lt;init&gt;(DFSClient.java:703)
        at org.apache.hadoop.hdfs.DFSClient.&lt;init&gt;(DFSClient.java:619)
        at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:149)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)
        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)
        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)
        at org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:38)
        at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:469)
        at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:454)
        at org.apache.hadoop.hive.ql.io.parquet.ParquetRecordReaderBase.getSplit(ParquetRecordReaderBase.java:79)
        at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.&lt;init&gt;(ParquetRecordReaderWrapper.java:75)
        at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.&lt;init&gt;(ParquetRecordReaderWrapper.java:60)
        at org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:92)
        at org.apache.hudi.hadoop.HoodieParquetInputFormat.getRecordReaderInternal(HoodieParquetInputFormat.java:89)
        at org.apache.hudi.hadoop.HoodieParquetInputFormat.getRecordReader(HoodieParquetInputFormat.java:83)
        at org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat.getRecordReader(HoodieParquetRealtimeInputFormat.java:74)
        at com.starrocks.hudi.reader.HudiSliceScanner.initReader(HudiSliceScanner.java:187)
        at com.starrocks.hudi.reader.HudiSliceScanner.open(HudiSliceScanner.java:205)
</pre>

<p>
我在github上看了一下 <a href="https://github.com/apache/hadoop/blob/release-2.7.4-RC0/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/PerformanceAdvisory.java">代码</a>， 这个类里面的确是有LOG字段的。
</p>

<p>
我遍历了一下hudi-reader-lib下面所有的jar, 看看那个jar里面包含了 `PerformanceAdvisory.java` 这个类
</p>
<ul class="org-ul">
<li>一个是 `hadoop-apache2-2.7.4-11.jar` 这个是pom.xml里面显示指定的</li>
<li>一个是 `hadoop-common-2.7.4.jar` 这个pom.xml是hadoop-aws间接引入的</li>
</ul>

<pre class="example" id="org07c1370">
hadoop-apache2-2.7.4-11.jar
org/apache/hadoop/util/PerformanceAdvisory.class

hadoop-common-2.7.4.jar
org/apache/hadoop/util/PerformanceAdvisory.class
</pre>

<p>
如果用javap查看这两个类，可以发现其实都有LOG，差别就是两者的类型不同
</p>

<pre class="example" id="org3c2be74">
sandbox-cloud :: be/lib/hudi-reader-lib ‹main*› » javap hadoop-apache2-PerformanceAdvisory.class
Compiled from "PerformanceAdvisory.java"
public class org.apache.hadoop.util.PerformanceAdvisory {
  public static final com.facebook.presto.hadoop.$internal.org.slf4j.Logger LOG;
  public org.apache.hadoop.util.PerformanceAdvisory();
  static {};
}

sandbox-cloud :: be/lib/hudi-reader-lib ‹main*› » javap hadoop-common-PerformanceAdvisory.class
Compiled from "PerformanceAdvisory.java"
public class org.apache.hadoop.util.PerformanceAdvisory {
  public static final org.slf4j.Logger LOG;
  public org.apache.hadoop.util.PerformanceAdvisory();
  static {};
}
</pre>

<p>
看上去去解决办法就是，把hadoop-common-2.7.4从hadoop-aws里面挪出去
</p>

<div class="org-src-container">
<pre class="src src-Xml">&lt;dependency&gt;
    &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
    &lt;artifactId&gt;hadoop-aws&lt;/artifactId&gt;
    &lt;version&gt;${hadoop.version}&lt;/version&gt;
    &lt;exclusions&gt;
        &lt;exclusion&gt;
            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
            &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;
        &lt;/exclusion&gt;
    &lt;/exclusions&gt;
&lt;/dependency&gt;
</pre>
</div>

<p>
测试下来hdfs是没有问题了。
</p>
</div>
</div>

<div id="outline-container-orgda63e48" class="outline-2">
<h2 id="orgda63e48"><span class="section-number-2">5.</span> NoClassDefFoundError</h2>
<div class="outline-text-2" id="text-5">
<p>
但是自测s3的时候又发现一个问题，错误信息如下
</p>

<pre class="example" id="org380a78a">
Exception in thread "Thread-9" java.lang.NoClassDefFoundError: org/apache/commons/lang/StringUtils
        at org.apache.hadoop.fs.s3a.BasicAWSCredentialsProvider.getCredentials(BasicAWSCredentialsProvider.java:37)
        at com.amazonaws.auth.AWSCredentialsProviderChain.getCredentials(AWSCredentialsProviderChain.java:101)
        at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3521)
        at com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1031)
        at com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:994)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:297)
        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)
        at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)
        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)
        at org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:38)
        at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:469)
        at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:454)
        at org.apache.hadoop.hive.ql.io.parquet.ParquetRecordReaderBase.getSplit(ParquetRecordReaderBase.java:79)
        at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.&lt;init&gt;(ParquetRecordReaderWrapper.java:75)
        at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.&lt;init&gt;(ParquetRecordReaderWrapper.java:60)
        at org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:92)
        at org.apache.hudi.hadoop.HoodieParquetInputFormat.getRecordReaderInternal(HoodieParquetInputFormat.java:89)
        at org.apache.hudi.hadoop.HoodieParquetInputFormat.getRecordReader(HoodieParquetInputFormat.java:83)
        at com.starrocks.hudi.reader.HudiSliceScanner.initReader(HudiSliceScanner.java:187)
        at com.starrocks.hudi.reader.HudiSliceScanner.open(HudiSliceScanner.java:205)
</pre>

<p>
这个问题看上去比较好理解：因为上面去掉了hadoop-common, 但是里面有个类被hadoop-aws又需要了。
</p>

<p>
找了一下，这个package在
</p>

<div class="org-src-container">
<pre class="src src-Xml">&lt;dependency&gt;
    &lt;groupId&gt;commons-lang&lt;/groupId&gt;
    &lt;artifactId&gt;commons-lang&lt;/artifactId&gt;
    &lt;version&gt;2.6&lt;/version&gt;
&lt;/dependency&gt;
</pre>
</div>

<p>
hadoop-apache2 jar其实里面也用到了StringUtils, 但是被放在了另外一个package下面
</p>

<pre class="example" id="org164c447">
hadoop-apache2-2.7.4-11.jar
org/apache/hadoop/util/StringUtils.class
org/apache/hadoop/util/StringUtils$TraditionalBinaryPrefix.class
org/apache/hadoop/util/StringUtils$1.class
com/facebook/presto/hadoop/$internal/org/apache/commons/codec/binary/StringUtils.class
com/facebook/presto/hadoop/$internal/org/apache/commons/lang/RandomStringUtils.class
com/facebook/presto/hadoop/$internal/org/apache/commons/lang/StringUtils.class
com/facebook/presto/hadoop/$internal/org/apache/commons/lang3/RandomStringUtils.class
com/facebook/presto/hadoop/$internal/org/apache/commons/lang3/StringUtils.class
</pre>
</div>
</div>

<div id="outline-container-orga466005" class="outline-2">
<h2 id="orga466005"><span class="section-number-2">6.</span> 总结</h2>
<div class="outline-text-2" id="text-6">
<p>
问题到这里， <b><b>基本</b></b> 算是被解决了：因为主要走这条代码逻辑，那么就不会出现class加载的问题。
</p>

<p>
但是这个并不能保证未来不会出问题，因为其他代码分支可能会加载新的class.
</p>

<p>
一些心得：
</p>
<ul class="org-ul">
<li>整个过程按了葫芦起了瓢：现在s3不行，修复后hdfs不行，修复后s3不行</li>
<li>Java可以自己写class loader, 但是需要管理好依赖</li>
<li>一旦出现依赖冲突，就只能找到冲突的包，把包挪出去</li>
<li><b><b>这个冲突可能是动态发生的，没有办法通过静态方法发现。</b></b></li>
</ul>
</div>
</div>
</div>
</body>
</html>
