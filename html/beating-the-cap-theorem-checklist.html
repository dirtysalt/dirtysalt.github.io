<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Beating the CAP Theorem Checklist</title>
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="dirtysalt" />
<link rel="shortcut icon" href="/themes/favicon.ico" /><link rel="stylesheet" type="text/css" href="/themes/simple.css"/></head>
<body>
<div id="content">
<h1 class="title">Beating the CAP Theorem Checklist</h1>
<p>
Your ( ) tweet ( ) blog post ( ) marketing material ( ) online comment
advocates a way to beat the CAP theorem. Your idea will not work. Here is why
it won't work:
</p>

<ul class="org-ul">
<li>( ) you are assuming that software/network/hardware failures will not happen
</li>
<li>( ) you pushed the actual problem to another layer of the system
</li>
<li>( ) your solution is equivalent to an existing one that doesn't beat CAP
</li>
<li>( ) you're actually building an AP system
</li>
<li>( ) you're actually building a CP system
</li>
<li>( ) you are not, in fact, designing a distributed system
</li>
</ul>

<p>
Specifically, your plan fails to account for:
</p>

<ul class="org-ul">
<li>( ) latency is a thing that exists
</li>
<li>( ) high latency is indistinguishable from splits or unavailability
</li>
<li>( ) network topology changes over time
</li>
<li>( ) there might be more than 1 partition at the same time
</li>
<li>( ) split nodes can vanish forever
</li>
<li>( ) a split node cannot be differentiated from a crashed one by its peers
</li>
<li>( ) clients are also part of the distributed system
</li>
<li>( ) stable storage may become corrupt
</li>
<li>( ) network failures will actually happen
</li>
<li>( ) hardware failures will actually happen
</li>
<li>( ) operator errors will actually happen
</li>
<li>( ) deleted items will come back after synchronization with other nodes
</li>
<li>( ) clocks drift across multiple parts of the system, forward and backwards in time
</li>
<li>( ) things can happen at the same time on different machines
</li>
<li>( ) side effects cannot be rolled back the way transactions can
</li>
<li>( ) failures can occur while in a critical part of your algorithm
</li>
<li>( ) designing distributed systems is actually hard
</li>
<li>( ) implementing them is harder still
</li>
</ul>

<p>
And the following technical objections may apply:
</p>

<ul class="org-ul">
<li>( ) your solution requires a central authority that cannot be unavailable
</li>
<li>( ) read-only mode is still unavailability for writes
</li>
<li>( ) your quorum size cannot be changed over time
</li>
<li>( ) your cluster size cannot be changed over time
</li>
<li>( ) using 'infinite timeouts' is not an acceptable solution to lost messages
</li>
<li>( ) your system accumulates data forever and assumes infinite storage
</li>
<li>( ) re-synchronizing data will require more bandwidth than everything else put together
</li>
<li>( ) acknowledging reception is not the same as confirming consumption of messages
</li>
<li>( ) you don't even wait for messages to be written to disk
</li>
<li>( ) you assume short periods of unavailability are insignificant
</li>
<li>( ) you are basing yourself on a paper or theory that has not yet been proven
</li>
</ul>

<p>
Furthermore, this is what I think about you:
</p>

<ul class="org-ul">
<li>( ) nice try, but blatantly false advertising
</li>
<li>( ) you are badly reinventing existing concepts and should do some research
</li>
<li>( ) in particular, you should read the definition of the word 'theorem'
</li>
<li>( ) also you should read the definition of 'distributed system'
</li>
<li>( ) you have no idea what you are doing
</li>
<li>( ) do you even know what a logical clock is?
</li>
<li>( ) you shouldn't be in charge of people's data
</li>
</ul>
</div>
</body>
</html>
