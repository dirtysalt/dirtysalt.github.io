<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Beating the CAP Theorem Checklist</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="dirtysalt" />
<link rel="shortcut icon" href="/themes/favicon.ico" /><link rel="stylesheet" type="text/css" href="/themes/simple.css"/></head>
<body>
<div id="content">
<h1 class="title">Beating the CAP Theorem Checklist</h1>
<p>
Your ( ) tweet ( ) blog post ( ) marketing material ( ) online comment
advocates a way to beat the CAP theorem. Your idea will not work. Here is why
it won't work:
</p>

<ul class="org-ul">
<li>( ) you are assuming that software/network/hardware failures will not happen</li>
<li>( ) you pushed the actual problem to another layer of the system</li>
<li>( ) your solution is equivalent to an existing one that doesn't beat CAP</li>
<li>( ) you're actually building an AP system</li>
<li>( ) you're actually building a CP system</li>
<li>( ) you are not, in fact, designing a distributed system</li>
</ul>

<p>
Specifically, your plan fails to account for:
</p>

<ul class="org-ul">
<li>( ) latency is a thing that exists</li>
<li>( ) high latency is indistinguishable from splits or unavailability</li>
<li>( ) network topology changes over time</li>
<li>( ) there might be more than 1 partition at the same time</li>
<li>( ) split nodes can vanish forever</li>
<li>( ) a split node cannot be differentiated from a crashed one by its peers</li>
<li>( ) clients are also part of the distributed system</li>
<li>( ) stable storage may become corrupt</li>
<li>( ) network failures will actually happen</li>
<li>( ) hardware failures will actually happen</li>
<li>( ) operator errors will actually happen</li>
<li>( ) deleted items will come back after synchronization with other nodes</li>
<li>( ) clocks drift across multiple parts of the system, forward and backwards in time</li>
<li>( ) things can happen at the same time on different machines</li>
<li>( ) side effects cannot be rolled back the way transactions can</li>
<li>( ) failures can occur while in a critical part of your algorithm</li>
<li>( ) designing distributed systems is actually hard</li>
<li>( ) implementing them is harder still</li>
</ul>

<p>
And the following technical objections may apply:
</p>

<ul class="org-ul">
<li>( ) your solution requires a central authority that cannot be unavailable</li>
<li>( ) read-only mode is still unavailability for writes</li>
<li>( ) your quorum size cannot be changed over time</li>
<li>( ) your cluster size cannot be changed over time</li>
<li>( ) using 'infinite timeouts' is not an acceptable solution to lost messages</li>
<li>( ) your system accumulates data forever and assumes infinite storage</li>
<li>( ) re-synchronizing data will require more bandwidth than everything else put together</li>
<li>( ) acknowledging reception is not the same as confirming consumption of messages</li>
<li>( ) you don't even wait for messages to be written to disk</li>
<li>( ) you assume short periods of unavailability are insignificant</li>
<li>( ) you are basing yourself on a paper or theory that has not yet been proven</li>
</ul>

<p>
Furthermore, this is what I think about you:
</p>

<ul class="org-ul">
<li>( ) nice try, but blatantly false advertising</li>
<li>( ) you are badly reinventing existing concepts and should do some research</li>
<li>( ) in particular, you should read the definition of the word 'theorem'</li>
<li>( ) also you should read the definition of 'distributed system'</li>
<li>( ) you have no idea what you are doing</li>
<li>( ) do you even know what a logical clock is?</li>
<li>( ) you shouldn't be in charge of people's data</li>
</ul>
</div>
</body>
</html>
