<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Large-Scale Deep Learning for Intelligent Computer Systems</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="dirtysalt" />
<link rel="shortcut icon" href="/themes/favicon.ico" /><link rel="stylesheet" type="text/css" href="/themes/simple.css"/></head>
<body>
<div id="content">
<h1 class="title">Large-Scale Deep Learning for Intelligent Computer Systems</h1>
<p>
用大规模的深度学习来构建智能计算机系统
</p>

<hr />
<p>
DL在Google内部大行其道，遍布各个产品线。也说明了产品往后发展也会越来越更加智能。
</p>


<div class="figure">
<p><img src="images/google-growing-use-of-dl.png" alt="google-growing-use-of-dl.png" />
</p>
</div>

<hr />
<p>
G内部有两代深度学习系统
</p>

<p>
Two generations of deep learning software systems:
</p>
<ul class="org-ul">
<li>1st generation: DistBelief [Dean et al., NIPS 2012].</li>
<li>2nd generation: TensorFlow (unpublished)</li>
</ul>
<p>
An overview of how we use these in research and products Plus, &#x2026;a new approach for training (people, not models)
</p>

<p>
第二代系统相比第一代更加侧重： 1. 支持更大的计算量  2. 支持更大的数据量（文字，图片，声音，日志，知识图谱）
</p>

<ul class="org-ul">
<li>Text: trillions of words of English + other languages</li>
<li>Visual data: billions of images and videos</li>
<li>Audio: tens of thousands of hours of speech per day</li>
<li>User activity: queries, marking messages spam, etc.</li>
<li>Knowledge graph: billions of labelled relation triples</li>
</ul>

<hr />
<p>
Parallelism
</p>

<p>
Model Parallelism + Data Parallelism.
</p>

<p>
Model并行化和DL网络结构相关，比如CNN里面的Local Receptive Field就决定了Model Parallelism是可行的。
</p>

<p>
Data Parallelism可以通过类似Parameter Server的方案来解决
</p>


<div class="figure">
<p><img src="images/google-parameter-server-for-adsdg.png" alt="google-parameter-server-for-adsdg.png" />
</p>
</div>

<p>
Can do this synchronously:
</p>
<ul class="org-ul">
<li>N replicas eqivalent to an N times larger batch size</li>
<li>Pro: No noise</li>
<li>Con: Less fault tolerant (requires recovery if any single machine fails)</li>
</ul>

<p>
Can do this asynchronously:
</p>
<ul class="org-ul">
<li>Con: Noise in gradients</li>
<li>Pro: Relatively fault tolerant (failure in model replica doesn’t block other replicas)</li>
</ul>

<p>
(Or hybrid: M asynchronous groups of N synchronous replicas)
</p>

<hr />
<p>
后面还有一些Google在DL方面取得的成果，以及Tensorflow的介绍
</p>
</div>
</body>
</html>
