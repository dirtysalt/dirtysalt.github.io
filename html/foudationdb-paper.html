<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>FoundationDB: A Distributed Unbundled Transactional Key Value Store</title>
<meta name="author" content="dirtysalt" />
<meta name="generator" content="Org Mode" />
<link rel="shortcut icon" href="/themes/favicon.ico" /><link rel="stylesheet" type="text/css" href="/themes/simple.css"/></head>
<body>
<div id="content" class="content">
<h1 class="title">FoundationDB: A Distributed Unbundled Transactional Key Value Store</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org7997fab">1. abstract</a></li>
<li><a href="#org7f19f3b">2. introduction</a></li>
<li><a href="#org06bec6d">3. design</a>
<ul>
<li><a href="#orgd858b8d">3.1. principles</a></li>
<li><a href="#org0a3eb54">3.2. APIs</a></li>
<li><a href="#orgf9a89d6">3.3. archtecture</a></li>
</ul>
</li>
<li><a href="#orgbfb3c20">4. txn management</a>
<ul>
<li><a href="#orgeb8fcb1">4.1. process</a></li>
<li><a href="#orgaa8a4a2">4.2. check conflict</a></li>
<li><a href="#org2eaa1c2">4.3. commit</a></li>
<li><a href="#orgd3d22c2">4.4. recovery</a></li>
</ul>
</li>
<li><a href="#orgd746eeb">5. simulation testing</a></li>
<li><a href="#orga333085">6. 5s MVCC Window</a></li>
</ul>
</div>
</div>
<p>
<a href="https://www.foundationdb.org/files/fdb-paper.pdf">https://www.foundationdb.org/files/fdb-paper.pdf</a>
</p>

<div id="outline-container-org7997fab" class="outline-2">
<h2 id="org7997fab"><span class="section-number-2">1.</span> abstract</h2>
<div class="outline-text-2" id="text-1">
<p>
fdbæœ‰è¿™ä¹ˆå‡ ä¸ªç‰¹ç‚¹ï¼š
</p>
<ul class="org-ul">
<li>ACID kv/nosql æ”¯æŒäº‹åŠ¡çš„KVæ•°æ®åº“</li>
<li>unbundled database. æ•°æ®åº“çš„å¤šä¸ªç»„ä»¶æ˜¯è¿›ç¨‹ä¹‹é—´éš”ç¦»çš„</li>
<li>simutation testing. å¯ä»¥è¿›è¡Œç¡®å®šæ€§çš„æ¨¡æ‹Ÿæµ‹è¯•</li>
<li>å¦å¤–å°±æ˜¯åœ¨äº‹åŠ¡å®ç°ä¸Šæ¯”è¾ƒæœ‰ç‰¹ç‚¹ï¼Œäº‹åŠ¡æ—¶é—´æœ€é•¿æ˜¯5s</li>
</ul>
</div>
</div>

<div id="outline-container-org7f19f3b" class="outline-2">
<h2 id="org7f19f3b"><span class="section-number-2">2.</span> introduction</h2>
<div class="outline-text-2" id="text-2">
<p>
fdbç›®æ ‡æ˜¯ç»™åˆ†å¸ƒå¼ç³»ç»Ÿæä¾›å¯ç”¨çš„æœåŠ¡ï¼Œæ‰€ä»¥åœ¨è®¾è®¡ä¸Šå¯èƒ½æ¯”è¾ƒç®€å•ï¼Œå°±æ˜¯æ”¯æŒäº‹åŠ¡çš„KVå­˜å‚¨ç³»ç»Ÿã€‚
</p>

<blockquote>
<p>
FoundationDB (FDB) [5] was created in 2009 and gets its name from the focus on providing what we saw as the foundational set of building blocks required to build higher-level distributed systems. It is an ordered, transactional, key-value store natively supporting multi-key strictly serializable transactions across its entire key- space. Unlike most databases, which bundle together a storage engine, data model, and query language, forcing users to choose all three or none, FDB takes a modular approach: it provides a highly scalable, transactional storage engine with a minimal yet carefully chosen set of features. It provides no structured semantics, no query language, data model or schema management, secondary indices or many other features one normally finds in a transactional database. Offering these would benefit some applications but others that do not require them (or do so in a slightly different form) would need to work around. Instead, the NoSQL model leaves application develop- ers with great flexibility. While FDB defaults to strictly serializable transactions, it allows relaxing these semantics for applications that donâ€™t require them with flexible, fine-grained controls over conflicts.
</p>
</blockquote>

<p>
è®¸å¤šåˆ†å¸ƒå¼æ•°æ®åº“å¯ä»¥å°†fdbç”¨äºlower half, ç„¶åè‡ªå·±å®Œæˆä¸Šé¢çš„upper half.
</p>

<blockquote>
<p>
One of the reasons for its popularity and growing open source community is FoundationDBâ€™s focus on the â€œlower halfâ€ of a data- base, leaving the rest to its â€œlayersâ€â€”stateless applications devel- oped on top to provide various data models and other capabilities. With this, applications that would traditionally require completely different types of storage systems, can instead all leverage FDB. Indeed, the wide range of layers that have been built on FDB in recent years are evidence to the usefulness of this unusual design. For example, the FoundationDB Record Layer [28] adds back much of what users expect from a relational database, and JanusGraph [9], a graph database, provides an implementation as a FoundationDB layer [8]. In its newest release, CouchDB [2] (arguably the first NoSQL system) is being re-built as a layer on top of FoundationDB.
</p>
</blockquote>

<p>
fdbè‡ªå¸¦çš„simutation testing frameworkå¯ä»¥ä½¿ç”¨ç¡®å®šæ€§çš„æ¨¡æ‹Ÿå›æ”¾æ¥å‘ç°è®¸å¤šbugs, å¹¶ä¸”åˆ›å»ºå›å½’cases. è¿™ä¸ªä¸œè¥¿æ²¡æœ‰åŠæ³•æµ‹è¯•æ€§èƒ½ï¼Œä½†æ˜¯æµ‹è¯•æ­£ç¡®æ€§åº”è¯¥å¾ˆæœ‰ç”¨ã€‚
</p>

<blockquote>
<p>
Testing and debugging distributed systems is at least as hard as building them. Unexpected process and network failures, mes- sage reorderings, and other sources of non-determinism can expose subtle bugs and implicit assumptions that break in reality, which are extremely difficult to reproduce or debug. The consequences of such subtle bugs are especially severe for database systems, which purport to offer perfect fidelity to an unambiguous contract. More- over, the stateful nature of a database system means that any such bug can result in subtle data corruption that may not be discovered for months. Model checking techniques can verify the correctness of distributed protocols, but often fall short of checking the ac- tual implementation. Deep bugs [46], which only happen when multiple crashes or restarts occur in a particular sequence, pose a challenge even for end-to-end testing infrastructure. The develop- ment of FDB took a radical approachâ€”before building the database itself, we built a deterministic database simulation framework that can simulate a network of interacting processes and a variety of disk, process, network, and request-level failures and recoveries, all within a single physical process. This rigorous testing in simulation makes FDB extremely stable, and allows its developers to introduce new features and releases in a rapid cadence. This is unusual not only for distributed databases, but even for centralized systems.
</p>
</blockquote>

<p>
fdbæ˜¯unbundledæ¶æ„ï¼Œæ¯ä¸ªç»„ä»¶éƒ½æ˜¯ç‹¬ç«‹çš„è¿›ç¨‹å­˜åœ¨ã€‚æ¯”å¦‚control planeä½¿ç”¨active disk paxosæ¥åšåˆ°é«˜å¯ç”¨ï¼ˆåé¢è¯´ä»–ä»¬æœ€å¼€å§‹æ—¶ä½¿ç”¨ZKçš„ï¼‰ï¼Œä½¿ç”¨OCC/MVCCæ¥å®ç°äº‹åŠ¡ä¸€è‡´æ€§ï¼Œå¥½å¤šç»„ä»¶å…¶å®ä¸æ˜¯quorumæ¥å®ç°çš„ï¼Œè€Œæ˜¯å®Œå…¨åŸºäºæœ€ä¸Šé¢çš„control planeæ¥åšçš„ï¼Œæ‰€ä»¥å¯ä»¥åšåˆ°f+1 replicasæ»¡è¶³f failures. cross regioné«˜å¯ç”¨æ€§å®ç°å¥½åƒä¹Ÿæ¯”è¾ƒä¸­è§„ä¸­çŸ©ã€‚
</p>

<blockquote>
<p>
FDB adopts an unbundled architecture [50] that comprises a con- trol plane and a data plane. The control plane manages the metadata of the cluster and uses Active Disk Paxos [27] for high availability. The data plane consists of a transaction management system, re- sponsible for processing updates, and a distributed storage layer serving reads; both can be independently scaled out. FDB achieves strict serializability through a combination of optimistic concur- rency control (OCC) [44] and multi-version concurrency control (MVCC) [18]. Besides a lock-free architecture, one of the features distinguishing FDB from other distributed databases is its approach to handling failures. Unlike most similar systems, FDB does not rely on quorums to mask failures, but rather tries to eagerly detect and recover from them by reconfiguring the system. This allows us to achieve the same level of fault tolerance with significantly fewer resources: FDB can tolerate ğ‘“ failures with only ğ‘“ + 1 (rather than 2ğ‘“ + 1) replicas. This approach is best suited for deployments in a local or metro area. For WAN deployments, FDB offers a novel strategy that avoids cross-region write latencies while providing automatic failover between regions without losing data.
</p>
</blockquote>
</div>
</div>

<div id="outline-container-org06bec6d" class="outline-2">
<h2 id="org06bec6d"><span class="section-number-2">3.</span> design</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-orgd858b8d" class="outline-3">
<h3 id="orgd858b8d"><span class="section-number-3">3.1.</span> principles</h3>
<div class="outline-text-3" id="text-3-1">
<p>
è®¾è®¡åŸåˆ™å¦‚ä¸‹ï¼š
</p>
<ul class="org-ul">
<li>divide-and-conque è¿™ä¸ªè¿˜æ˜¯unbundledåšæ³•ï¼Œå°±æ˜¯æŠŠç»„ä»¶æ‹†åˆ†æˆä¸ºå¤šä¸ªè¿›ç¨‹ï¼Œè¿™æ ·ä¹‹é—´ä¸ä¼šç›¸äº’å½±å“ï¼Œå¯ä»¥ç‹¬ç«‹éƒ¨ç½²å’Œæ¢å¤ã€‚</li>
<li>failure as common case å°†é”™è¯¯å¸¸æ€åŒ–ï¼Œè¿™æ ·çš„è¯å¯ä»¥åªæµ‹è¯•æ¢å¤è·¯å¾„ï¼Œå¹¶ä¸”å°½å¯èƒ½åœ°å°†æ¢å¤æ—¶é—´å‡å°‘ï¼Œåé¢è¯´åˆ°äº†è¿™æ ·å‡çº§çš„è¯å¯ä»¥ä¸€èµ·å‡çº§ã€‚</li>
<li>fail fast and recover fast. è¿™ä¸ªä¸Šé¢æåˆ°äº†ï¼Œå¯¹äºåˆ†å¸ƒå¼ç³»ç»Ÿfailureåº”è¯¥æ˜¯å¾ˆå¸¸è§çš„ï¼ŒåŒ…æ‹¬å‡çº§ç³»ç»Ÿï¼Œæ‰€ä»¥ä¹Ÿå‡å°‘æ¢å¤æ—¶é—´ã€‚</li>
<li>simulation testing. å¯ä»¥ç”¨å¾ˆå°‘çš„ä»£ä»·æ¥å‘ç°deep bugs.</li>
</ul>

<blockquote>
<p>
Divide-and-Conquer (or separation of concerns). FDB decou- ples the transaction management system (write path) from the distributed storage (read path) and scales them inde- pendently. Within the transaction management system, pro- cesses are assigned various roles representing different as- pects of transaction management, including timestamp man- agement, accepting commits, conflict detection, and logging. Furthermore, cluster-wide orchestrating tasks, such as over- load control, load balancing, and failure recovery are also divided and serviced by additional heterogeneous roles.
</p>

<p>
Make failure a common case. For distributed systems, failure is a norm rather than an exception. In the transaction man- agement system of FDB, we handle all failures through the recovery path: instead of fixing all possible failure scenar- ios, the transaction system proactively shuts down when it detects a failure. As a result, all failure handling is reduced to a single recovery operation, which becomes a common and well-tested code path. Such error handling strategy is desirable as long as the recovery is quick, and pays dividends by simplifying the normal transaction processing.
</p>

<p>
Fail fast and recover fast. To improve availability, FDB strives to minimize Mean-Time-To-Recovery (MTTR), which in- cludes the time to detect a failure, proactively shut down the transaction management system, and recover. In our produc- tion clusters, the total time is usually less than five seconds (see Section 5.3).
</p>

<p>
Simulation testing. FDB relies on a randomized, deterministic simulation framework for testing the correctness of its dis- tributed database. Because simulation tests are both efficient and repeatable, they not only expose deep bugs [46], but also boost developer productivity and the code quality of FDB.
</p>
</blockquote>
</div>
</div>


<div id="outline-container-org0a3eb54" class="outline-3">
<h3 id="org0a3eb54"><span class="section-number-3">3.2.</span> APIs</h3>
<div class="outline-text-3" id="text-3-2">
<p>
APIæ¯”è¾ƒç®€å•ï¼Œå°±æ˜¯get/set/getRange/clearRange. æœ€åå°±æ˜¯commit. key/value size(10KB/100KB)ä»¥åŠäº‹åŠ¡å¤§å°(10MB)æœ‰é™åˆ¶ï¼Œå› ä¸ºä¸€ä¸ªäº‹åŠ¡æ˜¯ä¸€æ¬¡æ€§å‘é€çš„ï¼Œæ‰€ä»¥ä¸å¯èƒ½å¤ªå¤§ã€‚
</p>

<blockquote>
<p>
FDB exposes operations to read and modify single keys as well as ranges of keys. The get() and set() operations read and write a single key-value pair, respectively. For ranges, ğ‘”ğ‘’ğ‘¡ğ‘…ğ‘ğ‘›ğ‘”ğ‘’() returns a sorted list of keys and their values within the given range; and ğ‘ğ‘™ğ‘’ğ‘ğ‘Ÿ () deletes all keys-value pairs whithin a range or starting with a certain key prefix.
</p>

<p>
An FDB transaction observes and modifies a snapshot of the database at a certain version and changes are applied to the under- lying database only when the transaction commits. A transactionâ€™s writes (i.e., set() and clear() calls) are buffered by the FDB client until the final commit() call, and read-your-write semantics are preserved by combining results from database look-ups with uncommitted writes of the transaction. Key and value sizes are limited to 10 KB and 100 KB respectively for better performance. Transaction size is limited to 10 MB, including the size of all written keys and values as well as the size of all keys in read or write conflict ranges that are explicitly specified.
</p>
</blockquote>
</div>
</div>


<div id="outline-container-orgf9a89d6" class="outline-3">
<h3 id="orgf9a89d6"><span class="section-number-3">3.3.</span> archtecture</h3>
<div class="outline-text-3" id="text-3-3">
<p>
ä¸‹é¢è¿™ä¸ªæ˜¯æ¶æ„å›¾ï¼Œå¯ä»¥çœ‹åˆ°åˆ†ä¸ºcontrol plane/data plane.
</p>

<p>
Control Plane:
</p>
<ul class="org-ul">
<li>Coordinators ä¹‹é—´ä½¿ç”¨Active Disk Paxosç»„æˆé«˜å¯ç”¨ç»„ï¼Œä¸Šé¢è¿˜ä¼šå­˜å‚¨æŸäº›é…ç½®æ–‡ä»¶æ¯”å¦‚log servers/storeage serversç­‰ç­‰ã€‚</li>
<li>Coordinators ä¼šé€‰å‡ºä¸€ä¸ªCluster Controller.ä¼šç›‘æ§æ‰€æœ‰çš„æœºå™¨å¹¶ä¸”é€‰æ‹©ä¸‹é¢3ä¸ªè§’è‰²ï¼šSequencer, DataDistributor, RateKeeper</li>
<li>å…¶ä¸­ä¸Šé¢Sequencerå·²ç»è¿›å…¥åˆ°äº†Data Planeå±‚é¢äº†ï¼Œå®ƒä¼šç»§ç»­é€‰æ‹©å‡ºä¸€ç»„Proxies, Resolvers, LogServers.
<ul class="org-ul">
<li>æˆ‘çš„ç†è§£æ˜¯å¦‚æœProxies/Resolvers/LogServerså‡ºç°downçš„è¯ï¼Œé‚£ä¹ˆSequencerå°±ä¼šç›´æ¥é€€å‡ºï¼Œå¼€å§‹é‡å¯æµç¨‹ã€‚</li>
<li>Sequencerä¸»è¦ç›®çš„å°±æ˜¯æ¥åˆ†é…read/commit version.</li>
<li>Proxies ç”¨æ¥å‘Šè¯‰Clientåº”è¯¥å»å“ªé‡Œè¯»å–æ•°æ®ï¼Œä»¥åŠå¸®åŠ©æ£€æŸ¥å’Œæäº¤äº‹åŠ¡çš„</li>
<li>Resolvers ç”¨æ¥æ£€æŸ¥äº‹åŠ¡æ˜¯å¦å¯ä»¥æäº¤</li>
<li>LogServers ç”¨æ¥å­˜å‚¨äº‹åŠ¡log. è¿™äº›logæœ€ç»ˆä¼šè¢«SSå¼‚æ­¥æ‹‰å–åˆ°æœ¬åœ°è¿›è¡Œå›æ”¾ã€‚</li>
</ul></li>
<li>SS. è¿™ä¸ªå°±æ˜¯æœ€ç»ˆçš„å­˜å‚¨æœåŠ¡å™¨ï¼Œä»LSä¸Šæ‹‰å–æ—¥å¿—ä¸‹æ¥è¿›è¡Œå›æ”¾ï¼ŒLSå’ŒSSä¹‹é—´çš„å¯¹åº”å…³ç³»åº”è¯¥æ˜¯ç¡®å®šçš„ã€‚LS/SSå„è‡ªä¼šç»´æŠ¤ä¸€å®šçš„key range.</li>
</ul>


<div id="org9aab383" class="figure">
<p><img src="../images/Pasted-Image-20240307111640.png" alt="Pasted-Image-20240307111640.png" />
</p>
</div>

<blockquote>
<p>
Control Plane The control plane is responsible for persist- ing critical system metadata, i.e., the configuration of transac- tion systems, on Coordinators. These Coordinators form a disk Paxos group [27] and select a singleton ClusterController. The ClusterController monitors all servers in the cluster and re- cruits three singleton processes, Sequencer, DataDistributor, and Ratekeeper, which are re-recruited if they fail or crash. The Sequencer assigns read and commit versions to transactions. The DataDistributor is responsible for monitoring failures and balanc- ing data among StorageServers. Ratekeeper provides overload protection for the cluster.
</p>
</blockquote>

<blockquote>
<p>
DataPlane FDB targets OLTP workloads that are read-mostly, read and write a small set of keys, have low contention, and re- quire scalability. FDB chooses an unbundled architecture [50]: a dis- tributed transaction management system (TS) performs in-memory transaction processing, a log system (LS) stores Write-Ahead-Log (WAL) for TS, and a separate distributed storage system (SS) is used for storing data and servicing reads. The TS provides transaction processing and consists of a Sequencer, Proxies, and Resolvers, all of which are stateless processes. The LS contains a set of Log- Servers and the SS has a number of StorageServers. This scales well to Appleâ€™s largest transactional workloads [28].
</p>
</blockquote>

<blockquote>
<p>
The Sequencer assigns a read version and a commit version to each transaction and, for historical reasons, also recruits Proxies, Resolvers, and LogServers. Proxies offer MVCC read versions to clients and orchestrate transaction commits. Resolvers check for conflicts between transactions. LogServers act as replicated, sharded, distributed persistent queues, where each queue stores WAL data for a StorageServer.
</p>
</blockquote>

<blockquote>
<p>
The SS consists of a number of StorageServers for serving client reads, where each StorageServer stores a set of data shards, i.e., contiguous key ranges. StorageServers are the majority of processes in the system, and together they form a distributed B-tree. Currently, the storage engine on each StorageServer is a modified version of SQLite [41], with enhancements that make range clears faster, defer deletion to a background task, and add support for asynchronous programming.
</p>
</blockquote>

<p>
æ–‡ç« æåˆ°äº†å¯åŠ¨å’Œreconfigurationéƒ¨åˆ†ï¼Œå…¶å®éå¸¸ç±»ä¼¼ã€‚å’Œæˆ‘ä¸Šé¢æƒ³çš„ä¸€æ ·ï¼Œsequencerä¼šç›‘æ§å¯åŠ¨çš„proxiers, resolverså’ŒLSs, å¦‚æœå¤±æ•ˆçš„è¯é‚£ä¹ˆsequencerä¼šé€€å‡ºè§¦å‘é‡æ–°å¯åŠ¨æµç¨‹
</p>

<blockquote>
<p>
Reconfiguration Whenever there is a failure in the TS or LS, or a database configuration change, a reconfiguration process brings the transaction management system to a new configuration, i.e., a clean state. Specifically, the Sequencer process monitors the health of Proxies, Resolvers, and LogServers. If any one of the monitored processes fails or the database configuration changes, the Sequencer process terminates. The ClusterController will detect the Sequencer failure event, then recruit a new Sequencer, which follows the above bootstrapping process to spawn the new TS and LS instance. In this way, transaction processing is divided into epochs, where each epoch represents a generation of the transaction management system with its unique Sequencer process.
</p>
</blockquote>
</div>
</div>
</div>

<div id="outline-container-orgbfb3c20" class="outline-2">
<h2 id="orgbfb3c20"><span class="section-number-2">4.</span> txn management</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-orgeb8fcb1" class="outline-3">
<h3 id="orgeb8fcb1"><span class="section-number-3">4.1.</span> process</h3>
<div class="outline-text-3" id="text-4-1">
<p>
txnæµç¨‹å…¶å®åœ¨ä¸Šé¢é‚£ä¸ªæ¶æ„å›¾å°±æœ‰äº†
</p>

<blockquote>
<p>
End-to-end Transaction Processing As illustrated in Figure 1, a client transaction starts by contacting one of the Proxies to obtain a read version (i.e., a timestamp). The Proxy then asks the Se- quencer for a read version that is guaranteed to be no less than any previously issued transaction commit version, and this read version is sent back to the client. Then the client may issue multiple reads to StorageServers and obtain values at that specific read version. Client writes are buffered locally without contacting the cluster. At commit time, the client sends the transaction data, including the read and write sets (i.e., key ranges), to one of the Proxies and waits for a commit or abort response from the Proxy. If the transaction cannot commit, the client may choose to restart the transaction from the beginning again.
</p>
</blockquote>

<blockquote>
<p>
A Proxy commits a client transaction in three steps. First, the Proxy contacts the Sequencer to obtain a commit version that is larger than any existing read versions or commit versions. The Sequencer chooses the commit version by advancing it at a rate of one million versions per second. Then, the Proxy sends the transac- tion information to range-partitioned Resolvers, which implement FDBâ€™s optimistic concurrency control by checking for read-write conflicts. If all Resolvers return with no conflict, the transaction can proceed to the final commit stage. Otherwise, the Proxy marks the transaction as aborted. Finally, committed transactions are sent to a set of LogServers for persistence. A transaction is consid- ered committed after all designated LogServers have replied to the Proxy, which reports the committed version to the Sequencer (to ensure that later transactionsâ€™ read versions are after this commit) and then replies to the client. At the same time, StorageServers continuously pull mutation logs from LogServers and apply com- mitted updates to disks.
</p>
</blockquote>
</div>
</div>

<div id="outline-container-orgaa8a4a2" class="outline-3">
<h3 id="orgaa8a4a2"><span class="section-number-3">4.2.</span> check conflict</h3>
<div class="outline-text-3" id="text-4-2">
<p>
åœ¨æäº¤çš„æ—¶å€™ï¼Œä»sequencerä¸Šé¢ä¼šæ‹¿åˆ°txn id(LSN), ä»¥åŠ previous LSN(è®ºæ–‡ä¸Šè¯´ç¡®ä¿ä¹‹å‰ä¸å­˜åœ¨gap). è¿™ä¸ªæˆ‘æ²¡æœ‰å¤ªæ˜ç™½æ˜¯ä»€ä¹ˆæ„æ€ã€‚åœ¨txn dataé‡Œé¢ï¼Œæ¯ä¸ªread rangeéƒ½æ˜¯éƒ½ä¸€ä¸ªread versionçš„ï¼Œè¿™ä¸ªä¼šåœ¨resolversé‚£è¾¹è¿›è¡Œæ£€æŸ¥. resolversé‚£è¾¹ä¼šç»´æŠ¤ä¸€ä¸ªlast commitè·³è¡¨å®ç°ï¼Œé‡Œé¢å†…å®¹æ˜¯ranger-&gt;commit versionçš„æ˜ å°„ã€‚ç„¶åæˆ‘ä»¬ä¼šæ£€æŸ¥æ¯ä¸ªread rangeæ˜¯å¦éƒ½å¤§äºè¿™ä¸ªcommit version, å¦åˆ™å°±ä¼šè®¤ä¸ºè¯»å–åˆ°çš„æ˜¯ä¸ä¸€è‡´çš„æ•°æ®è€Œç›´æ¥abort txn.
</p>

<blockquote>
<p>
FDB implements Serializable Snapshot Isolation (SSI) by combining OCC with MVCC. Recall that a transaction ğ‘‡ğ‘¥ gets both its read version and commit version from Sequencer, where the read version is guaranteed to be no less than any committed version when ğ‘‡ğ‘¥ starts and the commit version is larger than any existing read or commit versions. This commit version defines a serial history for transactions and serves as Log Sequence Number (LSN). Because ğ‘‡ğ‘¥ observes the results of all previous committed transactions, FDB achieves strict serializability. To ensure there is no gaps between LSNs, the Sequencer returns the previous commit version (i.e., previous LSN) with commit version. A Proxy sends both LSN and previous LSN to Resolvers and Log- Servers so that they can serially process transactions in the order of LSNs. Similarly, StorageServers pull log data from LogServers in increasing LSNs as well.
</p>

<p>
Algorithm 1 illustrates the lock-free conflict detection algorithm on Resolvers. Specifically, each Resolver maintains a history ğ‘™ğ‘ğ‘ ğ‘¡ğ¶ğ‘œğ‘šğ‘šğ‘–ğ‘¡ of recently modified key ranges by committed trans- actions, and their corresponding commit versions. The commit request for ğ‘‡ğ‘¥ comprises two sets: a set of modified key ranges ğ‘…ğ‘¤ , and a set of read key ranges ğ‘…ğ‘Ÿ , where a single key is converted to a single key range. The read set is checked against the modi- fied key ranges of concurrent committed transactions (line 1â€”5), which prevents phantom reads. If there are no read-write conflicts, Resolvers admit the transaction for commit and update the list of modified key ranges with the write set (line 6â€”7). For snapshot reads, they are not included in the set ğ‘…ğ‘Ÿ . In practice, ğ‘™ğ‘ğ‘ ğ‘¡ğ¶ğ‘œğ‘šğ‘šğ‘–ğ‘¡ is represented as a version-augmented probabilistic SkipList [56].
</p>
</blockquote>



<div id="org0becd8a" class="figure">
<p><img src="../images/Pasted-Image-20240307115515.png" alt="Pasted-Image-20240307115515.png" />
</p>
</div>

<p>
è¿™é‡Œé¢proxyè¿˜éœ€è¦å°†txnåˆ†æ´¾åˆ°ä¸åŒçš„resolversä¸Šé¢å»æ£€æŸ¥ï¼ŒæŒ‰ç…§key rangeè¿›è¡Œåˆ’åˆ†ã€‚æ‰€ä»¥è¿™é‡Œä¼šå‡ºç°ä¸€äº›é—®é¢˜å°±æ˜¯ï¼ŒæŸäº›resolveè®¤ä¸ºOKï¼Œä½†æ˜¯å¦å¤–ä¸€äº›è®¤ä¸ºä¸OKçš„è¯ï¼Œå°±ä¼šå‡ºç°æŸäº›resolverè®¤ä¸ºcommitæˆåŠŸï¼Œrangeä¸Šæ›´æ–°äº†æœ€æ–°çš„commit version. ä¼šå¯¼è‡´æŸäº›false positiveçš„cases. ä½†æ˜¯è®ºæ–‡è®¤ä¸ºè¿™ä¸ªæ²¡æœ‰å…³ç³»ï¼Œå› ä¸ºæ‰€æœ‰çš„txn 5så°±ä¼šè¿‡æœŸï¼Œclientå¯ä»¥é‡å¯å‘èµ·è¯»å–ã€‚è¿˜æœ‰å°±æ˜¯å®é™…ä¸­å¤§éƒ¨åˆ†åªæœ‰ä¸€ä¸ªresolveråœ¨æ£€æŸ¥å†²çªã€‚
</p>

<blockquote>
<p>
The entire key space is divided among Resolvers so that the above read-write conflict detection algorithm may be performed in parallel. A transaction can commit only when all Resolvers admit the transaction. Otherwise, the transaction is aborted. It is possible that an aborted transaction is admitted by a subset of Resolvers, and they have already updated their history of ğ‘™ğ‘ğ‘ ğ‘¡ğ¶ğ‘œğ‘šğ‘šğ‘–ğ‘¡, which may cause other transactions to conflict (i.e., a false positive). In practice, this has not been an issue for our production workloads, because transactionsâ€™ key ranges usually fall into one Resolver. Additionally, because the modified keys expire after the MVCC window, the false positives are limited to only happen within the short MVCC window time (i.e., 5 seconds). Finally, the key ranges of Resolvers are dynamically adjusted to balance their loads.
</p>
</blockquote>

<blockquote>
<p>
The OCC design of FDB avoids the complicated logic of acquiring and releasing (logical) locks, which greatly simplifies interactions between the TS and the SS. The price paid for this simplification is to keep the recent commit history in Resolvers. Another draw- back is not guaranteeing that transactions will commit, a challenge for OCC. Because of the nature of our multi-tenant production workload, the transaction conflict rate is very low (less than 1%) and OCC works well. If a conflict happens, the client can simply restart the transaction.
</p>
</blockquote>
</div>
</div>

<div id="outline-container-org2eaa1c2" class="outline-3">
<h3 id="org2eaa1c2"><span class="section-number-3">4.3.</span> commit</h3>
<div class="outline-text-3" id="text-4-3">
<p>
å¦‚æœäº‹åŠ¡æ£€æŸ¥é€šè¿‡çš„è¯ï¼Œé‚£ä¹ˆproxyå°±å¼€å§‹å¾€LSä¸Šè¿›è¡Œæäº¤ï¼ŒåŒæ ·æ˜¯æŒ‰ç…§key rangesæ¥åšæäº¤ã€‚æäº¤å®Œæˆä¹‹åä¼šæ›´æ–°è‡ªå·±çš„ `know committed version` åˆ°æœ€æ–°çš„LSNä¸Šï¼Œç»´æŠ¤è¿™ä¸ªKCVä¸»è¦æ˜¯ä¸ºäº†æ•…éšœæ¢å¤ä½¿ç”¨çš„ã€‚æäº¤åˆ°LSä¹‹åï¼Œé‚£ä¹ˆå°±å¯ä»¥è®¤ä¸ºcommit pathå®Œæˆäº†ï¼Œæ¥ç€å°±æ˜¯SSå»æ‹‰å–LS. è¿™ä¸ªå»¶è¿Ÿé€šå¸¸åœ¨4mså·¦å³ï¼Œæœ€å¤§å»¶è¿Ÿåœ¨208ms. å¦‚æœç”¨æˆ·è¯»å–ä¸åˆ°æœ€æ–°æ•°æ®ï¼Œå¯ä»¥ç­‰å¾…ä¹Ÿå¯ä»¥æ›´æ¢ss replica.
</p>


<div id="orgd110bca" class="figure">
<p><img src="../images/Pasted-Image-20240307142452.png" alt="Pasted-Image-20240307142452.png" />
</p>
</div>

<blockquote>
<p>
After a Proxy decides to commit a transac- tion, the log message is broadcast to all LogServers. As illustrated in Figure 2, the Proxy first consults its in-memory shard map to determine the StorageServers responsible for the modified key range. Then the Proxy attaches StorageServer tags 1, 4, and 6 to the mutation, where each tag has a preferred LogServer for storage. In this example, tags 1 and 6 have the same preferred LogServer. Note the mutation is only sent to the preferred LogServers (1 and 4) and an additional LogServer 3 to meet the replication re- quirements. All other LogServers receive an empty message body. The log message header includes both LSN and the previous LSN obtained from the Sequencer, as well as the known committed version (KCV) of this Proxy. LogServers reply to the Proxy once the log data is made durable, and the Proxy updates its KCV to the LSN if all replica LogServers have replied and this LSN is larger than the current KCV.
</p>
</blockquote>

<blockquote>
<p>
Shipping the redo log from the LS to the SS is not a part of the commit path and is performed in the background. In FDB, StorageServers aggressively fetch redo logs from LogServers before they are durable on the LS, allowing very low latency for serving multi-version reads. Figure 3 shows the time lag between StorageServers and LogServers in one of our production clusters for a 12-hour period, where the 99.9 percentile of the average and maximum delay is 3.96 ms and 208.6 ms, respectively. Because this lag is small, when client read requests reach StorageServers, the requested version (i.e., the latest committed data) is usually already available. If due to a small delay the data is not available to read at a StorageServer replica, the client either waits for the data to become available or issues a second request to another replica [32]. If both reads timed out, the client gets a retryable error to restart the transaction.
</p>
</blockquote>

<blockquote>
<p>
Because the log data is already durable on LogServers, Storage- Servers can buffer updates in memory and only persist batches of data to disks with a longer delay, thus improving I/O efficiency by coalescing the updates. Aggressively pulling redo logs from LogServers means that semi-committed updates, i.e., operations in transactions that are aborted during recovery (e.g., due to Log- Server failure), need to be rolled back (see Section 2.4.4).
</p>
</blockquote>
</div>
</div>


<div id="outline-container-orgd3d22c2" class="outline-3">
<h3 id="orgd3d22c2"><span class="section-number-3">4.4.</span> recovery</h3>
<div class="outline-text-3" id="text-4-4">
<p>
æ¢å¤è¿‡ç¨‹æ¯”è¾ƒè½»é‡ï¼ŒSequencerä¼šé‡æ–°é€‰æ‹©å‡ºä¸€ç»„log serverså‡ºæ¥ï¼Œè¿™é‡Œæœ‰å¥½å‡ ä¸ªæ¦‚å¿µï¼š**ï¼ˆæœ‰ç‚¹çœ‹çš„åŠæ‡‚ä¸æ‡‚çš„æ„Ÿè§‰ï¼‰**
</p>
<ul class="org-ul">
<li>DV(Durable Version). è¿™ä¸ªè¡¨ç¤ºæ¯ä¸ªLSèŠ‚ç‚¹ä¸Šçœ‹åˆ°çš„æœ€å¤§LSN. è¡¨ç¤ºåœ¨LSä¸Šå­˜å‚¨åˆ°çš„æœ€æ–°æ•°æ®åˆ°å“ªé‡Œäº†ã€‚</li>
<li>RV(recovery version). RVåˆ™æ˜¯æ‰€æœ‰LS groupä¸Š DVçš„æœ€å°å€¼ï¼Œ å¯ä»¥è®¤ä¸ºæ‰€æœ‰LSèŠ‚ç‚¹ä¸Šæœ‰éƒ½æœ‰è‡³å°‘åˆ°RVè¿™ä¸ªç‰ˆæœ¬çš„æ•°æ®ã€‚</li>
<li>PEV(previous epoch version). è¿™ä¸ªæ˜¯æ‰€æœ‰LS groupçš„KCVæœ€å¤§å€¼ï¼Œå¯ä»¥è®¤ä¸ºPEVä¹‹å‰éƒ½å·²ç»å®Œå…¨commitäº†ã€‚</li>
<li>é‚£ä¹ˆä» `[PEV+1, RV]` ä¹‹é—´çš„æ•°æ®éœ€è¦ä»old log serversåˆ°new log servers.</li>
<li>ç„¶åSSä¸¢å¼ƒRVä¹‹åçš„æ•°æ®ï¼Œä»PEVä¹‹åçš„æ•°æ®å¼€å§‹è¿›è¡Œå›æ”¾. <b><b>æˆ‘ç†è§£å¯¹KVç³»ç»Ÿçš„è¯ï¼Œé‡å¤å›æ”¾å‡ æ¬¡æ˜¯æ²¡æœ‰ä»€ä¹ˆé—®é¢˜çš„ã€‚æ‰€ä»¥æ²¡æœ‰å¤ªæ˜ç™½ä¸ºä»€ä¹ˆéœ€è¦ä¸¢å¼ƒRVä¹‹åçš„æ•°æ®</b></b></li>
</ul>

<blockquote>
<p>
he essence of the recovery of old LogServers is to determine the end of redo log, i.e., a Recovery Version (RV). Rolling back undo log is essentially discarding any data after RV in the old LogServers and StorageServers. Figure 4 illustrates how RV is determined by the Sequencer. Recall that a Proxy request to LogServers piggy- backs its KCV, the maximum LSN that this Proxy has committed. Each LogServer keeps the maximum KCV received and a Durable Version (DV), which is the maximum persisted LSN. During a recov- ery, the Sequencer attempts to stop all ğ‘š old LogServers, where each response contains the DV and KCV on that LogServer. As- sume the replication degree for LogServers is ğ‘˜. Once the Se- quencer has received more than ğ‘š âˆ’ ğ‘˜ replies 1, the Sequencer knows the previous epoch has committed transactions up to the maximum of all KCVs, which becomes the previous epochâ€™s end version (PEV). All data before this version has been fully replicated. For current epoch, its start version is ğ‘ƒğ¸ğ‘‰ + 1 and the Sequencer chooses the minimum of all DVs to be the RV. Logs in the range of [ğ‘ƒğ¸ğ‘‰ + 1,ğ‘…ğ‘‰] are copied from previous epochâ€™s LogServers to the current ones, for healing the replication degree in case of LogServer failures. The overhead of copying this range is very small because it only contains a few secondsâ€™ log data.
</p>
</blockquote>

<blockquote>
<p>
When Sequencer accepts new transactions, the first is a spe- cial recovery transaction that informs StorageServers the RV so that they can roll back any data larger than RV. The current FDB storage engine consists of an unversioned SQLite [41] B-tree and in-memory multi-versioned redo log data. Only mutations leaving the MVCC window (i.e., committed data) are written to SQLite. The rollback is simply discarding in-memory multi-versioned data in StorageServers. Then StorageServers pull any data larger than version ğ‘ƒğ¸ğ‘‰ from new LogServers.
</p>
</blockquote>


<div id="orgcd0de6f" class="figure">
<p><img src="../images/Pasted-Image-20240307145339.png" alt="Pasted-Image-20240307145339.png" />
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgd746eeb" class="outline-2">
<h2 id="orgd746eeb"><span class="section-number-2">5.</span> simulation testing</h2>
<div class="outline-text-2" id="text-5">
<p>
è¿™ä¸ªä¸œè¥¿ä¸å¾—äº†ï¼Œåº”è¯¥å°±æ˜¯FDBçš„ç§˜å¯†æ­¦å™¨äº†ï¼Œä½¿ç”¨è¿™ä¸ªæ¡†æ¶å¯ä»¥ä»¥è¾ƒå°çš„æˆæœ¬æ¥æ¨¡æ‹Ÿå‘ç°åˆ†å¸ƒå¼ç³»ç»Ÿä¸­æ—¶åºé—®é¢˜ã€‚
</p>

<p>
ä»£ç ç»“æ„ä¸Šï¼Œå°†IOéƒ¨åˆ†å·²ç»æ‹†åˆ†æˆä¸ºäº†sim ioå’Œreal io. è¿™æ ·å¯ä»¥ä»workloadsé‡Œé¢æ¥ç»™sim ioè¾“å…¥ã€‚å¦å¤–å°±æ˜¯è¿™ä¸ªå®ç°æ¡†æ¶ï¼Œæ˜¯ä¸€ä¸ªC++çš„actoræ¨¡å‹ï¼ŒåŸºäºäº‹ä»¶å“åº”æ¥å®ç°çš„ï¼Œæ‰€ä»¥å¯èƒ½å¯ä»¥æ›´å®¹æ˜“åœ°è¿›è¡Œæ³¨å…¥å’Œæ¨¡æ‹Ÿã€‚
</p>

<blockquote>
<p>
Testing and debugging distributed systems is a challenging and inefficient process. This problem is particularly acute for FDB, which offers a very strong concurrency control contract, any fail- ure of which can produce almost arbitrary corruption in systems layered on top. Accordingly, an ambitious approach to end-to-end testing was adopted from the beginning of FDB â€™s development: the real database software is run, together with randomized synthetic workloads and fault injection, in a deterministic discrete-event sim- ulation. The harsh simulated environment quickly provokes bugs (including but not limited to distributed systems bugs) in the data- base, and determinism guarantees that every bug found this way can be reproduced, diagnosed, and fixed.
</p>
</blockquote>


<div id="org80a152e" class="figure">
<p><img src="../images/Pasted-Image-20240307145858.png" alt="Pasted-Image-20240307145858.png" />
</p>
</div>

<blockquote>
<p>
Deterministic simulator. FDB was built from the ground up to make this testing approach possible. All database code is determin- istic; accordingly multithreaded concurrency is avoided (instead, one database node is deployed per core). Figure 6 illustrates the simulator process of FDB, where all sources of nondeterminism and communication are abstracted, including network, disk, time, and pseudo random number generator. FDB is written in Flow [4], a novel syntactic extension to C++ adding async/await-like concur- rency primitives. Flow provides the Actor programming model [13] that abstracts various actions of the FDB server process into a number of actors that are scheduled by the Flow runtime library. The simulator process is able to spawn multiple FDB servers that communicate with each other through a simulated network in a single discrete-event simulation. The production implementation is a simple shim to the relevant system calls.
</p>
</blockquote>

<blockquote>
<p>
The simulator runs multiple workloads (also written in Flow) that communicate with simulated FDB servers through the simulated network. These workloads include fault injection instructions, mock applications, database configuration changes, and direct internal database functionality invocations. Workloads are composable to exercise various features and are reused to construct comprehensive test cases.
</p>
</blockquote>

<p>
Fault injection å¯ä»¥åœ¨åˆé€‚çš„åœ°æ–¹æ³¨å…¥é”™è¯¯, è¿™é‡Œæ³¨å…¥é”™è¯¯ä¸ä»…é™äºå¤–ç•ŒIOéƒ¨åˆ†ï¼Œè¿˜å¯ä»¥åœ¨æŸäº›è¿”å›å€¼çš„åœ°æ–¹ç›´æ¥è¿”å›é”™è¯¯ï¼Œå°±æ˜¯ä¸‹é¢è¯´çš„"buggification". swarm testingå¯ä»¥ä½¿ç”¨ä¸åŒéšæœºå‚æ•°æ¥ç”Ÿæˆcluster size, configuration, è¿”å›é”™è¯¯ä½ç½®ç­‰ç­‰ã€‚ä¸ºäº†æ£€æŸ¥æ¨¡æ‹Ÿå¯¹äºæŸäº›åˆ†æ”¯çš„æ•ˆæœï¼Œè¿˜å¯ä»¥çœ‹æœ‰å¤šå°‘æ¬¡è·‘åˆ°äº†æŸäº›åˆ†æ”¯ä¸Šã€‚æŒºäº†ä¸èµ·çš„ã€‚
</p>

<blockquote>
<p>
Fault injection. The FDB simulator injects machine, rack, and data-center level fail-stop failures and reboots, a variety of network faults, partitions, and latency problems, disk behavior (e.g. the corruption of unsynchronized writes when machines reboot), and randomizes event times. This variety of fault injection both tests the databaseâ€™s resilience to specific faults and increases the diversity of states in simulation. Fault injection distributions are carefully tuned to avoid driving the system into a small state-space caused by an excessive fault rate.
</p>
</blockquote>

<blockquote>
<p>
FDB itself cooperates with the simulation in making rare states and events more common, through a high-level fault injection tech- nique informally referred to as â€œbuggificationâ€. At many places in its code-base, the simulation is given the opportunity to inject some unusual (but not contract-breaking) behavior such as unnecessarily returning an error from an operation that usually succeeds, inject- ing a delay in an operation that is usually fast, choosing an unusual value for a tuning parameter, etc. This complements fault injection at the network and hardware level. Randomization of tuning pa- rameters also ensures that specific performance tuning values do not accidentally become necessary for correctness.
</p>
</blockquote>

<blockquote>
<p>
Swarm testing [40] is extensively used to maximize the diversity of simulation runs. Each run uses a random cluster size and con- figuration, random workloads, random fault injection parameters, random tuning parameters, and enables and disables a different random subset of buggification points. We have open-sourced the swarm testing framework for FDB [7].
</p>
</blockquote>

<blockquote>
<p>
Conditional coverage macros are used to evaluate and tune the effectiveness of the simulation. For example, a developer concerned that a new piece of code may rarely be invoked with a full buffer can add the line TEST( buffer.is_full() ); and analysis of simulation results will tell them how many distinct simulation runs achieved that condition. If the number is too low, or zero, they can add buggification, workload, or fault injection functionality to ensure that scenario is adequately tested.
</p>
</blockquote>

<p>
è¿è¡Œè¿™äº›æµ‹è¯•å¦‚æœä¸æ˜¯CPUæ“ä½œçš„è¯ï¼Œé‚£ä¹ˆå¯èƒ½å¯ä»¥å¾ˆå¿«ï¼Œå› ä¸ºå¯ä»¥å°†æ—¶é’Ÿç›´æ¥å‰æ‹¨è€Œä¸ç”¨çœŸæ­£è¿›è¡Œç­‰å¾…ã€‚è¿™ä¸ªçœŸçš„éå¸¸æœ‰ç”¨ï¼Œæ—¶é—´è¶ŠçŸ­æ„å‘³ç€å¯ä»¥è¿è¡Œæ›´å¤§è§„æ¨¡çš„éšæœºæµ‹è¯•ã€‚
</p>

<blockquote>
<p>
Latency to bug discovery. Finding bugs quickly is important both so that they are encountered in testing before production, and for engineering productivity (since bugs found immediately in an individual commit can be trivially traced to that commit). Discrete-event simulation can run arbitrarily faster than real-time if CPU utilization within the simulation is low, as the simulator can fast-forward clock to the next event. Many distributed systems bugs take time to play out, and running simulations with long stretches of low utilization allows many more of these to be found per core second than in â€œreal-worldâ€ end-to-end tests.
</p>
</blockquote>

<blockquote>
<p>
Additionally, bugs can be found faster simply by running more simulations in parallel. Randomized testing is embarrassingly par- allel and FDB developers can and do â€œburstâ€ the amount of testing they do immediately before major releases, in the hopes of catching exceptionally rare bugs that have thus far eluded the testing process. Since the search space is effectively infinite, simply running more tests results in more code being covered and more potential bugs being found, in contrast to scripted functional or system testing.
</p>
</blockquote>
</div>
</div>

<div id="outline-container-orga333085" class="outline-2">
<h2 id="orga333085"><span class="section-number-2">6.</span> 5s MVCC Window</h2>
<div class="outline-text-2" id="text-6">
<blockquote>
<p>
FDB chooses a 5-second MVCC window to limit the memory usage of the transaction system and storage servers, because the multi- version data is stored in the memory of Resolvers and Storage- Servers, which in turn restricts transaction sizes. From our experi- ence, this 5s window is long enough for the majority of OLTP use cases. If a transaction exceeds the time limit, it is often the case that the client application is doing something inefficient, e.g., issuing reads one by one instead of parallel reads. As a result, exceeding the time limit often exposes inefficiency in the application.
</p>
</blockquote>

<blockquote>
<p>
For some transactions that may span more than 5s, many can be divided into smaller transactions. For instance, the continuous backup process of FDB will scan through the key space and create snapshots of key ranges. Because of the 5s limit, the scanning process is divided into a number of smaller ranges so that each range can be performed within 5s. In fact, this is a common pattern: one transaction creates a number of jobs and each job can be further divided or executed in a transaction. FDB has implemented such a pattern in an abstraction called TaskBucket and the backup system heavily depends on it.
</p>
</blockquote>
</div>
</div>
</div>
</body>
</html>
