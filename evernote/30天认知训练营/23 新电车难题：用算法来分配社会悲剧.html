<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 9.5.14 (465167)"/><meta name="author" content="章炎(印象)"/><meta name="created" content="2019-04-02 17:49:54 +0000"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2019-04-02 17:53:08 +0000"/><title>23 新电车难题：用算法来分配社会悲剧</title></head><body><h2 style="padding: 5px; border-bottom: 2px solid lightgrey; line-height: 40.5px; color: rgb(102, 102, 102); font-family: Georgia, &quot;Microsoft Yahei&quot;, &quot;WenQuanYi Micro Hei&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><span style="border-bottom: 2px solid lightgrey; line-height: 40.5px; color: rgb(102, 102, 102); font-family: Georgia, &quot;Microsoft Yahei&quot;, &quot;WenQuanYi Micro Hei&quot;; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;">23 新电车难题：用算法来分配社会悲剧
</span></h2><div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">今天我们要讨论的是一个两难问题，面对社会困境，有没有一个社会能够长期接受的选择。
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">先从一个经典的伦理学思想实验，电车难题开始讲起。我把这个问题稍稍做了一些转化，把它放在了人工智能时代。
</span></div><h2 style="padding: 5px; border-bottom: 2px solid lightgrey; line-height: 40.5px; color: rgb(102, 102, 102);"><span style="border-bottom: 2px solid lightgrey; line-height: 40.5px; color: rgb(102, 102, 102);">机器人三定律
</span></h2><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">人工智能时代已经开始，如何与人工智能共处，或者说在我们还有能力的时候，给人工智能定什么规矩，不再是抽象的哲学问题，也不必上升到谁做主宰这类终级追问。它变得极为具体，比如下面这个场景：
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="font-weight: bold;-en-paragraph:true;">行人横穿马路，刹车来不及，如果不转向，会撞死行人；如果转向，乘客会死于翻车。自动驾驶汽车应该作何选择？</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">这个问题已经迫在眉睫。自动驾驶汽车是最接近大规模商用的人工智能应用。无论中国，还是美国，多家公司已经上路实测，不止一家宣布要在一两年内推出自动出租车。
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">有汽车就有事故，有事故就有死伤，由人工智能来断谁该死谁该无恙，它该怎么断？
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;-en-paragraph:true;">很早以前，人工智能、机器人刚刚出现在人类想象之中，人们就已想到要给它们定规。科幻小说大师阿西莫夫提出影响力极大的机器人三定律。
</span></div><ul style="padding-left: 10px;"><li style=""><div><span style="font-weight: bold; background-color: rgb(255, 250, 165);-evernote-highlight:true;">第一定律：机器人不得伤害人类，或者因不作为而使人类受到伤害；</span></div></li><li style=""><div><span style="font-weight: bold; background-color: rgb(255, 250, 165);-evernote-highlight:true;">第二定律：除非违背第一定律，否则机器人必须服从人类的命令；</span></div></li><li style=""><div><span style="font-weight: bold; background-color: rgb(255, 250, 165);-evernote-highlight:true;">第三定律：在不违背第一及第二定律下，机器人必须保护自己。</span></div></li></ul><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">机器人三定律定义严密，层层递进，它能解决自动驾驶汽车的选择困境吗？
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">不能。
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">看第一定律：机器人不得伤害人类，或者因不作为而使人类受到伤害。自动汽车不转向就撞行人，转向则乘客死伤，都会伤害人类，它应该作为还是不作为，还有，哪个算作为哪个算不作为？我觉得机器人想这些问题能想到死机。
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;"><span style="-en-paragraph:true;">面对类似挑战，阿西莫夫后来给机器人三定律打了个补丁，在最前面加上第零定律：</span><span style="font-weight: bold;-en-paragraph:true;">机器人不得伤害人类种族，或者因不作为而使人类种族受到伤害。</span><span style="-en-paragraph:true;"
/></span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">可以把第零定律理解为要机器人的选择与人类的最大整体利益相符。问题是怎么辨别最大整体利益是什么？像金庸小说《笑傲江湖》中神医平一指那样救一人杀一人，算不算？救一个小孩杀一个老人呢？救两人杀一人呢？救两个胖子杀一个瘦子呢？救两个女人杀一个男人呢？
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">无穷无尽的计算，根本没有正解。机器人还不如死机算了。
</span></div><h2 style="padding: 5px; border-bottom: 2px solid lightgrey; line-height: 40.5px; color: rgb(102, 102, 102);"><span style="border-bottom: 2px solid lightgrey; line-height: 40.5px; color: rgb(102, 102, 102);">你想要什么样的道德算法
</span></h2><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">自动驾驶汽车眼前就要上路了，机器人三定律不够用，怎样给它立个什么规矩？换句话说，你想机器人按什么道德算法来运行？
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="font-weight: bold;-en-paragraph:true;">首先，你得了解自己到底想要的是什么。 </span><span style="-en-paragraph:true;">我给你推荐个自测工具，道德机器（Moral Machine），它放在麻省理工大学网站上（<a href="http://moralmachine.mit.edu">http://moralmachine.mit.edu</a>）。点进去，你会遇到13种情境，形形色色的人群组合，老的小的男的女的好的坏的胖的瘦的。面对着自动汽车，假设你是乘客，你希望自动汽车牺牲谁拯救谁？
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">我的推荐我先测，于是知道了我自己的偏好：
</span></div><ul style="padding-left: 10px;"><li style=""><div>孩子重于老人，胖瘦男女对我完全没差别；</div></li><li style=""><div>多个人重于一个人，不论是什么人；</div></li><li style=""><div>人重于动物，遇到撞人还是撞狗，永远选择撞狗；</div></li><li style=""><div>如果转向不转向都撞到同样多人，那就不转向；</div></li><li style=""><div>如果转向的后果是我自己完蛋，那就绝不转向。</div></li></ul><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">推荐你们也去做一遍，一分钟做完，对自己了解得很多。
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">道德机器不是做着玩的，研究者用它来搜集在自动驾驶问题上社会的道德判断。此前已经在顶级期刊《科学》（</span><span style="font-style: italic;-en-paragraph:true;">Science</span><span style="-en-paragraph:true;">)发表论文，标题就叫“自动驾驶的社会困境”（</span><span style="font-style: italic;-en-paragraph:true;">The Social Dilemma of Autonomous Vehicles</span><span style="-en-paragraph:true;">）。
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">读过论文，我发现自己的选择很有代表性。总的来说，我的选择是 </span><span style="color: rgb(232, 115, 31);-en-paragraph:true;">功利主义(utilitarianism)</span><span style="-en-paragraph:true;"> 的。如果一个选择比其他选择更有效用，我就选它。
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">一般来说，对社会而言，救多个人比救一个人更有效用，救人比救动物更有效用。研究者发现，效用主义深入人心，绝大多数人支持用效用主义来给自动汽车编制算法。
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="font-weight: bold;-en-paragraph:true;">更有意思的是，研究里说了，人工智能采用功利主义算法来做决策，本身并不会让人们感到特别不舒服。就是说，在没有自动汽车的时候，我们在开车面临这些情境时的选择是功利主义的，如果这些选择将来由人工智能替我们做了，这件事本身不会那么令人难接受。</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">我们知道人生有许多悲剧，必须有取舍，谁来做都得取舍。取舍就有错，人能够接受机器犯错。问题在于，人是自相矛盾的。
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">以我为例，我认可效用主义算法，但如果自动汽车按这个算法来作选择，我却不想坐更不会买。救多个人优于救一个人，哪怕这个人是乘客，这样做决定的自动汽车，你敢坐吗？你想买吗？
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;"><span style="-en-paragraph:true;">研究者发现，绝大多数人不敢，不想。他们希望买的是那种永远优先保护乘客的自动汽车。也就是说， </span><span style="font-weight: bold;-en-paragraph:true;">绝大多数人都支持自动汽车使用功利主义算法，支持别人买这样的车，但自己不买。这就会造成典型的社会困境。你希望别人做的事，自己不做。结果就是谁也不做，最后这种自动汽车根本就没人买。</span><span style="-en-paragraph:true;"
/></span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">功利主义不行，并不是说换个算法就行。假如换个算法，永远优先保护乘客，你倒是愿意买了，但公众能允许这样赤裸裸地以行人为壑的做法吗？ 
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">与效用主义针锋相对的另一种道德算法是</span><span style="color: rgb(232, 115, 31);-en-paragraph:true;">康德式道德律令</span><span style="-en-paragraph:true;"> ，它认为人是目的不是手段，一个人等于全人类，那更是让人工智能无从抉择。
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">说到这里，对道德哲学有了解的朋友，你应该已经对我开头说的，自动汽车撞谁不撞谁的算法问题，就是古老的电车难题在今天的重现，有了更深的理解。
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">电车难题是这样的： </span><span style="font-weight: bold;-en-paragraph:true;">电车失控，转向要伤人，不转向也要伤人，如果你是司机，该作何选择？百年来各种道德思想流派竞相抢答，没有一个公认的正解。今天无非是我们把司机换成了人工智能。</span><span style="-en-paragraph:true;"
/></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">这道选择题人类给不出正解，人工智能自然也给不出。
</span></div><h2 style="padding: 5px; border-bottom: 2px solid lightgrey; line-height: 40.5px; color: rgb(102, 102, 102);"><span style="border-bottom: 2px solid lightgrey; line-height: 40.5px; color: rgb(102, 102, 102);">摆脱“新电车难题”的困境
</span></h2><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">难道自动汽车就上不了路吗？
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">这倒绝不会。
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="font-weight: bold;-en-paragraph:true;">第一个摆脱困境的思路来自人工智能专家。 </span><span style="-en-paragraph:true;">他们认为既然解决不了这个问题，那就消灭它。
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">谷歌的自动驾驶工程师说，道德算法是假问题。自动汽车能高速处理速度、距离、路况、天气等信息，用激光雷达和各种传感器提前感知，提前计算出最合理方案，使那些难以抉择的危险情境根本就没有机会发生。
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">问它撞一个人还是撞两个人，这个问题它回答不了，但是这问题在它那里不存在。工程师对技术魔法有谜之自信，不管你信不信，反正我是不信。
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="font-weight: bold;-en-paragraph:true;">第二个思路是寻找与困境并存的策略。</span><span style="-en-paragraph:true;">人类古往今来一直在做这件事。
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">电车难题、人工智能的道德算法，本质上都是把悲剧分配给谁的问题。在理论上不存在满足各种公平正义要求的正解，但实践中则随时随地都在分配，一刻也没有因为不够公平而停止过，问题只在于它是如何分配的。</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">这正是卡拉布雷西（Guido Calabresi）名著《悲剧性选择》（</span><span style="font-style: italic;-en-paragraph:true;">Tragic Choices: The conflicts society confronts in the allocation of tragically scarce resources</span><span style="-en-paragraph:true;">） 最有洞察力的地方。
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;"><span style="-en-paragraph:true;">卡拉布雷西与科斯、波斯纳并列法经济学三位创始人之一，做耶鲁法学院院长多年，美国法律界的泰斗级人物。这本书的副题是“社会在分配悲剧性稀缺资源时面临的冲突”，专讲社会怎么分配悲剧： </span><span style="font-weight: bold;-en-paragraph:true;">怎么确定悲剧总量，以什么方式分配给谁。</span><span style="-en-paragraph:true;"
/></span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;-en-paragraph:true;">泰坦尼克号撞冰山，谁先上救生船？计划生育，如何分配生育指标？器官移植，谁优先获得器官？以及今天的新问题：自动汽车撞谁不撞谁？等等等等？
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;"><span style="-en-paragraph:true;">回答这些问题只能是全社会的责任。在《悲剧性选择》中，卡拉布雷西说， </span><span style="-en-paragraph:true;">社会分配悲剧有四个策略：市场、政府、摇号、惯例，但没有哪个能长期维持分配的稳定性。</span></span><span style="-en-paragraph:true;"
/></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="font-weight: bold;-en-paragraph:true;">1.</span><span style="-en-paragraph:true;"> 市场化分配是分散决策，价高者得。
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">但是这种机制，不可避免地将已有的财富不平等，延展到当下分配的不平等，并且注定将人们认为不可定价的东西，比如生命，也贴上价签。
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">没有一个社会能允许用市场化分配一切。
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="font-weight: bold;-en-paragraph:true;">2.</span><span style="-en-paragraph:true;"> 政治分配的好处是较能反映民意，如果政治力量的对比产生自选票的话；但这也使得混乱成为现代社会常态：
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">如果政府直接出面分配悲剧，那就成为社会价值观冲突的替罪羊；民意如潮汐，总在分配谁去承受悲剧的政府，无法长期稳定地获得民意支持。
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="font-weight: bold;-en-paragraph:true;">3.</span><span style="-en-paragraph:true;"> 抽签把一切交给运气，看上去是绝对平等，但抹杀了被社会珍视的另外一些平等观：为什么不先救孩子？为什么不把机会让给那些有巨大贡献的人？
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">而且，抽签撕下了面纱，让悲剧无法避免赤祼祼地摆到社会面前。社会其实不能承受这种真相。
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="font-weight: bold;-en-paragraph:true;">4. </span><span style="-en-paragraph:true;">所以，社会也用惯例、习俗、文化来掩盖对悲剧的分配，比如印度留存至今的种姓制度。可是，这种表面上无分配的分配有其代价：社会假装习以为常，披上虚伪面纱。
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;"><span style="-en-paragraph:true;">总而言之，没有一个单一的办法能够长期地解决问题，抽签、习俗、政府、市场。社会要保有道德自信，就得将悲剧的分配伪装起来。卡拉布雷西认为，</span><span style="font-weight: bold;-en-paragraph:true;">单一策略的效果往往不如混合策略，就是既不是全靠市场也不能全靠政府，或者是抽签或惯例，而是多种分配方式的杂糅。</span><span style="-en-paragraph:true;"
/></span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;-en-paragraph:true;">即便如此，稀缺永恒，每个社会都在做六个杯子五个盖的腾挪，能玩就玩下去，玩不下去了，社会就只好重置游戏。
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;"><span style="-en-paragraph:true;">也就是说，</span><span style="font-weight: bold;-en-paragraph:true;">社会的道德算法必然会周期性切换：因为无法在冲突的价值观中作出取舍，所以社会都保有它们，一个都不能少，但在时间上分开。 </span><span style="-en-paragraph:true;"
/></span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;-en-paragraph:true;">某个时段某个价值观占上风，直到它承载的负能量过多，终被另一个价值观所取代。风水轮流转。这是社会道德算法的跨时多元化策略，为相互冲突但都被珍视的基本价值观留下火种，并缓解冲击。
</span></div><hr/><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">回到自动汽车的话题，自动汽车时代注定很快到来，哪怕不会有一个公认“正确”的道德算法。未来自动汽车里植入的算法是厂商自决、市场选择还是政府规定，都有可能。
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;-en-paragraph:true;">惟一确定的就是不管用哪个算法，都必然将制造属于它特有的那一类悲剧，等到这些悲剧沉重到社会必须切换另一种悲剧来承受时，齿轮转动，算法重置，悲剧的分配重新开始。</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">今天我们讨论的，是人工智能时代下的新电车难题。
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="font-weight: bold;-en-paragraph:true;">无论是怎样的道德算法，本质上讨论的，都是面对社会困境把悲剧分配给谁。理论上不存在满足各种公平正义要求的正解，但实践中我们随时都在分配。</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">推荐给你一本书，卡拉布雷西的名著《悲剧性选择》。
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">今天留一个互动： </span><span style="font-weight: bold;-en-paragraph:true;">如果你是乘客，你愿意让人工智能汽车在面对可能的车祸时，替你作出决定吗？你希望他用哪些道德算法来做决定？ </span><span style="-en-paragraph:true;"
/></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">欢迎你到文稿下方留言区留言。也欢迎你把今天的内容分享给你的朋友，邀请他一起来讨论。
</span></div><div style="margin-top: 1em; margin-bottom: 1em;"><span style="-en-paragraph:true;">明天，我们进入30天认知训练的最后一个模块。我会跟你讨论一个有趣的问题，关于女性，男人什么都不懂。
</span></div></div><div><br/></div></body></html>