
<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="viewport" content="width=device-width, initial-scale=1" />
<title>036 | 机器学习排序算法：列表法排序学习</title>
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="dirtysalt" />

<style type="text/css">html {
    font-family: Georgia, "Microsoft Yahei", "WenQuanYi Micro Hei";
}

/* pre { */
/*     background-color: #eee; */
/*     box-shadow: 5px 5px 5px #888; */
/*     border: none; */
/*     padding: 5pt; */
/*     margin-bottom: 14pt; */
/*     color: black; */
/*     padding: 12pt; */
/*     font-family: Consolas; */
/*     font-size: 95%; */
/*     overflow: auto; */
/* } */

.title  { /* text-align: center; */
          margin-bottom: 1em; }
.subtitle { /* text-align: center; */
            font-size: medium;
            font-weight: bold;
            margin-top:0; }
.todo   { font-family: monospace; color: red; }
.done   { font-family: monospace; color: green; }
.priority { font-family: monospace; color: orange; }
.tag    { background-color: #eee; font-family: monospace;
          padding: 2px; font-size: 80%; font-weight: normal; }
.timestamp { color: #bebebe; }
.timestamp-kwd { color: #5f9ea0; }
.org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
.org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
.org-center { margin-left: auto; margin-right: auto; text-align: center; }
.org-ul { padding-left: 10px; }
.org-ol { padding-left: 20px; }
ul { padding-left: 10px; }
ol { padding-left: 20px; }

.underline { text-decoration: underline; }
#postamble p, #preamble p { font-size: 90%; margin: .2em; }
p.verse { margin-left: 3%; }
pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
}
pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
}
pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
}
pre.src:hover:before { display: inline;}
pre.src-sh:before    { content: 'sh'; }
pre.src-bash:before  { content: 'sh'; }
pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
pre.src-R:before     { content: 'R'; }
pre.src-perl:before  { content: 'Perl'; }
pre.src-java:before  { content: 'Java'; }
pre.src-sql:before   { content: 'SQL'; }

table { border-collapse:collapse; }
caption.t-above { caption-side: top; }
caption.t-bottom { caption-side: bottom; }
td, th { vertical-align:top;  }
th.org-right  { text-align: center;  }
th.org-left   { text-align: center;   }
th.org-center { text-align: center; }
td.org-right  { text-align: right;  }
td.org-left   { text-align: left;   }
td.org-center { text-align: center; }
dt { font-weight: bold; }
.footpara { display: inline; }
.footdef  { margin-bottom: 1em; }
.figure { padding: 1em; }
.figure p { /* text-align: center; */ }
.inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
}
#org-div-home-and-up
{ text-align: right; font-size: 70%; white-space: nowrap; }
textarea { overflow-x: auto; }
.linenr { font-size: smaller }
.code-highlighted { background-color: #ffff00; }
.org-info-js_info-navigation { border-style: none; }
#org-info-js_console-label
{ font-size: 10px; font-weight: bold; white-space: nowrap; }
.org-info-js_search-highlight
{ background-color: #ffff00; color: #000000; font-weight: bold; }

/* http://www.yinwang.org/main.css */

body {
    /* font-family:"lucida grande", "lucida sans unicode", lucida, helvetica, "Hiragino Sans GB", "Microsoft YaHei", "WenQuanYi Micro Hei", sans-serif; */
    font-size: 18px;
    margin: 5% 5% 5% 5%;
    padding: 2% 5% 5% 5%;
    width: 80%;
    line-height: 150%;
    border: 1px solid LightGrey;
}

H1 {
    /* font-family: "Palatino Linotype", "Book Antiqua", Palatino, Helvetica, STKaiti, SimSun, serif; */
}

H2 {
    /* font-family: "Palatino Linotype", "Book Antiqua", Palatino, Helvetica, STKaiti, SimSun, serif; */
    margin-bottom: 60px;
    margin-bottom: 40px;
    padding: 5px;
    border-bottom: 2px LightGrey solid;
    width: 98%;
    line-height: 150%;
    color: #666666;
}


H3 {
    /* font-family: "Palatino Linotype", "Book Antiqua", Palatino, Helvetica, STKaiti, SimSun, serif; */
    margin-top: 40px;
    margin-bottom: 30px;
    border-bottom: 1px LightGrey solid;
    width: 98%;
    line-height: 150%;
    color: #666666;
}


H4 {
    /* font-family: "Palatino Linotype", "Book Antiqua", Palatino, Helvetica, STKaiti, SimSun, serif; */
    margin-top: 40px;
    margin-bottom: 30px;
    border-bottom: 1px LightGrey solid;
    width: 98%;
    line-height: 150%;
    color: #666666;
}


li {
    margin-left: 10px;
}


blockquote {
    border-left: 4px lightgrey solid;
    padding-left: 5px;
    margin-left: 20px;
}


pre {
    font-family: Inconsolata, Consolas, "DEJA VU SANS MONO", "DROID SANS MONO", Proggy, monospace;
    font-size: 75%;
    border: solid 1px lightgrey;
    background-color: Ivory;
    padding: 5px;
    line-height: 130%;
    margin-left: 10px;
    width: 95%;
}


code {
    font-family: Inconsolata, Consolas, "DEJA VU SANS MONO", "DROID SANS MONO", Proggy, monospace;
    font-size: 90%;
}


a {
    text-decoration: none;
    # cursor: crosshair;
    border-bottom: 1px dashed Red;
    padding: 1px;
    # color: black;
}


a:hover {
	background-color: LightGrey;
}


img {
    box-shadow: 0 0 10px #555;
    border-radius: 6px;
    margin-left: auto;
    margin-right: auto;
    margin-top: 10px;
    margin-bottom: 10px;
    -webkit-box-shadow: 0 0 10px #555;
    width: 100%;
    max-width: 600px;
}

img.displayed {
    display: block;
    margin-left: auto;
    margin-right: auto;
}

#table-of-contents {
    border-bottom: 2px LightGrey solid;
}</style>

</head>

<body>

<div class="outline-2">
<h2>036 | 机器学习排序算法：列表法排序学习</h2>
<div class="outline-text-2">
<p>本周我们已经分别讨论了最基本的单点法排序学习（Pointwise Learning to Rank）和配对法排序学习（Pairwise Learning to Rank）两种思路。单点法排序学习思路简单实用，目的就是把经典的信息检索问题转化成机器学习问题。配对法排序学习则是把排序的问题转化成针对某个查询关键字每两个文档之间的相对相关性的建模问题。不过，这两种思路也都有很明显的问题，需要进一步对算法进行优化，以实现我们需要的最终目标。</p>
<p>今天我就来讲直接优化排序问题的“终极方法”：列表法排序学习（Listwise Learning to Rank）  。相对于尝试学习每一个样本是否相关或者两个文档的相对比较关系，列表法排序学习的基本思路是尝试直接优化像NDCG（Normalized Discounted Cumulative Gain）这样的指标，从而能够学习到最佳排序结果。</p>
<h2>列表法排序学习的历史</h2>
<p>2000年后，学术界和工业界都开始研究如何用机器学习来解决最优排序问题，五六年之后，研究者们才开始尝试直接优化整个排序列表。</p>
<p>这方面的研究工作很多都来自微软研究院。比如2007年左右的AdaRank，就来自微软亚洲研究院的徐君和李航。这篇论文算是较早提出列表法排序观点的研究工作。同一年在国际机器学习大会ICML 2007（International Conference on Machine Learning）上发表的ListNet算是从理论上开启了列表法的大门。这篇论文也来自微软亚洲研究院，是刘铁岩等人的重要工作。类似的研究工作在这一年里如雨后春笋般涌现。</p>
<p>另外一个方向，接下来我会提到，LambdaRank出现稍早，而LambdaMART则稍微晚一点。这方面的工作是在微软西雅图的研究院开发的。主导人是克里斯托弗·博格斯（Christopher J.C. Burges）。博格斯2016年退休，在微软工作了16年，可以说，他领导的团队发明了微软的搜索引擎Bing的算法。</p>
<h2>列表法排序学习详解</h2>
<p>列表法排序学习有两种基本思路。<strong>第一种，就是直接针对NDCG这样的指标进行优化</strong>。目的简单明了，用什么做衡量标准，就优化什么目标。<strong>第二种，则是根据一个已经知道的最优排序，尝试重建这个顺序，然后来衡量这中间的差异</strong>。</p>
<p>我先来说一下第一大思路，直接针对NDCG这样的指标进行优化。</p>
<p>首先，重温一下排序测试集的测试原理。总体来说，所有的基于排序的指标都要考察测试集上，对于某一个查询关键字来说，某一组文档所组成的排序是否是最优的。有两种比较通用的做法。第一个方法主要适用于二分的相关信息，对于某一个查询关键字，针对排序产生的“顶部的K”个文档进行评估，查看精度（Precision）、召回（Recall）等。第二种方法，利用五级相关信息定义，在这样的情况下，就可以利用类似于NDCG这样的评价指标。具体解读你可以回到本周前面两期我们讲解过的内容进行复习。</p>
<p>那么，<strong>直接优化排序指标的难点和核心在什么地方呢？</strong></p>
<!-- [[[read_end]]] -->
<p>难点在于，希望能够优化NDCG指标这样的“理想”很美好，但是现实却很残酷。NDCG以及我之前说过的基于“顶部的K”的精度，都是在数学的形式上的“非连续”（Non-Continuous ）和“非可微分”（Non-Differentiable）。而绝大多数的优化算法都是基于“连续”（Continuous ）和“可微分” （Differentiable）函数的。因此，直接优化难度比较大。</p>
<p>针对这种情况，主要有这么几种方法。</p>
<p><strong>第一种方法是，既然直接优化有难度，那就找一个近似NDCG的另外一种指标</strong>。而这种替代的指标是“连续”和“可微分”的 。只要我们建立这个替代指标和NDCG之间的近似关系，那么就能够通过优化这个替代指标达到逼近优化NDCG的目的。这类的代表性算法的有SoftRank和AppRank。</p>
<p><strong>第二种方法是，尝试从数学的形式上写出一个NDCG等指标的“边界”</strong>（Bound），然后优化这个边界。比如，如果推导出一个上界，那就可以通过最小化这个上界来优化NDCG。这类的代表性算法有SVM-MAP和SVM-NDCG。</p>
<p><strong>第三种方法则是，希望从优化算法上下手，看是否能够设计出复杂的优化算法来达到优化NDCG等指标的目的</strong>。对于这类算法来说，算法要求的目标函数可以是“非连续”和“非可微分”的。这类的代表性算法有AdaRank和RankGP。</p>
<p>说完了第一大思路后，我们再来看看第二大思路。这种思路的主要假设是，已经知道了针对某个搜索关键字的完美排序，那么怎么通过学习算法来逼近这个完美排序。<strong>我们希望缩小预测排序和完美排序之间的差距</strong>。值得注意的是，在这种思路的讨论中，优化NDCG等排序的指标并不是主要目的。这里面的代表有ListNet 和ListMLE。</p>
<p>讲了这两大思路以后，最后我再来提一下第三类思路。<strong>这类思路的特点是在纯列表法和配对法之间寻求一种中间解法</strong>。具体来说，这类思路的核心思想，是从NDCG等指标中受到启发，设计出一种替代的目标函数。这一步还和我刚才介绍的第一大思路中的第一个方向有异曲同工之妙，都是希望能够找到替代品。</p>
<p>这第三类思路更进一步的则是<strong>找到替代品以后，把直接优化列表的想法退化成优化某种配对</strong>。这第二步就更进一步简化了问题。这个方向的代表方法就是微软发明的LambdaRank以及后来的LambdaMART。微软发明的这个系列算法成了微软的搜索引擎Bing的核心算法之一。</p>
<p>我这里简单提一下LambdaRank这个系列模型的基本思想。</p>
<p>首先，微软的学者们注意到，一个排序算法是否达到最优的情况，简单来看，就是查看当前的排序中，相比于最优的情况，有哪些两两文档的关系搞错了。<strong>学习最优排序的问题就被转化成了减小这些两两排错的关系</strong>。更进一步，在设计这个优化过程中，我们其实并不需要知道真正的目标函数的形式，而仅仅需要某种形式的梯度（Gradient）。</p>
<p>这里有这样一个洞察，对于绝大多数的优化过程来说，目标函数很多时候仅仅是为了推导梯度而存在的。而如果我们直接就得到了梯度，那自然就不需要目标函数了。最后，通过实验，微软的学者们把这个NDCG通过梯度变化的差值再乘以这个梯度，这样就达到了增强效果的目的。</p>
<p>早期的LambdaRank，特别是RankNet是采用了神经网络来进行模型训练，而LambdaMART则采用了“集成决策树”的思想，更换到了基于决策树的方法。<strong>后来实践证明，基于决策树的方法对于排序问题非常有效果，也就成了很多类似方法的标准配置</strong>。</p>
<p>最后，有一点需要你注意，我们讨论了不同的列表法思路，列表法从理论上和研究情况来看，都是比较理想的排序学习方法。因为列表法尝试统一排序学习的测试指标和学习目标。尽管在学术研究中，纯列表法表现优异，但是<strong>在实际中，类似于LambdaRank这类思路，也就是基于配对法和列表法之间的混合方法更受欢迎</strong>。因为从总体上看，列表法的运算复杂度都比较高，而在工业级的实际应用中，真正的优势并不是特别大，因此列表法的主要贡献目前还多是学术价值。</p>
<h2>小结</h2>
<p>今天我为你讲了列表法排序学习。你可以看到，列表法排序有很多种思路，在2000年到2010年之间是一个非常活跃的研究领域，积累了大量的成果。</p>
<p>一起来回顾下要点：第一，简要介绍了列表法排序学习的历史。第二，详细介绍了列表法排序学习的三大思路以及每个思路里的主要细节和方法。</p>
<p>最后，给你留一个思考题，列表法是不是就完全解决了排序算法的问题呢？</p>
<p>欢迎你给我留言，和我一起讨论。</p>
<p><strong>参考文献</strong></p>
<ol>
<li>
<p>Jun Xu and Hang Li. AdaRank: a boosting algorithm for information retrieval. <em>Proceedings of the 30th annual international ACM SIGIR conference on research and development in information retrieval</em>, 391-398，2007.</p>
</li>
<li>
<p>Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, Hang Li. Learning to rank: from pairwise approach to listwise approach. <em>ICML</em>, 129-136, 2017.</p>
</li>
<li>
<p>Q. Wu, C.J.C. Burges, K. Svore and J. Gao. Adapting boosting for information retrieval measures. <em>Journal of Information Retrieval</em>, 2007.</p>
</li>
<li>
<p>C.J.C. Burges, R. Ragno and Q.V. Le. Learning to rank with non-smooth cost functions. <em>Advances in Neural Information Processing Systems</em>, 2006.</p>
</li>
<li>
<p>C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton and G. Hullender. Learning to rank using gradient descent. <em>Proceedings of the twenty second international conference on machine learning</em>, 2005.</p>
</li>
<li>
<p>F. Xia, T.-Y. Liu, J. Wang, W. Zhang, and H. Li. Listwise approach to learning to rank — Theorem and algorithm. <em>ICML</em>, 1192–1199, 2008.</p>
</li>
<li>
<p>S. Chakrabarti, R. Khanna, U. Sawant, and C. Bhattacharyya. Structured learning for non-smooth ranking losses. <em>SIGKDD</em>, 88–96, 2008.</p>
</li>
<li>
<p>T. Qin, T.-Y. Liu, and H. Li. A general approximation framework for direct optimization of information retrieval measures.<em>Technical Report, Microsoft Research</em>, MSR-TR-2008-164, 2008.</p>
</li>
<li>
<p>M. Taylor, J. Guiver, S. Robertson, and T. Minka. SoftRank: Optimising non-smooth rank metrics. <em>WSDM</em>, 77–86, 2008.</p>
</li>
<li>
<p>J.-Y. Yeh and J.-Y. Lin, and etc. Learning to rank for information retrieval using genetic programming.  <em>SIGIR 2007 Workshop in Learning to Rank for Information Retrieval</em>, 2007.</p>
</li>
<li>
<p>Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A support vector method for optimizing average precision. <em>SIGIR</em>, 271–278, 2007.</p>
</li>
</ol>
<p></p>

</div>
</div>

</body>
</html>