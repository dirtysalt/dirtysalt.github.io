#+title: How to build your own AlphaZero AI using Python and Keras

原文链接 [[https://medium.com/applied-data-science/how-to-build-your-own-alphazero-ai-using-python-and-keras-7f664945c188]]. 文章里面还给出了另外两个链接：
- http://tim.hibal.org/blog/alpha-zero-how-and-why-it-works/ MCTS是如何工作的
- https://medium.com/applied-data-science/alphago-zero-explained-in-one-diagram-365f5abf67e0 (alphago zero cheat sheet)

从 alphago zero cheat sheet 这个图里面了解到，AlphaGo Zero分为三个部分，并且这几个部分是并行执行的
1. 自我对弈 self play. 对弈结果会将比赛结果用于训练神经网络
2. 重新训练网络 retrain network. 关于这个网络的cost function以及network architecture在图片和文章里面都提到了。
3. 评估神经网络。新旧网络进行对弈，胜率必须超过55%才认为新网络比旧的好。

这个network同时输出value(价值)和policy(策略). 其中value表示当前状态的好坏，在cost function里面和Z(win or loss)做mean square error. 而policy则是根据当前状态判断下一步的概率分布如何，在cost function和MCTS的策略做cross entropy.这个神经网络的初始化应该是个问题，如果使用alphago训练出来的网络参数初始化的话，可以显著加快训练速度，不过也有可能无法显著好于alphago的风险。

我fork了文章里面的 [[https://github.com/dirtysalt/DeepReinforcementLearning][项目]]， 并且添加了一些注释。为了调试方便，可以将 `config.py` 里面的一些值修改小比如 `EPISODES` 和 `MCTS_SIMS`
