#+title: On the future of neural networks

[[http://neuralnetworksanddeeplearning.com/chap6.html#on_the_future_of_neural_networks]]

Addressing these questions comprehensively would take a separate book. Instead, let me offer one observation. It's based on an idea known as [[http://en.wikipedia.org/wiki/Conway%27s_law][Conway's law]]:

#+BEGIN_QUOTE
Any organization that designs a system... will inevitably produce a design whose structure is a copy of the organization's communication structure.
#+END_QUOTE

This is a common pattern that has been repeated in many well-established sciences: not just medicine, but physics, mathematics, chemistry, and others. The fields start out monolithic, with just a few deep ideas. Early experts can master all those ideas. But as time passes that monolithic character changes. We discover many deep new ideas, too many for any one person to really master. As a result, the social structure of the field re-organizes and divides around those ideas. Instead of a monolith, we have fields within fields within fields, a complex, recursive, self-referential social structure, whose organization mirrors the connections between our deepest insights. And so the structure of our knowledge shapes the social organization of science. But that social shape in turn constrains and helps determine what we can discover. This is the scientific analogue of Conway's law.

My frank opinion is this: it's too early to say. As the old joke goes, if you ask a scientist how far away some discovery is and they say "10 years" (or more), what they mean is "I've got no idea". AI, like controlled fusion and a few other technologies, has been 10 years away for 60 plus years.
