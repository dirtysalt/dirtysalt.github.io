#+title: 语言检测和编码检测

今天想到一个问题，网络抓取的时候经常会抓到很多乱码数据。通常来说这些乱码数据能只是因为错误的编码方式才呈现乱码，如果使用另外的编码方式或许就会得到有意义的文本。是否可以自动地进行编码检测呢？

我觉得可以从语言检测这个问题入手。对于一段乱码或者是无意义的文本，语言检测可能没有办法得到某种显著的结果，比如概率分布或许是(en=0.3, pt=0.3, zh=0.3)这样的。而如果是有意义的文本的话，通常可以很容易地被识别出某种语言，比如概率分布或许是(zh=0.95, en=0.03).

阅读了一下python langdetect https://github.com/Mimino666/langdetect 的代码，大致思路是这样的：
1. 首先做各种语言的原始文本做1,2,3-gram 可以计算出各种words的出现次数
2. 将各种语言的words出现次数汇总，形成一个map<string, vector>. 其中string是word, vector是每种语言对应的概率。这个概率只需要在语言内部归一化即可，不用跨语言的归一化。
3. 对输入文本进行清洗，比如针对url, mail过滤掉，将空格合并等操作。
4. 对输入文件切分按照1,2,3-gram进行切分，这样同样可以得到许多words。
5. 将这些words多次地随机地在map中去查找，将prob相乘，这样可以得到最终每种语言的最终概率。

如果按照这种思路来做乱码检测和纠正的话，可以这样完成。
1. 枚举所有可能的编码方式。网络上常用的编码方式不多，UTF-8, GBK, ISO-8859-1, latin-1 这些是常用的
2. 针对每种编码方式解码，将解码后的文本应用langdetect. 如果某个语言的概率显著的高比如>90%的话，那么基本可以认为是这种编码方式。
