#+title: 机器学习系统设计(Building Machine Learning Systems with Python)

-----
然而根据亲身经验，我们知道做这些很"酷"的事--使用和调整机器学习算法比如SVM，NNS，或者同时支持两者--其实只需要耗费一位优秀机器学习专家的一点时间。看看下面这个典型的工作流程，你就会发现绝大部分时间花费在一些相当平凡的任务上：1）读取和清洗数据；2）探索和理解输入数据；3）分析如何最好地讲数据呈现给学习算法；4）选择正确的模型和学习算法；5）正确地评估性能。

你通常不会直接将数据输入机器学习算法，而是在训练前对部分数据进行提炼。很多时候，使用机器学习算法会让你得到性能提升的回报。一个简单算法在提炼后数据上的表现，甚至能够超过一个非常复杂的算法在原始数据上的效果。这部分机器学习流程叫做特征工程(feature engineering)，通常是一个非常令人兴奋的挑战。你有创意和智慧，便会立即看到效果。

好特征的目标是在重要的地方取不同值，而在不重要的地方不变。一个很自然就会想到的问题式，我们能否自动滴把好特征选取出来。这个问题叫做特征选择(feature selection). 人们已经提出了很多方法来解决这个问题，但是在实践中，极简单的想法可能已经可以做得很好。

要提升效果，我们基本上有如下选择：1）增加更多的数据[learning_curve]；2）考虑模型复杂度[cross_validation and validation_curve]；3）修改特征空间；4）改变模型。

-----
逻辑回归中的逻辑函数引入是这样的：
- 线性回归的回归函数式 y = w * x
- 逻辑回归中我们使用 log(p / (1-p) 来代替 y.
- 逻辑函数h(x) = p = 1 / (1 + e^{-w * x})

-----
朴素贝叶斯分类器要求所有特征之间相互独立。虽然在实际应用中很少有这种情况，但是在实践中它仍然能够达到非常好的正确率。
- 我们要求解在已知特征F1,F2情况下样本属于某类别C的概率P(C|F1,F2). # 后验概率
- 根据贝叶斯公式P(C|F1,F2) * P(F1,F2) = P(F1,F2|C) * P(C). # P(C)先验概率(prior) P(F1,F2|C)似然性(likehood)
- 预测时因为P(F1,F2)都一样所以我们有时可以不用计算。
- P(F1,F2|C) = P(F1|C) * P(F2|C) 这是因为F1,F2两个特征相互独立。
- 实际过程中可能P(F1,F2) = 0. 那么可以通过加法平滑或是拉普拉斯平滑(laplacian smoothing), 又或是Lidstone平滑来处理。
- 又因为在实际计算时多个p1 * p2...会出现精度问题，所以可以转为log(p1) + log(p2)...来处理。

-----
回归惩罚函数
- Ordinary Least Squares(OLS) 普通最小二乘法，普通线性回归
- L1惩罚(L1范数, L1 norm)则是在OLS上增加a * |w|. Lasso法
- L2惩罚(L2范数, L2 norm)则是在OLS上则加a * |w|^2. Ridge regressin(岭回归)
- L1 + L2则是在OLS上增加a * |w| + b * |w|^2. Elastic Net(弹性网)

-----
整个购物篮分析领域有时又叫做关联规则挖掘(association rule mining). 这些规则式：如果一个顾客购买了X的话，相对于基线，那么他更有可能购买Y。有一个指标来衡量每个规则的价值，称为提升度。提升度就是规则和基线所得到的概率之间的比值：life(X->Y) = P(Y|X) / P(Y). 其中P(Y|X)就是规则对应的概率，而P(Y)则是基线。Apriori是这方面问题的经典算法。

-----
下面这些理由会告诉你为什么在实践中应该尽可能消减维度：
- 多余的特诊会影响或误导学习器。并不是所有机器学习方法都会有这种情况（SVM), 但是大多数模型在维度较小的情况下会比较安全。
- 另一个反对高维特征空间的理由是，更多特征意味着更多参数需要调整，过你喝的风险也越大。
- 我们用来解决问题的数据的维度可能只是虚高，真实维度可能比较小。
- 维度越少意味着训练越快，更多东西可以尝试，能够得到更好的结果。
- 如果我们想要可视化数据，就必须限制在两个或者三个维度上。
