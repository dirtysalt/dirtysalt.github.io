# Chapter 1: 从Transformer到自由探索：AI研究的困境与出路

故事的主人公，一位曾参与发明Transformer的先驱，如今却站在了AI研究的十字路口。他坦言，尽管Transformer取得了巨大成功，但这个领域已变得过度饱和。他深知，没有人比他或Transformer其他六位共同作者更长时间地投入其中。因此，他做出了一个大胆的决定：大幅减少对Transformer的研究，转而投向更广阔的探索，寻求真正的创新。他渴望“增加研究中的探索量”，去尝试一些真正不同的东西。

带着这份探索精神，他推出了“连续思维机器”（Continuous Thought Machine），这项创新成果将在2025年的NeurIPS大会上亮相。这款模型拥有原生的自适应计算能力，以一种全新的方式构建循环网络，利用更高层次的神经元概念和同步表示，旨在以更接近人类思维的方式解决问题，其灵感源于生物学和自然界。

回首Transformer诞生之初，AI研究的氛围截然不同。那时，研究是自下而上的，没有宏大的自上而下规划。一群研究者在午餐时随意交流，思考当前问题，并拥有数月甚至更长时间的自由去尝试新想法，最终催生了Transformer这样的革命性架构。然而，如今的AI研究环境却缺乏这种自由，这让他感到担忧。他曾提出通过大规模扩展进化搜索算法来寻找新突破，尽管投入数亿美元的计算资源，但由于当时业界对单一技术的狂热追捧，他的提议无人问津。这促使他创立了自己的公司——Sakana，以追求这些被忽视的方向。

在Sakana公司内部，他深受肯尼斯·斯坦利《伟大无法规划》一书的启发，坚信研究者应“不受目标和委员会束缚，追随自己的兴趣梯度”。他将此视为“认知觅食”，认为过多的议程只会导致“灰色黏液”，扼杀新颖性和多样性。作为公司创始人之一，他的核心职责便是保护研究人员的自由。他深知，随着公司发展，商业压力会不可避免地侵蚀这种自由，但他希望能尽可能长久地维持这份特权。

他观察到，尽管AI行业涌入了前所未有的资金和人才，但这反而加剧了竞争压力，迫使人们追求短期回报和产品产出，从而削弱了研究的自主性。他将这种现象比作“技术捕获”，类似于“受众捕获”，担心Transformer的巨大成功可能将整个领域困在一个“局部最优”中，难以跳脱。

他回忆起Transformer之前的RNN时代，当时RNN在序列到序列学习上取得了巨大突破，翻译质量大幅提升，业界也曾一度认为RNN是终极技术，只需不断完善。无数研究者投入到RNN的细微改进中，如LSTM、GRU的变体，每次的性能提升都微乎其微，但足以发表论文。然而，当Transformer出现后，他的团队将深度Transformer模型应用于语言建模，立即取得了远超RNN的性能，使得之前所有对RNN的精妙研究瞬间变得“完全多余”，仿佛“浪费了时间”。

他担忧，如今的AI研究正重蹈覆辙，大量论文只是在Transformer架构上进行细枝末节的调整，这可能同样是在“浪费时间”。他坚信Transformer并非最终架构，未来必将出现新的突破，届时，现在看似重要的工作可能再次变得无关紧要。他认为，我们正成为自身成功的“受害者”，陷入一个“吸引力盆地”，这是一种“架构彩票”效应。

他不同意“多样化技能正在消亡”的观点，而是认为有大量才华横溢、富有创造力的研究者，但他们缺乏施展才华的自由。学术界的发表压力和企业界的商业目标，使得研究者倾向于选择“安全”而非“大胆”的路径。即使有更好的新架构出现，例如一些研究已证明比Transformer更优的模型，但由于整个行业已深度绑定Transformer生态系统——包括熟悉的训练方法、内部机制、微调流程以及完善的软件工具链——迁移成本巨大，使得这些更优的架构难以被广泛采纳。

# Chapter 2: Transformer的引力与AI的未来

在AI研究的广阔天地中，Transformer架构无疑是当下的霸主。它以其卓越的性能，让许多人坚信，只要投入更多的计算资源和数据，便能“条条大路通罗马”，解决一切难题。这种“通用表征”的哲学，认为Transformer所捕捉的模式与大脑中的通用模式不谋而合，从而导致了一种观念：既然规模和算力就能解决问题，何必另辟蹊径？

然而，对话揭示了一个令人深思的困境：尽管研究领域中已涌现出一些在特定任务上表现优于Transformer的架构，但它们却难以撼动Transformer的统治地位。原因何在？并非它们不够好，而是它们不够“碾压式地好”。整个行业对Transformer的投入已是天文数字——从熟悉度、训练方法、微调技巧，到庞大的软件生态系统，一切都围绕着它构建。要让这个庞大的机器转向，仅仅“更好”是远远不够的，必须是“明显碾压式地更好”。

回溯历史，Transformer之所以能取代RNN，正是因为它带来了训练速度和准确率的巨大飞跃，让人无法忽视。深度学习革命亦是如此，它以无可辩驳的优势击败了符号主义的怀疑论者。但如今，这种“碾压式”的门槛反而成了创新的阻碍。每当有新的、更精巧的架构出现，OpenAI等巨头只需将Transformer模型扩大十倍，便能再次超越，使得“Transformer已经足够好”的引力将研究者们一次次拉回原点。

更深层次的问题在于，当前的语言模型可能存在“捷径学习”的缺陷，以及一种“海市蜃楼”般的假象。我们正在“异化”Transformer架构，通过打补丁的方式强行添加自适应计算和不确定性量化等功能，而非从根本上设计出能内在实现这些特性的架构。这导致了AI的“锯齿状智能”——它能解决博士级别的复杂问题，却又在下一秒犯下显而易见的低级错误。这并非偶然，而是当前架构可能存在根本性缺陷的体现。

一个生动的例子是“智能矩阵指数化”论文中对“螺旋数据集”的分类。经典的ReLU多层感知机和tanh多层感知机都能准确分类螺旋上的所有点，技术上“解决”了问题。但它们的决策边界是无数细小的分段线性区域，通过“蛮力”拟合出螺旋形状。而该论文提出的M层架构，其决策边界本身就是一条螺旋！它以螺旋的方式来表示螺旋数据。这引发了一个核心问题：如果数据是螺旋，我们难道不应该以螺旋的方式来表示它吗？这种“螺旋式”的表示不仅能正确分类，还能正确地“外推”螺旋的延续。

这揭示了当前AI的“冒名顶替者”困境：它们只是“描摹”了模式，而非抽象地、建设性地理解了模式。就像视频生成模型早期在处理人手时，常常出现手指数量错误的问题。虽然通过增加数据和算力，现在它们能画出五根手指，但这究竟是真正理解了“手”的概念，还是仅仅通过“蛮力”记忆了五根手指的像素模式？这种“规模化”的成功，反而让人们轻易地将这些根本性问题“扫到地毯下”。

正是在这样的背景下，一种名为“连续思维机器”（Continuous Thought Machines, CTM）的新架构应运而生。它并非是对现有技术的彻底颠覆，而是一种“简单而受生物启发”的理念——神经元同步。CTM的研发过程也颇具启发性：团队在没有被“抢先”的压力下，得以从容地进行科学研究，打磨论文，最终获得了欧洲的聚光灯奖。这证明了鼓励研究者承担风险，探索那些看似投机、长期的想法是多么重要。CTM正是为了更直接地解决自适应计算和不确定性量化等内在需求而设计。Luke作为该项目的主要研究员之一，介绍了CTM的诞生历程，它耗时八个月，最初名为“异步思维机器”，后因“异步”概念易混淆而更名。CTM的核心在于其三大创新，其中之一便是“内部思维维度”。

展望未来，AI是否能成为科学研究的主导力量？对话者认为，AI模型驱动进步是可能的，但短期内不太可能完全取代人类。他们曾发布一个“AI科学家”系统，能从一个研究想法开始，自主完成代码编写、实验运行、结果收集乃至论文撰写，甚至有一篇100%由AI生成的论文被研讨会接受。然而，理想的未来是人机交互式的研究：人类提出想法，AI生成更多想法，双方讨论，AI编写代码，人类审查代码，并共同讨论结果。这种互动至关重要，因为人类拥有深厚的理解、丰富的经验和路径依赖，能够进行直觉性的创造性探索，而这正是当前AI所缺乏的。就像指导实习生一样，需要持续的反馈和引导。当然，就像国际象棋中AI最终超越了人机协作一样，总有一天，AI的输入可能会变得对人类有害，但那将是更遥远的未来。

# Chapter 3: 连续思维机器：探索AI新范式的核心奥秘

在Sakana AI的实验室里，一位资深研究科学家正热情洋溢地分享着他们团队的最新力作——“连续思维机器”（CTM）。他坦言，这项研究耗费了整个团队大约八个月的时间才最终成文，这在当下追求速度的AI研究领域，无疑是一段漫长而严谨的旅程。最初，他们曾将其命名为“异步思维机器”，但由于“异步”一词常引人困惑，最终更名为“连续思维机器”，以更直观地表达其核心理念。

CTM的精髓在于其三大创新之处。首先是引入了“内部思维维度”。这并非全然新颖，它与“潜在推理”的概念异曲同工，本质上是将计算应用于一个序列维度。当研究者以这种框架审视问题时，他们发现许多看似智能的问题解决方案，往往都具有序列性。以CTM的“Hello World”问题——迷宫求解为例，对于深度学习而言，如果任务被简化，直接输入迷宫图像并输出路径图像，这轻而易举。然而，一旦我们要求机器以更像人类的方式解决迷宫，即通过一系列“向上、向右、向左”的指令逐步探索，问题便陡然变得极具挑战性。CTM正是通过这种内部序列思维维度来攻克这一难题。

其次，团队对“神经元”的定义进行了颠覆性重构。传统的深度学习神经元，如ReLU，非开即关，是对生物神经元的高度抽象。CTM则大胆设想，让每一个神经元本身就是一个“小模型”，能够处理更复杂的动态信息。这种设计不仅带来了系统内部的丰富动态，也为模拟生物神经元的复杂行为提供了可能。

第三项创新则聚焦于“思想的表示”。科学家们提出，思想并非仅仅是神经元在某一时刻的状态，它是一个随时间展开的过程。那么，如何在工程上捕捉这种“随时间存在的思想”呢？CTM给出的答案是：测量神经元之间的“同步性”。系统不再仅仅关注循环模型的状态，而是通过计算神经元对之间如何同步，来构建其内部的思考表示。这种基于同步性的表示方式，为系统带来了前所未有的丰富性和表达力。

当被问及CTM的“规划”能力时，这位科学家解释道，CTM的思考过程能够将复杂问题分解。例如，在迷宫任务中，如果试图一次性预测100或200步的路径，即使是CTM也无法做到。为此，他们开发了一个“自动课程学习系统”：模型首先学习预测第一步，成功后再学习第二、第三步，以此类推，形成一种“自举机制”。这种逐步学习的方式，使得模型能够处理那些一次性解决会呈指数级增长难度的复杂问题。

CTM还展现了卓越的“自适应计算”能力。在ImageNet图像分类任务中，模型会根据问题的难易程度，自然地调整思考步数。简单样本几乎瞬间解决，而复杂样本则会投入更多思考时间，充分利用其内部思维维度。这种灵活性使得CTM能够更高效地分配计算资源。

关于神经元级模型（NLM）和同步性的具体实现，科学家进一步阐述道，每个神经元不再是简单的激活函数，而是接收一段有限的激活历史作为输入，并输出一个激活值。同步性则通过计算两个神经元激活时间序列的点积来衡量。虽然这种方法在计算上可能导致二次时间复杂度，但通过子采样等技术可以优化。令人惊喜的是，CTM在实验中展现出极高的稳定性，并且同步性机制有助于梯度在时间维度上的有效传播，解决了传统循环网络中常见的学习中断问题。

此外，CTM引入了“指数衰减率”来处理同步性，允许不同神经元以不同的时间尺度进行同步，有的快速，有的缓慢，这极大地丰富了模型的表示空间，使其能够捕捉到生物大脑中多样化的时间尺度现象。科学家们坚信，CTM这种架构，凭借其独特的序列推理能力、对离散和稀疏领域的适应性以及样本效率，有望在诸如ARC挑战等需要深层推理的任务上，超越Transformer等现有模型，开辟“思维链推理”这一新的扩展维度。

# Chapter 4: CTM：超越Transformer的思考之道

在对Sakana AI团队的“连续思维机器”（CTM）深入探讨中，对话揭示了其超越传统Transformer架构的独特潜力。主持人首先抛出疑问：CTM在推理、离散稀疏领域以及样本效率方面，为何能显著优于Transformer，尤其是在应对ARC挑战这类任务时？

研究者解释道，近年来语言模型领域最引人入胜的突破之一，便是“思维链推理”作为一种新的“扩展维度”，它本质上为系统增加了更多计算资源。CTM将这种思想推向极致，其内部思维维度允许推理过程完全内化，并以某种序列方式运行，这被视为一个深刻的突破。与Alex Graves的神经图灵机（NTM）相比，CTM巧妙地避开了NTM在读写内存时离散动作的挑战，而是让推理在一个丰富且潜在的空间中展开，以适应不同类型的任务。

一个引人深思的例子是图像分类任务。传统的ViT或CNN模型必须将所有推理逻辑并行地嵌套在同一空间中，无论是识别一只简单的猫，还是一个复杂、罕见的类别，都必须在最后一层进行分类。然而，CTM能够将问题分解，在不同时间点进行决策，对于简单的任务可以迅速给出答案，而对于复杂的任务则能投入更多“思考时间”，自然地将任务按难度分段处理。这种“课程学习”的理念，与人类的学习方式不谋而合，如果能通过架构自然实现，无疑是值得深入探索的方向。

更令人惊喜的是，CTM展现出近乎完美的校准性。众所周知，许多训练充分的神经网络往往校准不佳，它们可能对错误的预测过于自信，而对正确的预测却犹豫不决。CTM在训练后却能保持高度校准，这并非刻意追求的目标，而是一种自然涌现的特性，被视为其设计优越性的“确凿证据”。同样，自适应计算时间（Adaptive Computation Time），通常需要复杂的惩罚项来平衡计算量与损失，但在CTM中，它却能自然而然地涌现。这体现了Sakana AI团队的独特研究哲学：尊重大脑和自然，构建受其启发的系统，然后观察会发生什么，追随“有趣性梯度”去探索。

对话进一步深入到“路径依赖”的概念。构建一个理解世界、能够持续学习的智能体，其“如何”达到某个理解，与“是什么”理解本身同样重要。CTM的架构允许智能体在空间中探索轨迹，找到最佳路径，从而构建出一种“路径依赖”的理解，以其独特的方式“雕刻”世界。这为理解语言模型中的“幻觉”现象提供了新的视角——也许那只是模型以一种我们不认可的方式“雕刻”了世界。CTM的目标是让模型能够隐式地意识到它正在以不同的方式“雕刻”世界，并探索这些可能性，以一种自然、无过多“技巧”的方式将问题分解为可解决的小部分。

迷宫任务成为了CTM能力的一个绝佳例证。研究者观察到，在训练过程中，CTM智能体在迷宫中会先沿着一条路径前进，发现错误后能及时回溯，然后尝试另一条路径，最终找到解决方案。更令人惊叹的是，当思考时间受限时，CTM会学习一种“跳跃式”算法：它会迅速跳到迷宫的近似终点，然后向后填充路径，再向前跳跃，如此往复。这种基于系统约束而产生的算法，引发了关于人类在受限与开放环境下思考方式的深刻哲学问题。

展望未来，团队正积极探索CTM在长时记忆方面的潜力。例如，在一个只能看到周围5x5区域的迷宫中，CTM智能体如何构建和检索记忆，以解决复杂的导航问题。此外，将CTM与群体方法和集体智能相结合，探索模型间的权重共享，有望进一步拓展其能力边界，构建出真正能够持续学习和适应的智能体。

# Chapter 5: 探索AI的深层智能：长时记忆与变体数独的挑战

在一次深入的探讨中，我们触及了人工智能未来发展的两个核心命题：长时记忆与高级推理。团队目前正积极探索AI的长时记忆机制，这对于构建一个真正智能的系统至关重要。想象一下，我们将一些智能体置于一个巨大的迷宫之中，它们视野受限，只能看到周围5x5的区域。这些智能体被赋予了存储和检索记忆的能力，它们的任务是找到迷宫的出口。这意味着模型必须学会如何构建记忆，以便在走错路时能够回溯到之前的某个关键点，并选择一条不同的路径。更进一步，当多个智能体在同一个迷宫中并行探索，并共享一个记忆结构时，会发生什么？这就像形成了一种“文化记忆”，所有智能体都能访问并共同解决这个复杂的全局任务。毫无疑问，记忆将是未来AI发展中一个极其关键的要素。

话题随后转向了AI的推理能力。尽管人们普遍认为AI在推理方面取得了巨大进步，但我们最近发布了一个名为“数独基准”（Sudoku Bench）的数据集，旨在挑战这一认知。这个基准测试并非普通的数独，而是“变体数独”。这些变体数独在标准规则（每行、每列、每个九宫格填入1到9）之上，叠加了几乎无限种额外的手工定制规则。这些规则千变万化，有些甚至需要极强的自然语言理解能力。例如，有一个谜题的规则本身就包含元推理，要求模型在解题前先理解规则的含义；另一个谜题则将迷宫叠加在数独上，老鼠必须通过遵循特定数字路径才能找到奶酪。这些谜题的复杂性难以言喻，以至于我们相信，任何能够攻克这个基准的模型，都必然拥有一个极其强大的推理系统。目前，最优秀的模型也只能解决其中最简单的15%左右。即使是GPT-5，虽然表现有所提升，但仍无法解决人类能够轻松应对的谜题。

创建这个数据集的初衷，源于对“思维轨迹”的渴望。我们拥有海量的互联网文本数据，但真正有价值的，是人类在创造这些文本时的“思维轨迹”。这种数据是否存在呢？答案在一次偶然的休闲时光中浮现——YouTube频道“Cracking the Cryptic”。两位英国绅士以令人难以置信的细节，讲解他们如何解决那些极其困难的数独谜题，有时一个视频长达四小时。他们精确地阐述了每一步的推理过程，这正是我们梦寐以求的“高质量人类推理思维轨迹”。在获得他们的许可后，我们抓取了数千小时的视频内容，将其转化为可用于模仿学习的数据。然而，这个基准的难度超出了预期，当前的AI模型无法轻易泛化，它们无法找到谜题中那些独特的“突破点”（breakins），而是退回到枯燥的试错法，这与人类灵活的推理方式截然不同。

关于推理模式的多样性，我们探讨了“知识的演绎闭包”——一个庞大的推理之树。我们越聪明、知识越渊博，就能在这棵树上走得越深。理想情况下，AI应该能够从第一性原理进行推理。但现实是，我们更像是在“钓鱼”，寻找那些可以应用于特定问题的“乐高积木”般的推理模式。人类在解决谜题时，会实时学习并发现这棵树的新部分，这是一种关于推理的元任务。他们会识别出特定的模式，比如“这看起来像一个Parask谜题”，或者“我应该拿出路径工具来追踪一下”。他们拥有海量的“推理乐高积木”，能够直观地知道从何处着手，即使是他们也会尝试、回溯，然后选择另一条路径——这是当前AI所不具备的能力。这棵推理之树极其庞大，不同推理模式之间的“系统发育距离”巨大，难以跳跃。这或许解释了为何集体智能如此有效，因为我们能够共同探索并连接这棵树的不同部分。当前的强化学习算法之所以在此处失效，正是因为那些“突破点”所需的特定推理模式极其稀有，难以通过采样学习。这并非简单的“我们有RL就能解决一切”的问题。最后，我们诚挚邀请有志之士加入我们的团队，共同探索AI的未知领域，我们提供最大的研究自由，相信这将带来许多令人兴奋的发现。

