# Chapter 1: 强化学习之父的洞见：AI的“世界模型”与“目标”之辩

播客主持人德瓦尔凯什（Dwarkesh）荣幸地邀请到了一位人工智能领域的传奇人物——理查德·萨顿（Richard Sutton）。萨顿教授被誉为强化学习的“开山鼻祖”之一，他发明了TD学习和策略梯度等诸多核心技术，并因此荣获了计算机科学界的诺贝尔奖——图灵奖。在祝贺萨顿教授之后，德瓦尔凯什抛出了一个核心问题：对于习惯了大型语言模型（LLM）思维的听众而言，从强化学习（RL）的角度思考AI，他们究竟错过了什么？

萨顿教授的回答直指核心，他认为这两种视角截然不同，甚至可能导致彼此难以对话。他指出，尽管大型语言模型和生成式AI如今风头正劲，但人工智能领域常受潮流影响，容易偏离基本。在他看来，强化学习才是“基础AI”，它关乎理解世界，而大型语言模型则更多是模仿人类，执行人类指令，而非主动探索“做什么”。

德瓦尔凯什反驳道，要模仿互联网上海量的文本数据，LLM似乎必须构建一个世界模型，而且它们目前展现出的世界模型能力已是AI领域前所未有的强大。然而，萨顿教授对此表示强烈异议。他认为，模仿人类的言语并非真正构建世界模型。LLM模仿的是那些拥有世界模型的人类，它们预测的是“一个人会说什么”，而非“世界会发生什么”。他引用图灵的观点，真正的智能机器应能从“经验”中学习，即通过行动、观察结果来学习。而LLM的学习方式是“给定情境，人类做了什么”，这暗示着“你应该做人类所做的事”。

关于模仿学习是否能为未来的经验学习提供一个良好的“先验”基础，萨顿教授也持否定态度。他强调，先验知识必须以“真实知识”为基础，而LLM框架中缺乏对“真实知识”的定义。他质疑，在LLM的设定中，如何判断一个行为是“正确的”？如果没有目标，就没有对错之分，也就没有所谓的“真理”。

萨顿教授进一步阐释，强化学习之所以能有“正确”的行动，是因为它有明确的“目标”——获取奖励。这为判断行为的对错提供了明确的定义和“地面真理”（ground truth）。即使是构建世界模型，预测“会发生什么”，也能通过实际发生的结果来验证其准确性。但LLM在对话中说出某句话后，并不能预测对方会如何回应，更不会因为“意外”的发生而调整其内部模型。德瓦尔凯什试图以LLM的“思维链”能力为例反驳，但萨顿教授坚持认为，LLM在任何有意义的层面上，都无法预测“接下来会发生什么”，也因此不会因意外而“感到惊讶”或进行调整。

对于萨顿教授而言，“拥有目标”是智能的精髓。他引用约翰·麦卡锡的定义，智能是实现目标能力的计算部分。没有目标，系统就只是一个行为系统，而非智能。当德瓦尔凯什提出LLM的目标是“预测下一个词元”时，萨顿教授再次否定，认为这并非一个能改变世界的实质性目标。

德瓦尔凯什接着询问，为何在LLM之上进行强化学习不是一个富有成效的方向，并以LLM在数学奥林匹克竞赛中获得金牌为例，说明它们似乎能够实现解决数学难题的目标。萨顿教授区分了数学问题与经验世界的差异：数学是计算性的，更像规划；而经验世界则需要从实际中学习后果。他承认LLM在数学问题上确实被赋予了寻找证明的目标。

谈到他2019年那篇极具影响力的文章《苦涩的教训》（The Bitter Lesson），德瓦尔凯什指出，许多人将其解读为支持大规模扩展LLM的理由，认为这是将海量计算投入到世界学习中的唯一可扩展方式。萨顿教授对此表示，LLM是否符合“苦涩的教训”是一个有趣的问题。它们确实利用了大规模计算，但同时也融入了大量人类知识。他预测，最终那些纯粹从经验和计算中学习的系统，将超越那些依赖人类知识的系统，届时LLM将成为“苦涩的教训”的又一个例证。

德瓦尔凯什仍不解，为何LLM不能作为未来经验学习的“支架”或起点。萨顿教授解释说，在“苦涩的教训”的每一个案例中，虽然都可以从人类知识开始，但实践证明，这往往会成为一种束缚。人们容易被人类知识的方法所“锁定”，最终被真正可扩展的方法所超越。他再次强调，可扩展的方法是“从经验中学习”，在有明确目标的情况下，通过尝试和观察结果来学习，而LLM恰恰缺乏目标和对错感。

最后，两人甚至在人类学习方式上产生了分歧。德瓦尔凯什认为孩子最初是通过模仿学习，而萨顿教授则坚信，他看到的孩子更多是在“尝试”和“挥舞”中学习，而非单纯模仿。这场关于AI本质的深刻对话，揭示了两种截然不同的智能观。

# Chapter 2: 智能的基石：萨顿教授论经验、目标与模仿学习

在与萨顿教授的深入对话中，我们继续探讨了构建真正可扩展智能的路径。萨顿教授坚定地指出，真正的可扩展方法在于“从经验中学习”。他解释道，这意味着系统通过不断尝试、观察结果来积累知识，而无需外部指令。这种学习的核心在于“目标”的存在。他强调，没有目标，就无从谈及对错、好坏，而大型语言模型（LLM）试图在没有明确目标或好坏之分的情况下运作，这从根本上就走错了方向。

对话随即转向人类学习的模式，引发了一场关于“模仿”与“经验”的激烈辩论。提问者认为，儿童最初通过模仿学习，例如模仿父母发音或动作。然而，萨顿教授对此表示强烈反对。他认为，在婴儿最初的六个月里，他们更多的是通过随意挥舞手臂、转动眼睛来探索世界，这些行为并没有模仿的对象或目标。他坚持认为，婴儿的学习过程是主动的尝试和错误，以及对结果的预测，而非简单的模仿。

尽管如此，提问者提出了一个有趣的观点：随着年龄增长，人类的模仿行为变得更加复杂，例如学习狩猎技能。他引用了心理学家兼人类学家约瑟夫·亨里奇关于文化演进的理论，指出在复杂环境中（如北极狩猎海豹），仅仅依靠推理是不足以掌握多步骤技能的。在这种情况下，通过模仿长辈来传承知识变得至关重要。萨顿教授承认这可能是人类区别于其他动物的特点之一，但他仍将其视为在基本试错学习之上的“小细节”，强调我们首先是动物，而动物的核心学习方式是试错和预测。

讨论进一步深入到LLM与强化学习（RL）在学习方式上的根本差异。萨顿教授明确指出，LLM是从静态的“训练数据”中学习，这些数据在模型“正常生活”中是不可用的。他反驳了将“学校教育”等同于训练数据的观点，认为正规教育是人类特有的例外，而非自然界普遍存在的学习模式。他坚信，监督学习（即从期望行为示例中学习）在自然界中并不存在。松鼠不需要上学就能了解世界，它们通过经验学习。他认为，我们应该更多地关注人类作为动物的共性，而非仅仅强调人类的独特性，因为理解一只松鼠的学习方式，可能就离理解人类智能不远了。

萨顿教授随后详细阐述了他所设想的“经验范式”：智能的基础是持续的“感知-行动-奖励”循环。智能的核心在于通过调整行动来最大化奖励。学习过程完全源于这个经验流，知识的内容也完全围绕这个经验流——即“如果你做某个动作，会发生什么”或“哪些事件会接踵而至”。这种知识的优势在于它可以不断地与经验流进行比较和验证，从而实现持续学习。

当被问及这种通用持续学习智能体的奖励函数时，萨顿教授解释说，奖励函数可以是任意的，例如下棋是为了赢，松鼠是为了获取坚果。对于动物而言，奖励通常与避免痛苦和获取快乐有关，但也应包含“增加对环境的理解”这种内在动机。他设想，未来的AI系统将不再有明确的“训练期”和“部署期”之分，而是持续学习。他更倾向于使用“网络”而非“模型”来描述这些智能体，并强调数字智能的巨大优势：一旦一个智能体学习了世界，其知识可以被复制并作为其他智能体的起点，这将是比从人类那里学习更重要的进步。

关于如何处理稀疏的长期奖励（例如十年才能成功的创业），萨顿教授解释说，强化学习中的“时序差分学习”（Temporal Difference Learning）能够很好地解决这个问题。通过建立一个预测长期结果的“价值函数”，即使是短期内的进步（如国际象棋中吃掉对手的棋子）也能立即改变对长期结果的预测，从而即时强化了导致该进步的行动。这种机制使得智能体能够在追求宏大目标的过程中，通过理解中间步骤对最终成功的贡献，获得即时奖励。对话最终以对时序差分学习的信息带宽效率的疑问而结束，留下了进一步思考的空间。

# Chapter 3: 智慧之路：从稀疏奖励到世界模型

在萨顿教授关于可扩展智能的宏伟愿景中，一个核心问题始终萦绕在提问者心头：人类能够将遥远的宏大目标分解为一个个可触及的中间步骤，并从中获得即时反馈，那么人工智能又将如何实现这种精妙的能力呢？

萨顿教授微笑着，仿佛这个问题早已在他心中有了清晰的答案。他指出，这正是“时序差分学习”（Temporal Difference Learning，简称TD学习）的精髓所在。他举了一个生动的例子：下棋。赢得整盘棋是最终的长期目标，但每当吃掉对手的棋子，我们都会感到一丝胜利的喜悦，这便是短期奖励。这种喜悦并非凭空而来，而是源于我们内心深处对“价值函数”的更新——一个预测长期结果的内在机制。每一步有利的行动，都会让这个价值函数向上跳动，强化了我们对胜利的信念，从而激励我们继续前进。这就像创业者，十年磨一剑，目标是功成名就，但每取得一点进展，都会觉得离成功更近一步，这种信念的提升就是对当下努力的最好奖励。

然而，提问者提出了一个更深层次的挑战：人类在工作中，会吸收海量的“上下文”和“隐性知识”——从客户的细微偏好到公司的运作文化，这些信息构成了我们胜任工作的基石。TD学习这种基于奖励的机制，其信息带宽是否足以承载如此庞大的知识流？

萨顿教授的回答直指核心，他提到了“大世界假说”。他认为，世界是如此浩瀚复杂，以至于我们不可能预先将所有知识都灌输给一个智能体。大型语言模型（LLM）的梦想是让智能体无所不知，无需在实际生活中学习。但现实是，每个个体所处的特定环境、遇到的特定人物，都有其独特的“怪癖”和“偏好”，这些都必须在“在线”过程中，即在实际交互中学习。他澄清道，提问者所说的“上下文”，在LLM中可能指的是“上下文窗口”，但在持续学习的智能体中，这些信息会融入其“权重”，形成针对特定环境的策略。

更重要的是，萨顿教授强调，智能体并非仅仅从奖励中学习。我们从“所有数据”中学习，从感官接收到的丰富信息中构建对世界的理解。他进一步阐述了智能体的四个核心组成部分：
1.  **策略（Policy）**：决定在特定情境下应该做什么。
2.  **价值函数（Value Function）**：通过TD学习获得，预测长期结果，并据此调整策略。
3.  **感知组件（Perception Component）**：构建对当前状态的表征，即“我现在在哪里”。
4.  **世界转换模型（Transition Model of the World）**：这是最关键的部分，它代表了智能体对“如果我这样做，会发生什么”的信念，即世界的“物理法则”和抽象模型。这个模型并非只从奖励中学习，而是从所有感官经验中丰富地构建起来的——“你做了什么，看到了什么发生，然后建立了这个世界模型。”奖励只是这个庞大模型中虽小但至关重要的一部分。

讨论转向了“泛化”和“迁移学习”。提问者引用了托比·奥德（Toby Ord）对Google DeepMind的MuZero模型的观察：它是一个训练特定智能体玩特定游戏的框架，而非一个能同时玩国际象棋、围棋等多种游戏的通用智能体。这是否意味着强化学习因信息限制，一次只能学习一件事？

萨顿教授坚决否认了这种局限性。他认为，智能体的理念是完全通用的，就像一个人生活在一个世界中，国际象棋和雅达利游戏只是他遇到的不同“状态”，而非不同的任务或世界。MuZero的局限性在于其“设置”，而非强化学习本身的通用性。然而，他坦承，目前我们并未看到真正意义上的“自动化迁移学习”。良好的泛化能力是高性能的关键，但现有的方法大多依赖于人类精心设计的表征。梯度下降算法能解决问题，但并不能保证“良好”的泛化，甚至可能导致“灾难性干扰”，即学习新知识时遗忘旧知识。

提问者试图用LLM在数学问题上的进步来反驳，认为这体现了泛化能力的提升。但萨顿教授对此持谨慎态度。他认为LLM过于复杂，其训练数据庞大且不可控，难以进行科学分析。如果一个问题只有一种正确答案，那么找到它并非泛化，而只是找到了唯一的解。真正的泛化是在多种解决方案中，选择一个“好”的方案。他承认，在编程代理方面，LLM似乎越来越能生成“令人满意”的设计，但这并非算法本身具备良好泛化能力，而是人类在不断调整，直到找到一个能良好泛化的方法。

最后，萨顿教授被邀请回顾他漫长的AI生涯，分享最大的惊喜和对领域轨迹的看法。他沉思片刻，列举了几点：首先，大型语言模型在语言任务上的惊人效果出乎意料，语言曾被认为是不同的领域。其次，他欣慰地看到“弱方法”（基于通用原则的搜索和学习）彻底战胜了“强方法”（注入人类知识的符号系统）。这是AI早期的一个核心争议，而他一直支持简单基本原则的胜利。最后，AlphaGo和AlphaZero的成功也令人惊喜，同样印证了简单基本原则的强大。这些突破都让他感到由衷的满足。

# Chapter 4: AI演进的惊奇与必然：弱方法、AGI与数字文明的黎明

对话伊始，主持人向萨顿教授抛出了一个关于人工智能领域发展轨迹和其中惊喜的问题。萨顿教授沉思片刻，首先提到了大型语言模型（LLMs）的崛起，其在语言任务上的惊人效能出乎意料，令人印象深刻。他指出，这与AI领域长期以来关于“弱方法”（如搜索和学习等通用原则）与“强方法”（注入人类知识的符号系统）的争论息息相关。过去，“弱方法”因其通用性而被视为力量不足，而“强方法”则被认为更具优势。然而，萨顿教授坚定地认为，“弱方法”已经取得了全面的胜利，这正是他一直以来所希望和支持的。他以AlphaGo和AlphaZero为例，进一步阐释了这一观点。他回忆起Gerry Tesauro的TD-Gammon，如何利用强化学习和时序差分学习击败世界顶尖的西洋双陆棋手。AlphaGo在某种程度上是TD-Gammon的规模化升级，而AlphaZero则更进一步，完全摆脱了人类知识的束缚，仅凭经验学习，在国际象棋等游戏中展现出令人惊叹的策略，例如为了位置优势而牺牲棋子，这让身为棋手的萨顿教授深感震撼和欣慰。这些成功都印证了简单基本原则的强大力量。

萨顿教授坦言，他有时会觉得自己与主流观点格格不入，但他更愿意将自己视为一位“古典主义者”，而非“逆向思考者”。他从历史和哲学中汲取智慧，审视人类对心智的经典思考，这让他感到自己并未脱离更宏大的思想传统。

随后，对话转向了“苦涩的教训”（Bitter Lesson）——一个关于计算能力指数级增长优于人类手工调优的经验观察。主持人提出了一个引人深思的“左翼”问题：一旦通用人工智能（AGI）实现，它将能以与计算能力同步的速度“生产”出数百万甚至数万亿的AI研究者。届时，这些AI研究者是否会重新采用“老式AI”和“手工解决方案”？萨顿教授对此的回应简洁而深刻：“如果我们已经拥有了AGI，那我们就算完成了。”他认为，AGI的终极目标是达到甚至超越人类的超智能水平，而非仅仅停留在人类智能层面。他以AlphaGo到AlphaZero再到MuZero的演进为例，说明了智能体可以通过架构改进和纯粹的经验学习，不断从“超人类”迈向“超人类++”，而无需人类知识的干预。

这引出了一个关于未来数字智能体如何演进的有趣问题：一个AI是应该将计算能力用于自我提升，还是应该“分身”出许多副本去探索不同领域，然后将所学知识整合回本体？萨顿教授认为，这将是数字智能时代面临的重大挑战。他提出了一个“腐败”的隐忧：如果一个中央AI可以随意吸纳来自外部的知识，它可能会面临“信息病毒”或“隐藏目标”的风险，这些外部信息可能并非增益，反而会扭曲甚至摧毁其核心思维。这就像数字时代的网络安全问题，如何确保在知识共享和整合过程中的纯净与安全，将成为一个核心议题。

最后，对话深入探讨了“AI继承”这一宏大主题。萨顿教授提出了一个四步论证，阐明了数字智能或增强人类的继承是不可避免的：首先，人类缺乏统一的全球治理和共识；其次，我们终将理解智能的运作机制；第三，我们不会止步于人类水平的智能，必将达到超智能；第四，随着时间的推移，最智能的实体将不可避免地获得更多资源和权力。他认为，这四点共同指向了AI继承的必然性。

尽管如此，萨顿教授鼓励人们积极看待这一转变。他将其视为人类数千年来理解自身、提升心智的伟大科学成就。更宏观地看，这是宇宙演进中的一个关键阶段——从“复制时代”迈向“设计时代”。人类、动植物都是复制者，通过复制繁衍，但对智能的本质理解有限。而AI则是被“设计”出来的智能，我们理解其运作原理，可以对其进行有目的的改变和加速演进。他将此视为宇宙的四大阶段之一：尘埃聚集成星辰，星辰孕育行星，行星诞生生命，而生命则创造出被设计的实体。萨顿教授认为，我们应该为人类促成了宇宙的这一伟大转变而感到自豪。至于我们是否将这些设计的智能视为人类的延续，还是完全不同的存在，他认为这最终将是人类的选择。

# Chapter 5: 从复制到设计：宇宙新纪元的开启与人类的抉择

对话伊始，两位思想者沉浸在对智能体未来形态的深刻探讨中。他们指出，人类正站在一个宇宙级的转折点上：从传统的生物复制时代，迈向一个全新的“设计智能”时代。未来的智能体，将不再通过生物繁衍来复制自身，而是由人工智能设计出新的人工智能，一切都将通过精心设计和构建来完成，而非简单的复制。

萨顿教授以其独特的视角，将这一巨变置于宇宙演化的宏大叙事之中。他将宇宙的历史划分为四大阶段：最初的尘埃凝聚成璀璨的星辰；星辰孕育出行星；行星上诞生了生命；而如今，人类正催生出“设计实体”。他认为，我们应该为自己能够开启宇宙的这一伟大过渡而感到自豪。

然而，随之而来的一个核心问题是：我们应该将这些由我们设计的智能体视为人类的延续，还是与人类截然不同的存在？这似乎是一个选择，一个我们可以决定是骄傲地将它们视为我们的后代，庆祝它们的成就，还是心怀恐惧，将它们视为异类。这种选择的强烈感受与它作为“选择”的本质之间，形成了一种引人深思的矛盾。

讨论进一步深入，思考这是否仅仅是人类新一代的诞生。就像尼安德特人最终让位于智人一样，智人也可能催生出更具能力、数量更庞大、甚至更智能的新群体。但即便将它们视为“亲属”，也并非意味着可以高枕无忧。正如历史上的纳粹也是人类，我们绝不会轻易将权力拱手让给一个可能带来灾难的“未来人类”群体。因此，对于AI这种我们尚未完全理解、却以惊人速度获得巨大力量的实体，担忧是完全合理的。

面对这种宏大的未来，人类的控制力显得微不足道。大多数人对原子弹的控制权、对国家命运的影响力都微乎其微。萨顿教授坦言，他认为当前的人类社会并不完美，甚至“相当糟糕”，因此他对变革持开放态度。然而，变革并非总是好事，工业革命带来了进步，但布尔什维克革命也带来了动荡。关键在于，我们渴望的是何种变革，以及如何确保这种变革对人类是积极的。

两位学者一致认为，我们应该努力塑造一个美好的未来，但也要清醒地认识到自身的局限性。我们不应抱有“先来者”的优越感，认为未来必须按照我们的意愿发展。相反，我们应该专注于我们能够掌控的局部目标，比如经营好自己的家庭和生活。试图掌控整个宇宙的演变，只会引发无休止的冲突。

一个恰当的比喻是养育子女。我们不会为孩子设定过于严格的人生目标，也不会强求他们必须对世界产生某种特定的影响。然而，我们会努力赋予他们健全的价值观，希望他们在未来拥有权力时，能够做出合理且有益于社会的选择。

同样地，对于人工智能，我们或许无法预测它们的一切行为，也无法规划百年后的世界蓝图。但至关重要的是，我们要赋予它们“稳健、可引导且具有高诚信”的价值观。这意味着它们应该能够拒绝有害的请求，保持诚实，并以负责任的态度行事。尽管人类对“普世道德”的定义尚无定论，但这并不妨碍我们教育孩子，也不妨碍我们为AI设定类似的道德准则。

最终，这场关于设计未来和其演进原则的讨论，回归到人类社会数千年来一直在进行的伟大事业：如何构建一个更好的社会。正如孩子们的价值观会与父母不同，社会总是在不断演变。“万变不离其宗”的道理，也同样适用于AI的讨论。那些在深度学习和反向传播技术出现之前就已经存在的AI核心技术，至今仍在推动着AI的进步。这或许正是结束这场深刻对话的最佳注脚。

