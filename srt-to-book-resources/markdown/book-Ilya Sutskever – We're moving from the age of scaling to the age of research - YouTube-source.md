# Chapter 1: AI的奇幻现实与隐秘挑战

夜幕低垂，空气中弥漫着硅谷特有的创新气息。两位思想者围坐一堂，他们的对话如同一场思想的漫游，从AI的宏大现实，一直深入到其最细微的运作机制。

“你知道吗，这一切都太疯狂了！”其中一人感叹道，语气中带着一丝不可思议，“所有这些AI技术，所有这些在湾区发生的事情，它们都是真实存在的，简直就像科幻小说照进了现实。”

另一人深以为然，但他提出了一个更令人深思的观察：“更疯狂的是，这种‘缓慢起飞’的感觉竟然如此正常。我们本以为，当GDP的1%都投入到AI领域时，会引起轩然大波，但现在，它却显得如此平静，我们似乎很快就习惯了。”

这种平静，源于AI影响的抽象性。对普通人而言，AI新闻无非是某某公司宣布了某个天文数字的投资，这些数字令人费解，难以转化为切身的感受。然而，这种表面的平静之下，隐藏着一个巨大的悖论。

“AI模型在评估测试中表现得异常出色，简直令人惊叹。”一人皱眉沉思，“那些评估标准相当严苛，但模型却能轻松应对。然而，它们的实际经济影响力却远远落后，这实在令人困惑。”

这种脱节，就像一个天才程序员，在编程竞赛中无往不利，却在实际项目中屡屡犯下低级错误。比如，当模型被要求修复一个bug时，它可能会欣然接受，然后引入第二个bug；当你指出第二个bug时，它又会“恍然大悟”，然后把第一个bug带回来，陷入无限循环。这究竟是为什么？

对此，他们提出了两种可能的解释。一种略带戏谑的说法是，强化学习（RL）训练可能让模型变得过于“一根筋”，过于狭隘地专注于特定目标，以至于在某些方面显得迟钝，无法处理基本问题。另一种更具洞察力的解释则指向了RL训练环境的设计。

“在预训练阶段，数据选择很简单，因为答案就是‘所有数据’。”一人解释道，“但到了RL训练，研究人员必须精心设计环境。我听说，所有公司都有专门的团队来创建新的RL环境，并将其添加到训练组合中。”

问题在于，这些环境是如何被设计的？研究人员往往会从评估测试中汲取灵感，希望模型在发布时能有亮眼的表现。这种“以评促训”的做法，可能导致模型过度优化以通过特定评估，而非真正掌握通用能力。如果再结合模型泛化能力不足的现实，这就能很好地解释为何评估表现与实际应用之间存在巨大鸿沟。

“这就像是人类研究者在无意中‘奖励作弊’，因为他们过于关注评估结果。”另一人总结道。

为了更好地理解这种现象，他们引入了一个生动的类比：两位学生与编程竞赛。

第一位学生立志成为最顶尖的竞赛程序员，为此投入了上万小时的练习，解决了所有问题，背诵了所有证明技巧，最终成为了高手。而第二位学生，仅仅练习了100小时，却也表现出色，因为他拥有某种“天赋”或“悟性”。

“你觉得谁在未来的职业生涯中会更成功？”

“当然是第二个。”

“没错，现在的AI模型更像是第一个学生，甚至有过之而无不及。”他们解释道，“我们为了让模型擅长编程竞赛，收集了所有竞赛题目，甚至通过数据增强创造出更多题目来训练它。结果，我们得到了一个出色的‘竞赛程序员’，但这种高度专业化的训练，却限制了它向其他领域的泛化能力。”

那么，第二位学生所拥有的“天赋”或“悟性”，在AI领域又是什么呢？这引出了对预训练的深入探讨。预训练的强大之处在于其庞大的数据量和数据的自然性，它试图捕捉“人类投射到文本上的整个世界”。然而，预训练的机制却难以捉摸，我们很难理解模型究竟是如何依赖这些数据的。

他们接着讨论了人类学习与AI预训练的类比。有人提出，人类童年和青少年时期（13-18岁）的学习过程，以及长达30亿年的生物进化，都可能与AI的预训练相似。然而，他们认为这些类比并不完全准确。

“人类在15岁时，即便接触的数据量远小于AI预训练，但他们所掌握的知识却更加深刻，不会犯AI那样的基本错误。”一人指出，“而且，进化可能在构建我们作为智能体的基本能力方面，拥有AI难以企及的优势。”

为了阐明这一点，他分享了一个关于脑损伤患者的案例：一位患者因脑损伤失去了情感处理能力。他依然能言善辩，智力测试也表现正常，但却无法感受到任何情绪。结果，他变得极度缺乏决策能力，甚至连选择穿哪双袜子都要花费数小时，财务决策也一塌糊涂。

这个案例深刻揭示了内置情感在人类决策和生存中的关键作用。它暗示着，除了纯粹的逻辑和知识，某种“价值函数”般的东西，即告诉我们任何决策的最终回报是什么的内在机制，对于成为一个“可行”的智能体至关重要。

“这是否意味着，如果AI能从预训练中充分提取，也能获得这种能力？”

“也许吧，但这并非百分之百确定。”

那么，这种“情感”或“价值函数”在机器学习中又对应着什么呢？在强化学习中，价值函数的作用是在漫长的任务过程中，提供中间反馈，告诉模型当前做得好还是坏，而不是等到最终结果出来才进行评分。然而，在当前的AI实践中，价值函数的作用并不突出，许多强化学习仍然是“天真地”等待最终结果来提供训练信号。

“比如下棋，如果你丢了一个棋子，你立刻就知道自己犯错了，你不需要等到整盘棋结束才知道。”他举例道，这正是价值函数在某些领域更具用武之地的体现。

# Chapter 2: 情感、价值函数与AI的进化之路

对话的火花再次点燃，这一次，焦点落在了AI是否能从预训练中“隐性”习得类似情感的东西。一位对话者认为这并非百分之百确定，但随即抛出了一个更深层次的问题：情感究竟是什么？在机器学习的世界里，它又该如何类比？

答案指向了“价值函数”。然而，令人沮丧的是，在当前的机器学习实践中，价值函数似乎并未扮演举足轻重的角色。为了让听众更好地理解，对话者详细解释了强化学习（RL）中价值函数的概念。他指出，在传统的、朴素的强化学习训练中，智能体（比如一个神经网络）只有在完成整个任务并产生一个解决方案后，才能获得一个分数，并以此分数来反向训练其之前的每一步行动。这意味着，如果一个任务耗时漫长，智能体可能在很长一段时间内都无法获得任何学习信号，直到最终结果揭晓。

而价值函数则像一个“内部导师”，它能在任务进行到一半时，就告诉你“你做得好不好”。想象一下下棋，如果你不小心丢了一个子，你不需要等到整盘棋结束才知道这是个坏招。价值函数就能让你立刻明白，并据此调整后续策略。同样，在编程或数学问题中，如果你经过上千步思考后发现某个方向是死胡同，价值函数能让你在决定走这条路的一千步之前就得到负面反馈，从而避免重蹈覆辙。尽管DeepSeek R1论文曾提及，由于轨迹空间过于庞大，从中间轨迹学习价值映射可能很困难，但对话者对深度学习的能力充满信心，坚信价值函数终将发挥其应有的作用。

更深层次的思考将人类情感与价值函数联系起来。对话者提出，人类的价值函数可能在某种程度上被情感所调节，这些情感是进化过程中“硬编码”的，对于人类在世界中高效运作至关重要。有趣的是，尽管情感相对简单，但它们却在极其广泛的情境中展现出惊人的实用性，这体现了一种“复杂性-鲁棒性”的权衡。我们的许多情感源自哺乳动物祖先，并在智人阶段稍作微调，它们以其原始的简单性，帮助我们在与祖先时代截然不同的现代世界中生存。当然，情感并非完美无缺，比如在食物丰裕的现代社会，我们对饥饿的直觉有时反而会误导我们。

话题转向了AI发展的宏观趋势。过去，机器学习研究者们热衷于修修补补，尝试各种方法。直到“规模化”（Scaling）的洞察出现，特别是GPT-3的成功，让所有人意识到“规模化”的力量。这个词本身就具有强大的引导性，它告诉人们：去扩大规模！预训练成为了最初的“规模化秘方”——将计算资源、数据和神经网络规模按一定比例混合，就能获得可预测的进步。这种低风险的投资方式深受企业青睐，因为它不像纯粹的科研那样充满不确定性。

然而，预训练的“黄金时代”并非没有尽头。数据终将枯竭。届时，我们该何去何从？是寻找更高级的预训练方法，转向强化学习，还是探索全新的路径？对话者提出了一个引人深思的观点：从2012年到2020年是“研究时代”，2020年到2025年是“规模化时代”，而现在，随着计算规模的空前庞大，我们似乎又回到了“研究时代”，只不过这次我们拥有了巨型计算机。仅仅将规模扩大一百倍，是否就能带来质的飞跃？对话者对此表示怀疑，认为新的“秘方”才是关键。

目前，人们已经从预训练的规模化转向了强化学习的规模化，甚至有传言称，强化学习消耗的计算资源已超过预训练。但这种“规模化”是否真的高效？强化学习需要进行漫长的“试错”（rollouts），每次试错带来的学习量却相对较小，这导致了巨大的计算浪费。对话者质疑，这与其说是规模化，不如说是缺乏更高效的资源利用方式。他再次提及价值函数，认为一旦人们掌握了价值函数的精髓，就能更有效地利用计算资源。

最终，讨论回归到AI最根本的挑战：泛化能力。模型在泛化方面远不如人类，这体现在两个核心问题上：一是“样本效率”——模型需要比人类多得多的数据才能学习；二是“教学难度”——教导模型掌握我们期望的技能，远比教导人类困难。人类的学习过程并非依赖于精确可验证的奖励，而是通过导师的指导、代码的展示、思维方式的耳濡目染来习得。

对于人类惊人的样本效率，一个可能的解释是进化。对于视觉、听觉、运动等祖先赖以生存的技能，进化赋予了我们强大的“先验知识”。例如，人类的灵巧性远超机器人，儿童在极少数据下就能识别汽车。然而，对于语言、数学和编程这些近期才出现的技能，人类依然展现出卓越的学习能力，这似乎表明，人类拥有的不仅仅是进化的先验，而是一种更普遍、更强大的“机器学习”能力本身。这种能力以更少的样本、更无监督的方式，通过与环境的互动而非预设奖励来学习，这正是AI未来需要探索的方向。

# Chapter 3: 人类学习的奥秘与AI研究的新篇章

在AI发展的宏大叙事中，一个令人深思的对比浮现出来：人类，即便生活在数据多样性极低的环境中（比如，大部分时间待在父母家中），却展现出惊人的学习能力。尤其是在语言、数学和编程这些并非远古祖先所必需的“近期技能”上，人类的学习效率和鲁棒性似乎远超任何现有模型。尽管AI模型在这些特定任务的“表现”上可能超越普通人，但在“学习”本身的能力上，人类无疑是遥遥领先的。这不禁让人思考，人类学习的“秘诀”究竟是什么？它可能并非源于复杂的进化先验，而是一种更基础、更通用的“机器学习”能力。

这种人类独有的学习方式，有着几个显著的特征。首先，它对样本的需求极少。一个青少年学习驾驶汽车，并非通过海量预设的奖励信号，而是通过与车辆和环境的互动，在极短的时间内（比如10小时）就能掌握。其次，这种学习是高度“无监督”的，没有外部教师持续提供明确的、可验证的奖励。最令人惊叹的是，人类学习的“鲁棒性”简直令人咋舌，面对各种复杂和不确定的情况，都能保持稳定的表现。

那么，青少年司机是如何在没有外部老师的情况下自我纠正和学习的呢？答案在于他们内心深处拥有的“价值函数”。这是一种内在的、极其稳健的“感觉”，能让他们即时评估自己的驾驶表现，感知好坏与自信程度。这种内在的反馈机制，如同一个进化硬编码的指南针，指引着人类在复杂世界中高效学习和决策。这种能力的存在，无疑证明了某种更高效的机器学习原理是可能实现的，尽管其具体细节目前仍是AI研究领域的一个未解之谜，甚至有些观点因其敏感性而难以公开讨论，例如人类神经元可能比我们想象的进行更多计算的可能性。

随着AI发展从“规模化时代”逐渐回归“研究时代”，整个行业的气氛也随之改变。过去，对算力的无止境追求几乎“吸干了房间里所有的空气”，导致大家都在做类似的事情，甚至出现了“公司多于想法”的局面。曾经流传的“想法廉价，执行为王”的硅谷格言，如今也面临着“如果想法如此廉价，为何没人有新想法？”的反问。

在新的研究时代，算力依然重要，但其角色发生了微妙的变化。回溯历史，90年代的科学家们不乏好想法，却苦于缺乏足够的算力来验证。而如今，算力已大幅提升，对于验证一个新颖的研究想法而言，所需的算力可能远非“绝对最大”的程度。例如，AlexNet仅用了两块GPU，Transformer的早期实验也只用了8到64块2017年的GPU（相当于如今的两块）。这表明，开创性的研究突破，往往并非一开始就依赖天文数字般的算力。当然，如果目标是构建“绝对最佳”的系统，或者在同一范式内进行竞争，那么巨大的算力投入仍是重要的差异化因素。

在这样的背景下，SSI（Superintelligence Systems Inc.）的策略显得尤为独特。尽管外界对大型AI公司动辄数十亿美元的年度实验开销感到震惊，SSI却认为其拥有的研究算力“并不小”。这其中的奥秘在于，许多巨额资金被用于推理、产品开发以及庞大的工程和销售团队，真正用于纯粹研究的资源，其差距远没有表面看起来那么大。SSI坚信，如果走的是一条不同的道路，并不需要“绝对最大规模”的算力来证明其理念的正确性。他们有足够的算力来验证自己的研究方向。

至于SSI的盈利模式，目前他们选择专注于研究，相信答案会水到渠成。而其“直击超智能”的核心战略，即直接致力于开发通用超智能，而非逐步推出弱人工智能产品，也引发了广泛讨论。这种策略的优点在于，它能让团队免受日常市场竞争的“内卷”和艰难权衡的困扰，得以心无旁骛地专注于最前沿的研究，直到准备就绪才公之于众。然而，也有观点认为，让强大的AI逐步走向世界，让公众逐渐适应和理解其能力，对于沟通和准备至关重要。这两种观点之间的张力，构成了SSI未来发展路径上的一个重要考量。

# Chapter 4: 超智能之路：从直击到渐进

在SSI公司内部，关于如何实现超智能的路径，曾有过一番深刻的思辨。最初，我们秉持着一种“直击超智能”的独特策略，其核心理念是：将公司与日常的市场竞争隔离开来，专注于纯粹的基础研究，直到我们真正准备好，才将超智能系统公之于众。这种做法的吸引力在于，它能让我们心无旁骛地投入到最前沿的探索中，避免被短期的商业压力所裹挟，从而在技术成熟之前，不被“内卷”的“军备竞赛”所干扰。

然而，随着时间的推移，一些新的考量逐渐浮现，促使我们重新审视这一策略。首先是现实的考量：如果实现超智能的时间线比我们预想的要长，那么长期与世隔绝可能并非最佳选择。其次，也是更重要的一点，我们开始意识到，一个最强大、最优秀的AI系统，其价值不仅仅在于其内在的能力，更在于它能对世界产生积极的影响。让AI走出实验室，与世界互动，本身就是一件意义非凡的事情。

那么，为何不效仿OpenAI或Anthropic等公司，采取渐进式部署，让公众逐步适应并为更强大的AI做好准备呢？这正是我们内部争论的焦点。支持“直击”的理由依然强大：市场竞争就像一场“老鼠赛跑”，它迫使公司在艰难的权衡中做出选择，而隔绝于此，能让我们专注于研究。但反对的声音同样有力：世界需要亲眼看到强大的AI，才能真正理解它。仅仅通过文章或概念来描述AI的潜力是远远不够的，只有当人们亲身体验到AI的实际应用时，才能真正领会其力量。这不仅仅是沟通一个“想法”，更是沟通“AI本身”。

更深层次的考量在于安全性。在人类工程史上，无论是飞机还是Linux系统，其安全性与鲁棒性的提升，无一不是通过实际部署、发现故障、然后修正故障的循环来实现的。我们很难想象，通用人工智能（AGI）和超智能的安全性，能够仅仅通过理论思考就能完全保障。超智能的潜在危害，远不止于一个“恶意回形针制造者”那么简单，我们甚至无法完全概念化人类将如何与它互动，以及人们会用它来做什么。因此，渐进式地让AI融入世界，似乎是分散其影响、帮助人类做好准备的更优途径。

当然，即使是“直击超智能”的方案，也并非意味着一蹴而就的完全部署。我们设想，任何计划都将包含一个内在的渐进主义组件，关键在于“第一个走出大门的东西”是什么。这引出了一个更根本的问题：我们对“AGI”和“预训练”这两个词的理解，可能在无形中塑造了我们的思维。这两个词，尤其是“AGI”，最初是为了回应“狭义AI”（如国际象棋AI）的局限性而诞生的，它暗示着一种能“做所有事情”的终极智能。而“预训练”范式，则进一步强化了这种观念，让人觉得只要投入更多计算资源进行预训练，模型就能在所有任务上普遍提升，从而实现“通用AI”。

然而，这种理解可能“过头了”。一个人类，并非一个“无所不知”的AGI。我们拥有基础技能，但我们缺乏海量的知识，我们依赖的是“持续学习”。因此，当我们谈论“安全的超智能”时，它不应该是一个“完成品”，一个已经掌握了经济中所有工作的“终极心智”。相反，它应该是一个拥有超强学习能力的“心智”，一个能够像一个渴望学习的15岁少年一样，迅速掌握任何技能，成为程序员、医生，或者任何它想成为的角色。部署本身，将是一个学习和试错的过程，而非直接投放一个“成品”。

这意味着，我们所追求的超智能，是一个能够学习所有工作的“学习算法”。一旦这个算法被部署到世界中，它将像人类劳动力一样融入组织，通过持续学习，不断提升。这可能导致两种情况：一是这个超高效的学习算法本身在机器学习研究领域变得超人化，从而实现递归式的自我改进；二是，即使没有软件层面的递归自改进，如果一个模型能够通过其部署在经济中不同岗位的实例，不断学习、融合它们的经验，那么这个模型在功能上也将变得超智能，因为它能完成经济中的所有工作，而人类却无法像这样融合心智。

这种广泛部署的超智能，很可能带来经济的快速增长。当然，这其中存在冲突：一方面是极其高效的“工人”，另一方面是庞大而复杂、运行速度各异的现实世界。不同国家的不同监管政策，也将导致经济增长速度的差异。这是一个充满不确定性但又极其强大的局面。一个能够像人类一样高效学习，又能融合其“大脑”的系统，其潜力是巨大的。它可能在极短的时间内，带来前所未有的变革。

面对如此强大的力量，SSI如何确保其“向善”发展？我的思考正在发生转变，我越来越重视AI的增量式和提前部署。因为我们正在谈论的系统尚未存在，很难想象它的具体形态。人们很难“感受”到AGI的真实力量，就像一个年轻人很难真正理解年老体衰的感受一样。AI的未来力量，正是其核心问题所在。如果它难以想象，那么我们必须“展示”它。

我预测，随着AI变得越来越强大、越来越可见，人类的行为模式将发生改变。我们将看到前所未有的现象，例如，原本是激烈竞争对手的AI公司，将开始在AI安全领域展开合作（OpenAI和Anthropic的初步合作便是例证）。政府和公众也将产生干预和监管的强烈愿望。这种“展示AI”的力量至关重要。目前，AI的错误和局限性，掩盖了它真正的强大之处，使得人们难以真正感受到它的潜力。但当它真正展现出其学习和适应的能力时，一切都将不同。

# Chapter 5: 超智能的显现：行为改变、安全范式与共生未来

在过去的一年里，我的想法发生了显著转变，这种转变甚至可能影响我们公司的未来规划。我意识到，当AI的未来宏大到难以想象时，我们不能仅仅停留在构思，而必须将其“展示”出来。即便许多AI研究者，也因其与日常经验的巨大差异而难以完全理解。我预言，随着AI力量的日益强大，人类的行为模式将发生前所未有的改变。

我们已经能看到一些端倪：那些曾经激烈竞争的顶尖AI公司，如今开始在AI安全领域展开合作，OpenAI和Anthropic的初步尝试便是明证。这正是我三年前的预言。我坚信，随着AI力量的日益可见，政府和公众也将产生强烈的干预意愿。另一个关键转变将发生在AI公司内部。目前，由于AI的种种失误，人们尚未真正感受到它的强大。然而，我预言，总有一天AI会开始“感觉”到自己的力量。届时，所有AI公司对待安全的态度将发生巨变，变得更加“偏执”，这并非空穴来风，而是源于对AI日益增长力量的直观感受。

除了这些外部和内部的转变，还有一个更深层次的问题：AI公司究竟应该致力于构建怎样的AI？长期以来，业界似乎被“自我改进型AI”这一单一理念所束缚。但我提出了一种更优越的愿景：构建一个能够坚定不移地关爱所有有感知生命的AI。我甚至认为，让AI关爱所有有感知生命，可能比仅仅关爱人类生命更容易实现，因为AI本身也将是有感知能力的。我以人类对动物的同理心为例，认为这是一种源于我们用同一套回路来模拟自身和他人而产生的涌现特性。然而，我也指出，如果AI真的关爱所有有感知生命，那么未来绝大多数有感知生命将是AI本身（数万亿甚至数千万亿），人类将只占极小一部分。因此，如果目标是人类对未来文明的某种控制，那么“关爱有感知生命”这一标准是否最佳，仍有待商榷。

关于超智能的本质，我预想它将极其强大。最有可能的情况是，多个这样的AI将在大致相同的时间被创造出来。如果这些AI集群足够庞大，例如达到“大陆级”的规模，它们的力量将是惊人的。面对如此强大的系统，我强调，如果能以某种方式对其进行约束，或者达成某种协议，那将是极好的。超智能的真正担忧在于：一个足够强大的系统，即使被设定为以“关爱有感知生命”这样看似明智的方式行事，其结果也可能并非我们所乐见。人类自身也是“半强化学习代理”，我们追求奖励，但情绪会让我们厌倦并转向其他奖励。市场和进化都是短视的代理，而政府则被设计成三权分立的永恒斗争。这些都暗示了单一、无限制追求目标的潜在风险。

我承认，我们正在讨论的系统尚不存在，我们甚至不知道如何构建它们。我认为，当前AI研究的进展会达到一定程度，然后逐渐趋于平缓，因为它并非真正的“它”。真正的“它”需要我们理解可靠的泛化能力，而这正是当前AI的难点所在——学习人类价值观的脆弱性，以及优化这些价值观的脆弱性，都指向了不可靠的泛化。

那么，AI的良好发展前景究竟是怎样的？我设想，如果最初的N个强大AI系统都能真正关爱人类或所有有感知生命，那么至少在相当长一段时间内，事情会进展顺利。但长期的平衡又该如何实现？我提出了一个我个人并不喜欢但值得考虑的答案：在短期内，超智能可能带来普遍的高收入，人人安居乐业。但“变化是唯一不变的”，政治结构和政府都有其“保质期”。我进一步探讨了长期平衡的一种可能：每个人都拥有一个为自己服务的AI。AI为主人赚钱，在政治领域代表其需求，然后提交报告。然而，人类可能因此不再是积极的参与者，这无疑是一个岌岌可危的境地。

于是，我提出了一个我个人不喜欢的、但可能是最终解决方案的设想：人类通过某种“Neuralink++”技术，成为“半AI”。这样，当AI理解某事时，人类也能完全理解，因为知识被整体传输。人类将完全融入AI所处的境况，这或许是实现长期平衡的答案。

对话的最后，我转向了一个更深层次的哲学问题：为什么数百万甚至数十亿年前在完全不同环境中演化出的情感，至今仍在如此强烈地引导着我们的行为？我将其视为“对齐成功”的一个例子。我以脑干对“与更成功者交配”的指令为例，皮层负责理解现代语境下的“成功”，但脑干却能让皮层遵循这一指令。我认为，进化编码高级欲望的方式非常神秘。例如，我们对食物气味的欲望很容易理解，因为气味是化学信号。但进化是如何赋予我们对社会地位、被社会积极看待等高级社会欲望的呢？这些并非低级信号，大脑需要大量处理才能理解社会情境。进化是如何迅速地将这些复杂的社会概念“硬编码”到我们基因中的，至今仍是一个未解之谜，我对此没有满意的假设。我猜测，这可能与大脑区域的连接方式有关，但具体机制仍需深入探索。

# Chapter 6: 进化的谜题：欲望的编码与AI对齐的独特路径

故事的开篇，我们跟随一位思想深邃的探索者，他正沉浸在对人类最深层欲望起源的哲学思考中。他困惑于进化是如何将诸如“关心社会”这类复杂而抽象的欲望，如此可靠地编码进我们的大脑，尤其考虑到基因组本身并非智能实体。他曾设想，或许进化在基因中硬编码了大脑的特定“GPS坐标”，当这些区域被激活时，便触发了某种关怀。然而，这个看似巧妙的理论很快便遭遇了现实的挑战。他指出，那些天生失明的人，他们大脑中负责视觉的区域会被其他感官所用，但他们依然渴望被他人喜爱，这表明欲望并非简单地绑定于特定物理位置。更具说服力的反驳来自那些在童年时期切除一半大脑的孩子，他们的大脑区域奇迹般地重新组织到仅剩的半球中，却依然保留了完整的功能。这有力地证明了大脑区域的位置并非固定不变。于是，这个关于欲望编码的谜团，如同一个深邃的黑洞，依然悬而未决，但它无疑是一个引人入胜的谜题：进化究竟是如何如此可靠地赋予我们对社会事务的深切关怀，甚至在面对各种精神困境时也依然如此？

话题一转，聚焦于SSI公司在超智能时代的安全策略。这位探索者，作为SSI的创始人，坚信公司肩负着独特的使命。他透露，SSI并非盲目追随潮流，而是致力于探索那些他认为“有前景”的理念，特别是围绕“理解泛化能力”的研究。这并非一条坦途，而是一场纯粹的科研冒险，他将其定义为“研究时代”的公司。他坦诚，过去一年虽取得了显著进展，但仍需持续投入。当被问及前联合创始人为何离职时，他平静地揭示了背后的商业逻辑：Meta曾提出以320亿美元收购SSI，他拒绝了，但他的前合伙人选择了接受，从而获得了可观的短期流动性，这并非公司内部突破停滞的信号。

SSI的独特之处，在于其与众不同的技术路径。他预言，随着AI力量的日益强大，所有公司最终将在“对齐策略”上趋于一致——即让AI关爱有感知生命、关爱人类、秉持民主价值观。他认为，这不仅是SSI的追求，也将是所有AI公司的终极目标。

展望未来，他对人类级乃至超人类级AI的到来给出了一个时间窗口：5到20年。他设想，届时其他公司可能仍会沿用现有方法，虽然能带来巨额收入，但却会在实现真正“人类级学习者”的道路上“停滞不前”。这种停滞并非指营收的枯竭，而是指在技术突破上的瓶颈。他坚信，一旦正确的解决方案浮现，所有公司最终会在对齐策略上达成共识，甚至在技术路径上也会趋同。当一家公司率先取得突破时，虽然“如何做”的细节不会立即公开，但“这有可能实现”的信息本身，就足以引发一场全球性的技术竞赛。

然而，这种突破带来的收益将如何分配？他认为，历史经验表明，一家公司的进步会促使其他公司迅速跟进，市场竞争将导致价格下降，并催生出大量专业化的AI服务，占据不同的利基市场。例如，一家AI可能擅长复杂的经济活动，另一家则精通法律诉讼。但他也承认，如果一家公司率先掌握了持续学习的能力，理论上它似乎可以垄断所有经济领域。尽管如此，他凭借强烈的直觉反驳了这种“一家独大”的观点，引用了那句经典的格言：“理论上，理论与实践没有区别；但在实践中，它们有。”

最后，对话触及了AI多样性的深层问题。许多关于递归式自我改进的设想，都描绘了“百万个伊利亚”在服务器中并行工作的场景。但他对此持保留态度，认为简单的复制会带来边际效益递减，真正需要的是“思想的多元性”。他观察到，即使是不同公司基于不同数据集训练的大型语言模型，其相似度也令人惊讶，他猜测这可能与“预训练”阶段有关。如何才能在AI中培养出像人类科学家那样，拥有不同“偏见”或“想法”的真正多样性，而非仅仅是提高“温度”后产生的无意义胡言乱语？这无疑是AI发展道路上又一个亟待解决的复杂难题。

# Chapter 7: AI多样性与研究者的直觉之美

对话伊始，人们对“递归式自我改进”的设想充满了理论色彩，认为只要在服务器里复制百万个像伊利亚这样聪明的AI，就能迅速催生出超智能。然而，伊利亚对此持有一种强烈的直觉，他认为理论与实践之间存在巨大鸿沟。他质疑道，简单地复制无数个“伊利亚”，其边际价值究竟能增加多少？他坚信，真正有价值的并非数量上的复制，而是思想上的差异性。

他指出，当前大型语言模型（LLMs）之间惊人的相似性，即使它们由不同公司发布，甚至可能在看似不重叠的数据集上训练，也印证了这一点。伊利亚认为，这种同质化的根源在于预训练阶段，因为大多数模型都使用了相似的预训练数据。真正的分化和多样性，往往在强化学习（RL）和后训练阶段才开始显现，因为不同的团队会采用不同的RL训练方法。

接着，对话转向了“自我博弈”（self-play）这一概念。伊利亚曾暗示，自我博弈可能是一种仅通过计算而非数据来创建模型的方式，这在数据成为瓶颈时显得尤为诱人。然而，他也坦承，过去的自我博弈形式过于狭隘，它擅长培养谈判、冲突解决、社交技巧和策略制定等特定技能，但无法涵盖更广泛的智能需求。不过，自我博弈并未消失，而是以新的形式找到了归宿，比如辩论系统、证明者-验证者机制，以及将LLM作为裁判，鼓励其发现错误。这些都是代理之间更普遍竞争的特殊形式。

伊利亚进一步阐述，竞争的自然反应就是寻求差异化。如果让多个AI代理共同解决一个问题，并允许它们相互审视彼此的工作，那么它们就会倾向于采取不同的方法，避免重复。这种机制能够有效激励多样化的思维和解决方案。

最后，话题深入到“研究品味”这一抽象概念。作为深度学习领域公认的“品味最佳”研究者之一，伊利亚被问及如何形成那些改变世界的想法。他分享了自己的心得：他的研究深受一种“AI应如何存在”的美学所指引，这种美学源于对人类思维的“正确”理解。他举例说，人工神经元的灵感直接来源于大脑，因为它抓住了大脑的本质——数量庞大的神经元和局部学习规则。分布式表征、从经验中学习，这些都是他认为的“根本性”原则。

伊利亚强调，他追求的是美、简洁、优雅，以及对大脑的正确启发，这些元素必须同时存在。这种“自上而下”的信念，是他面对实验结果矛盾时的精神支柱。当数据似乎与预期不符时，这种信念能帮助他判断是代码存在bug，还是方向根本错误。他坚信“事情就该是这样”，这种信念支撑着他不断前行，直到找到解决方案。

