# Chapter 1: 谷歌双星：AI先驱的早期足迹与技术洞察

今日，我们有幸与谷歌首席科学家杰夫·迪恩（Jeff Dean）和AI革命的关键人物诺姆·沙泽尔（Noam Shazeer）进行了一场深度对话。杰夫在谷歌的25年职业生涯中，参与了无数现代计算的变革性系统，从MapReduce、BigTable、TensorFlow到AlphaChip，如今更是Gemini项目的核心领导者之一。而诺姆，作为Transformer、Mixture of Experts等现代大型语言模型（LLM）核心架构和技术的发明者或共同发明者，无疑是当前AI革命的幕后推手。他们二人与另一位同事共同领导着谷歌DeepMind的Gemini项目。

当被问及在谷歌早期，他们是否曾对公司的一切了如指掌时，诺姆回忆起2000年底加入谷歌的情景。那时，公司实行导师制，他被分配了一位无所不知的导师——而这位导师正是杰夫。杰夫笑着补充道，并非所有谷歌员工都无所不知，只是他恰好写下了大部分代码。他分享了公司成长的几个阶段：从最初的25人，到能记住所有人的名字；再到后来，虽然记不住所有人的名字，但至少知道所有软件工程师；接着是知道所有项目；直到有一天，你收到一封邮件说“Project Platypus”即将上线，你却一无所知，只能惊叹于公司的蓬勃发展。他强调，即使在高层次上，了解公司动态并建立广泛的人脉网络，对于解决问题至关重要。

谈及加入谷歌的经历，杰夫表示是他主动联系了谷歌。而诺姆的故事则更为曲折。1999年，他在招聘会上看到了谷歌，当时他以为这已是一家庞然大物，加入已无意义，因为他认识的每个人都在使用谷歌（这可能与他当时在伯克利读研有关）。他曾多次辍学，但最终在2000年，他抱着试试看的心态投递了简历，因为谷歌是他最喜欢的搜索引擎。面试时，他被一群聪明人、有趣的工作以及墙上那张每日搜索查询量呈指数级增长的蜡笔图所吸引。他心想：“这些人一定会非常成功，而且有很多有趣的问题可以解决。”于是他决定先在这里工作一段时间，赚够钱后就能随心所欲地投入AI研究。他笑着说，这完全按照他的计划实现了，因为谷歌最终成为了一个绝佳的AI研究平台。他早在1999年就对AI充满热情，认为谷歌“组织全球信息，使其普遍可访问和有用”的宏伟愿景，本身就需要先进的AI技术来实现。

对话转向了摩尔定律在过去二三十年间对系统设计和项目可行性的影响。杰夫指出，二十年前，硬件每18个月就能翻倍，开发者无需过多干预。但近十年来，通用CPU的性能提升放缓，制造工艺改进周期变长，多核处理器架构的提升也不再像以前那样显著。然而，与此同时，机器学习加速器（如TPU和专注于ML的GPU）的出现，使得现代计算能够实现极高的性能和效率，这与传统的C++代码运行模式截然不同。诺姆补充说，算法正在追随硬件的脚步。如今，算术运算变得极其廉价，而数据传输则相对昂贵得多。深度学习的崛起正是基于这一事实：它主要由矩阵乘法构成，其运算量（N的三次方）远大于数据传输量（N的平方）。杰夫强调，正是这种转向专门为低精度线性代数设计的硬件（如TPU），才促使算法去利用这些优势。

他们进一步探讨了“机会成本”的概念，认为芯片设计应充分利用晶圆面积，填充更多的算术单元，即使是低精度的。这需要算法和数据流的相应改变。诺姆指出，如果内存成本下降幅度大于算术成本，AI可能会呈现出完全不同的面貌，或许会更像20年前的AI，但方向相反，更多地依赖于对超大内存的查找。杰夫回忆起2012年他将诺姆“哄”回谷歌大脑团队的经历，诺姆也打趣说他似乎每12年就会重返谷歌（2000年、2012年、2024年）。

展望未来的TPU版本，他们正在考虑如何更好地整合算法。一个显著趋势是模型量化和低精度计算的进步。从TPUv1开始，他们就尝试用8比特整数进行模型服务，如今，训练和推理的精度已降至INT4、FP4，甚至有人尝试用1比特或2比特进行量化。杰夫解释说，这必须是算法和硬件的协同设计。如果算法设计师不了解低精度能带来的巨大性能提升，他们自然会拒绝，因为这会增加风险和麻烦。但当芯片设计师告诉他们，通过量化，模型速度可以提升三倍时，即使有些麻烦，也必须接受。最后，杰夫回顾了自己1990年关于反向传播的本科毕业论文，以及2007年他们训练的2万亿词元N-gram语言模型。他当时在并行计算课程中接触到神经网络，并实现了两种并行化反向传播训练的方法（模型并行和数据并行），在32处理器超立方体机器上进行了对比，对神经网络的抽象能力感到非常兴奋。

# Chapter 2: 从反向传播到猫咪神经元：AI先驱的早期探索

在人工智能的早期探索中，量化技术被视为提升系统效率的关键。它虽然可能带来一些“恼人”的细节处理，但能显著提高吞吐量与成本比，让模型运行速度快上三倍，这无疑是工程师们必须面对的甜蜜负担。

杰夫·迪恩和诺姆·沙泽尔的职业生涯中，不乏与当今生成式AI惊人相似的早期工作。早在1990年，杰夫的本科毕业论文就深入探讨了反向传播算法。他回忆道，当时在一门并行计算课程中首次接触到神经网络，便被其抽象的魅力深深吸引。为了完成荣誉论文，他与教授决定实现两种并行化反向传播训练神经网络的方法：模型并行和数据并行。他将这些方法命名为“模式分区”等有趣的名字，并在一台拥有32个处理器的超立方体机器上进行了实践。他曾天真地以为，32个处理器足以训练出“非常棒”的神经网络，但事实证明，要让神经网络真正解决实际问题，还需要大约一百万倍的计算能力。直到2008年至2010年间，随着摩尔定律的持续演进，计算能力才终于达到足以让神经网络发挥作用的水平，杰夫也正是在那时重新投入到神经网络的研究中。

而在那之前，2007年发生了一件里程碑式的事件。当时，谷歌的机器翻译研究团队在弗朗茨·奥赫的带领下，每年都会参加DARPA（美国国防高级研究计划局）的翻译竞赛，将中文和阿拉伯语翻译成英文。谷歌团队以显著优势赢得了比赛，杰夫兴奋地询问弗朗茨何时能将这项技术推向市场。然而，弗朗茨的回答却令人沮丧：“我们无法发布，因为它翻译一句话需要12个小时，根本不实用。” 杰夫对此感到震惊，并决定亲自介入。他发现，该系统在翻译每个词时，需要对一个庞大的语言模型进行多达10万次的磁盘寻道操作，这无疑是其速度缓慢的症结所在。于是，杰夫与团队花了大约两三个月的时间，设计了一种内存中压缩的N-gram数据表示方法。N-gram模型本质上是统计特定N个词序列在一个巨大语料库中出现的频率。当时，他们处理了高达2万亿个词，并决定使用五元语法（five-grams），即统计每五个词序列的出现频率。杰夫构建了一个数据结构，使其能够在200台机器上将所有这些数据存储在内存中，并通过批处理API实现并行查询。这一改进让翻译时间从漫长的12小时骤降至惊人的100毫秒！

这不禁让人想起那些关于杰夫·迪恩的“查克·诺里斯式”趣闻，比如“对杰夫·迪恩来说，NP问题就是‘没问题’（no problemo）”，以及“光速曾是每小时35英里，直到杰夫·迪恩在一个周末将其优化”。这些玩笑虽然夸张，却也从侧面反映了他在优化方面的非凡能力。

回想起来，这种通过分析词语之间的关系来构建整个互联网潜在表示的想法，正是大型语言模型（LLM）和Gemini的雏形。当时，这仅仅被视为一种翻译技术，还是预示着一种全新的范式？杰夫认为，一旦这种大型语言模型服务被用于翻译，很快就被扩展到其他应用，比如自动补全功能，以及诺姆·沙泽尔在2001年开发的拼写纠正系统。诺姆的拼写纠正系统令人惊叹，他曾向全公司展示，即使输入“scrumbled uggs Bundict”这样的错误拼写，系统也能准确识别出“scrambled eggs benedict”。

然而，在开发这些系统时，他们是否预见到，随着模型复杂度的提升，从考虑五个词到一百个、一千个词，这种潜在表示最终会演变为“智能”？杰夫坦言，他从未觉得N-gram模型会“席卷世界”或成为人工智能本身。当时，很多人对贝叶斯网络更感兴趣。但诺姆则认为，早期的神经网络语言模型展现出的魔力，以及其“预测下一个词的概率分布”这一简单而又拥有几乎无限训练数据（如网络文本）的特性，使其成为世界上最好的问题。这种从世界观察中学习的能力，最终被证明是“AI完备”的，即如果能很好地解决它，就能解决几乎所有问题。

关于伟大思想的诞生，是“在空气中弥漫”的必然，还是从某个切线方向偶然撷取？杰夫和诺姆都认为，这两种情况兼而有之。一些想法，如神经图灵机、注意力机制等，确实“在空气中弥漫”，但总需要某个团队去将其实现。许多想法是现有研究的融合与创新，结合了已有的灵感和尚未解决的新问题，最终催生了突破性的研究成果。

杰夫回忆起谷歌大脑团队早期的一个关键时刻。当时，团队致力于构建能够训练超大型神经网络的基础设施。由于数据中心还没有GPU，他们利用大量CPU协同工作，通过模型并行和数据并行训练大型神经网络。他们在一个系统上进行了无监督学习，使用了1000万帧随机选择的YouTube视频，通过重建高层表示来构建无监督表示。这个系统在2000台计算机、16000个核心上运行。一段时间后，模型在最高层竟然能够构建出一种表示，其中一个神经元专门对猫的图像产生兴奋，尽管它从未被告知“猫”是什么。同样，其他神经元也对人脸、行人背影等产生反应。这种从无监督学习原理中构建出高级表示的能力令人惊叹。随后，他们利用这个模型在ImageNet 20000类别挑战赛中取得了巨大成功，相对改进了60%的最新技术水平。这个神经网络比之前训练的模型大了50倍，并取得了优异的成绩，这让杰夫坚信，扩大神经网络的规模是一个正确的方向，值得继续推进。

这些例子完美诠释了AI系统如何融入谷歌“组织信息”的核心使命。AI通过发现信息和概念之间的关系，帮助用户更快地获取所需信息。如今，随着BERT等AI模型在谷歌搜索中的应用，它们不仅擅长信息检索，更能编写整个代码库，完成实际工作，这已超越了单纯的信息检索范畴。那么，如果谷歌正在构建通用人工智能（AGI），它是否仍然是一家“信息检索公司”？杰夫认为，谷歌是“组织世界信息”的公司，这比信息检索更广泛。它现在是“组织并根据你的指导创造新信息”的公司。无论是帮助用户撰写给兽医的信件，还是总结视频内容，亦或是处理多模态数据和编程问题，这些模型的能力都令人兴奋，并且正在快速提升。谷歌的未来，将是理解世界所有模态的信息，并将其转化为有用的洞察，帮助人们完成各种任务，无论是娱乐、解决复杂问题，还是处理多模态数据和编程挑战。

# Chapter 3: 谷歌使命新篇章：从信息组织到智能创造

在一次深入的对话中，关于谷歌未来走向的讨论如火如荼。主持人抛出了一个核心问题：如果谷歌正在构建通用人工智能（AGI），它是否还能仅仅被定义为一家“信息检索公司”？

杰夫·迪恩和诺姆·沙泽尔对此给出了一个富有远见的答案。他们认为，谷歌的使命早已超越了简单的信息检索，而是演变为“组织世界信息，并根据用户指导创造新信息”。这意味着，未来的AI系统将不再仅仅是信息的图书馆管理员，更是能动地为用户完成实际工作的智能助手。想象一下，你只需描述症状，AI就能为你起草一封给兽医的信；或者，它能分析一段视频，每隔几分钟就生成一份内容摘要。这不仅仅是文本处理，更是对世界各种模态信息的深刻理解，无论是人类可读的文本、视频、音频，还是自动驾驶汽车的激光雷达数据、基因组信息，甚至是复杂的健康数据。AI的目标是将这些信息提取、转化，为人们提供有用的洞察，并帮助他们完成各种任务，从寻求娱乐到解决复杂问题，甚至处理多模态内容或编程难题。

诺姆激动地指出，如果说组织信息是一个万亿美元的机会，那么让这些系统真正为人类“做事情”，比如编写代码或解决人类自身难以解决的问题，则是一个万万亿美元的机遇。这不仅仅是金钱的堆积，更是为世界创造巨大价值的潜力。杰夫也对此深表认同，他认为，为了实现这一目标，模型的能力必须快速、灵活地提升。

讨论很快转向了当前AI面临的关键技术挑战——“长上下文”问题。谷歌搜索拥有整个互联网的索引，但其检索是“浅层”的；而语言模型虽然上下文有限，却能进行“深度思考”，这简直是“黑魔法”。杰夫解释说，当前模型的幻觉和事实性问题，部分源于训练数据被“搅拌”成参数后变得模糊。而模型输入窗口中的信息则因Transformer的注意力机制而“清晰锐利”。虽然现在的模型可以处理数百万个token的上下文（相当于数百页PDF或数十小时的视频），但理想状态是模型能“关注”到数万亿个token，比如整个互联网，甚至用户的全部个人信息（在获得许可的前提下）。然而，朴素的注意力算法是二次方的，这使得直接扩展到万亿token成为巨大的计算挑战，因此需要大量的创新算法近似。

谈到代码生成，杰夫透露，谷歌已经将内部代码库用于Gemini模型的进一步训练，以提高内部开发者的效率。目前，谷歌代码库中约有25%的新增字符是由AI编码模型在人工监督下生成的。他们设想，在未来一两年内，研究人员与AI的互动将发生革命性变化：AI可以根据高层级的想法或论文描述，自动生成实验代码，甚至探索新的架构方向，从而将研究生产力提升10倍甚至100倍。诺姆甚至提到一个外部用户用AI模型在C语言中实现了一个SQL数据库系统，这展示了AI作为“自主软件工程师”的巨大潜力。

面对未来可能出现的“数百万AI员工”同时工作的场景，杰夫和诺姆认为，管理这种并行化可能类似于管理大量顶尖机器学习研究人员的协作。AI可以进行大规模的探索性工作，尝试数百万种想法来寻找突破，因为现代实验室的计算能力已是Transformer训练时的百万倍。这种“并行搜索”有望加速科学发现，甚至可能实现“每天一个突破”。他们相信，通过在小规模问题上验证大量想法，再将有前景的方案放大，将是未来研究的有效路径。

最终，他们描绘了一个宏伟愿景：让世界上任何信息都能被任何人使用，无论他们讲何种语言。这意味着，无论你使用数千种语言中的哪一种，任何内容、任何视频都能被你理解和使用。这虽然尚未完全实现，但已清晰可见于地平线上，预示着一个真正无界的信息与智能时代即将到来。

# Chapter 4: 智能浪潮：突破、规模与算法的秘密

故事从一个残酷的现实开始：即使是最杰出的头脑，一项开创性的AI研究成功的几率也微乎其微，大约只有2%。大多数尝试都注定失败。然而，如果能尝试一百次、一千次，甚至一百万次，那么总有可能触及到某个惊人的发现。幸运的是，我们现在拥有前所未有的计算能力。现代顶尖实验室所拥有的计算资源，可能比训练Transformer模型所需的计算量还要多出一百万倍。

这引出了一个引人深思的问题：假设全球有大约一万名AI研究人员（实际上，仅NeurIPS会议就有1.5万人参加，总数可能高达十万），每年他们中产生Transformer级别突破的几率是10%。那么，如果这个研究社区的规模扩大一千倍，通过并行探索更好的架构和技术，我们是否能实现每天都有一个突破？这听起来令人振奋，但机器学习研究的实际情况又是如何呢？

人们常常倾向于将实验运行到最大规模，这似乎是人类的一种本能。然而，更有效的方法或许是先在一个千分之一规模的问题上验证十万个想法，然后只将那些最有前景的方案放大。

一个可能被世界低估的趋势是：虽然人们普遍意识到，将模型规模扩大一百倍，其计算难度会呈指数级增长，例如从Gemini 2到Gemini 3的飞跃。但与此同时，Gemini 3这样的模型本身也在不断提出新的架构理念，进行尝试，并从中学习。这种持续的算法进步，使得训练下一代模型变得越来越容易。这种反馈循环能走多远？

事实上，模型代际间的改进，部分源于硬件升级和规模扩大，但同样重要甚至更重要的是，它们由重大的算法创新、模型架构调整以及训练数据混合方式的改进所驱动。这些进步使得模型在每单位浮点运算（flop）下表现得更好。

如果能实现想法的自动化探索，我们将能验证更多的理念，并将其融入到下一代模型的实际生产训练中。这无疑将带来巨大的帮助，因为目前这项工作主要由众多杰出的机器学习研究人员手动完成：他们审视大量想法，筛选出在小规模下表现良好的，再在中等规模下测试，最终将其引入大规模实验，并决定将其添加到最终的模型配方中。如果能通过自动化搜索过程，让这些研究人员只需温和地引导，而不是亲手“照看”大量实验，那么整个过程的速度将提升一百倍，这将是革命性的。

当然，最大规模的实验仍然无法加速。这些“N=1”的实验，依然需要一群顶尖的专家齐聚一堂，深入审视，找出成功或失败的根本原因。对于这些，更强大、更先进的硬件仍然是不可或缺的解决方案。

# Chapter 5: 智能计算的潮汐：从即时推理到分布式训练的未来展望

在AI的广阔天地中，计算的需求如同潮汐般变幻莫测。当用户焦急等待响应时，即时推理（inference）的计算需求便如奔涌的浪潮，需要高度专业化的解决方案来确保毫秒级的低延迟。然而，并非所有推理任务都如此紧迫。想象一下，一个AI工具被赋予一项宏大的使命：“去研究可再生能源的历史，分析风能、太阳能等技术的成本趋势，并整理成一份八页的报告，附上五十条参考文献。”这样的任务，用户并不需要一秒钟内完成，它可以在后台默默运行一两分钟，最终呈现出令人惊叹的成果。这便是异步推理的魅力，它为我们带来了全新的用户界面挑战：当用户同时启动二十个这样的后台任务，而其中一些任务又需要额外信息（比如“我找到了去柏林的航班，但没有直飞，您介意转机吗？”），如何优雅地处理这种人机交互，让AI在获取信息后继续其后台工作，将是一个引人入胜的课题。

为了提升推理效率，研究者们正绞尽脑汁。传统的Transformer模型在训练时能将序列长度作为批次处理，但在推理时，由于一次只生成一个词元（token），效率便大打折扣。为此，一种巧妙的算法改进——“草稿模型”（drafter models）应运而生。它利用一个小型语言模型快速预测出几个词元，然后将这些预测结果交给大型模型进行验证。如果大型模型同意前三个词元，那么它就能一次性“跳过”这三个词元，实现并行计算，而非逐个生成，从而大大缓解了单词元解码的瓶颈。大型模型在这里的角色，更像是一个高效的“验证者”。

计算的版图也在不断扩展。关于核电站供电能力不足以支撑单个AI园区巨大计算需求的讨论不绝于耳，那么，训练模型是否可以更加分布式？答案是肯定的。我们已经开始实践多数据中心训练，例如Gemini 1.5的训练就横跨了多个大都市区，通过高带宽、相对高延迟的网络连接这些数据中心。对于训练而言，每一步通常需要几秒钟，因此几十毫秒的网络延迟影响不大，关键在于带宽能否在每一步内同步所有模型参数并聚合梯度。

在训练方式上，异步与同步之争也从未停歇。早期的CPU时代，由于计算资源有限，我们不得不采用异步训练：每个模型副本独立计算，然后将梯度更新异步发送到中央系统。这种方式虽然让模型参数“摇摆不定”，理论保证不那么严谨，但在实践中却能有效扩展。然而，当TPU芯片和Pod出现后，其内部芯片间惊人的带宽，以及数据中心和跨都市区网络的高速连接，使得完全同步训练成为可能。同步训练的优势在于实验结果的可复现性，这让研究人员倍感欣慰，因为他们不再需要担心“网络爬虫是否在同一台机器上运行”这类外部因素干扰实验。尽管如此，随着模型规模的进一步扩大，未来我们可能仍需在系统中引入更多异步性，并探索如何实现“异步但可复现”的理想状态，例如通过记录操作序列来确保结果的重现。

然而，规模化并非没有代价。当系统变得庞大，各种意想不到的“敌人”便会浮现：可能是量化过度，可能是数据问题，甚至是代码中那些看似无害的bug。神经网络对噪声的容忍度极高，这使得问题难以察觉——一个bug可能让模型变差，也可能让它变得更好，甚至有时会带来意想不到的发现。调试过程也因此变得复杂。研究通常从小型实验开始，快速验证新想法。一旦有前景，便逐步扩大规模，并将多个改进叠加起来。然而，现实是残酷的：大约一半的情况下，这些看似独立的改进并不能很好地协同工作，它们之间可能存在意想不到的交互，需要深入调试才能理解。如何在追求极致性能的同时，保持系统和代码库的简洁性，是研究团队面临的永恒挑战。

展望未来，AI的发展速度引发了深刻的思考。我们正处于一个关键时期：AI是会像过去几十年那样缓慢演进，还是会因为强大的反馈循环而实现指数级飞跃？许多人，包括对话者本人，都倾向于后者——一个充满加速的未来。模型能力正以惊人的速度迭代提升，未来两到三代模型将能够将复杂任务分解成数百甚至数千个子任务，并以极高的准确率完成。这意味着AI将从“优秀的机器学习研究员”迅速跃升至“超人智能”。这种加速发展将对社会产生巨大影响：一方面，AI有望在教育、医疗等领域带来革命性进步，让信息触手可及；另一方面，我们也必须警惕其潜在风险，如虚假信息传播和自动化网络攻击。因此，制定并遵循负责任的AI原则，理解模型能力，并部署必要的安全防护措施，对于确保AI造福人类至关重要。

# Chapter 6: AI的飞跃与责任：从模型进化到智能未来

对话者们沉浸在对人工智能未来趋势的探讨中，语气中既有兴奋，也带着一丝审慎。他们指出，当前的AI模型正以惊人的速度迭代，每一代都比前一代强大得多，这种进步的势头在未来几代内似乎不会减缓。想象一下，一个现在只能将简单任务分解成10个子部分并以80%的准确率完成的模型，在两三代之后，将能够把一个高度复杂的任务拆解成上百甚至上千个细微环节，并以90%的成功率完成。这无疑是模型能力的一次质的飞跃，让人们不得不认真思考这一领域正在发生的深刻变革。

这种强大的AI能力，无疑将在各个领域大放异彩。对话者们对AI在教育和医疗领域的潜力充满期待，认为它能让信息触手可及，惠及每一个人，极大地改善人类生活。然而，硬币的另一面也同样清晰可见：AI可能被滥用于制造虚假信息，甚至自动化攻击计算机系统。因此，建立尽可能多的防护措施、缓解机制，并深入理解模型的能力边界，变得至关重要。谷歌在这方面秉持着一套负责任的AI原则，这套框架为如何在不断提升AI系统能力的同时，确保其安全、避免产生有害内容提供了指导。

然而，一个更深层次的担忧浮现出来：如果AI系统在自我优化的反馈循环中出现偏差，其目标与人类的初衷不符，那将是一个难以挽回的错误。设想一下，一个在短短几年内，甚至更短时间内，就能达到甚至超越顶尖AI研究者（如Jeff Dean或Noam Shazeer）水平的AI，并且有数百万个这样的“邪恶版本”在运行，那将是比核战争更可怕的灾难。这种“智能爆炸”一旦失控，后果不堪设想。

面对这种潜在的风险，对话者们认为，人类的角色并非袖手旁观，而是要积极“塑造AI”的发展方向。这不仅仅是一个技术问题，更是一个工程挑战：如何设计出安全的系统？他们将其比作航空软件开发，一个在风险极高的领域中，通过严谨开发确保系统安全可靠的典范。一个令人鼓舞的观点是，分析文本似乎比生成文本更容易，这意味着AI模型本身可能具备分析其自身输出，识别潜在问题或危险的能力，从而成为解决控制问题的关键。

当然，这需要人类的持续监督。即使AI能探索算法研究的新思路，最终的决策权仍应掌握在人类手中，确保每一次系统核心代码的修改都经过审慎评估。通过API或用户界面暴露模型能力，并设定使用边界，也是确保AI行为符合预设标准的重要手段。目标是赋能人类，但绝不能允许AI被用于制造“百万邪恶软件工程师”来伤害他人，这是不可逾越的底线。

话题随后转向了轻松的回忆。在过去25年里，最令人怀念的时光莫过于谷歌早期的四五年。那时，作为少数几个负责搜索、抓取和索引系统的人之一，亲眼见证流量的爆炸式增长，将索引更新频率从每月一次提升到每分钟一次，那种构建被数十亿人使用的产品的满足感无与伦比。而如今，与Gemini团队的合作也同样令人兴奋。在过去一年半中，模型能力的飞速提升，让团队成员充满激情。想象一下，20年前甚至5年前的人，看到今天的AI能完成如此复杂的任务，一定会觉得不可思议，这种成就感令人陶醉。

这种协作精神在谷歌的“微型厨房”里体现得淋漓尽致。在名为“梯度天蓬”（Gradient Canopy）的新大楼里，微型厨房不仅提供咖啡和零食，更是一个充满活力的交流空间，五十多张办公桌让研究人员可以随时随地进行面对面的思想碰撞，分享灵感，解决问题。当然，全球各地的团队成员也通过上百个聊天室紧密联系，伦敦团队的最新进展，数据团队面临的挑战，都在这些虚拟空间中实时分享，形成了一个高效而充满活力的全球协作网络。

展望未来，对话者们再次回到对计算需求的预判。谷歌在早期就预见到对TPU等专用计算硬件的巨大需求，这种前瞻性思维令人称奇。到2030年，随着AI模型成为各项服务的核心，持续的推理和未来的训练将需要天文数字般的计算资源。如果提升模型质量的一个方法是增加推理计算量，那么一次简单的文本生成请求可能就会消耗当前50到1000倍的计算资源。同时，随着全球用户对聊天式AI界面的认知度从目前的10%-20%提升到100%，并更频繁地使用，这将带来一到两个数量级的需求增长。模型本身的规模扩大，又会带来一到两个数量级的计算需求。最终，我们需要的将是极其高效的推理硬件。

想象一下，到2030年，AI可能成为无处不在的个人助手，集成在眼镜中，能访问所有数字信息；或者像总统的耳麦，实时提供建议，解决问题。AI将分析周围的一切，为人类提供有益的洞察。届时，全球GDP中将有多少比例用于AI？这无疑是一个巨大的、持续增长的市场，预示着一个由AI深度赋能的未来。

# Chapter 7: AI计算的未来：从个人助手到模块化智能体的演进

对话伊始，我们便被带入了一个令人惊叹的未来图景：AI计算的需求正以惊人的速度飙升。如今，生成少量文本所需的计算量，相比过去已暴增50到1000倍，即便输出内容相同。这仅仅是冰山一角。目前，全球仅有10%到20%的电脑用户领略过聊天式AI界面的魔力，但随着这一比例逼近100%，且人们对AI服务的依赖日益加深，计算需求还将再攀升一到两个数量级。更庞大的模型、更复杂的任务，将带来额外的数量级增长，对推理计算的效率提出了前所未有的要求，呼唤着极致高效的专用硬件。

展望2030年，AI将如何融入我们的生活？它或许会成为你眼镜中的个人助手，洞察周遭一切，连接你的数字信息与全球知识库。又或者，它会是国家领导人耳畔的实时智囊，在内阁会议上提供即时建议，解决难题，给出精辟见解。想象一下，一个AI能分析你周围的一切，为你发掘潜在的价值。如果多投入一倍的计算资源，你的AI助手就能聪明5到10个IQ点，你是否愿意每天多花10美元，换取一个更智能的伙伴？它不仅是生活助手，更是工作利器，能将一个“10倍工程师”的能力提升至“100倍甚至1000万倍”。

从第一性原理出发，人们将愿意把全球GDP的相当一部分投入到AI中。而AI，凭借其“人工工程师”大军，将推动全球GDP实现前所未有的飞跃，甚至可能比今天高出两个数量级。届时，无限能源和碳排放问题或许已迎刃而解，数百万乃至数十亿的机器人将为我们建造数据中心。太阳的能量是如此浩瀚，而每个人所用的AI计算量也将达到天文数字。为了让这些强大的AI能力普惠大众，拥有极其廉价的硬件平台至关重要。通过硬件与模型协同设计，我们有望大幅提升AI的效率，降低其使用成本。

谈及AI模型的持续演进，一个引人深思的概念是“持续学习”——模型无需从头开始，而是随着时间推移不断自我完善。对话者是稀疏模型（如“专家混合模型，Mixture of Experts”）的忠实拥趸。他解释道，这类模型中，不同部分（即“专家”）各有所长，例如一个擅长数学，另一个精通识别猫咪图片。这使得模型在推理时既能拥有庞大容量，又能保持高效，因为每次只激活一小部分专家。然而，目前的专家混合模型结构仍过于规整。未来的愿景是构建更“有机”的结构，让模型的各个模块能够独立开发。设想一下，一群专注于特定语言或编程语言的小团队，可以独立训练出高质量的模块，再将其无缝接入一个更大的模型，从而提升其在东南亚语言或Haskell代码推理方面的能力。

这种模块化方法带来了巨大的软件工程优势，将复杂的AI开发从“整体式”流程分解为更易管理的组件。这意味着Google内部的数百个团队，乃至全球各地的开发者，都能协同工作，共同改进模型。这正是持续学习的一种高级形式。这种“即插即用”的模式，允许我们像外科手术般对模型进行“切入”或“生长”，升级某个模块而无需废弃整个系统。虽然从头训练有助于受控实验和快速进展，但通过模块化的版本控制系统，我们也能实现类似的效果，从而让AI研究变得更便宜、更快，并更易于并行化。

Google的Pathways系统正是为实现这一愿景而生，它支持这种“扭曲、奇特”且能异步更新不同部分的模型结构。尽管Gemini模型已在Pathways上训练，但其部分高级功能尚未完全启用。此外，“蒸馏”技术被视为一种强大的工具，能将大型、复杂的模型转化为更小、高效的版本，尤其适用于低延迟推理。这种蒸馏过程甚至可以在模块层面持续进行：一个大型模块不断将其知识蒸馏给一个小型版本，待小型版本成熟后，大型版本便可被替换，并增加新的参数容量以学习更多知识，如此循环往复。这不仅能实现推理的动态扩展，还能根据任务难度将请求路由到不同大小的模块。

对于专家混合模型中“专家”的可解释性，尽管公众研究常认为难以理解，但作为专家混合模型的发明者，对话者指出，早期的专家模型相对容易理解，例如，某个专家可能专注于“圆柱形物体”，另一个则擅长处理“日期”。虽然运行时有学习型路由器负责调度，无需人类理解每个专家，但模型可解释性研究（如Anthropic的Chris Olah发现的“金门大桥神经元”）仍能应用于专家层面。最终，只要模型的整体表现优异，我们不一定需要理解每一个神经元的具体功能。这种模块化、稀疏化的模型架构，虽然每次查询只激活一小部分参数，但整个模型仍需加载到内存中，这使得Google投资的由数百甚至数千个TPU组成的Pod集群，在未来AI基础设施中将发挥不可估量的价值。

# Chapter 8: 智能巨兽的诞生：有机进化与模块化未来

在AI的宏大叙事中，我们常常纠结于是否需要理解模型中每一个神经元的具体作用。然而，对话者们提出了一种更为务实的视角：如果一个AI系统的整体输出和特性都表现出色，那么我们或许不必深究其内部每一个微观细节。这正是深度学习的魅力所在——它无需我们手工设计每一个特征，而是通过学习自行涌现出强大的能力。

然而，这种能力的实现并非没有挑战。当前，一个拥有数百亿参数的模型可能只需几块GPU就能运行。但随着“专家混合模型”（Mixture of Experts, MoE）的兴起，情况变得复杂起来。尽管每次查询可能只激活模型总参数的一小部分，但为了高效运行，整个模型——包括所有专家——都必须加载到内存中。这颠覆了人们对MoE的普遍误解，即未使用的专家无需占用内存。实际上，为了在大型批处理中实现效率，所有专家都必须随时待命。正因如此，Google在TPU Pods上的巨大投入显得尤为宝贵，这些由数百甚至数千个TPU组成的集群，正是支撑这种庞大而复杂的MoE架构的关键基础设施。

未来的MoE模型将更加精妙。目前的专家通常计算成本相近，处理的批次大小也大致相同，以维持负载均衡。但设想一下，如果专家们的计算成本能相差百倍甚至千倍，有些路径深入多层，有些则仅需单层甚至跳过连接，那将是怎样一番景象？为了应对这种复杂性，推理过程将需要异步进行，让系统能够根据不同请求的特点，灵活地调度计算资源。Google的Pathways系统正是为此而生，它能够编排这些可变成本的组件，让模型根据具体示例选择性地激活子集。

这种技术趋势也带来了深远的影响：未来，只有像Google这样拥有庞大数据中心和尖端技术的公司，才可能承载并运行这样一个“单一的智能巨兽”。这个巨兽将不再是简单的模型副本，而是一个能够根据负载特性，智能地复制常用专家（比如处理数学问题的专家），甚至将不常用专家（比如塔希提舞蹈专家）分页到DRAM的有机系统。它将成为Google所有产品的核心，无论是搜索、图片还是Gmail，都可能通过这个庞大而专业的专家混合体来提供服务。Gemini模型已经开始展现这种潜力，通过指令而非微调，在Google的各项服务中发挥作用。

更令人兴奋的是，这种架构将支持高度的模块化和定制化。一个巨大的基础模型可以根据不同场景添加定制模块，甚至实现数据访问限制。例如，Google内部员工可以使用基于内部数据训练的模块，而这些模块对外部用户不可见；或者YouTube的数据训练的模块只能在YouTube产品中使用。这种数据控制的模块化，将极大地提升AI的灵活性和安全性，甚至可以为每个用户创建专属的“个人模块”，存储其私有数据。

当然，实现这一切并非易事，系统工程和机器学习的挑战并存。但其潜在的优势——更高的质量、更快的并行开发速度——足以激励我们深入探索。想象一下，这个“智能巨兽”不仅能自我构建，还能自我优化，不断告诉我们如何让它变得更好，甚至实现增量式的有机成长。这种灵感来源于生物大脑的结构，它拥有不同专业区域，并能根据需求有机地增长专业能力。模型与硬件的连接也应如此，芯片内部连接紧密，跨芯片连接稀疏，跨数据中心连接更稀疏，只传递最关键的信息，且这种连接模式应由硬件特性有机地塑造。

未来的AI增长模式将不再是简单地增加更多实例（横向扩展），而是通过激活“巨兽”内部不同的、更复杂的模式（纵向深化），以满足从简单到极其困难任务的巨大计算需求差异。一个请求可能需要迭代地通过模型，根据中间结果决定是否激活更多部分。尽管这种复杂系统部署起来充满挑战，但“模型蒸馏”技术提供了解决方案。我们可以定期将这个庞大而有机的系统蒸馏成一个针对特定任务高度优化的、高效部署的小模型。因此，我们迫切需要更快速、更高效的蒸馏技术。

此外，当前的训练方法也亟待改进。仅仅预测下一个词元，似乎未能充分挖掘每个数据点的价值。人类的学习方式远不止于此，我们阅读一章书后回答问题，从视觉数据中获取信息。我们需要更具挑战性的训练目标，比如让模型在关键时刻（如“答案是”之后）投入更多计算，或者像视觉模型那样，通过扭曲或隐藏部分信息来迫使模型从局部信息中推断整体。即使文本数据看似有限，但通过更高效的学习策略，我们仍能从现有数据中训练出远超想象的强大模型。人类仅凭数十亿词元就能掌握大量知识，这为AI的样本效率设定了一个引人深思的基准，促使我们重新思考训练目标，并更充分地利用多模态数据，以期实现更接近人类的学习效率。

# Chapter 9: AI学习的未来：从数据效率到创新哲学

对话伊始，我们深入探讨了AI模型在数据利用效率上的挑战。图像处理领域曾因缺乏标注数据，不得不催生出如Dropout等创新技术。尽管Dropout最初为图像而生，但在文本模型中却鲜有应用。有人提出，若能让模型在海量文本数据上进行百次迭代并辅以Dropout，或许能显著提升模型能力并避免过拟合，但这无疑是巨大的计算开销。尽管外界担忧文本数据即将耗尽，但我们坚信，现有文本数据仍有巨大潜力可挖，足以训练出远超当前水平的模型。毕竟，人类仅凭有限的输入（约十亿个token）就能掌握诸多技能，这本身就是一个引人深思的参照点。

关于AI与人类在学习效率上的巨大差异，我们提出了两种视角：一是认为大型语言模型（LLMs）仍有巨大的进步空间，若能达到人类的样本效率，将带来数量级的提升；二是反思，或许AI的学习机制与人类根本不同。那么，如何才能让AI模型像人类一样高效学习呢？

我们认为，关键在于改变训练目标。仅仅预测下一个词元，与人类的学习方式相去甚远。人类会通读一章书，然后回答问题，这是一种更深层次的理解与应用。此外，AI对视觉数据的利用也远远不足。尽管我们已开始涉足视频数据训练，但与人类从海量视觉输入中学习的能力相比，仍是冰山一角。更重要的是，人类通过主动探索世界、采取行动并观察结果来高效学习。婴儿通过反复抓取和丢弃物品来理解重力，这种主动学习远比被动观察庞大数据集有效。我们设想，如果模型能像Gato一样，在学习过程中主动采取行动并观察相应结果，那将是巨大的飞跃。甚至，人类还能通过“思想实验”进行学习，如爱因斯坦和牛顿的例子，以及国际象棋AI通过自我对弈提升能力。这表明，AI在没有外部数据的情况下，也能在特定领域实现大量学习，甚至通过“自言自语”变得更聪明。

随后，话题转向了Google在AI研究成果发布上的策略。Noam Shazeer在2017年发布的Transformer论文，为其他公司创造了数百亿美元的市场价值。回首往昔，是否会后悔如此慷慨地分享这些核心技术？答案是肯定的“不”。我们认为，看到其他公司在Transformer基础上取得的成就，反而让我们看到了这个领域的巨大机遇，这并非一个“固定蛋糕”的市场。AI的进步将带来GDP、健康和财富等方面的数量级增长。因此，Transformer的普及是件好事。

当然，如今Google在发布研究成果时会更加审慎。这其中存在一个权衡：是立即发布所有细节，还是先将其融入产品（如Gemini模型），待产品发布后再选择性地公开研究论文，或者根本不发布？以Pixel手机的计算摄影技术为例，如“夜视”功能，我们通常会先将创新技术集成到产品中，待产品发布后再发表详细的研究论文。对于某些被认为极其关键的技术，我们可能选择不发布；而对于那些既有趣又能提升产品体验的技术，则可能在产品发布后，选择性地进行轻量级讨论，而非披露所有细节。但我们依然坚信开放出版、推动领域发展的重要性，因为这是整个社区共同受益的方式。

我们还探讨了Google在AI领域长期耕耘，但为何在某些时期，竞争对手的模型似乎更胜一筹。Google在语言模型领域有着悠久的历史，从2001年的拼写纠正，到2007年的大规模语言模型，再到seq2seq、word2vec、Transformer和BERT。甚至在ChatGPT问世之前，Google内部就有一个名为Meena的聊天机器人系统，在疫情期间为员工提供了陪伴。我们最初从搜索的角度看待这些模型，认为它们存在大量“幻觉”和不准确性，因此在事实准确性上达不到搜索的理想标准。我们低估了它们在非搜索场景下的巨大实用性，比如撰写给兽医的便条或总结文本。同时，我们也高度关注安全问题，确保模型不会产生冒犯性内容。正是这种对准确性和安全性的坚持，让我们花费了更多时间。但我们相信，这条道路是正确的，如今的Gemini模型正变得越来越强大。

最后，我们分享了在AI领域保持职业生涯长青和广度的秘诀。Jeff Dean强调了持续学习和跨领域合作的重要性。他喜欢探索新领域，与拥有不同专业知识的同事合作，共同解决问题。这种跨学科的交流不仅能拓宽个人技能，也能催生出单打独斗无法实现的创新。Noam Shazeer则强调了“谦逊”和“放弃”的能力。他认为，要敢于承认当前的工作并非最好，并乐于接受更好的想法，甚至完全放弃旧方向。他提到Google Brain曾采用一种“自下而上”的芯片资源分配模式（类似“UBI”），鼓励实验和放弃。而Gemini项目则更多采用“自上而下”的模式，虽然促进了协作，但也可能削弱了放弃不成功项目的动力。未来，我们认为需要结合这两种模式，以兼顾协作与灵活性。Jeff Dean补充说，作为领导者，他会通过分享“杰夫的古怪想法”这样的内部幻灯片，来启发团队探索新的产品方向，而非强制命令，从而激发大家的积极性和创造力。

