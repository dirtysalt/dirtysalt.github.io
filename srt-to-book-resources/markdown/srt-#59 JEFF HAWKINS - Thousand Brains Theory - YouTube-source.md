# Chapter 1

in spite of the steady accumulation of detailed knowledge

how the human brain works is still profoundly mysterious

[music] the ultimate goal of neuroscience is to learn how the brain gives rise to human intelligence

and what it means to be intelligent

understanding how the brain works is considered one of humanity's greatest challenges

i got into this field because i just was curious as to who i am

you know how you know

how do i think what's going on in my head when i'm what i'm thinking

what does it mean to know something

you know

i can ask what it means for me to know something independent of how i learned it from you

or from someone else or from society

what does it mean for me to know that i have a model of you in my head

what does it mean to know

i know what this microphone does and how it works physically

even when i can't see it right now

how do i know that

what does it mean

how do the neurons do that at the fundamental level of neurons and synapses

and so on

those are really fascinating questions

the neuroscientist francis crick observed that scientists have been collecting data on the brain for decades

they knew a lot about the workings of the brain

but they hadn't converged on any meaningful theory of how the brain worked

how intelligence emerges from low level cells in your head is still a profound mystery

to be intelligent

the brain has to learn a great many things about the world

to understand how the brain creates intelligence

we need to figure out how the brain learns a model of the world and everything in it

jeff hawkins thinks that the reality we perceive is a kind of simulation

a hallucination

a confabulation

he thinks

that our brains are a model of reality based on thousands of information streams originating in the senses in our body

critically

hawkins doesn't think that there's only one model

but rather thousands

jeff has just released his new book

a thousand brains

a theory of intelligence

it's an inspiring and well written book

and i really hope after watching this show

you'll be inspired to read it too

now

jeff is a sub symbolist

he thinks that our knowledge is sliced and distributed over the substrate of synapses in our brain

but similarly that our simulated models of the world are also distributed over hundreds and thousands of cortical columns

this means that when we recognize a dog

there's a society of cortical columns

all independently predicting that the dog is there and mutually calibrating after a consensus is reached

now jeff hawkins is a materialist

he thinks that despite appearances to the contrary

mental states are just physical states and that the latter emerges from the former

there's no inception

no ghost in the machine

no cartesian theater

if i had to summarize into one

big idea is that we think of the the brain

the neocortex is learning this model of the world

but what we learned is actually there's tens of thousands of independent modeling systems going on

and so each

what we call a column in the cortex is about 150

000 of them is a complete modeling system

so it's a collective intelligence in your head in some sense

so the thousand brains theory says

well

where do i have knowledge about you know this coffee cup

where is the model of this cell phone

it's not in one place

it's in thousands of separate models that are complementary and they communicate with each other through voting

hawkins says that the neocortex is the outermost and most recently evolved layer of the mammalian brain

a bit like a wrinkly napkin

which wraps around the old brain

it occupies about 70 percent of the volume of your brain

and it's responsible for all aspects of your intelligence

that is to say your sense of vision

touch

hearing

language

in all its forms and even abstract thinking

such as mathematics and philosophy

the neocortex is surprisingly different to other parts of the brain

it has no visually obvious divisions

the anatomical organization is strikingly similar

nevertheless

different parts of the neocortex still perform different functions like vision or hearing

for example

the complex circuitry of the neocortex looks remarkably alike in visual regions and language regions and even touch regions

it looks similar across species such as rats

cats and humans

the variations between regions are relatively small compared to their similarities

now

hawkins argues that the complexity of the brain is in the content of its connections and its wiring

which is an emergent property of a simple learning algorithm

i mean

look at the openai microscope

their tool for visualizing trained convolutional neural networks

as an example

the learning algorithm is simple

but all of this beautiful complexity emerges as a result of training

even after this type of visualization

it's not really understandable by us humans

it's possible that any successful knowledge

representational substrate for some domains of data may never be understandable by humans

now the neocortex is arguably one of the main organs of intelligence

it gives us sensory perception

motor control and abstract thought

the key attributes it has are its ability to learn continuously learn rapidly

its power efficiency and its flexibility

which is to say its ability to learn diverse and novel tasks

now hawkins argues that the neocortex learns a model of the world

that each cortical column is a complete sensory motor modeling system

he thinks that cortical columns use reference frames to store knowledge and generate behavior

our brains learn about the outside world by processing our sensory inputs and movements

the neocortex anticipates the sensory results of movement when we touch objects or observe the world around us

our brain receives a sensory motor sequence of information

when moving our fingers over familiar objects

we quickly notice discrepancies suggesting we make tactile predictions that are specific to particular objects

hawkins believes that the prediction of sensory stimuli after movements is the fundamental primitive of cognition

the brain does not just memorize the sensory information as this would become quickly intractable

instead

hawkins claims that evolution repurposed the ancient systems that evolved to model spatial relationships

such as grid and place cells

it generalized those systems to model abstract concepts in abstract spaces

in this system

thinking and reasoning becomes a kind of traversal through a complex topology of relationships in the brain

analogous to the traversal of a physical space

the whole cortical sheet is made of collections of primitive columnar units called mini columns

each cortical area is a collection of millions of cortical mini columns

these mini columns are organized in about six layers with specific types of neurons

and connection patterns in each layer

neighboring mini columns have the same receptive field

so they have the same inputs from other cortical areas

and these mini columns form complete fundamental and primitive units known as macro columns

or as hawkins calls them cortical

columns

now pyramidal neurons are the typical excitatory neurons in the neocortex

their name comes from the triangular shape of their cell body

the dendrites of each pyramidal neuron connect to tens of thousands of excitatory synapses equally split from local

and remote sources

the neurons can look significantly different in their connection arrangements

depending on which neocortex layer

they're situated in the thalamus is the main input and output subcortical structure to the neocortex

it could be viewed as the seventh layer of the neocortex

it primarily sends sensory or pre-processed information into layer four of the neocortex

moreover

columns in the neocortex learn to model whatever systems are wired to them

columns wired to the ear

for example

learn to process sound

columns

wired to the retina

rods and cones

learn to process images

the neocortex can retain remarkable plasticity even later into life

perhaps the most striking examples are those of post post-injury plasticity where the brains overcome injury

by rewiring remaining neurons to regain some or even full functionality

the cortical regions are densely interconnected with many feedback and feed-forward skip connections across the hierarchy

and all functional areas in the neocortex are bi-directionally connected with other brain structures

but their main inputs and outputs come from other cortical areas via long-distance connections

all cortical regions are densely connected

but there's some apparent asymmetry and even hierarchy

detectable in the overall structure

sensory areas tend to be lower in the hierarchy than associative or motor areas

for example

and bottom-up information flows are known as feed-forward and top-down connections are known as feedback connections

what should be pretty clear from looking at this structure is that there's no blank slate

there's a very clear cognitive architecture

and much of this high-level structure is there from birth

there are many skip connections

a high level of recurrence

with many feedback loops and the process is highly distributed at this point

i want to draw your attention to a paper published last year by psychologist joseph cesario

it was titled your brain is not a tiny onion with a reptile inside

joseph pointed out several misunderstandings of nervous system evolution

which he says stems from the work of paul mclean

who in the 1940s began to study the brain region

which he called the limbic

system

mclean later proposed that humans possess a so-called triune brain consisting of three large divisions that evolved sequentially

the oldest

the reptilian brain controls basic functions such as movement and breathing

next

the limbic system

which controls emotional responses

and finally

the cerebral cortex

which pretty much means the neocortex

but you know

he thought that that controlled language and reasoning

joseph said that mclean's ideas were already understood to be incorrect by the time he published his 1990 book

i mean

since the 1970s

many in developmental neuroscience thought that the uh

the ideas from mclean were a myth

you know due to its longevity

the triune brain idea has been called by uh

neuroscientist

lisa feldman barrett

as one of the most successful and widespread errors in all of science

joseph pointed out

that the problem with this view is a simple conception of evolution being arranged sequentially from the simplest to the most complex organisms

this view implies that anatomical evolution proceeds in the same fashion as geological strata

with new layers adding on top of existing ones

instead

most evolutionary change consists of transforming existing parts

joseph says that the neocortex is not an evolutionary novelty unique to humans or primates or mammals

all vertebrates possess structures evolutionarily related to our cortex joseph concluded

that these ideas are consistent with traditional views of human nature as rationality

battling emotion

the tripartite

platonic

soul

freudian psychodynamics and religious approaches to humanity

it's also a simple idea that can be distilled and communicated to a lay audience

look

i'm not suggesting for a second that hawkins subscribes to the idea of a tree in brain

but i think it's possible to get this impression

reading his book

which is why i wanted to bring this up

it turns out it's very tricky to get neurons to do this

to build a map of an environment

it's just and so we now know there's this

these famous studies that's still very active about place cells

and grid cells

and other types of cells in the older parts of the brain

and how they build these maps of the world

it's really clever

it's obviously been under a lot of evolutionary pressure over a long period of time

they get good at this

so animals not know where they are

what we think has happened

uh

and there's a lot of evidence to justice

is that that mechanism we learned to map like a space is was repackaged

the same type of neurons was repackaged into a more compact form

and that became the cortical column

in 1972

john o'keefe discovered the first component of an internal gps system in the brain

he discovered place cells in the hippocampus which activate

when a rat is situated in specific locations in a room

more than three decades later

in 2005

mae britt and edward moser

discovered another key component of the brain's positioning system

grid

cells

which represented a coordinate system

which allowed for precise positioning and pathfinding

these discoveries solved the problem that has occupied philosophers and scientists for centuries

how the brain creates a map of the complex world around us

and how we navigate our way through it

what's intriguing about the brain solution is that it seems to create multiple hexagonal grids with slightly different orientations

and scales

because these grids are orientated and scaled differently

each point in space activates a unique combination of grid cells

in other words

the brain encodes locations using bits in a sparse distributed representation

jeff thinks that sparse representations are crucial to the success of the human brain

and should guide our construction of artificial intelligence

what's intriguing about the brain solution is that it seems to create multiple hexagonal grids with slightly different orientations

and scales

because these grids are orientated and scaled differently

each point in space becomes a unique combination of grid cells

in other words the brain encodes these cells using a sparse distributed representation

jeff thinks that such sparse representations are crucial to the success of the human brain

and should guide our construction of artificial intelligence

if you're going to build a model of a house

in a computer

they have a reference name

and you can then reference them like cartesian coordinates

# Chapter 2

and how we navigate our way through it

what's intriguing about the brain solution is that it seems to create multiple hexagonal grids with slightly different orientations

and scales

because these grids are orientated and scaled differently

each point in space activates a unique combination of grid cells

in other words

the brain encodes locations using bits in a sparse distributed representation

jeff thinks that sparse representations are crucial to the success of the human brain

and should guide our construction of artificial intelligence

what's intriguing about the brain solution is that it seems to create multiple hexagonal grids with slightly different orientations

and scales

because these grids are orientated and scaled differently

each point in space becomes a unique combination of grid cells

in other words the brain encodes these cells using a sparse distributed representation

jeff thinks that such sparse representations are crucial to the success of the human brain

and should guide our construction of artificial intelligence

if you're going to build a model of a house

in a computer

they have a reference name

and you can then reference them like cartesian coordinates

like x y and z axes

so i could say

oh

i'm going to design a house

i can say

well

the

the front door is at this location

xyz

and the roof is at this location

xyz

and so on

that's a type of reference frame

so it turns out for you to make a prediction

and i walk you through the thought experiment in the book

where i was predicting what my finger was going to feel

when i touched the coffee cup

it was a ceramic coffee cup

but this one will do

and what i realized is

that to make a prediction with my finger's going to feel like it's going to feel different than this

which would feel different if i touch the hole or the thing on the bottom

make that prediction

the cortex needs to know where the finger is

the tip of the finger relative to the coffee cup and exactly relative to the coffee cup

and to do that

i have to have a reference frame for the coffee

jeff says that after many years of thinking about the function of the neocortex

he deduced that it must store everything that we know

all of our knowledge

right

using something called a reference frame

but what exactly does jeff mean by a reference frame

well

consider a paper map as an analogy

a map is a type of model

right

so a map of a town is a model of a town and the grid lines

such as the

the latitude and longitude lines are a type of reference frame

a maps

gridlines is its reference frame

so they provide a structure of the map

a reference frame tells you where things are located relative to everything else

and it can tell you how to achieve goals such as how to go from one location to another location

now

jeff realized that the brain's model of the world is built using map-like reference frames

not one reference frame but hundreds of thousands of them

jeff thinks that most of the cells in your neocortex are dedicated to creating

and manipulating reference frames which the brain uses to plan and think

our brain's knowledge representation is a simulation of reality

and this applies in concept space as well as physical space

as was the case with grid and place cells in other parts of your brain

so jeff now thinks that the way we think is analogous to how we navigate spaces

he says

that the similarity of circuitry observed in all cortical regions is strong evidence

that even high-level cognitive tasks are learned

and represented in a location-based framework

to be an expert in any domain requires having a good reference frame

a good map

two people observing the same physical object will likely end up with similar maps

for example

it's hard to imagine how the brains of two people observing the same chair would arrange its features differently

jeff said in his book that being an expert is mostly about finding the reference frame to arrange facts

and observations

i mean albert einstein

for example

started with the same facts as his contemporaries

however he found a better way to arrange them

a better reference frame that permitted him to see analogies and make predictions that were surprising

what's most fascinating about einstein's discoveries relating to special relativity is

that the reference frame he used to make them were everyday objects

right

he thought about trains and people and flashlights

he started with the empirical observations of scientists such as the absolute speed of light

and then used everyday reference frames to deduce the equations of special relativity

because of this

almost anyone can follow his logic and understand how he made the discoveries

in contrast

einstein's general theory of relativity required reference frames based on mathematical concepts called field equations

which are not easily related to everyday objects einstein found this much harder to understand

as does pretty much everyone else

jeff hawkins is one of the ultimate gentlemen scientists of our age

his first major project was palm computing

which is a company that he founded

he invented the palm pilot in the mid-1990s and despite his incredible success in the nascent mobile computing industry

his heart was never in it

his passion was always for theoretical neuroscience

he knew the biggest prize was understanding human intelligence

and then using that knowledge to create human level machine intelligence

so in 2005 he co-founded numenta in redwood city in california

nomenta

is a machine intelligence company that has developed a cohesive

theory

core software technology

and applications based on the principles of the neocortex

its dual mission is to understand how the brain works

and to apply those principles of real intelligence to create intelligent machines

neuroscientists were publishing thousands of papers a year

covering every single detail of the brain

but there was a lack of systemic theories that tied all of those details together

nimenta decided to first focus on understanding a single cortical

column

right

they knew that cortical columns were doing something physically complex and therefore must be doing something complex

now

last week

we had ben gertzel on the show

and he is convinced that artificial general intelligence must be a hybrid of many underlying algorithms

not a single learning algorithm

jeff hawkins doesn't agree

hawkins thinks that all the magic of intelligence could emerge from a single cortical learning algorithm

andrew ng said that as a young professor and after he read hawkins first book on intelligence

he also became convinced that a simple scaled-up learning algorithm could reach artificial general intelligence

now

what does seem to really distinguish hawkins ideas is that intelligence must emerge from diverse and strongly multi-modal inputs

perhaps that intelligence is somehow emerging from the nature of physical embodiment

now

jeff argues that we're the first species on earth to know the age and the size of the universe

he thinks that humans are the first species to be known by their knowledge and not by their genes

that's the beauty of this discovery that this guy

vernon

mount castle made many many years ago

which is that there's

there's a single cortical algorithm underlying everything we're doing

the mindful brain is a small book

it's about 100 pages long

and published in 1978 and it contains two essays about the brain from two prominent scientists

one was written by vernon

mel castle

a neuroscientist at john hopkins university

now

jeff hawkins

cites mount castle as being one of his biggest inspirations

jeff says that it remains one of the most iconic and important essays ever written about the brain

mount castle proposed a new way of thinking about the brain

that is elegant

a hallmark of great theories

but it's also kind of surprising and it continues to polarize the neuroscience community

now mount castle noted that the brain grew really large

by adding new brain parts on top of old brain parts

the older parts control more primitive behaviors

while the newer parts create more sophisticated ones

however mount castle goes on to say that while much of the brain got bigger

by adding new parts on top of old parts

that's not how the neocortex grew to occupy 70 percent of our brain

the neocortex got big by making copies of the same basic thing

the same circuit

he says that every single part of the neocortex is the same basic circuit

different parts of the neocortex are different not in their intrinsic function but rather in what they are connected

to

the implications of this are huge

if we understand how one part of the neocortex works

we understand how it all works and how all aspects of intelligence can emerge from a single cortical

algorithm

mount castle pointed out that the neocortex grew really quickly given the short evolutionary time

now darwin's big idea is that the diversity of life emerged from a single algorithm

similarly mount castle proposed that the diversity of intelligence also emerged from a single basic algorithm

the difference is that darwin knew what the algorithm was

random variation and natural selection

darwin didn't know where the algorithm was in the body

the discovery of dna came

much later

mount castle knew where the algorithm resided

but not what it did

mount castle said that there's about 150 000 cortical columns in the neocortex

a bit like 150 000 little pieces of spaghetti stacked next to each other

scientists knew that these columns existed because they all respond to different sensory inputs

be it from a patch of skin or a signal from the retina

but the columns are wired to different sensory inputs from the body

there's a wonderful anecdote in jeff's book about the last time he met mount castle

jeff gave a speech at john hopkins university

and at the end of the day he met with mount castle and the dean of the department

the time had come for him to leave

and you know jeff had a flight to catch

so they said their goodbyes and the car was waiting for jeff

outside

as jeff walked through the office door

mount castle intercepted him

put his hand on jeff's shoulder and said in here is some advice for you

kind of tone of voice

you should stop talking about hierarchy

it doesn't really exist

jeff was stunned

mount castle was one of the foremost experts on the neocortex

and he was telling jeff that one of its largest and most well-documented features didn't exist

jeff was surprised

right

it was as if francis crick had said to him

oh

that dna molecule doesn't really encode your genes

so jeff didn't know how to respond

he just said nothing

as jeff sat in the car on his way to the airport

he tried to make sense of those parting words

today

jeff's understanding of hierarchy in the neocortex has changed dramatically

it's much less hierarchical than he previously thought

did vernon mount castle know this back then

did he have a theoretical basis for saying that hierarchy didn't really exist

was he thinking about the experimental results that jeff didn't know about

he died in 2015

and jeff will never be able to ask him after his death

jeff took it upon himself to re-read many of his books and papers

his thinking and writing always

very insightful

his 1998 perceptual neuroscience

the cerebral cortex is a physically beautiful book and remains one of jeff's favorites about the brain

when jeff thinks back on that day

he really laments

and he kind of wished

that he would have chanced missing his flight for that last opportunity to talk of mount castle further

even now he wishes he could talk to mount castle about his current ideas

he'd like to believe that mount castle would have enjoyed the thousand brands theory of intelligence

so if you have many brains

who are you

then

so it's interesting

we have a singular perception

right

you know

we think

oh

i'm just here

i'm looking at you

but it's

it's composed of all these things

there's sounds and there's and there's

uh

there's vision and there's touch and all kinds of inputs

yeah

we have the singular perception and what the thousand brain theory says

we have these models that are visual models

we have a lot models of auditory models

models

matters of tactile models

and so on

but they vote

and so

um

they send in the cortex

you can think about these columns as that

like little grains of rice

150 000 stacked next to each other

and

um

each one is its own little modeling system

but they have these long-range connections that go between them

and we call those voting connections or voting neurons

um

and so the different columns try to reach the consensus

like what am i looking at

okay

you know

each one has some ambiguity

but they come to a consensus

oh

there's a water bottle

i'm looking at um

we are only consciously able to perceive the voting today

the most common way of thinking about the neocortex is a bit like a flow chart

right

information from the senses is just processed sequentially step

by step as it passes from one region to the next

in this notion

every step of neural processing refines a representation from the low level to the high level

incrementally

scientists refer to this as a hierarchy of feature detectors

but as jeff points

out

even basic study of how the brain works will tell you that cognition is a interactive process

right

depending on movement

for example

to learn what a new object looks like

we hold it in our hand

and we rotate it this way and that way and we see what it looks like from different angles

and once learned

we're able to recognize entire objects from the touch of a single finger

or a fleeting glimpse of a small part of the object

jeff's proposal of reference frames in cortical columns

suggests a different way of thinking about how the neocortex works

thinking of cortical columns as cognitive primitives

even in low-level sensory regions that are capable of learning and recognizing complete objects

jeff's theory explains how a mouse with a mostly one-level visual system can see

and recognize objects in the world

but where is the knowledge stored in the brain

jeff thinks that our knowledge of objects are distributed over many cortical

columns

so when i pick up a pen there isn't a single model of this pen but rather thousands

i have visual models

i have sensory models

i have auditory models and everything created in between

right from a rich topology of reference frames

binding them all together and every cortical column models

hundreds if not thousands of complete objects at multiple scales

the long-range connections between the columns and the regions of the neocortex communicate at the level of classified objects

not features

jeff thinks that his theory solves the age-old binding problem in artificial intelligence

which is the challenge of mapping sensory input to discrete mental categories

and how these discrete categories can be combined into a single lived experience

jeff thinks

that the binding problem is a side effect of a flawed assumption

that the connection topology of the brain is convergent rather than divergent

the solution to the binding problem is that your cortical columns vote

your perception is the consensus

which is reached from the columns

voting on what they recognize

the voting works across sensory modalities

when you grasp an object in your hand

jeff believes that the tactile columns representing your fingers share another piece of information

their relative position to each other

which makes it even easier to figure out what they're touching

the brain wants to reach a consensus

now in jeff's book he shows an example of an image which can appear as either a vars

or two faces

in examples like this

the columns can't decide which is the correct object

because it's ambiguous

as if it's as if they have two maps for two different towns

but the maps at least in some areas are identical

vars

town and faces town

they're just too similar

so the voting layer wants to reach a consensus

but it doesn't permit two objects to be the same simultaneously

# Chapter 3

which is reached from the columns

voting on what they recognize

the voting works across sensory modalities

when you grasp an object in your hand

jeff believes that the tactile columns representing your fingers share another piece of information

their relative position to each other

which makes it even easier to figure out what they're touching

the brain wants to reach a consensus

now in jeff's book he shows an example of an image which can appear as either a vars

or two faces

in examples like this

the columns can't decide which is the correct object

because it's ambiguous

as if it's as if they have two maps for two different towns

but the maps at least in some areas are identical

vars

town and faces town

they're just too similar

so the voting layer wants to reach a consensus

but it doesn't permit two objects to be the same simultaneously

so you have to pick one possibility

you can perceive faces or of ours

but not both at the same time

and the process of cognition allows us to move between the alternatives to reason interactively over time

in his book

jeff makes the powerful argument that thinking is simply traversing a topology of reference

and displacement frames in your brain

jeff thinks that this reasoning is movement evolved to extend the physical world of spaces

and time to our worlds of abstract thought

the succession of thoughts that we experience

when thinking is analogous to the succession of sensations we experience

when moving our finger over an object or walking around a town

perhaps the reason why albert einstein was so smart was

because of the unique topology of reference frames in his brain his information architecture

if you will

must have been arranged as a function of his life experiences as well as his biology

traversing his brain

topology allowed him to make powerful abstract inferences that other people couldn't make for him

it must have felt a bit like the borg traversing their wormhole network in the delta quadrant

for those of you who are fans of star trek voyager

jeff says that learning conceptual knowledge can be difficult if i give you 10 historical events related to democracy

how should you arrange them in your brain

one teacher might show you the events arranged on a timeline

you know

in a one-dimensional reference frame

it's useful for assigning the temporal order of the events and which events might be causally related

by temporal proximity

but another teacher might arrange the same historical events geographically on a map of the world

timelines and geography are both valid ways of organizing historical events

yet they lead to different ways of thinking about history

they might lead to different conclusions and different predictions

the best structure for learning about democracy

might even require an entirely different map

a map with multiple abstract dimensions that correspond to fairness or rights

for example

so what does the thousand brains theory tell us about machine intelligence

intelligent machines need to learn a model of the world

inference

prediction

planning and motor behavior are all based on this model

the model is distributed among many

nearly identical units that vote to reach consensus

this gives us robust prediction

it scales well and it works with any kind of sensor array and modality

and voting solves the binding problem in each unit

knowledge is stored in a reference frame and is learned via sensory motor interaction

this means that we can learn unsupervised fast and the motor behavior is integrated

this is matthew taylor from numenta

this place looks really familiar

but i can't remember how i got here

it's almost like someone severed all of the distal connections between the pyramidal neurons and my neocortex

many years ago

nomenta used to refer to their overarching theory as htm theory

but now they use the terminology thousand brains

the htm

or hierarchical temporal memory algorithm was a particular implementation of the early ideas of the thousand brains theory

the original guiding principles of htm were that it was a sequence memory algorithm

numenta

thinks that every neuron in your brain is learning a pattern of sequences

it's needed to support continual learning

and critically

it wasn't an artificial neural network

which they argued were not biologically inspired

or at least not in their popular configuration

at the time

the core data structure of the algorithm was called an sdr or a sparse distributed representation

this was a large bit

mask

think of it as a large ordered collection of ones and zeros

the representation is sparse

which means that it typically only contains about one percent of ones instead of zeros

and the values represented the state of neurons in different regions of your neocortex

so nomenta really leans into this idea of sparsity in the brain

and its necessity to build any intelligent system

the reason why sparsity is so powerful is that there are factorially

many permutations of values

i mean

for example

there are about 175 million values

if you had four on bits in an sdr of length 256

right

because it's 256 choose 4.

so this means that the possibility of getting false positives is negligibly small

it's also space efficient

because those four bits which could represent 175 million things could be stored in a 32-bit array

right

which is four times eight bits

the notion of similarity between these sdrs is their intersection or their hamming distance

and again

the really clever thing is just how robust these representations are to noise

you could add about 33 percent of random noise to both of the sdrs

and it would barely affect the overlap metric

you can also union the sdrs together

and not much information about the patterns would be lost in the mix so the htm algorithm needed encoders to take any data structure

and represent it as a sparse distributed representation

encoding the information into an sdr

is an important consideration for htm

much as it is with any other machine learning model

but i was really excited

you know when i learned about htm

because it seems so audacious

and each column has a connection to this input space and it has a receptive field

so each column is connected to different bits in the input space and these are proximal

dendritic

connections feed forward input into the system

and each one of the cells within the column shares that receptive field through its proximal

connection

we also have these other connections between cells within the structure

and here's a third cell

with another four synapses on its segment

these are distal connections

so the cell body or the soma has got

uh

different areas of receptivity

the feed forward proximal input comes from below

and the contextual information or the distal connections come laterally from other cells within the structure

we're comparing a biological neuron to the htm

you know

neuron in software that we're creating

we have the feed forward input

which is the proximal dendritic input from the input space in in both sides

and then the distal input

from lateral connections to other cells within the space for context

now this htm neuron is showing that there's feed forward input

but it's also showing that it can have one or many distal connections

these are distal segments

each one of these segments could potentially have one

or many synapses or connections to other cells within the htm structure

each one of those cells may be in an on and off state

so at any time

if a cell wants to decide whether it's going to go into a predictive state or not

it can look at all of its segments

and its connections across all of their synapses

and if any one of those summed across all the synapses breach some threshold

which is configurable

then that cell goes into a predictive state based upon its connections

its contextual connections to the other cells within the structure

so what i'm doing is i'm feeding in a four note sequence

and then resetting and restarting the sequence over

so every time it sees f-sharp the first note in the sequence

it's seeing it for

without any context

nothing came before it

so the algorithm goes and looks into every cell in in every active column

only cells within active columns become activated

because these activations are completely driven by the proximal segments to the input space

there's only one segment on the cell

because it's one color

it's magenta color

and all of the cells that it's connected to are active in the current time step

that's why it's predictive

because it looked at its segment and it looked

it summed up all the synapses and they were all one apparently

and that breached its threshold to become predictive

so it is in a predictive state

i've never seen another machine learning algorithm quite like it

you know

almost all of them are continuous rather than discrete

and even the discrete ones like decision

trees they still assume an ordinal value on the dimensions

hdm takes it one step further

its representations are also distributed over the features

the only encoding rules of sdrs were that semantically similar data should have a significant overlap on the sdrs

the encoder should be deterministic and the output should have a fixed dimensionality

and the sparsity level should be similar across the input domain

now the thing that was missing to some extent from htm was the notion of representation learning

like we have in neural networks

you have to do it all yourself in the encoder

which is probably the main reason why htm never took off for unstructured data problems

the reason why cnn's dominated computer vision was because it learned representations that were better than any hand-crafted representations

the locality prior

and the weight sharing and the stochastic gradient descent made it computationally tractable

the htm neuron was inspired by pyramidal neurons in the neocortex

a neuron is receiving sdrs from distilled apical dendrites higher up in the hierarchy

and from basal dendrites from the same region of the hierarchy

and also from proximal dendrites from lower level of the hierarchy

or some sensory input

which represented the classical receptive field of a neuron

now all of these neurons are receiving a stream of sdrs and they're figuring out

when to fire and turn on their on bit

or you know when they should go into a predictive state

which can tell other neurons when to fire

right from the very beginning

numenta knew that real neurons are not simple point neurons

the synapses on active dendrites detect dozens of sparse contextual patterns and can learn complex temporal sequences

active dendrites enable flexible context

integration in the layers of neurons

there are two primary phases of the temporal memory algorithm

the first is to identify which cells within active columns will become active on this time step

the second phase

once those activations have been identified is to choose a set of cells to put into a predictive

state

this means that these cells will be primed to fire on the next time step

htm implemented dendrite branch specific plasticity

so if a cell becomes active and there's a prediction

it reinforces that dendritic segment

if there is no prediction

it grows the connections by sub-sampling the previously active cells

and if the cell is not active and there was a prediction

it weakens the dendritic segments

now i learned all about the htm algorithm from watching the htm school series of videos from matthew taylor

who worked at nomenta

matt tragically passed away last year and i wanted to personally pay tribute to him here

his passion and enthusiasm was infectious and he'll be greatly missed by the entire machine learning community

well

sparsity is something that doesn't run really well on existing hardware

it doesn't really run really well

um

on gpus

um

and on cpus

and so that would be a way of sort of bringing more

and more brain principles into the existing system on a commercially valuable basis

there's a large body of work on training dense networks to yield sparse networks for inference

but this limits the size of the largest trainable

sparse model to that of the largest trainable dense model

this was the case actually

until relatively recently

now

since the 1980s

we've known that it's possible to eliminate a significant number of parameters from the neural network without affecting accuracy

or inference time

pruning can substantially reduce the computational demands of inference

when the appropriate hardware is utilized to do so

when the goal is to reduce inference costs

pruning often occurs late in training

now

in 1995

researchers discovered that retrospectively pruning low magnitude connections worked impressively well

later researchers found that retraining the prune connections produced even better results or even better

still rinsing repeating the process with multiple rounds of pruning and retraining

other approaches explored adding connections back in at random

or even focusing on non-uniform sparsity

which is to say adding the connections in where they are most needed in the network

jonathan frankl

who was on the show last year

by the way

released his lottery ticket hypothesis in 2019

which demonstrated that if we can find a sparse neural network with iterative magnitude pruning

then we can train that sparse network from scratch to the same level of accuracy

however

as the demands of training have exploded

researchers have begun to investigate the possibility that networks can be pruned early in training or even before training

the benefit of doing so could reduce the cost of training existing models

and make it possible to continue exploring the phenomena that emerge at larger scales

recently

several methods have been proposed specifically for pruning at initialization

snip

aims to prune weights that are least salient for the loss

grasp

aims to prune weights that most harm or least benefit gradient flow and sin flow

which janet made a video about

by the way

aims to iteratively prune weights with the lowest synaptic strengths in a data independent manner with the goal of avoiding layer collapse

where pruning concentrates on certain layers

now

um frankel pointed out in his recent summary paper

that magnitude pruning after training outperforms all of these pre-initialization methods

most of these methods effectively prune the layers

not the weights

which to say you can perform similarly well

even if you randomly shuffle the weights that they prune in each layer

now interestingly sin flow and magnitude pruning work quite well at initialization time without seeing any data

now frankel didn't identify a single cause for why these methods struggled to prune in a specific fashion

initialization time

and thought that this is an important question for future investigation

perhaps there are properties of optimization that make pruning specific weights difficult or impossible

initialization perhaps because the training occurs in multiple phases

now combining gradient descent training with an optimal sparse topology can lead to state-of-the-art results with smaller networks in the brain

numenta argues that sparsity is key for how information is stored and processed

they also believe it to be one of the most important missing ingredients in modern deep learning

we reached out to the mentor after the show and they're vps of machine learning

architecture and research and engineering

so lawrence bracklund and subtite

# Chapter 4

where pruning concentrates on certain layers

now

um frankel pointed out in his recent summary paper

that magnitude pruning after training outperforms all of these pre-initialization methods

most of these methods effectively prune the layers

not the weights

which to say you can perform similarly well

even if you randomly shuffle the weights that they prune in each layer

now interestingly sin flow and magnitude pruning work quite well at initialization time without seeing any data

now frankel didn't identify a single cause for why these methods struggled to prune in a specific fashion

initialization time

and thought that this is an important question for future investigation

perhaps there are properties of optimization that make pruning specific weights difficult or impossible

initialization perhaps because the training occurs in multiple phases

now combining gradient descent training with an optimal sparse topology can lead to state-of-the-art results with smaller networks in the brain

numenta argues that sparsity is key for how information is stored and processed

they also believe it to be one of the most important missing ingredients in modern deep learning

we reached out to the mentor after the show and they're vps of machine learning

architecture and research and engineering

so lawrence bracklund and subtite

ahmed told us that at a high level

the biggest difference is

that they view sparse networks as a unique stand-alone class of artificial neural networks

that mirror the sparsity exhibited in the brain versus being a derivative of dense networks created

by pruning

so not really removing redundant connections but creating networks that are designed to be sparse

in 2019

the mentor released a paper called sparsity enables 50 times performance acceleration in deep learning networks

in that paper

they pointed to the scaling challenges faced by the current state-of-the-art neural networks

they said that the brain is highly efficient

right

requiring a mere 20 watts to operate

which is less power than a light bulb

contrast that to gpt3 which costs millions of dollars to train

numenta

believe that by studying the brain and understanding what makes it so efficient

they can create new algorithms that approach the efficiency of the brain

they think that the core reason the brain is so efficient is the notion of sparsity

a sparse network is one

where all of the neurons are not densely connected to every other in the same cortical

area

the brain stores and processes information as sparse representations

you know

at any given time

only a small percentage of the neurons in the brain are active

disparity

may vary

you know

from less than one percent to a few percent of neurons being active

but it's always sparse

sparsity will lead to a massively smaller memory footprint

because only the non-zero elements are stored

enabling the hardware to run more networks

simultaneously

gpus and tensor processing units

so tpus

they are dense execution engines

they perform the same computation task on an entire vector or matrix of data

this is a wise approach

when the vector or matrix is dense

which is to say it's all non-zero but in the dense environment

we gain efficiency by executing a single instruction to be applied to all of the data

this is an approach called simd

but when the data is predominantly zeros

then a prodigious amount of computation is wasted

so if you're keeping up to date on ai hardware

you might have heard of graphcorp or cerebros cerebrus

in particular

actually

they've developed this epic microprocessor with 850 000 cores and 40 gigabytes of memory on board

it's absolutely insane

not only that the chip has been designed to support sparsity from the ground up

these cerebros cores never multiply by zero

the scheduling operates at the granularity of a single data value

so all of the zeros are filtered out

and this in turn provides a performance advantage

by doing useful work during those cycles which otherwise would have been wasted

not to mention the power and efficiency savings

now

recently

in a presentation

they showed this graphic claiming to achieve near linear speed up in respective sparsity

about 84

speed up for 94

sparsity

numenta released this paper a couple of years ago before the current sparse hardware was released

and at the time they chose an fpga which is a field programmable gate array as the platform to run their performance tests

because of the flexibility it provided and handling sparse data efficiently

in addition

random access to memory is far more granular and efficient on an fpga

enabling fpga implementations to efficiently handle the unstructured access patterns in sparse networks

now in their paper

as well as confirming the previous results in the literature about the robustness of sparse networks to noise

and variance error

they also realized a significant performance gain from using the specialized fpga hardware

now my intuition is

that there's no difference at all between the representational power of a discrete htm type model which they used in their previous generation of algorithmic approaches versus an artificial neural network

the obvious difference is that feed forward monolithic vector space models are more amenable to training

given today's hardware

pretty much

everyone agrees that sparse networks are better

but is there something fundamentally special about sparsity

numenta certainly seem to think

so they anecdotally point to the brain as being sparse

but the brain is probably sparse for the same reason that i don't decide to get up every morning

and travel to every city in the uk doesn't seem like a profound insight

to be honest

we can take it as a given that sparse networks suffer less from overfitting

because they're not going to be using their precious representational power

memorizing individual challenging or non-representative examples in the training data

it remains unknown if the performance of the best pruning algorithms is an upper bound on the quality of sparse models

there's actually some really interesting papers out there now like momentum resnet which allow us to train huge neural networks with a small memory footprint

but the key question is are sparse networks functionally better than huge

densely connected neural networks

researchers from google released the paper rigging the lottery

making all tickets winners back in 2019

the rigging the lottery algorithm starts with a randomly initialized connection topology and then layer

by layer adds and removes connections

densifying the layer and then specifying again using a traditional weight magnitude heuristic

the algorithm achieves higher accuracy than all previous techniques for a given computational cost at all levels of sparsity

and then scored higher accuracy than the dense to sparse algorithms

now nimenta say that they've also produced a similar algorithm

although

as far as i know

it's not been made public yet

so i assume it's pretty similar to google's approach

as they pointed us in this direction

but unlike traditional dense to sparse

iterative magnitude

pruning google's algorithm allows the topology to grow

also during the optimization

which can apparently help overcome some of the local minima

obviously the most accurate sparsity algorithms required

at a minimum

the cost of training a large dense network in terms of memory and computational horsepower

but that approach has serious limitations

right

the size of the sparse model you can learn is strongly bounded on the size of the larger dense model

which you're specifying from as a starting point

it's simply too inefficient to waste computation on so many parameters

which would end up being zero

anyway

google's algorithm seems unequivocally better than dense

iterative magnitude

pruning

which is somewhat surprising to be honest

given that it's

uh

it seems to be doing the same thing

but only one layer at a time

now training mobile net one and two on imagenet

with this form of sparse training was instructive

it was possible to train a sparse network with nearly the same accuracy in about 30 percent of the compute time

it was also possible to train a large sparse network

which was five percent better

accuracy in roughly double the flops of training

the original dense uh version

today's neural networks have something called the point neuron

which is a very simple model of a neuron

and uh

by adding dendrites to them

just one more level of complexity

that's in biological systems

you can solve problems in continuous learning

um

and rapid learning

so we're trying to take

we're trying to bring the existing uh field and we'll see if we can do it

we're trying to bring the existing field of machine learning commercially

along with us

you brought up this idea of keeping

you know

paying for it commercially

along with us

as we move towards the ultimate goal of the true ai system

now nimente is working on some really cool stuff behind the scenes

unfortunately there's very little information about it in the public domain

yet jeff's main aim is to realize the vision of the thousand brains theory in an efficient computational algorithm

sparse networks are just a tiny part of this vision

the next step is implementing continual learning with active dendrites

and this essentially means that they need to be able to add new synapses

and train them independently of the existing ones

this is going to require a specified version of backprop

which will also require specialized hardware and algorithms to implement

they also mentioned to us that they want to exploit activation and weight sparsity

simultaneously mirroring the neocortex

anyway

next week

on street talk

[music] another book that i read around the same time that had a big impact on me

uh

and and there was actually a little bit of overlap with john pierre as well

and i read it around the same time

is jeff hawkins on intelligence

which is a classic

and he has this vision of the people of the main championship

i hope you enjoyed it

a multiscale hierarchy of temple prediction modules

and these ideas really resonated with me

like the the notion of a modular hierarchy

um

of you know

potentially

um of compression functions or prediction functions

i thought it was really really interesting and it reshaped

uh

the way it started thinking about how to build minds

let's kick off with the main show

hope you enjoy it

folks

well

i fell in love with brains actually

when i read an article by francis crick

who was one of the co-discoverers of dna

and francis had later in his life turned his interest to neuroscience

and he wrote this essay

that appeared in scientific american

where soon the emperor has no closed type of essay

he said

you know what

we have all this data about the brain and it's really wonderful

we collected all this data and we've got decades and decades

but no one has a clue what the hell is going on

and um

and he says

you know

we need new ways of thinking about the brain

we don't really necessarily need more data

and

and that just struck me

i was 22 and i was like holy crap

you know

that's just a puzzle

we have this

we have these pieces and someone has to put the pieces together

and that seemed like something i would be good at

or at least i would enjoy

and that's got me going

that was

that was the thing that just i said i'm going to make a career out of this

and then

very quickly

i realized that

well

you know

this is the long thing

it's going to take a long time

and

uh

but if we do this

um

then

if we really figure out how the brain works and what it does

then we will have real big insights into how to make intelligent machines

and so i said

oh my god

the implications are here to not just from a neuroscience but from an ai point of view

and so that got me going

uh

on this on this journey

and at that point i decided to change careers from engineering to neuroscience

uh

computer engineering

computer science and neuroscience and start all over

i just got my

my degree from the university

so i was like

yup

starting again

here

we are

40 years later

it's been a long journey though

as you probably probably know

there's been a lot of twists and turns

too

absolutely

absolutely

well

we really enjoyed reading your book

but

um

i wanted to talk a little bit about some of the tribalism in the machine learning community

so i've been doing a bit of research online

and your mission right now is to try and convince other people of of your ideas

you've got this incredibly exciting idea of the brain

and as you just said that in in some level of abstraction

the brain is infinitely complicated

but actually

if you think about it in terms of simple rules that can produce a lot of complexity

it's not that complicated

but you know

i've noticed that when you speak to some of the machine learning folks

they are very quick to dismiss your ideas and they say

well

there's there's no material difference with monolithic neural networks with point neurons and back props and back prop

and you know

it's good that people are so passionate about what they believe in

but by the same token

it means that science only advances one funeral at a time

yeah

um

it's a very complex issue you bring up here

and um

uh

i i think our mission isn't to convince people

our first

our mission was to figure out what the hell is going on in our heads

that was the first thing we had to do

and we made a lot of progress on that

now we have to sell those ideas

we have to sell them to neuroscientists

so we publish papers

up speaker conferences

people cite our work

they test it

and so on

that has to happen

it takes time

um

then

uh

then we now have a road map we can see

we can now

uh

from the position i'm in and some of the people work with me

we can see where the shortcomings are in current ai techniques

uh

we can

we can see

oh

the brain's doing it differently

it's doing it this way

and and we have

we can have two approaches to go forward

we could say

well

we need to convince everybody else

which is not really a fun thing to do

um

or we can

we can just put our ideas out there

document them

show people them and then work at doing this ourselves like

just demonstrate it

start building things

um

making things that work

making things to solve problems that other people have been struggling with

so we're doing a bit of all these things

we're promoting the neuroscience theory

we're promoting um

these ideas in the machine learning community

and we're also implementing this stuff

uh

because in the end

you

if you don't

if you

you know

you can't implement it unless you understand it

so it's a good test for us too

um

and you would wish you know

everyone wish like

oh

i wish everyone

just you know

agreed with us

where everyone you know got what we understand

but the reality is it's a big world of lots of big little

um

and there's a lot of um people who want to dismiss new ideas

um

but that's just the nature of the of the beast

right

we just have to accept that

um

science is not just science

you have to promote your ideas as well as discover them

and the same is true

uh

in the machine learning world

well

one thing that's kind of interesting about that

you know

philosophical aspect that you have of look

it's a big world

there's lots of people doing a lot of different things

that's kind of the heart of evolution

right

is that there's variation

people taking different approaches and natural selection will kind of sort out

you know which ones work and don't work

and that's one thing i've always thought about

a lot of the folks that don't want to pay attention to what the brain does is

they don't have a sufficient appreciation for how much you know

uh

let's say

work has gone into designing the human brain right

i mean a billion years of life

however many

hundreds of millions of years of intelligent evolution

so certainly we can learn things from the neuroscience

right

i mean

and

and of course that was the original inspiration for the artificial neuron

but that was just one tiny

simplest possible abstraction of a neuron

that happened

what fit

yeah

i don't know what

50 years ago

right

six years

there's got to be more we can learn right

but yeah

but you know

the way i look at it is

we're all trying to reach the same end goal

we're all trying to figure out how to build intelligent machines

they're truly intelligent

we all

we're either intelligent

uh

we can talk about the implications of that

um

and i don't think there will be multiple ways of doing this

there's like in the end

there aren't multiple ways of building computers

they're all some sense of universal turing machines and we have variations on that

um

and so i think that's what's gonna happen here too now

a priori

how would i know that engineers couldn't figure this all out

just by thinking about and doing engineering stuff

um

you know

my guess was that we'd have to figure out how the brain works first

uh

and who knows i could have been wrong

that was a bet

maybe people would figure it out

but here we are 67 years later in the field of ai

# Chapter 5

uh

we can talk about the implications of that

um

and i don't think there will be multiple ways of doing this

there's like in the end

there aren't multiple ways of building computers

they're all some sense of universal turing machines and we have variations on that

um

and so i think that's what's gonna happen here too now

a priori

how would i know that engineers couldn't figure this all out

just by thinking about and doing engineering stuff

um

you know

my guess was that we'd have to figure out how the brain works first

uh

and who knows i could have been wrong

that was a bet

maybe people would figure it out

but here we are 67 years later in the field of ai

and i still

i think today's ai systems are still incredibly limited

they don't really do much at all with

it's intelligent

that's my opinion

um

they're very very restricted

they don't generalize

they don't create behaviors

they don't

it's just so many things that we're so far away and now we took a long time

but now we figured out a lot about how the brain works

so now we have a roadmap

so

um

i think it's going to be a lot easier

it's now it's not just like

oh

i think it's going to be quicker to study the brain and other people say

no

i don't think we'll be

i said

well

we now have some things we can we really understand about the brain

we don't have to worry about that anymore

so

um

the approach we took has been fruitful

um

and um

and i should also say

just be clear

today's ai is really useful

so i have nothing against it

yeah

it's great

um

it's just not intelligent and i want to build soldier machines

yeah

it's quite a

it's really fascinating to get to talk to you

uh

your book on intelligence is actually one of the very first books of it about ai

i ever read it

so

uh

edward is your fault

that part of this field

now i always feel bad when people tell me

i hope it worked out

okay

for you

it's been

it's been a wonderful journey

and i'm really glad that it was one of my first exposures to the field

in a way it was so full of like

really interesting ideas especially for the at the time seemed very revolutionary

um

and a very well shaped my picking

so i want to ask you a little bit

um

appears to me as you know

someone who works

sorry

i work with deep learning

you know a lot

of course

nowadays

it feels to me that a lot of your ideas seem

uh

validated in a lot of dealery practices

um

a

like a

currently kind of like emerging trend is how

um

with

like these me

furtive societies transfer models that have become very popular

recently that scaling these larger is just stacking these transformer layers

seems to currently at least increase performance in

like a

you know in a pretty consistent manner

and i wonder

well

it feels like that

it kind of validates in a way

it's like a different proposal to what you say

but like accordingly before we

do

you think there's any kind of connection

ah

you know

i don't think about

i'll answer your question

but let's just say i'm always forward thinking

i never look back

and so i don't worry about

hey

what the hell

was i right about this or not

or just

you know

somebody get credit for with this or that

i don't really care

oh so it's like let's go forward

what's the best thing you can do right now

and um

and so you know

i think there's you can

obviously

as i said

really

we're all going to converge on the same thing right eventually

i think so ideas become

you know

i got ideas from other people other than people

get ideas

for me

the history of how that milieu of ideas travels

it's very difficult to point to

um

so i i

i'm very reluctant to claim precedent than anything

i just feel like

hey

you know

if we're all moving towards the same idea

that's great

i don't think transformers are an implementation of the cortical

column or the cortical um algorithm idea that we're not there yet

because we know we now know what that

the core of that algorithm is

it has to do with movement and reference frames

and

uh

transformers don't have that

in some sense

they have a very primitive sort of um

attentional movement

if you want to think of it that way

um

um

but nothing like kind of moving and referencing we talk about

so

um

so it's a little bit in the right direction

um

and of course they're really impressive

um

but it's

it's not

it's not close to what i think we need to get to yeah it's really fascinating

though

because transformers are strange beasts

they perform a kind of information rooting

and it's quite esoteric exactly how they work

and i think in some sense they're similar to capsules

i know capsules have equated to some of your work

but i wanted to talk a little bit more deeper than that

though

so a lot of um

your ideas from a machine learning point of view

come down to the fundamental dichotomy of discrete representations versus continuous representations

which is what we use in deep learning

we use vectors

and there are some advantages to using these continuous vector spaces

namely that you get for free

some spatial priors

so you can encode semantic similarity as a function of how close they are

and yeah

and also you have gradients

which are useful for stochastic gradient descent and

and interestingly

the manifold hypothesis states that most natural data falls on very smooth and low dimensional manifolds

so all of the human faces could be projected onto a low dimensional manifold

but um

i i know that a lot of your work has has been really solely focused on on these um

sparse distributed representations and then um

building on top in in the encoding

for example

so if i'm encoding a date representation

i would do it such

that this sparse distributed representation would have a significant overlap for date times that are in the vicinity

yeah

and so semantically similar things should intersect each other

and spatial relations between bits can be encoded using a receptive field similar to

you know how it works in in cnn

so

um

but nimenta has asserted that intelligence wouldn't be possible without these sdrs and they're the primitive of of intelligence

so what do you think about the kind of comparison between vectors

and and these um discrete representations that that you were never

yeah

well

let's just start

you know

the whole idea of these sparse representations comes because that's how brains work

i mean that's

there's no compression about it

uh

if you look in the brain

you look at any population of cells that are representing something at any point in time

most of the cells are inactive

uh

it can be anywhere from

um

you know

90 percent to 99 percent of the cells interact

you just don't see of the information carrying neurons or some neurons that fire all the time

we're not talking about those

but the ones like information

you just do not see any kind of dense representations

that is a

when you

when that happens

you're having a seizure

um

and we also see that in in in real neurons that they're not very high fidelity

they don't carry individual spiking rates are very crude

um

there's many instances where even the single

the first spike is actually the most important thing recovering information

so we're not like

this isn't so

on a high resolution

you know

four bits or three bits

or even

even often

one bit of precision and firing of a nerve

all right

so this doesn't exist in the brain

um

so we just take that okay

well

we need to understand why that is

and it's not just

it's like more efficient

from an energy point of view

that's not what it is

um

it turns out that um

sparse representations have a lot of really desirable and interesting properties that you don't get with dense representations

um

we can talk about those

um

it's not like everything is

um

um

there's a spectrum here

right there are some parts of the brain where you have representations

which are a little bit less dense

maybe of the tempest are firing and there

the activity rates really do matter

um

they're still not high precision

but they really do matter

and then we believe there's other parts where they're very

very sparse

so i might have five thousand neurons and let's say two percent are active

so i have 100 active neurons and all the information is encoded

almost all the information is coding in the population code

not in the fire rate

and we can talk about that

if you want to dive into the details with the advantage of sparse distributed representations

um

but it is an empirical observation of the brain

so you know we're gonna start with that

it's okay

well

why is it that way

you know

well

one thing i wanted to understand

because you made the comment before that

um

they could improve generalization

but intuitively they're the opposite of improving generalization

because they have such an incredible specificity

so

for example

when i was reading through some of your literature when you use boosting

for example in your algorithm it increases the specificity to such an extent

but then the way we generalize in our cognition isn't necessarily through the representation

it's through the abstractness of of the concept we're learning

would you argue

yeah

so i'll be honest with you

i've changed my opinion about this

um

um

i used to believe exactly what you're saying

which is like

oh

these overlaps between these sdrs are really how you're going to get generalization

we now realize it is a much more powerful form of generalization

um

i talked about it briefly in uh

my new book

um

a thousand brains

i don't know

if we really discussed it in any of our papers yet

um

and this is the idea that you're so i can switch into that

so i'm admitting right

yeah

i don't think it's what i said before

okay

all right

we learn

um

but there's a much more powerful form of generalization

if you want

i can delve into that right now

um

uh

it's not okay

so so we have

this is what we've learned is that each core of a column where you say the brain in general

um

it's building this sort of

uh

the model of the world you can think of as a graph

but it's

you can literally think of it as like

where are things relative to other things

it's like a computer-aided design

model

right

cad model

it's literally doing that

you have a model of something like a computer or your house

or room or bird

or ideas that they're structured

using reference frames and data

populating

reference locations and reference frames

and um

so you can imagine

uh

my knowledge of something again

as you know

in the book

i use the coffee cup example

quite a bit

but you can use anything

um

a knowledge of say a stapler is

is it has a certain set of parts

uh

which are then arranged in certain

uh

relative positions of each other and they have certain movements related to each other

and um

and that structure is

is that graph

if you will

is the uh

is the definition of that object

and that's your

that's your model of it

that's how you

that's how you represent it in your brain

now

when you

when you learn it

the way you do this is you attend individual components one at a time

so if i

if i were to look at a new object i haven't seen before

like a stapler

i would just attend to one end or look at what's what's this thing look like

then

what's that thing look like

and as you attend to these different parts

you're essentially building this graph

you're essentially building the model

you're saying

okay

this component

i recognize

that's like a hinge

this component over here looks like maybe a button

this component here looks a rubber pad

and i and i just start building this model in my head

but it looks my phone right thumb up there

um

i started building this mod

okay

so you got this

i'm trying to paint a picture in your head

how you you're doing this every moment of your life

as you look around

even like you're looking at the screen right now you're looking to see different things

you see where they are relative to each other

i think most generalization in the world comes about from a different from the process

of

if i see something new that i haven't seen before or i'm experiencing something new

doesn't matter revision

but just imagine you're seeing something new

you don't recognize it

you don't see this arrangement as a familiar arrangement

then you look at a subset of the components and you say

okay

let's just focus over here

and you say

well

these three components every year look like something i've seen before

these look like it might be a button that i pushed down and this thing over here looks

it might be a hinge

um

and then you

then you say

well

if it's a hinge

then a button with this thing

even i've never seen it before

maybe if i push on this button

the hinge will look

and and so you

you

basically your

your generalization comes about

because subsets of these graphs

that you build of the structure in the world are similar to subsets in other graphs you've learned

and you can say

well

these subsets are similar

therefore the behavior behaviors associated with those substances are similar

the performance of the substance is similar

so i now can ascribe to the stapler

some things i've learned about other things

um

and we keep

and if i see something really new

we just keep going down into small and smaller pieces until we find something i recognize

this next

okay

well

i thought i recognized

and and so that i believe

uh

is actually the primary way we generalize in the world

um

we can talk about how str's do that

do that too

but anyway

what you're talking about

it sounds a lot like sort of some of the graph isomorphism you know

work that that people talk about where

if you

if you've got this new thing and it has all these parts

and they're in a graph

you know

certain subsets of that graph may be isomorphic to previous

you know graphs that you've seen

and then therefore you can learn about it

um

and you said something really interesting

which is when you see something new

you know

you attend to kind of different parts of that and you're doing kind of a remapping there

and there's a lot of research on on grid cells

right that they remap

so when a rodent goes into a new

a new environment

it remaps the grid cells

can you talk some

i mean

is this related to the remapping

and i'm really curious about how does that remap

and happen in the neurons and how quickly does it take place

you know

yeah

well

so one of the the

the hypothesis

we have a thousand range theory is that

well

first of all

we deduce the idea that you need some that there is any in the cortex

there are these reference frames and these structures

these graphs

right

we can deduce that it's like okay

well

i know this is happening

i walked through the logic behind that

and then it was

at first to me

it was very odd

like

how could neurons build these graphs

right

how and how do they structure them i mean it

what with the way i used to focus is like my cortex knows

where my finger is relative to the thing it's touching

like relative to the thing

it's tilting

not relative to my body

and and so i use the example of coffee cup and the coffee cup blows around

so the reference brand has to move the coffee up

i was like holy crap

how does that happen

it's really hard for neurons to do this

and then

of course we looked into the literature of the hippocampal complex

which is grid cells in the anterional cortex

and place cells in the hippocampus and there's a whole bunch of stuff

and there's

there's a 20 year or 30 year history of research in this field

it's not in the neocortex

but it's it's something that's very related to the neurocortex

um

and and so

and that's what they

it's clear that grid cells and place cells and these other types

these vector cells that exist

there are building maps just like this

um

and so we said

okay

well

it's likely then that the the same mechanisms that are used

by grid cells are probably going to be used in the cortex

and the time we made that that was a purely evolutionary argument

it was like saying

it seems unlikely evolution is going to discover something like this twice

it's really hard to do right

it's really really hard to do this

so

and animals have to know where they were in the world a long time ago

so they probably evolved this complex mechanism from knowing where they are

and now the brain's gonna use that same mechanism for knowing where

like your eyes are or your fingers are

or you know

it's gonna be the same mechanism for everything else

and so we just said

hey

it's probably likely that the same basic mechanism that uses good cells

and place cells and vector cells is going on cortex

at the time we made that that prediction

we were not aware of any evidence of that was the case

um

and now there's a lot of evidence for it

now people are finding fit cells throughout the cortex

um

so so the general idea i'll keep get into your details in a moment

but the general idea that

hey

the same mechanisms are work in both places

seems to be right

and and now we don't understand those mechanisms completely

even though there are 30 years of research on grid cells and place cells

it's there's still some mysteries about them

um

but there's still a lot

but there's a lot that's known

and so

um

we can map on the concepts i was talking about like

okay

um

where am i

where am i in this graph

what's what do i place in that graph

you can think of that a little bit like place cells

um

in grid cells

you can think of remapping

is

um

there's two types of remapping like this is a whole new thing

# Chapter 6

even though there are 30 years of research on grid cells and place cells

it's there's still some mysteries about them

um

but there's still a lot

but there's a lot that's known

and so

um

we can map on the concepts i was talking about like

okay

um

where am i

where am i in this graph

what's what do i place in that graph

you can think of that a little bit like place cells

um

in grid cells

you can think of remapping

is

um

there's two types of remapping like this is a whole new thing

so let's just start with a new graph

or versus

let's fix the graph i already got

you know

and uh

and that's that's very much related

as you said to remapping of grid cells

not that we understand that completely

we don't

no one does

um

but we know a lot about it

it's a very

very technical field in the neuroscience solution

right

i've been fascinated by douglas hofstadter

he has a really um interesting idea on cognition

and he says that cognition is a little bit like the interstate freeway

um

in the sense that we we have this ability to make analogies

and this is something that current ai systems cannot do

and a lot of the symbolic folks are saying

well

of course

deep learning models can't do it

you're just interpolating on a manifold

for goodness sake

but um

your conception of reference

frames i i found fascinating because they're not just on your sensory inputs

there's you actually said that thinking is a little bit like traversing through concept space through these reference frames

and that's absolutely fascinating

so

but the thing is

when we learn information

many of us learn information differently

but we learn the same knowledge and the same facts

you know

we

we

we all know how to speak

um

the same language

and you did allude to this in your book

that there are some interesting differences in the topology of how you learn information

but we still seem to be able to reconcile it in the same way

yeah

well

sometimes you know it's it's like the way i view it

uh

again

we're telling stuff that some

some of this hasn't been published anywhere yet

um

but the way i view it is

um

if i think of myself as this part of the cortex says the cortical call

um

i don't know what come my input represents

and i don't know where i am in the world

i don't

i just don't know anything

i'm just a bunch of neurons right

and so i have to discover what i'm getting

i get two pieces of information

i get some sensory data and i also get information about how my sensor is moving in the world

yeah

we can talk about how that happens

but i get

i get some knowledge

and so i have to figure out on my own

what is the dimensionality of the space

what kind of space is it

i i don't have a preconceived

it's not like

oh

the world is three-dimensional i don't know that

um

i don't know if it's two-dimensional three-dimensional or in-dimensional i just kind of figure out

i have to figure out what is the structure of which i'm going to to place this information

and when you and i look at a chair we both see it as three-dimensional we

it is a three-dimensional object

we're going to build three-dimensional reference frames for that chair

they're going to be pretty similar

it's going to be hard for you to have a representation of the chairs

it's significantly different than mine

um

but when we look at things that we might not sense directly

something that someone tells me about or i'm reading about

in a book

the kind of reference frames you and i might create for the same facts could be very different

and and so we can arrange the same data in different reference frames

because we can't directly sense it we're basically relying on information coming from other people

and so we can end up with

like the same facts represented differently and reach to different conclusions

and this is why we have different beliefs about the world

it's almost always about things we can't sense directly

if we can sense them directly

then we generally have the same belief about it

you know

but numb the same kind of structure

so i think this is a fascinating idea

um

and um

uh

but it's

it's really fascinating to me

is

that there is no pre a notion about the dimensionality of the world that a column is looking at

and and the data

so it can represent almost any kind of data

as long as you can

could figure out from a movement vector

in a sense

lumen data and sense data

they can say

can i build a map of this thing that makes sense

the predictive map and um

uh

and if i can

i said

okay

that's my

that's my understanding of it

another

another beautiful thing about it is is the fact that your brain is

is creating these binary encodings at all

like i

when i was researching this

i found it very interesting that for grid cells

there is some some topography

at least in the visual cortex or in the visual layer

i'm sorry

so you know

grid cells that are nearby each other will have like a similar orientation

but maybe a different scale

but once you go up a level higher

so those things create the sdr

you know

they create these bits that are very sparsely populated

and then the place cells correspond to combinations of those

those bits to indicate a certain location

and once you're at that layer

there is no more topography

it's like now they've become truly digital encodings of information

right

well

i'm not sure

i'm not sure i understand that

because i

it's my

from my understanding

there's always going to be a topology

you're never going to lose it

uh

you may not see that topology in the place else

right

um

all right

that's that

i mean this

the neuroscientists who listen to white

listen to this

i don't

i don't want to insult them

because i'm going to make this very simple

i say you can think of like place cells are like what

and grit cells are like

right

okay

so

um

and you can say

well

this thing

the what thing is at this location

type of stuff

and so if you just look at the place cells

you don't

this isn't because they actually do uniquely encode places

so this is

it gets a little confusing here

but um

uh

but you can think of it just generally like that

um

if we're going to go deeper about this

we're going to start getting into all the unknowns about grid sales and play cells and they're very interesting

the unknown and weird properties

um

we actually think now that

um

this is work that one of our employees marcus

um

is doing is we think

actually

the

the better way to think about a coding is not

the location is not actually through the grid cells

but through these vector cells

things are things like optic vector cells

and so on

um

it's

it's in the

the grid cells actually might just play a very interesting singular role

they might play the role of what we call path integration

which is like

if i'm moving in a certain direction

i have to be able to predict where i'll be

uh

as i move

that's called path integration

i'm going to predict something i have to know where i'm going to be when i get there

um

and that most of the max

the graph itself is not built up of grid cells

it's built up these objects

these

these

more like these polar coordinate cells called vector cells

um

so again

this is an evolving field and you know

and even stuff

we wrote

a couple years ago

we're like

oh

maybe that's not right

it's like this

but the overall principles are the same

it's still a graph

it's still

you know data at locations in this graph

um

that hasn't changed

uh

it's just like

how's the graph actually constructed

that's a little tricky

right

i mean it's like it's fascinatingly deep

it's just a fascinatingly deep feel

but if i can just tie this back to sdrs

because i know there's

there's always a lot of really confusion or uncertainty about the value of sdrs

and just for our listeners sake

i wanted to share one intuition and get your take on it

which is if we are representing

let's say

location

uh

in some space

if all we need is 20 bits to do that

so we really only need 20 bits of information to represent a space location

to the resolution that we need

then

if you go further than that

if you have 10 000 bits and you decide i'm not going to use 20

i'm going to use all 10

000 put

like you know

little. 01 values kind of all throughout there

really

you're just creating noise and that other space that you could have been using for other purposes

right

and so i think that's one advantage of the sdrs

like

what do you think about that intuition

that's that's a little bit correct

i i

i

i haven't thought of it that way

um

let me give you some

let me just give you my take on some of the advantages of seos

so let's say i'm going to represent

uh

something

i have 5 000 neurons

i'm going to represent

so

all right

so let's talk about the capacity and let's say i'm going to use a 2 sparsities

so i have 100 active neurons and five four thousand

nine

hundred

um

they act if not

and

um

first thing

you say

what is the capacity of the system

how many different representations you can represent

so that's

that's just five thousand choose

you know 100

and it's

you know

gazillion because i'm in gaza

so then you can say okay

so there's no representational capacity

you

then you see if i randomly chose these sdrs

um

what is their overlap

well

they'll overlap by about two neurons each

because each neuron is going to participate in every 50 patterns

um

but not by a lot more

you won't find two patterns randomly overlapping by 50 bits or 30 bits

it's all statistically impossible

so you can

now you have all these these patterns

you can pick randomly all day long and they're not going to overlap

so they're very unique

each one is going to be extremely robust to noise

so you can add 50 noise to any of these patterns and they're still going to be recognized correctly

okay

so very noise

robust is another um thing

so high capacity

uh representation is very noisable

there's another property that

uh

we think is being used everywhere

which is really interesting

i don't know if there's any equivalent to it

other types of networks is what we call the union property

where i could invoke

um

not one pattern

but two

three

five

ten poundings at once

and so now

instead of having a hundred neurons active

i say i vote ten patterns

i'm gonna have a thousand neurons active

thousand out of five thousand

well

it turns out that all the processing that you do still works on all those

you can process them in parallel

there's you get

even though you're

you're mixing these things together

the other ends

the neurons that are that are detecting these patterns

don't get confused

and you can show this mathematically

so

instead of representing probabilities like a probability distribution

you might think about the brain often relies on this union property

which is

you know

there are multiple possibilities right now

it's not really a probability distribution

it's just like these are all possible things that could be happening right now

let's process them all in parallel and see which one works out a superposition

that's the kind of property that

yeah

i guess you could call that

yeah

um

and you only get the sparser

you are

the more of those you can do and the whole things become

so you got this huge capacity

super noise

robustness

um

it only takes

you know

if i want to recognize one of these patterns of 100 neurons

i only have to recognize i only have to connect to 20 of them

so so neurons typically don't really detect mana

they look for

maybe up to about 20 or 30 synapses at maximum

um at a time

so so all these

these are other properties

all these properties come about

yeah

since you did mention capacity

though i would like to

because i did see

you know some variable statements

sometimes i've seen no problem whatsoever with capacity

mathematically

we've got tons of it

but i was listening to one of your research calls

i guess

and you started to question

you know capacity

i think

at least in the

maybe the the grid cell

you know

the inter rhino cortex

or or some such

so i guess i have two questions

yeah

one is

is capacity a concern or not a concern

and also

uh

in order to make best use of the capacity

i'm sure the brain must have some type of entanglement

so you know

machine learned artificial networks

we know that they entangle representations

so the

you know

there's like weight sharing between different representations

do you think something similar is happening in

uh

biological networks

well

i i don't know

uh

the

the

the machine learning

uh

field that you're talking about

uh

and so i can't really comment on this idea of entitlement of weights in machine learning

um

i'm not familiar with it there

um

um

i do

i

i

i want to maybe correct something about sdrs

which relates to the question you're asking

uh

we think in some parts of the brain

some parts of the neocortex

these very sparse representations worked

as i just said

there are other places where there are less sparse

maybe 10 percent sparsity

grid cells is a great example

if i actually look at a population of grid cells

you know it's not like one percent sparsity

it's more like ten percent sparse

and and even and there the activation levels actually do matter

it's not like they matter

like three digits of precision

but maybe one digit of precision

and um

and they

and so that doesn't have all the properties

i was just talking about

um

when you get to like 10

you can't really do a lot of superposition patterns

so it looks like

in some parts of these algorithms

there are sparse patterns that work on some principles

and you know

and in other parts

they're very

very sparse

i work on other principles

it's not everything is super sgrs

you know

i used to think that

but i don't think that anymore

um

i just became enamored with seoul and i said

oh my god

this is really cool

you know how these things work

um

but i now we now know that there are in some parts of these algorithms

you need to have

um

still sparse

but more dense and activation of level matter

and then some of the capacity issues

uh

you don't have those

the capacity advantages

and so

um

and that's clearly true with with grid cell representations

um

but i i imagine it's true at other things too

i know it's true in other rares

too

so it's

it isn't one size fits all here

um

i do

i do say we could say for certainty that you don't find anywhere in the braid

where anything is represented by full active

you know

uh

fully dense

uh

networks where all the neurons are fire

developed that this does not exist

doesn't mean this couldn't be useful

and it doesn't mean that we could in the future do something like that

it just seems very unlikely to me

uh

i know that's how most

uh

you know

convolutional nora

that works with networks work today

um

but

um

right

but you probably know there's a lot of

there's a lot of effort going on in sparsity

right now

in the classic machine learning community

uh

people try to do it and we're working on that too

um

so so i i think there's something fundamental about the representations we use when we build ai algorithms

there's a dichotomy between

you know

the good old-fashioned ai people

they

they work at a level of abstraction higher

they try to implement the mind and us connectionists

we're trying to implement the brain

i think there's something fundamental about that level of abstraction that we work at

so there's a discussion between people like scott harrison

who think that computation is raw and should happen at the lowest possible level

so just think of how neural networks work

they distribute knowledge

don't they across lots and lots of different neurons

and then there's um

uh

people like krishna swami

and he's a computer scientist

and and he thinks that we should be dealing with much higher level computational primitives like functions and types

so

um

i i guess

from your perspective

because i i feel that all of this is strongly emergent

i feel that we could work at the very lowest primitive level of computation

and all of the magic emerges

do you think we should be doing that

or do you think we should be working higher up the stack

no

i

i

well

i think we have to working higher up the stack

um

i mean

of course you can break any system down and say it's just a bunch of atoms

it's just a bunch of molecules

right

it's just a bunch of cells

um

and that's true

and you can

theory

should be able to understand that

but just like thermodynamics says

it's not very useful to think about liquids as a bunch of atoms

right

you

you need to have something at a higher level than that

and and so you know what

i guess

the principle we

that we'd assume that we just we have

now

in some sense discovered that the brain uses this representation scheme using reference frames and movement

now i think that's the right level to think about it

i don't

i

i

you could try to extract it less than that

but why bother

that is the unit that we're talking about here

# Chapter 7

theory

should be able to understand that

but just like thermodynamics says

it's not very useful to think about liquids as a bunch of atoms

right

you

you need to have something at a higher level than that

and and so you know what

i guess

the principle we

that we'd assume that we just we have

now

in some sense discovered that the brain uses this representation scheme using reference frames and movement

now i think that's the right level to think about it

i don't

i

i

you could try to extract it less than that

but why bother

that is the unit that we're talking about here

we use models that are

but okay

that's

that's

that's the thing

though

but

um

the brain was evolvable

it evolved within biological and environmental limitations

and and go

five people say

well

you know what scrap that

let's just implement the mind directly

you know

and they think that functions and types

um

they're not an imaginary concept that have been invented by humans

they think that mathematical knowledge has

it's universal and it's been discovered

by us and we would be silly not to implement it directly in this higher level language

because if you think about it

even

um

the knowledge that we have

you write about this in your book

it's fascinating that now there's a separation between our genes and the knowledge we've created as the human race

and all of this stuff now is

is

is emergent

our society is emergent

so why not implement it directly

but why trust

because

because

because you're gonna

you're gonna guess wrong

right

you know

you're gonna say

okay

well

i will figure out what that structure is

what you know

what is that mind structure that we're gonna

you know

and and so

for example

most of the

you know

the good old-fashioned ai never considered movement in their

in their

in their fundamental aspects of representation space

right

um

and you know

or even

do you look for what dr lanatt did was psych

you know

same idea

right

um

yeah

and so

but but now i know

okay

the brain builds these structures using movement

so it it's not like it was a wrong idea of the old ai people

it just they just didn't pick the right schema

it was the wrong schema

well

could you expand on that

because the thing is

um

because we've got a go-fi person

we're very

we're very good friends with

and he is convinced that there is

you know

like the knowledge engineering bottleneck and yeah

it

it

it's very

very brittle and it didn't work very well

but um

but you're reading your chapter on knowledge representation

i think you were saying our brain is

it's like lots of models

when we model a stapler

it's actually lots of little models in our brain and it's it learns a kind of

um

um

it analogizes the knowledge in a really sophisticated and distributed way

so when we want to know what happens when we press down on the stapler

we run a simulation in our mind

you know what

what happens if i press down on it

what happens if i stretch it

and our models seem to um generalize to any version of a stapler we might ever find

so that almost convinced me that we do need to have a lower level representation of knowledge

lower than what

lower than the the reference frames i was talking about

well

lo

so the go five people say that we should use um functions and types and relations to describe knowledge

but do you think that's possible

i

i i don't want to misrepresent go fight because i haven't looked at that stuff in a long time

um

i i think you can't do this at the level of the neuron

i mean

obviously brains are made of neurons

so

but they're not uniform neurons

we have dozens of different types of neurons hooked up in very complex ways

they do different functions

it's not just one big neural network

uh

that structure is important

um

and so

uh

i think it's now become evident to me

and it wasn't this way five years ago

it's really evident to me now that at least in the brain

knowledge is represented in this structured form

uh

with reference

aims and movement

it's the same inspiration that um

uniting has with with capsules

but he didn't incorporate movement as part of that structure

um

you know

uh

that

with that

you know

but now i

i now i understand

okay

at least from a brain's point of view

these reference frames are constructed

as i mentioned earlier

they're discovered through through movement

observing movement of the of the sensor through the world

and

um

and so now we have this

the

the

the basic structure of

if you want to call it will fi is

is these

uh

movement related

um

um

reference frames and placing

you know

and stuff like that

i

i can't

i don't know how i would do it

less than that

i don't know

what how i want to get rid of that

now

what do i do

just ignore that and go back and just put a bunch of neurons together

i don't know

do you mean

could you come up with a genetic algorithm that could have discovered this using just traditional neural networks

sure

maybe you

could

you know

i don't know

which is the fastest way to get there

but now that i know it

why would i want to do something different

i

i

i

maybe i'm misunderstanding your questions

right

yeah

well

i

i think the the

the main issue is that we don't understand the representation

you know

if you look at the open ai microscope and you can visualize all the different neurons

and so on

these objects have been distributed in a way which is completely ineffable across all of these different neurons

and is that the most high fidelity possible representation of knowledge

well

i don't

i don't know

but well

i will say now is

i currently believe very strongly that

um

in the future

when we build ai systems

they're going to work more like on these brain structures

and they won't be just highly distributed

mysterious blobs of neurons right there

that's great

by the way

it's

it's great

uh

not only because of what they

how they perform

but it's great

because they'll understand about a whole lot better

this

you know

this touches on the issue of the

the threats of ai

right

you know

the so many of the threats of are based on the idea

that we don't know what's going to happen

well now you actually will know what's gonna happen

it's like

you know

you know

i just like to say it works like this

it's what's it

you know

i can't put it everything

but i

you know

i don't know what it's gonna learn

but i know how it's gonna learn and what it's what it's capable of

um

so i

i just

you know

i i don't want to debate people about this because i

you know there's a lot of smart people out there and they may have things

no things that i don't know and they can pursue things

but from where i see right now

um

it's very clear to me that

okay

this is how brains do this

this explains everything explains again

how we structure all knowledge

um

and how it evolved

that's a big part of this

and how you know how we have this common algorithm

and it's going to be based on these reference frames and movement and so on

and so okay

well

why wouldn't i just just just build that

let's go for that

um

and you know

why build something else

if i understand that

why build something else

i

i don't

i understand the point of

is it going to be better

you know

in the future

we will certainly take you

let's say i'm right

let's say i'm right

and we build machines that work on these principles that i outlined in the book

well

of course

we can very well go for that

we don't have to stick to the way biology

maybe we'll come up with a better way of representing things in structures and reference frames

maybe it won't be movement-based maybe something else

i don't know

but i try to do it

do it

build something and get it working

why would i go away from it

you know

yeah

go ahead to try and steal steel man

the go-fi people

it's because they think that knowledge is universal

we discover it

there is only one representation of knowledge

and clearly in the brain

you can learn things in different ways

you can

i can teach you a curriculum and i can teach you it in a different way

and maybe in some weird way

our brain does learn some latent representation of the knowledge

which is the universal knowledge

but they're making the argument that

why don't we just represent it the only way it can possibly

well

obviously

well

first of all

that's not

you know

it clearly isn't true

i give it

just like you just said

i gave the example in my book about taking a bunch of historical facts

and arranging them along a timeline

and arranging them on a map

and you end up with different inferences and different beliefs about the same set of facts

so whether there is a universal truth

i don't know

but clearly humans don't know that universal truth

um

we

we can't know everything

we can only sense a small part of the world and um

and in in

you know

the vast majority is invisible to us

so who the hell knows what that universal truth is

um

what we can do is build good models and

uh

and the models

there isn't a universal model

there isn't one correct model for for history

right

um

we might like to believe there is my models

writing yours was wrong

that kind of thing

but you know it's really easy to point

out

even non-controversial things like the timeline versus the map will lead to different models

the order in which you train someone will matter

you know

so i think we have to live with this um and i don't think you know

maybe there's some platonic

you know

go five people thinking there's some plot

universal knowledge that's correct all time

i don't know

but certainly i don't think that's accessible to us

if there was

we

just we

just we can't

we can't sense the world both physically and time wise

we

our senses only deal with a teeny part of the world

yeah

it's a good part

but it's not a little teeny part

we don't even know the universal truth about space and time

is

i mean

definitely true

if i could move on to some other questions about the brain

um

so you of course focus very much on new cortex as the

you know

seed fuel intelligence

and i think i think you're definitely on the right path with that

i think most people agree something similar to that

but there's also lots of other parts in the brain that see pretty important and i'm specifically

uh

interested

it's like one thing that machine learning people are interested in is the dopamine circuit

so like the cortical basal ganglia critical

and which seems really

it seems like alerted

it seems like some kind of learning is going on

there

could you maybe say some words about that

yeah

so obviously the neoprojects is connected to the rest of the brain in modern ways

um

myriad ways

um

uh

it's

it's not in isolation and it's a very complex relationship

so lots of other things

um

that doesn't mean you can't understand what its circuits are doing

um

but in a human and their cortex is complexly tied to things

and one of those things is that

if you think

if the neocortex is as this sort of map of the world or model of the world

well

it

what should it learn and what goals should it have

the dopamine circuitry is clearly associated with what should we learn

when should we take

when should we take the effort to learn something which is metabolically expensive

um

and um

and so somebody has to decide that

uh

my basic contention is the decision is not the neocortex's decision

right

mostly

it's somebody else's decision and

um

when we build intelligent machines

we'll have to figure out who's making that decision

and

um

it won't be in the neocortical equivalent

it'll be in another part of the intelligent machines equivalent

but uh

there's absolutely no reason we would have to model the goals and um

emotions and other things that other parts of the brain

do

unless you want to create something was human-like right

so you know

i used to joke that our dopamine signal in our models is a switch

i turn it on and turn it off

i can learn

now

stop learning now

right

just manually turn it on

manually turn it off

uh

when we run experiments

um

it doesn't have anything you know

more complicated than that

um

but there's it

you know

the only thing i'm claiming here is when we build intelligent machines

if we build them on the principles in the neocortex

they have to be embodied

i mentioned this

the whole chapter in the book about they have to be embodied in some sort physical or non-physical embodiment

but some embodiment

they have to have

uh

you know

very sort of safeguards built in

they have to have various mechanisms for what to learn

what not to learn

someone has to provide goals

uh

what should the goals of the system be

uh

what are we trying to achieve right now and

um

but so those things have to exist

but they don't have to be modeled on

i don't see any reason at all to model those on the neural circuits

the way the neurons do that that

now we're getting pure biology

we're now getting into what is a biological organism need

including

like keeping your heart going and breathing

and you know

making sure you have sex and all these things

um

yeah

i can ignore that

just if we're going to build intel's machines

just make them a little bit

um

to our purpose

i guess i would say another neuroscience question is that i know

um

you know

a lot of people are skeptical that the brain can do anything remotely like bat propagation

right

i mean there's sort of good arguments for why that's just not possible

but on the other hand

i don't know if you've seen kind of a lot of the recent work by blake richards

at mcgill university

where he talks about

you can do something that performs in a very similar way to back propagation

because

and especially in the neocortex

the

uh

the neurons there had these very long apical dendrites

and they had these sections with these calcium channels

that can do these very long lasting activations that trigger like spike trains

and he's saying that there are

there is evidence that you know

if you have feedback from a higher or downstream layer back into this

the

uh

the apical dendrite

you know

connections that that can have an effect of creating something like back propagation

it's like stochastic back propagation

uh

what are your thoughts on that

well

i'm

i'm a little bit familiar with that work and blood breakthroughs work a little bit

um

i viewed a little bit of a distraction because you know backpropagation's a cool idea

uh

it works on classic neural networks pretty well

um

i don't think the end result is like kind of nerf what you see in the brain at all

i don't think the kind of learning

that that back propagation does is the kind of learning we see in the brain

um

and and so

uh

and there's lots of other theories

what those apical dendrites are doing and why you have that stuff so

um

i'm not going to say that back propagation isn't occurring anywhere in the brain

i don't know

maybe it is

um

but it hasn't played a role in our theories about the brain

and i think a lot of people spend time trying to shoot her in there

like i'm going to prove it to you

see the brain really is doing this

it's like well

but the paintings look like anything else

none of this stuff

the brain doesn't look like anything like you're talking about here

you're just trying to put this

you know

one little piece back prop into this thing

i mean for example just take away learning

we know that learning in a neuron

lauren taught these dendritic branches and and the synapses are distributed on these branches

uh

well

this is

this is key

this is critical

you can't

you can't get around this

this is this

there's an important information

theoretic

reason for this

it's

it's not just some other reasons

it's doing something important from an information point of view

and and so the whole idea of backprop working with learning on individual dendritic segments is

i don't know what that means

right

it's like

it's like hard to fit in

so right to my mind

it's like noise

i kind of like tuning it out

saying

well

if they figure out something really important

they'll let you know

well

i mean i'll pay attention to it

yeah

i think i think more his his motivation

and i quite agree with this

it's about taking inspiration from the brain

so this algorithm that they discovered by kind of looking for things that could you know

at least enable

you know

feedback mechanisms for the training

is it

but are they

are they taking information from the brain

are they taking inspiration from the neural networks and trying to figure out what the brain could do

what they're trying to do is they're trying to figure out how can the brain do deep training

how can the brain do deeply

and they like assuming that that's what

that

assuming back prop is what they want

no

no

i just want to know

can it do

how can it do deep learning

and so then they look very carefully at the biology and they find this this alternative mechanism

# Chapter 8

it's about taking inspiration from the brain

so this algorithm that they discovered by kind of looking for things that could you know

at least enable

you know

feedback mechanisms for the training

is it

but are they

are they taking information from the brain

are they taking inspiration from the neural networks and trying to figure out what the brain could do

what they're trying to do is they're trying to figure out how can the brain do deep training

how can the brain do deeply

and they like assuming that that's what

that

assuming back prop is what they want

no

no

i just want to know

can it do

how can it do deep learning

and so then they look very carefully at the biology and they find this this alternative mechanism

which can produce results that are very similar to back propagation

but actually it's slightly better in some ways

and so it's leading to inspiration for how to improve ml

and i guess that's the question i really wanted to ask you

which is i always ask this what's missing question

okay

so we have artificial neural networks

clearly

they're a very stripped down

you know

micro abstraction of what happens in the brain

if you could add in a few things like just one

two

three kind of abstract properties to neural networks today

what would you suggest should be added to get the biggest bang for the buck like

how can we incrementally improve them

okay

well

so so that's what we're doing at nemesis right now

so

um

my colleague

subatomi

ahmad

who's really the machine learning expert

i'm more of the neuroscience guy

um

he put together a road map

well

we did it

together but he's implementing it

uh

a road map

the idea is like

hey

we have these grand theories about the neocortex and so on again

maybe the right

maybe they're not

but we think they're right

um

well

what do we do next

like what do we do right

so the answer question was well

how we incrementally get from where we are today

uh

machine learning to where we want

where we think we need to be some number of years from now

and so we literally have a roadmap and we started off by saying okay

uh

first i'm going to focus on sparsity

and so we've been doing that and we've been at

uh

we've been making really interesting progress

some of it

which we published and some

some of it

we haven't yet

uh

about introducing sparsity to

uh

first we did it for the convolution neural networks and we did for other types of nerves

now we're doing it for transformers

um

and showing that we can speed these things up and make them more robust

and and i'm talking speeding things up by a lot

not a little bit

but a lot

uh

depends on the network

depends on the world through all these different things

but uh

how do

and then how do you implement that on heart

so we start with sparsity

the next thing we're working on is the dendritic computation

because what the dendrites get you

and we haven't really talked about this much at all so far

but with the dendrites

each individual section of a dendrites like its own little complete computational pattern recognizer

um

they allow you to represent information in different contexts

and so um

uh

this is

we think it's going to make

it's far far less training data to train existing networks

um

um

and so and allow us to do continuous well

because when you train a neuron you don't train all the synapses

you only

you only modify a few on on one of these dendritic branches

and so you can

you can learn without forgetting things

um

so we're working on that

so okay

take existing networks make them so that they're going to be continually learning or they

they don't forget things

um

and make them so you can train them for fewer data points

and then so we're

we're well into that now

um

and then the third thing with the next big one

which is sort of biting off the whole concept of reference frames

and um

we're just starting that

um

and it probably will take several years

um

but and then finally we do the whole the whole thousand brains theory

we have lots of these columns and so on

so at least we put together

we have a road map

we're working to it

i think i i'll be honest with you

i think this

the progress we've made is actually far better than i thought it would be

i thought it would take us longer to achieve the things we're achieving

uh

the performance gains we're getting are really really something dramatic

and

uh

without losing accuracy

um

so

uh

that's

that's what nomento is working on

and i don't think we've talked about this road map

um

but we haven't given all the details of it

yeah

so it's not public yet

the roadmap

well

the rare map

because i've talked about it in like you know talks

i just i just spoke about it here

right

so i have pictures of that stuff

um

but the details how we're doing it

like we've created

we've discovered

like you talk about sparsity

the problem is you want to run sparse networks

you want to run them on some hardware

right

what kind of heart

are you going to run mods

is it gpus

cpus

fpgas

well

we're doing all of those

um

but they all have their problems and issues and because gpus don't like sparse networks

at least you don't get any benefit from them

and um

and fpgas are hard to use

but they're pretty

they're good

but they're really hard to use

um

and

and so that will be discovered and including

very recently we've discovered tricks

um

engineering tricks that allow you to map sparse networks onto some of these existing architectures

uh

better than we think anyone else has been able to do by pretty large margin

and so

um

that you know these are very engineering problems

uh

they're not pure

you know they're not pure theoretical problems

they're very big engineering problems and we have to be able to solve those

because it's no good to say

hey

spartan is great if you can't run it faster on something

you know

it's gotta really work in the real world

so it's exciting and we'll see how far we go

but right at the moment it's going great

i'm really thrilled about it

uh

we've actually decided to invest more in it

because

uh

we're hiring more people because it seems to be working so well

so we've been talking a lot about intelligence

uh

you know

however

that might exactly be defined

i think rural on campus pitch what we need when we talk about intelligence

the video cortex is this map

this ability

you know

however you might exactly want to define it

but we've also talked

you know

we

we've

um

a little bit moved

you talked about motivations about

you know

which may you know

relate more to dopamine or whatever you know in the brain there you just run on your cortical intelligence

so in your book use you

and of course dedicate a good chunk of it to these questions of motivations

risks of benefits

yeah

and you describe yourself as someone who is pretty skeptical about

like severe risks

um

so one of your

one of the things which i fully agree with

by the way

is you very phenomenally describe how intelligence is kind of like having a map

either

the map itself has no immunizations

it's not good or evil

it's

you know

it's just a tool it could be used to do something nice or something good

you know

or it could be used to do something bad

um

the map itself is

you know

it's orthogonal to these kind of questions of motivations and you know

good or bad

it kind of feel the human idea of you

we can't get it out from the news

but

at some point

of course

when we actually build these systems

we don't want a lot to be in there

right

you know

we do want our system to actually do things

at some point

we'd only grow about sitting there and know everything

but never take the actions

and the book

you

for example

gives you the example of

you know

we want them

you know to go to mars and then

you know

build a colony and not just sit around the sun all day

you know

thinking about the universe or whatever

so at some point we have to introduce some kind of motivational system

the brain seems to solve this

according to this

which called the old breed

and um

i'm not sure exactly which parts you found of the oval prey and versus the newborn

but some of perfect obvious

you know we have circuits that give us hunger

or you know you have other motivations or whatever

but it's and

but it seems like these parts are will be very important that

okay

let's assume your project goes fantastically here

20 years in a mentor

is the biggest ai company in the world

yeah

you figured out the neocortex

you've created a wonderful algorithm on

at some point

we have to inject motivation

i'm sure you agree with

yeah

and we have to assure that those motivations aligned here in like what we want

and that seems like a hard problem

maybe if you talk a little bit about that

well

i don't know if it's a heart problem

i think the concern that people have about this is that these machines will evolve their own motivations

that that

or somehow they

they are some sort of

you know

epiphenomena disappears

um

and

um

i i don't think that's right

um

i think we will have to work hard to put them in there

like i mentioned earlier

um

it will not be easy

uh

we'll have to design what how these systems work

um

unless we put this machine in some sort of evolutionary um structure

whether it's a self-replicating machine or it just uses evolutionary algorithms

genetic algorithms to decide what its motivations are

um

they won't modify themselves

they won't change on their own

you know

um

i i gave the very simplistic example of a self-driving car

but i think it's a correct example

you know

i tell the car where i want it to go

it's not going to decide to go someplace else

because today it feels different

you know it's

i could make a call that does that

but why would i do that

i'm not going to design a car to do that

so if we send roll rods to mars to build uh machines

i'll have to be motivated to solve the tasks they've been given

um

and

but we will also build in all kinds of safeguards

just like we built in safeguards into cars

and we won't allow the machines to

you know

it won't be a mysterious process

it'll be like a self-driving car

it's not going to be something we don't understand that happens and runs away from us

it will be difficult to do

we can make mistakes

we can put in bad things and machines will do bad

things

but it's not like the machines on their own are going to do this

it's not like we've just enabled this beast

that's just going to take over and decide on its own what its motivations are

so i make it very clear in the book that there are a lot of risks with ai

and a lot of bad things can happen with it

but i

my only contention is that the fears of existential risk

uh

are overblown

i don't think it's true at all

and that the arguments don't really hold up

um

because there are more arguments based on ignorance

as opposed to some detailed knowledge of how these things are going to work

so did i answer

i absolutely agree with you

uh

i

i happily agree with you that these

like epiphenomenal explorations or naive

you know

a lot of like sci-fi depictions are like

the robot becomes conscious

it wakes up and it decides to rebel against its master

of course that's a pretty silly person

not everyone thinks that's silly

by the way

so i

i

i know i i

and i'm well aware that

on everything facility

but i feel there is a stronger

a similar argument

that's

i don't consider the same argument but has similar consequences

and in a way it's kind of like

in your book

you have a chapter titled how the new tortex can thwart the old brain

and you yourself describe the old brain as the motivational system

and now you're describing the neocortex

the new brain plotting to thwart the goals put in by

you know the

you know

our creator

you know evolution or whatever

so it seems like our

why

wouldn't our machines maybe have similar capability interests to like modify their own old brain

quote

unquote

that's interesting

um

uh

yeah

i can see your point there

um

let me see

i

no one's ever asked me that before so let me just think about a second

um

well

um

the way i view that with the

you know

the old brain's trying to sort something okay

so

first of all

then

yeah

the new bridge is going to like overtake the open

so the new

the neocortex

that's

it has a model of the world

and the model is very consistent and it strives to have consistent models

it wants

it wants to make its model correct

that's one motivation it has if it sees something wrong

it tries to correct it in this model

so he has this model of the world and says

okay

the world ought to work like this

if a happens a b happens and c happens

and that's good

uh

but then you know

the old break comes along and says

nope

we

don't

we're not going to do that

we see a and b

we're going to go x and uh

and so it violates the model in some sense

right

it's like

while it's the model and so

um

generally

the new neocortex uh

is not able to overcome the older brain things

it just we just give in to our desires and emotions

uh

but we just struggled with it

um

so i guess you're saying

where does the motivation for the from the neocortex come

how does it decide that its model is more important than the old branch model

um

and i

i don't

it's a good question

i'll have to think about it some more

but i think that the root of it will be that the motivation

the true

and only motivations the neocortex has is to make its model the world correct and to fix errors

um

and um

that is

it says this isn't right

my model says this isn't right

uh

we

you know

i want to correct my model

but i'm being told not to

or i'm being told to do things that are violating the model

um

and so that is its motivation

now you could take that to some extreme

let's get a little sci-fi here right

you could take this to some extreme and

uh

say

well

um

the neocortex really figures out how the world works

the future brain ai and neophytes really figure out how the world works and humans got it all wrong

you know

uh

let's let's bring it to an example

we could

we can relate to

there is no god

all right

i know i figured this out

there's no god

you guys all believe in this

but it's not true

so

uh

what do we do

about that is

is this ai system

uh

put up with it

or is it

or does it do something about it

or does it just keep trying to prod humans to do something differently

um

i don't know

it's a good question

i think you bring up

that's a very interesting philosophical question

um

i will say this

i don't think this is something that's going to happen fast quickly overnight

you know it's

it's

it's not

this is something we don't have time to think about this quite a bit

um

but it does bring up a question

what if

what if

the the world as as a smarter machine unknows

it violates the world

as we'd like to believe it

and that's really the the

the friction you're talking about

and

um

what should we do about that

there's an even more direct like kind of physical assault on this

that doesn't require much philosophy

it's just what about the traditional mechanisms that created life

which is variation

and you know random variation and natural selection

so for example

if we send robots to mars to build a colony

they're going to get damaged and destroyed

a boulder is going to fall on them

so they're going to need to be programmed to replicate themselves they're going to have to be programmed to build replacements of themselves

and they're not going to be able to do that with absolute 100 fidelity

so errors will creep into the

the neurons or the silicon neurons or the programming or whatever

and so you've already got in place

the requirements for evolution

which is information transfer or self-replication oh

i'll push

i'll push back on that

first of all

uh

uh

hanging

uh

robots

self-replicate is really really hard

are they gonna build their own semiconductor chips

you know

are they gonna

you know

mind their own

you know

titanium

i mean i don't know

but

but but even if they do

evolution requires a very complex structure for

um

how information is represented and and requires that information be changed constantly in our offspring

right

you know

so when we build

# Chapter 9

hanging

uh

robots

self-replicate is really really hard

are they gonna build their own semiconductor chips

you know

are they gonna

you know

mind their own

you know

titanium

i mean i don't know

but

but but even if they do

evolution requires a very complex structure for

um

how information is represented and and requires that information be changed constantly in our offspring

right

you know

so when we build

we build

you know

computer chips of nvidia builds index chip

they

they're all pretty much going to be identical

um

so i i push back on the idea

that there's inherent evolution built in any kind of self-replication i don't think that's true

i think we're if it

if there's some variation

it would be very

very minor

and and and it's not going to be passed on genetically to somebody else

you know

there's going to be some blueprint

how to build this ship

which is the neocortex

the blueprint itself isn't changing

so if i

if

i

if i build a neocortex from the blueprint that slightly got an error in it

that's not going to be propagated to its children

um

so i think there's a lot of ways you

there's a lot of things you have to do to create evolution

it's not easy

um

and i don't think that's going to happen accidentally

so i'm pushing back

keith

i don't think that's going to happen

you know it's good

it's good to think about it

ask that question

right

right

but you know

i don't think

so

i think

in reality

if you're sending robots to mars

but you know

in the chapter in the book i mentioned this point that we could send robots across the universe

and then they would have to self-replicate right

and after i wrote it

i said

oh damn

i don't know how to do that

i just don't know how you know

a fleet of robots

you know

ai systems appear on another planet in some distant part of the galaxy

and they have to replicate themselves

what would be the physical form

of those ai systems that they would be able to do that in any reasonable amount of time

and effort

because you know

just think about again

i mentioned

like semiconductor factories

right

you know

it's like they're gonna build those things

you know

how do they do this

so it made me it

it was a hole in my argument that we could send ai systems across the universe

and i didn't put out

pull out the whole

i said to myself

well

i

i excuse myself in this regard by saying

well

you know what

if we're going to send ai systems to other parts of the universe

they're not going to be built of silicon chips there may have to be some other manifestation that

like biology is able to replicate

uh

on its own without the use of

um

these complex

uh

other systems like semiconductor factories and

um

but we're so far from knowing how to do anything like that today that

um

i don't know

and then maybe we'd be in trouble

right

we send these things over there

they evolved and they come back

and yes

i don't know

but we're talking a long time from now

yeah

that's for sure

there is a kind of failure mode of thinking

you know too far ahead into the sci-fi future and you're just kind

of generalizing your fictional evidence

yeah

you know

yeah

i guess what i

um

so what i have taken from

like your

your obsession

your book didn't for now

kind of about like motivations and essential risk is that i think

actually

uh

at least except essential risk

people that i personally find interesting

they agree with you in like a lot of ways

actually

in that

i think a lot of them are very reasonable in the sense of saying

like

you know whether it's fast or short

it's kind of like

you know

it's kind of beside the point and you try to see

you know

humans already

kind of you know

hard to control and like you know it's like

if you actually try to think about

how would we write a motivation system

because you know it can't really be a learning system because you know

like in your book

you give an example

where if the brain comes up with two ways to get to food

and one of the path has a tiger on it

the old brain will say you know kind of like

oh no

don't go there

but you know the whole brain

how does it know that a tiger is bad

that's like also like

not obvious to me

how we would necessarily learn that

so i guess it just it feels to me that

um

as you so say you're focused on the neocortex

which i think is an important thing to be working on

but maybe we can agree on that

some work on these motivational systems and control problems might also be justified

oh yeah

sure

i'll totally agree with that

um

i

i think it's justified and necessary

but i'm not scared of it

and you know the the conversation

sometimes people we shouldn't be doing this research because it's going to get out of control

aren't you kidding me

it's gonna be so hard to do this at all

you know nothing's gonna get out of control

um

so we need to do that

we should do it

the conversation we're having today is great about that kind of stuff

but i guess it's just not

you know this

there are quite a few people feel this existential risk is

you know

upon us

and

and in any day now

you know it'll be too late

therefore

you know we have to stop all this stuff

and i

it's just i can't see that in any possible scenario

but yeah

we have to think about these things right

we have to figure out

not because i'm

it's scary or dangerous

because we have to figure out

because we have to do it and and i use the example of the book about

you know

we put in safeguards

my car doesn't always listen to me when i

if i'm about to hit something

my car puts the brakes on

even if i put the accelerator down

ignores me

well

we have to put in some some fail-safe systems too

um

in these systems

so they don't

you know

end up damaging things

but it all has to be done

i had a question on on intelligence

i'm fascinated on the nature of intelligence

and you were saying in your in your book that you know the traditional go-fi people had a task

specific skill conception of intelligence

and then it moved towards more of a flexibility model

so being able to learn

and there's also a really interesting tradition in cognitive science about embodiment

which i think is fascinating

and you know

my favorite person is francois chorley

he says that intelligence is the infamous

well

i think he says it's the task acquisition efficiency and generalization

but but also

quite

you know

now we've been talking about this a lot

and there's this interesting idea that you can think of intelligence as the ability to um acquire knowledge

so almost all of the knowledge that humans acquire in their lifetime is

is not empirical in trial and error

it's given through instruction or reasoning and deduction and some

some magic happens with these intelligent systems

you know

why

why can we deduce so much knowledge

is that a question to me

yes

uh

why why i mean mechanistically

well

how do we do that

i mean i

i think i've

i thought i answered this earlier

so maybe i'm

i don't want to repeat myself too much

um

and maybe i don't understand the question

um

well

for example

you know

you knock the beer bottle off the table

you can now reason that the floor is wet

someone might slip up on on the floor

yeah

so almost all of the the knowledge that we that we have around the world is deduced

or well

it's it's

it's deduced that knowledge that you had to learn that

through observation

if you had never experienced liquids and you never started slipping on liquids

you never experienced knocking a cup over

you wouldn't know what's going to happen

i mean that's what kids do

little babies do this

right

they're like

oh

look what happens that we could live in a world where liquids fall up

and that would be a perfectly good world

and that's what we learn

um

so i won't think we deduce

this

we observe it and and then by analogy

we we predict

uh

how other things that are similar would behave similarly

uh

but i think you know

to me

the way i look at it is you're born with this with this structure in your head

that is designed to learn the world through eyes

and ears and touch

um

but it really in the neocortex knows almost nothing about what it's going to learn

the old parts of the brain

that's not true

right refining refining questionnaire because i'm

i'm also fascinated

by this concept

that i learned it from the first time in your book

that the neocortex now i was going to say blank slate

but keith would kill me for saying that it's not a blank slate

it's a template

because there's actually there's a lot of evolutionary knowledge that's gone into there

yeah

yeah

because you

because you can think

you know

it's just learning all these signals

but just the way the ear has evolved over time

that the information gets encoded in a certain way

the cochlear has the logarithmically spaced bands and so on

so it's learnable because it's been encoded in in a certain way

but people like chomsky

they say that we have a kind of universal grammar or language built into our brains

and it was endowed

by evolution

so where do you kind of draw

that

does he say that about all knowledge about the actual language

i thought he just said that about language about language

yeah

yeah

yeah

so i don't know

i mean

but we're

we're talking about all knowledge now

right

so like it's so you know

so i think

look

there

is

there's all these assumptions about the how how the cortex is connected to things right

and how it's internally connected

that have been evolutionary advantageous

and so you know

we

we only see certain spectrum of light

because that's a good spectrum to look at

and we only certain types of you know sensory inputs

because those are things that we process them before they get to the near cortex

so that they're in the right form for the neocortex to work

um

we allocate a certain amount of our neocortex to these different sensory modalities

because that seemed to be the right thing to do from an evolutionary point of view

um

and so there's a good good example to say

it's like a template that's right

but the in the ins

what's actually learned in that template is is

um

is unknown in terms of language

i i hated it this

when i talked about language in the chapter about high level thought in the book

and i don't consider myself an expert in language at all

um

but it occurs to me that if vernon melcaster is right

that there is a single cortical algorithm that's basically running everywhere

the language has to be somehow fundamentally mapped on to this algorithm for century

motor modeling through reference frames

and i made the analogy about a big part of languages

it has to do with um recursion and a recursive structures

and and the algorithms in the in the column are really good at that

um

and so to me i would say that you know if i would take

this isn't it

i never thought of this

but if i would say

take the idea that there's a universal language

i would extend it beyond what we call language

i would say there's a universal

uh structure to everything in the world

that we can learn

not everything in the world but everything in the world that we can learn

and

um

that universal structure

is this this

uh reference frame

central motor idea

and um

that universal structure can learn any language

whether it's spoken language or computer language or written language

it can learn any structure that fits into that algorithm

and that could be

you know how staples work and how birds fly and how evolution occurs

these are all everything we know has to fit into that structure

so in some sense i would

i would agree with chomsky

i just think he only focuses on quote language

um

where i would say the universal algorithm is

language is a subset of the universal algorithm that chomsky talks about

that's an interesting idea

i never

i never said that before stuff man

we'll see how it feels about tomorrow

but but there's

there's a fascinating dichotomy

though

isn't that because it's similar to the um bias variance trade-off in machine learning

so you know

evolution has has given us a certain prior and a certain default encoding

and then we have this learnability algorithm

and then what fascinates me is how externalized so much of this stuff is

so there's embodiment

there's all the knowledge in in society that we

we learn language

we're brought up by our parents and and we we acquire the language around us

so how much of it is being pushed down from society

and how much of it is being pushed up from the prior knowledge that we've evolved and inherited

well

i would

i would say there's three things

there is the our knowledge we inherited

there is what we learn on our own just to exploratory behavior

and then what is passed down

which is always always through language

but it could be also through by observation of other humans

um

and um

and what's the balance between those two

well

i think in terms of the neocortex

it's not a lot from the biology and the evolution

in terms of the other parts of the brain

it's very much

so

you know

there's a

if there's evidence that says the prayer collectors is like the old visual system

um

it

it

it detects snakes and spiders

so people were scared of snakes or spiders

it's not the neocortex is the old by going

ah start a snake

it looks like a snake

and so you know that's not learned

it's there

it's hard to get rid of

um

so there's some of that

but you know

our ability to walk

we don't learn to walk

we actually are programmed to walk

it's just that we did

we haven't finished developing yet

until when we learn to walk

we're really just our nervous system is finished being growing

you don't really learn to walk

so these are these old priors

lots of them

um

and as a human

they're very

very important

you know eating and sex and

and

uh

body functions

and you know

survival

all these things and all there

uh

but from the neocortex point of view

i'd say there's very little

um

it's more just assumptions about the

the

the sensory types of sensory data you're going to get

i mean it's what you know

what strikes me

tim is

it's incredibly

we are so incredibly versatile on what we can learn

i mean just you know

think of all things we learned and i'm sure you know this

you thought about

i'm sure yourself that all the things we learned

we had no evolutionary pressure to do like running these

this kind of podcast and you know

broken computerism and talking about brains

it was incredible

it just just cries out that there's a universal method here that's being applied to any anything

that

not to everything

you know

brains can't learn everything

but they can learn a hell of a lot

and it seems to be

we haven't really discovered the extent of it yet

so this

this says

you know

there's

there is this sort of blank slatish um system

if you want to call it that way

with these assumptions about our bodies built into it

well

the only thing about the the blank slate thing is it's such a dichotomy

because i was reading your book and it seems to be so strongly determined

by the embodiment and the multi modality

you know

the way that we're wired

because you said yourself that ai will actually evolve towards

you know

robotics

basically

it's not just the brain

yeah

it's

it's how it acts in the environment and how it gets with those senses

but i wanted to ask one one final thing

so i went to a yoga retreat at the weekend

and after the physical components of the practice were concluded

matters of a philosophical nature

uh

were investigated

and the basic case that the yoga teacher was making is that we have two selves

right

we have the emergent social self and then we have the inner self

which i'm sure sam harris probably might spoke to about

you know the sense of being and the sense of being is

when you kind of ignore the virtual social program on the top

and you know a society

it is an emergent virtual program and it does many of the things that the cortical columns are doing

it has the error correction it had

you know

it's the externalization and distribution

so

even though we're programmed to care a lot about our social selves

um

we are very stressed when we lose control of our own narrative

because someone might be saying something bad about us on twitter

so anyway

this yoga teacher was saying

well

there's something deeply fulfilling

um

spiritually about just kind of um going into your mind and and just just that raw conscious experience

but then i felt like saying to him

well

you should read jeff hawkins book

because your mind is just a load of prediction models and all you're doing is traversing your reference frames

# Chapter 10

and you know a society

it is an emergent virtual program and it does many of the things that the cortical columns are doing

it has the error correction it had

you know

it's the externalization and distribution

so

even though we're programmed to care a lot about our social selves

um

we are very stressed when we lose control of our own narrative

because someone might be saying something bad about us on twitter

so anyway

this yoga teacher was saying

well

there's something deeply fulfilling

um

spiritually about just kind of um going into your mind and and just just that raw conscious experience

but then i felt like saying to him

well

you should read jeff hawkins book

because your mind is just a load of prediction models and all you're doing is traversing your reference frames

so what do you

what do you think i don't

i

well

i again asked me to talk about things i'm not too familiar with

but i'll tell you

i'll tell you a personal experience

i have

okay

i don't do meditation

and and sam didn't bring that up

but but i thought about it a bit

i do something equivalent to meditation

um

is that when i find things in the world

stressful

um

i just sort of shut out the world

and i think about interesting problems like the future of humanity or the future of intelligence

what's the nature of life and things like this

and so now i'm living in this world that is sort of

um

is in some sense

uh

pure

right

it's not

it's not being messed up with

like i'm hungry

or this person's being nasty to me or just like tuning it all out

like

oh my god

i'm not in trouble again

and then

so i think that's my own personal meditation

and i don't think i think it's useful actually

because

uh

i think it helps when you

when you think like that

you

you're

you're sometimes able to get to deeper truths

um

that you separate the body functions and the day-to-day stuff

and and i often do my best thinking when there are no distractions either on that

sometimes while i'm driving or sometimes i'm just awake at night lying in bed

um

but where there's like

i don't i don't have to deal with anything else

so that's kind of like meditation

and i find

i don't think it's useful

i think it's very useful because you

it allows the brain to sort of separate out from the old brain stuff

and um

and just have run a bit

sometimes it comes up some good ideas

you know exactly

i couldn't attest to that

well

um

jeff hawkins

thank you so much for joining us today

it's been an absolute honor

well

it's been a pleasure

you guys are great

i really

i enjoy all your questions

you're really fun and i think this is one of the

one of the more meaningful conversations i've had in a long time

i'm serious

you know

because hey

you know

you know what you're talking about

you know what i'm talking about

you ask great questions

you're really deep thinking about all this stuff

so i think that's wonderful

thank you so much appreciate that

thank you so much

all right

all right

well

i hope it helped

it was good for you guys

so that was a wrap

how was that guys

it's pretty great

it's pretty great

um

and honestly

yeah

i'm really pressed

it was really nice to talk to them

it's just clearly

you know

you know

it was like so much more than was in the book

uh

it

building

the book was really just scratching the surface

and

uh

i wish we had

like

you know

like just taking off for a beer and like really dig into some of the degrees

yeah

what i i love

uh

i love people like him because he's so smart and and smart and a very

um

he has such great common sense right

so

when you

when you ask him questions

he comes back with these really informative answers that are very

uh

concrete

and you understand what he's talking about

so it's really fun to talk to him

i mean

a very

very

great learning experience

yeah

i got the impression that his research focus has changed a little bit

so when we were doing some preparation for this

we were looking a lot into the htm algorithm

which i understand is now

um

they're pivoting away from that a little bit

so he was talking about some stuff that they're doing with transformers models and sparsity

and quite a few things i haven't heard about

and also he was really focused on

um

the particular way of thinking about cognition using these reference frames

which he spoke about in his book

which i think is actually a slight departure from from um

some of the stuff the mentor has had out about five years ago

yeah

but i definitely got the feeling that he had like so much interesting stuff

that's just not quite ready yet for public consumption

but

um

hopefully

in the book

i often have this feeling like

ah

the city

just you know

it feels like there's more to this than there is in the book

maybe

or maybe just needs more time

and i think i remember reading the book

it took like a year and a half to write

or so

so

maybe

even right here

you know

lots of things been happening

um

so i do look forward to whatever he's gonna publish in the future

well

what i love too is you know he's more than willing to admit

look

i've changed my views

you know

i think

yeah

lots of people run into are not able to do that

and if you agree with chalet's

you know

measure of intelligence

the mark of intelligence is the ability to assimilate new information and to learn from it

and most of the people we talk to just want to stay adamantly entrenched

you know

no

look

this thing that i called xyz back

you know

30 years ago

you're wrong that it was limited to this little area

it actually encompasses everything

you know

that that works now

like nobody ever wants to admit that they've learned or changed or that science has evolved

i want to push back on that a little bit because there is something really special

i know we were joking that science advances one funeral at a time

but there is something really special about people who have an idea and they see it through

just like yan lacoon and the deep learning pioneers

everyone was asking them why the hell are you doing this in the 1990s

and they and they hung on and it was worth hanging on

and it's a similar thing with

with some of the approaches from nomento

they've been at this for such a long time now

and there is some really really kind of biologically inspired

and plausible reasons why this might lead to something interesting in the future

but clearly they've been at this for a long time and and they've been sticking at it

well

nothing

nothing's wrong with perseverance

but perseverance without adaptation is wrong

it's just perseverance when you're right

is right

and prince raised when you're wrong

is bad

you're never 100

right

and you're right

you're never 100.

so that's the whole point is

you've got to be able to learn

you have to be able to

well

the learning rate needs to be about 0. 01

sure

and the party centers

3 e minus 4

i think is the correct learning rate for all matters in life

yeah

all the matters specifically

yeah

and i agree

like also

i

i'm really glad i got to talk to jeff about essential risk or something

because of course

you know

i take some songs pretty seriously

yeah

it's a very common experience to hear people dismiss and extensively

because they've heard the bad versions of the arguments

if there are some heroin encouragements of these arguments

that it's even from some very smart people like

yeah

some smart people i feel do the field a great justice than i just can't feel bad

they're against

um

and it was really nice to see you know him

say like

yeah

you know what something we're thinking about

and

um

i

i think

honestly

a lot of the disagreement comes down to that

yeah

like

i agree with him and almost everything really

so they're not those relationships

i was really impressed with with his response

and as you say

you can see it from so many different perspectives

so you can cover it from a grand perspective

talking about paper clips and um

oh my god

but

um

i lost my train of thought

yeah

so he

i read the

uh

the less wrong article which you linked to me was that steve burns

yeah

secrets

yeah

so that was fascinating

so so he was saying that

um

at the end of the day

you've got a robot rover on mars or something like that

and at some point you need to actually give it something to do

and you might give it an instruction

and then it might really want to complete that instruction

so it might predict that you're about to give it another instruction

it says

oh my god

he's about to give me another instruction

i've not done the first thing yet

so i need to kill him

and that's not completely beyond the realms of possibility

now i i agree with charlotte that i think intelligence is externalized

it is embodied and there are many like natural environmental

limiting steps to intelligence

and you could talk about all the replication stuff as hawkins did

but that's just a simple example

i'm just an intelligent agent on on mars and i i need to have motivation

because i need to be told what to do

what am i doing

am i building a base and then i can plausibly see how that could be

um

you know

misdirected

yeah

i feel like it's it the intelligence explosion

arguments

i'll get mixed up with these

like motivational

uh

concerns

which i think is unfortunate because they really aren't blocking all like you could like these

these concerns about

you

know

alignment and about motivation will crop up

whether you know

it takes 10 years or a thousand years for ai to come to be

at some point

s hawkins

i think

gonna pick

so when we put

the work needs to be done at some point

you just have to build a safety mechanism

at some point

you have to write a motivation system

so that's why i think this

you know

kind of research

you know

even if intelligence blows

it doesn't happen is still something needs to get done

yeah

and and there is this thing

um

i mean you can explain this better than me

but instrumental convergence

which is the idea that

um

you know

many seemingly innocuous goals incidentally lead to dangerous motivations like self-preservation and self-replication and goal preservation

and um

i i can sign on to that

that seems very plausible

yeah

exactly

i mean you give a perfect example

but like you know

you give the rover and the the the job

will

you know

build a thing

but actually it's like

oh

actually

i wanted to build something different

you know

still was motivated

but i went to the first thing

so unless we have

like some really sophisticated

you know

system inside of it allows to like

you could equally value

doing this or do with another human

self-encourageability and which we don't currently really love

but implement formally

um

it's very likely that it will

you know

having instrumental goals like keeping itself alive

so

and where you're stopping you from giving it your orders

like it maybe will damage its entirement or something

so you can't give it orders

or maybe

if you

if you're now

if you try to destroy the broker because this malfunctioning

maybe they'll retaliate

because if it's destroyed it can't build habitat

no

can it

so so yeah

there's a lot of ghouls like this

also also like delayed power seeking goals to sleep obviously

yeah

if you don't know how to achieve a goal

and gaining your power is probably like always a good move

you know

getting more money

more social capital

you know

controlling for people

the driver resources

like almost always a good thing

like almost

no matter how innocuous

the goal might seem

yeah

but see

like

on the other hand

i don't care if a rover on mars tries to retaliate

it has like no capability to retaliate on us at all

and so the only kinds of like artificial intelligence i worry about are ones

that are capable of expanding in some capacity

like they have to be able to build things

and they actually have to be able to build intelligent things

so that's why i brought up the point of

you know

if we have a robot

we're gonna need one that can build

you know

somehow

or another

replicas of other robots to expand

conduct more work

replace ones that were broken

whatever

and it's not so infeasible to believe that

at some point in the next few hundred years

you know we'll have 3d printing technology more than sufficient to build silicon chips and you know whatever else

and as soon as you have a system that's self-replicating

and we don't live in a perfect world and we have random variation

right

and they have sets of instructions

and also we're going to need them to adapt to new environments

like it's going to be diff

difficult

different

building it on the plains of mars versus deep down in the valleys and mining water or whatever

so they're going to have to have some capability to allow variation

you know

then you have the mixture for evolution

and once you have evolution

even if you had no goals

like

think of it this way

the ultimate thing which has no motivations whatsoever was inanimate matter

and that's what the earth was four billion years ago

and we wound up with animate objects that have goals

so despite philosophers

and this is kind of an interesting thing

philosophers are always like

you can never get ought from is

but interestingly enough

ought arose from just stuff

right

yeah

the very elegantly said

there's this weird kind of symmetry breaking that happens somewhere

i mean in evolution

the symmetry breaking happened is because of physics

because things that don't want to replicate

don't replicate

so we don't see them

so we found the first one from

is comes from just the fact that if there is a possibility of replicator and the possibility non-replicator eventually

you will only see replicators

and that's your first symmetry breaking

and corporate occurs

that hopefully be

you know

when we build intelligent systems is that maybe we can break the symmetry in a more

you know

in a less dystopian you know than a malthusian way and build systems that aren't more motivated

but i think to be considered better than just for your replication

but it's very non-obvious to me

you know how to do that safely

it's hard

it's a hard trouble

but hawkins did say though that he was more worried about the immediate threat of um humans

controlling the kind of ai that we have now

or the kind of ai we have now being used for bad purposes or or even things like

um

the spread of false beliefs

that's incredibly dangerous as well

and that that replicates like a virus

that's why that

um

you know that social dilemma documentary

like

i thought this was a very brilliant quote that was in there

and a lot of other people think it's like not an interesting quote

but to me it was beautiful when

uh

you know

undermining

exactly

he said we've always been worried about when ai would overcome our strengths

when we should have been worried about when it would overcome our weaknesses

right and that

and it's already

you know

even though we don't have

maybe we forget

if we have real intelligence or not

or whatever

clearly

these machine systems have overcome lots of our weaknesses

and they've created things that are more addictive than they've ever been found

ways to waste more of our time than now

of course there's offsetting things like

yeah

we also get greater productivity here

etc

etc

but we should be worried about

when it overcomes our weaknesses and what that's doing to us and our children

and you know

one of my favorite examples is it's called the food optimizer

so this comes from

and yeah

called roku

and he makes this example that

so

the way he describes it is

that in like the 19th century we collectively summoned a weak super intelligence to feed all humans

i think the deal was kind of this

this capitalist

you know

market system will produce really cheap food for us

but the system was optimizing a reward signal based on profit

of course

let's see how these things work

so it's kind of like a weak

it's not sent huge

it was obviously not intelligent

the weight you know humans are

but a weed is option

rising system

it optimized for more and more food

and now we're at the point that you know what

60 of adults in the western world are obese

something

we are literally eating ourselves to death

and this is a very

very weak intelligence quote and this is already powerful enough in a way to you know

get people to basically kill themselves voluntarily

so imagine if you gave such a motivation

you know

biggest

you know

food corporation in the world

you know

build their big

their big agi

you know

using hawkins

you know

thousand brandon algorithm

of course

uh

and give you the motivation of

you know

smith roth

yeah

but this

this gets to utility and similarly hawkins said

oh

you should never clone yourself

# Chapter 11

biggest

you know

food corporation in the world

you know

build their big

their big agi

you know

using hawkins

you know

thousand brandon algorithm

of course

uh

and give you the motivation of

you know

smith roth

yeah

but this

this gets to utility and similarly hawkins said

oh

you should never clone yourself

because you're not cloning

you're forking

i thought that was a bit of a contradiction

though

because he was talking about

um

you know

sending clones of yourself to mars or seeding mars

that's right

and that's the same thing

isn't it

you're actually forking the human race when you have a population of humans on on mars

i don't think

um

who doesn't like forking what's wrong with that

well

no

but the thing is because you

we

we're doing it because we want to leave a legacy

you know

i

i like doing this youtube channel because you know

after i'm dead

people can see who is that tim scarf guy

but um

but you know you might say i'm going to create a clone of myself

because i want to have a legacy

but it's not a legacy

because after

after the next day it becomes a

you know

it's a different person

it's not connor anymore

and similarly

when you were talking about

um

you know

why don't we restructure society because let's take away social media

let's let's fork and take away facebook and everything else

but what's your utility function

how could you

how could you justify that ourselves in that parallel universe would be better off than we are now

well

first of all

i mean i think it would be a legacy

so you

and i both think that that the pixels on youtube are going to be a legacy

and those are a pale pale shadow of a

you know

projection of us right

so i would

totally

you know

a clone of you or a robot programmed with your mind state or something that then can go off

and live its own life

that's exactly what a legacy is like that

that is a legacy

it's not me

it's a legacy

yeah

like i described to even more extreme crazy

uh

versions of this kind of thing is like i

i fundamentally believe

i'm like you know a non-dual identity

and like that

most of the kinds of identity is kind of incoherent

it's just it's just a convenient abstraction in humans

like humans tend to be discreet

that's just a convenient property that humans have to have happen to have

there's no reason that identity or minds in the sense have to be discrete like this

it just happens to be the case that humans come in packages

it looks like they become in

you know

one intelligence please one unit of intelligence

but there's everything you should study

they have it

they have a mark

they have to have a markov blanket exactly exactly

but there's no reason you can't have systems that don't really

there's not like a clear separation between them like the

the only reason i think really me and tim aren't the same person

it's because the bandwidth between our brains is really low is interbreed

it's lower that it is intraprey

like

why is the right heart for memory

maybe that's the person in the left hand

also a different person

depending on your definition

like you know

there's like split brain patients

where like the brain has to actually disagree and like take different actions and like fight with each other

so like really different people

i don't think

so i think

what makes identity a synchronization in a sense or like alignment

so you see is that identity

the best way to define identity is alignment with agua

is that

like

does the

the system coherently act without

you know

like shooting itself from the foot

so to speak

or like you know

doing at all

so like

that's why i think it's a really great argument that

for example

ant colonies

for one animal that happens to have

like

you know

very big individual parts

because in a way it is aligned

you know not all the parts were produced

you have a reproducing part

you have

you know

fighting for death

eating parts or whatever

food

getting parts or whatever

and they happen to not all be one body that's just an implication detail

you were also just colonies of cells

you know

it's

it's just

it's just implementation details and matters

is the coherency with the synchronization

of

you know

knowledge and goals in life

yeah

but again

a lot of that is is emergent

but you were saying as well that

um

there's a kind of there's a

there's an i o bottleneck when we communicate

and this is

um

a lot of people talk about neural link in this context as well

but i don't think there is

because when we communicate

there's common knowledge that we both have

so it's a fairly efficient communication mechanism

and we're invoking lots and lots of very complex knowledge that we've reconciled

so when we read a book or when we

when we take on board information

the bottleneck is comprehension

i don't think it's the

the bandwidth

sure

as it depends on how like

and by bandwidth

i also meant comprehension back

it's like if i could

literally

i imagine we

could

you know

take

two human brain would put them in the same skull

and could like to connect them through some super cleopas closeum or whatever

and you know

let the whole thing synchronize and you're not diet seizures immediately or whatever

i think there's an arsenal argument to be made

that is one entity

but you could also make an argument's two connected entities and simply just becomes continuous

like a classic play open when something seems confusing

if there's a discrete quantity

evolved

try try continualizing the quantity and see if the confusion disappears

very cool

well

gentlemen

it's been a pleasure

so see you next week

folks see you next week

