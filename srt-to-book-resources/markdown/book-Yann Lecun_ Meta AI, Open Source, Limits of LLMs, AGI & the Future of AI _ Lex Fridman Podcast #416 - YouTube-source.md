# Chapter 1: 勒昆的AI之路：大语言模型并非通往超人智能的坦途

故事从一场关于人工智能未来的激烈辩论开始。杨·勒昆，这位Meta的首席AI科学家、纽约大学教授、图灵奖得主，也是人工智能领域的奠基人之一，再次做客Lex Fridman的播客。他以其一贯的坚定立场，主张开源AI的发展，并身体力行地推动Meta开源了Llama 2，并即将推出Llama 3。勒昆对那些警告AGI（通用人工智能）将带来生存威胁的“末日论者”持批评态度，他坚信AGI终将到来，但它会是良善且可控的，不会逃脱人类的掌控，更不会毁灭人类。他认为，真正的危险在于少数公司通过专有AI系统集中权力，这远比其他任何威胁都大。他相信人性本善，开源AI能赋能人类的善意。

然而，勒昆的观点并非没有争议，尤其是在AI飞速发展的当下。他最近发表了一些关于人工智能未来的技术性声明，其中最引人注目的是：他认为像GPT-4、Llama 2这类自回归大语言模型（LLMs）并非通往超人智能的正确道路。

他解释道，原因有几点。首先，智能行为有几个核心特征：理解物理世界的能力、持久记忆、推理能力和规划能力。而当前的LLMs，要么完全不具备这些能力，要么只能以极其原始的方式实现。它们无法真正理解物理世界，没有持久记忆，无法真正推理，更谈不上规划。如果期望一个系统在不具备这些基本要素的情况下变得智能，那无疑是错误的。尽管LLMs在构建应用生态系统方面非常有用且有趣，但作为通往人类水平智能的路径，它们缺失了关键的组成部分。

勒昆还提出了一个令人深思的数据对比。LLMs通常在海量的文本数据上进行训练，这些数据量相当于一个人每天阅读八小时，持续17万年（约2乘以10的13次方字节）。这听起来知识量巨大，但与人类儿童所接收的信息量相比，却显得微不足道。一位发展心理学家指出，一个四岁的孩子在其生命中清醒了大约16000小时，通过视觉皮层接收到的信息量高达10的15次方字节。这表明，我们通过感官输入获取的信息远多于通过语言。我们大部分的知识，尤其是在生命最初几年学到的，以及动物所学到的，都与语言无关，而是通过与真实世界的观察和互动获得的。

Lex Fridman对此提出疑问，认为语言本身是高度压缩的，蕴含着丰富的智慧，是否可能仅凭语言就能构建出世界模型和对物理世界的理解？勒昆坚定地回应道，智能必须根植于某种现实，无论是物理现实还是模拟现实。他认为，环境的丰富性远非语言所能完全表达。语言只是对感知和心智模型的一种近似表述。我们完成许多任务时，例如建造物品或抓取物体，都需要在脑海中操作情境的心智模型，这与语言无关。我们通过想象一系列动作的结果来规划行动，而这些心智模型与语言关系不大。他认为，我们大部分的知识都源于与物理世界的互动。

勒昆引用了机器人学先驱汉斯·莫拉维克提出的“莫拉维克悖论”：计算机在下棋、解积分等高级复杂任务上表现出色，但在驾驶汽车、收拾餐桌、洗碗等我们日常生活中习以为常的简单任务上却举步维艰。LLMs可以通过律师资格考试，但却无法像一个17岁的青少年那样在20小时内学会开车，也无法像一个10岁的孩子那样一次性学会收拾餐桌和洗碗。这究竟是为什么？我们究竟缺少了哪种学习或推理架构？

当被问及LLMs是否能构建一个懂得驾驶和洗碗的世界模型，只是暂时无法处理视觉数据时，勒昆的简短回答是“不能”。他进一步解释说，虽然可以通过一些“技巧”让LLMs处理视觉信息，例如训练一个视觉系统将图像转换为类似文本的“token”，然后将其输入LLM。但这些本质上是“权宜之计”，因为这些系统并非真正为了理解世界而训练，它们没有通过视频进行训练，也无法真正理解直觉物理学。

勒昆强调，直觉物理学和对物理空间的常识性推理，是LLMs目前无法逾越的巨大鸿沟。他认为，我们目前使用的LLMs无法做到这一点，主要原因在于它们的训练方式。LLMs通过预测文本中缺失的词语（或下一个词语）来训练。它们生成的是一个词语的概率分布，然后从中采样一个词语，再将其作为输入来预测下一个词语。这种“自回归预测”的过程，与人类思考的方式截然不同。当人类思考并准备说话时，我们所构思的答案和计划，往往独立于我们最终将使用的语言。勒昆以数学概念为例，指出其思考过程与用法语、俄语或英语表达无关。尽管Lex Fridman对此表示怀疑，并以双关语为例，勒昆也承认在某些特定情境下（如双关语），语言确实会影响思考，但他坚持认为，对于大部分思考而言，存在一个先于语言的更抽象的表征。

# Chapter 2: 抽象思维的奥秘与AI的“世界模型”之争

杨·勒昆教授以他一贯的深刻洞察力，将我们带入了一场关于人类思维与人工智能本质的探讨。他首先抛出了一个引人深思的观点：在语言形成之前，存在着一种更为宏大、更为抽象的思维层次。他举例说，当我们思考一个复杂的数学概念，或者在脑海中构思一件木制品的结构，甚至只是想象一个水瓶旋转90度后的样子，这些思维过程与任何特定语言的内部独白都毫无关联。我们并非用法语或英语在脑中“说话”，而是构建着事物的心理模型，进行着纯粹的抽象思考。

勒昆教授接着将矛头指向了当前炙手可热的自回归大语言模型（LLMs）。他认为，LLMs的工作方式与人类的这种抽象思维截然不同。它们更像是一种“本能”的反应，一个词接一个词地生成，缺乏真正的“思考”和“规划”。他打了个生动的比方：这就像一个人在全神贯注做某事时，突然被问了一个简单问题，不假思索地自动给出答案。LLMs正是如此，它们依赖于海量知识的积累进行检索和输出，但并没有像人类那样在生成答案前进行深层次的规划。

对话随即转向了“世界模型”的核心问题。勒昆教授承认，通过预测来构建世界模型是可能的，但关键在于“预测什么”。他坚信，仅仅通过预测“词语”来构建世界模型是行不不通的，因为语言的“带宽”太低，信息量不足以支撑对世界的深刻理解。真正的世界模型，需要AI系统能够观察世界，理解其演变规律，并预测自身行动对世界状态的影响。

然而，构建这样的世界模型并非易事。勒昆教授回顾了他们在Meta AI（FAIR）长达十年的尝试，试图用生成模型来预测视频。他们曾尝试让系统观看一段视频，然后预测后续的帧，就像LLMs预测下一个词一样。但与文本不同，视频是高维且连续的，预测所有可能的像素分布几乎是不可能完成的任务。世界的信息量远超文本，一个摄像头在房间里平移，系统无法预测墙上画作的每一个笔触、沙发的纹理或地毯的细节。

他们尝试了各种方法，包括引入“潜在变量”来捕捉未被感知的信息，以期能更好地预测像素细节，但这些努力，无论是使用神经网络、GANs、VAEs还是各种自编码器，都以“彻底失败”告终。甚至将这些方法用于学习图像或视频的良好表征，以供后续识别任务使用，也未能成功。勒昆教授指出，这种通过重建被损坏图像或视频来学习表征的自监督方法，在文本领域（LLMs的原理）效果显著，但在视觉领域却举步维艰。

那么，问题究竟出在哪里？勒昆教授解释说，失败之处在于，尽管编码器架构本身可能很优秀（例如MAE，一种掩码自编码器，通过移除图像块并重建来训练），但这种重建式的自监督训练并不能让系统产生良好且通用的图像特征。相比之下，如果用带有标签的数据进行监督训练，同样的架构就能产生远超自监督重建的优秀表征。

面对这一困境，勒昆教授提出了一个充满希望的替代方案——“联合嵌入预测架构”（JEPA）。他解释道，JEPA不再试图重建所有像素细节。相反，它将完整图像和其被损坏或转换的版本同时输入编码器，然后训练一个预测器，从损坏版本的抽象表征中预测完整版本的抽象表征。这种方法的核心在于，它不追求预测所有原始输入细节，而是专注于提取输入中“相对容易预测”的抽象信息。

JEPA的早期版本，如对比学习，曾面临“模型崩溃”的挑战，即系统会忽略输入并产生恒定的表征。对比学习通过引入“负样本”（已知不同的图像对）来避免崩溃，但也有其局限性。令人振奋的是，在过去三四年里，非对比学习方法取得了突破，它们无需负样本，仅通过同一事物的不同视图进行训练，并辅以其他巧妙的调整来防止崩溃。

勒昆教授强调，JEPA与LLMs等生成架构的根本区别在于，JEPA不生成原始输入的所有细节，而是预测其抽象表征。这使得JEPA在资源消耗上更为高效，也更有可能提取出对世界理解至关重要的信息。他将JEPA视为迈向“高级机器智能”（AMI，他更偏爱的术语）的第一步，预示着AI在真正理解世界方面可能迎来新的曙光。

# Chapter 3: JEPA：从抽象表征到世界模型与规划

杨·勒昆教授以他一贯的深刻洞察力，为我们揭示了联合嵌入预测架构（JEPA）与当前主流生成式AI（如大型语言模型LLMs或基于重建的视觉系统）的本质区别。他指出，LLMs和那些试图通过重建来理解世界的视觉系统，其核心任务是巨细无遗地预测并生成原始输入——这意味着它们要耗费大量资源去捕捉每一个像素、每一个细节。这就像要求一个画家不仅画出树的形状，还要画出风中每一片树叶的精确摆动轨迹，既困难又低效。

然而，JEPA的哲学截然不同。它并不追求预测所有原始细节，而是专注于预测输入数据的“抽象表征”。勒昆教授生动地解释道，这就像我们人类观察世界一样，我们不会记住风中每一片树叶的精确摆动轨迹，因为那既不可预测也无关紧要。我们只关心“树叶在动”这个抽象事实。JEPA正是通过训练编码器，过滤掉那些不可预测的、无关紧要的“噪音”，只保留那些能够被有效建模和预测的关键信息。通过这种方式，系统能够学习到一种更高层次的抽象世界模型，将可预测的部分视为有意义的信号，其余则视为需要剔除的杂音，从而提升了表征的抽象层次。

勒昆教授强调，这种分层抽象的能力对智能系统至关重要。人类在描述现象时，总是在不同的抽象层次上进行，从不试图用量子场论来解释所有日常事件。JEPA的目标正是以自监督的方式学习这种抽象表征，甚至可以进行分层学习，这被他视为构建真正智能系统的核心要素。他甚至半开玩笑地说，语言模型之所以能“偷懒”，是因为语言本身就是一种高度抽象且信息密度高的形式，已经替模型完成了大部分抽象工作，所以它们可以直接预测词语。

当被问及JEPA是否仍是生成式模型时，勒昆教授肯定了这一点，但明确指出它是在“抽象表征空间”中进行生成。他进一步阐述了为什么感知输入（如视觉）比文本更适合自监督学习：因为感知数据中存在巨大的冗余。这种冗余性使得模型能够更好地捕捉世界的内在结构。相比之下，文本虽然信息量大，但经过高度压缩，冗余度较低，因此单纯的自监督学习效果可能不如在视觉领域。

关于将视觉和语言数据结合训练，勒昆教授持谨慎态度。他认为，过早地结合可能会导致“作弊”，即语言成为视觉系统缺陷的“拐杖”。他举例说，猫狗虽然没有语言，但它们对世界的理解和规划复杂行动的能力远超任何LLM。因此，他主张首先让机器学会如何理解世界运作的物理规律，然后再将其与语言结合，这样才能真正迈向高级智能。

勒昆教授对JEPA寄予厚望，认为它有望让机器获得类似猫狗那样的“常识”——一种能够预测世界如何运作、甚至如何“最佳地捉弄主人”的能力。他详细介绍了JEPA所采用的非对比式学习技术，包括基于蒸馏的方法（如BYOL、I-JEPA、DINO）和VICReg。这些方法的核心思想是：将原始输入（例如一张图片）通过编码器生成一个表征，然后对原始输入进行各种“破坏”（如裁剪、模糊、旋转或遮蔽），再将破坏后的输入通过另一个（或共享权重的）编码器，训练一个预测器来预测原始输入的表征。关键在于，只训练处理被破坏输入的那个分支，但由于权重共享，整个系统都在学习。通过巧妙的设计，这些方法能够避免模型“崩溃”——即忽略输入而产生无意义的输出。

他举例说明了不同数据类型下的“破坏”方式：DINO需要图像特定的几何变换和模糊，而I-JEPA则更通用，只需简单地遮蔽图像的某些区域。更令人兴奋的是V-JEPA（视频JEPA），它将I-JEPA的思想应用于视频。V-JEPA通过遮蔽视频中的“时间管”（即在连续帧中遮蔽同一区域），训练系统从部分遮蔽的视频中预测完整视频的表征。这项技术取得了显著进展，首次使得系统能够学习到高质量的视频表征，从而能准确识别视频中的动作。

更令人振奋的是，初步结果表明V-JEPA甚至能判断视频中的事件是否符合物理定律——例如，物体是否凭空消失或突然瞬移。这意味着系统开始捕捉到关于现实世界的物理约束。

然而，勒昆教授也坦承，要让JEPA达到足以驾驶汽车的“世界模型”水平，还需要时间。他设想了一种改进版的JEPA：将行车记录仪的视频作为输入，通过遮蔽未来的帧，并同时输入驾驶员的“动作”（如方向盘转动角度），来预测未来世界状态的抽象表征。如果能构建出这样的模型，机器就能拥有一个“内部世界模型”，从而实现LLMs目前无法做到的“规划”能力。

他解释说，有了这个世界模型，机器可以像人类一样，在脑海中模拟一系列动作的后果，评估这些后果是否能达成特定目标（比如将桌上的瓶子移到左边），然后规划出实现目标的最佳行动序列。这本质上就是“模型预测控制”——一种在工程领域（如火箭轨迹规划）已广泛应用的经典方法。

最后，关于“分层规划”，勒昆教授明确表示，这不会自动从JEPA中涌现，而是需要专门的架构来支持。他以从纽约到巴黎的旅程为例，说明了分层规划的必要性：一个宏大的目标需要被分解成一系列更小的、更具体的子目标（比如先去机场），才能逐步实现。

# Chapter 4: 分层规划的挑战与大模型的局限：勒昆对智能之路的深思

故事从一个看似简单的目标开始：从纽约前往巴黎。杨·勒昆教授用这个例子生动地阐释了“分层规划”的必要性。他指出，要完成这样一项复杂的任务，我们不可能从毫秒级的肌肉控制开始规划整个行程。想象一下，从纽约大学的办公室出发，第一步是去机场，然后是搭乘飞机。而“去机场”又分解为“下楼打车”，再细化为“去电梯”、“按按钮”，直至“从椅子上站起来”、“打开办公室门”……层层递进，最终落到最微观的肌肉动作。勒昆强调，这种从高层抽象目标到低层具体行动的分解，是复杂规划不可或缺的。因为在现实世界中，我们无法预知所有条件（比如打车需要多久，交通状况如何），也无法承担如此巨大的计算成本来规划每一个微小细节。然而，他坦言，目前的人工智能领域，还没有人真正知道如何训练一个系统，使其能够学习并运用这种多层次的表征，从而实现有效的层级规划。

当被问及大语言模型（LLMs）能否胜任这种分层规划时，勒昆给出了一个审慎的回答。他承认，LLMs在经过充分训练后，确实能在一定抽象层面回答这些问题，甚至能给出看似合理的计划。但他也指出，这些答案可能只是对训练数据中类似情景的“复述”或“幻觉”，它们无法真正为从未遇到的新情况进行规划，更无法触及毫秒级的物理控制。勒昆认为，LLMs擅长处理语言描述的抽象事物，而物理世界的经验，其带宽远超语言所能表达。因此，他提出，LLMs可以负责高层级的推理，比如预订机票、查询网站等，而联合嵌入预测架构（JEPA）则应承担与物理现实交互的低层任务，它能提升表征的抽象层次，而无需重建每一个细节。

对话随后转向了对自回归LLMs成功的质疑。勒昆坚定地表示，LLMs的巨大成功并非偶然，而是他多年来一直倡导的“自监督学习”的胜利。他解释说，通过让模型从被破坏的文本中重建缺失部分（如BERT模型），我们得以构建出能够理解语言、进行多语言翻译、总结和问答的强大系统。而自回归LLMs（如GPT-2）的“奇迹”在于，当它们被大规模扩展并训练海量数据时，展现出了惊人的语言理解能力。

然而，勒昆也清醒地指出，我们常常被LLMs的“流畅性”所迷惑，误以为它们具备了人类智能的所有特征。他认为这种印象是错误的，并直言图灵测试是一个糟糕的智能衡量标准，莫拉维克悖论依然适用——对人类而言容易的事，对机器却异常困难。他强调，尽管LLMs令人印象深刻，用途广泛，但它们有其固有的局限性，我们必须认识到这些局限，才能找到通往更高级智能的道路。

勒昆回顾了他近四十年来对“学习表征”的执着追求。从早期的监督学习，到与Yoshua Bengio和Jeff Hinton共同复兴无监督学习，再到2014年在FAIR大力推动自监督学习。他列举了自监督学习在自然语言处理（NLP）领域的辉煌成就，例如多语言翻译系统、内容审核，以及在语音识别领域的突破性进展——Wav2Vec（一个基于联合嵌入架构的系统），它能用极少量标注数据实现多语言语音识别，甚至能直接进行无文本的语种间语音翻译。

然而，在图像和视频领域，勒昆团队曾尝试用生成模型（预测像素）来学习世界表征，却屡屡碰壁，十年间未能取得理想效果。最终，他们放弃了预测每一个像素的思路，转而采用联合嵌入并在表征空间中进行预测。这一转变带来了成功。勒昆因此得出结论：我们无法通过生成模型来学习真实世界的良好表征。他甚至大胆地向同行们喊话：“如果你真的对人类水平的人工智能感兴趣，请放弃生成式人工智能的理念！”他坚信，联合嵌入表征才是通往常识推理和高层级推理的关键。

# Chapter 5: 放弃生成式AI：勒昆的坚定立场与常识之辩

杨·勒昆教授的观点如同一道闪电，划破了当前AI领域对“生成式AI”的狂热追捧。他直言不讳地指出，那些试图预测每一个像素的早期尝试，效果差强人意。真正的突破，在于转向联合嵌入（Joint Embedding）并在表征空间中进行预测——这正是JEPA（联合嵌入预测架构）的核心思想。勒昆教授斩钉截铁地表示，有充分证据表明，生成模型无法有效学习真实世界的良好表征。因此，他向所有追求人类水平AI的研究者发出呼吁：“放弃生成式AI的幻想吧！”

然而，主持人莱克斯·弗里德曼提出了一个关键疑问：仅仅依靠JEPA这种基于视频等感官数据学习的方法，能否触及高层次的推理，比如规划从纽约到巴黎的路线，或是理解全球政治局势？这些似乎是大型语言模型（LLMs）擅长的领域。

勒昆教授对此有清晰的界定。他认为，LLMs的成功主要源于对海量文本的自监督学习，但它们缺乏对物理世界的“低层次常识”理解。它们无法像人类一样，通过亲身经验感知重力、惯性等基本物理规律。莱克斯则反驳道，这些低层次的常识，即使没有明确的文字描述，也可能隐含在庞大的文本数据中。为了构建一个连贯的世界模型，LLMs或许能够从字里行间推断出这些物理法则。

但勒昆教授坚决不认同。他强调，现实世界中大量信息并未通过语言表达。他以人类婴儿为例：在学会说话之前，婴儿通过视觉、触觉、听觉等感官数据，积累了海量的世界知识。一个四岁的孩子，在清醒的16000小时里，仅通过视觉就能处理高达15字节的信息。他们学会了重力、惯性、稳定性，区分有生命和无生命的物体，甚至在18个月大时就能理解他人的意图并提供帮助。这些都是通过观察而非语言习得的。勒昆教授认为，这正是当前AI系统所缺失的。

对话转向了LLMs的另一个核心缺陷——幻觉（hallucinations）。勒昆教授解释说，这是自回归预测的固有问题。每当LLM生成一个词元（token），都有一个非零的错误概率。如果假设这些错误是独立的，那么随着生成序列的增长，答案保持正确的概率会呈指数级下降。错误会累积，导致答案越来越偏离事实，最终变得荒谬。莱克斯质疑，难道训练数据中“真相”的强大引力不足以纠正这种偏差吗？勒昆教授将其比作“与维度诅咒的斗争”。

他进一步阐述，虽然通过微调可以覆盖80%的常见问题，但人类可能提出的问题存在一个极其庞大的“长尾效应”。LLMs在训练数据之外的巨大提示空间中，很容易“失控”。一个随机的字符序列，或者仅仅将几个英文单词替换成其他语言的同义词，就可能让系统给出完全不着边际的回答，这本质上是“越狱”了系统的预设行为。勒昆教授认为，LLMs在这种情况下更像是一个“巨大的查找表”，而非真正的推理者。

他指出，LLMs的推理能力非常原始，因为它们为每个生成的词元分配的计算量是恒定的，无论问题简单还是复杂。这与人类解决问题的方式截然不同——我们面对难题时会投入更多的时间和精力。勒昆教授将LLMs比作人类的“系统一”思维：快速、直觉、无意识，比如熟练司机开车或经验丰富的棋手下棋。而真正的高级推理和规划，需要的是“系统二”思维，即深思熟虑、有意识的规划。他预言，未来几年会出现具备这种规划能力的系统，但它们的架构将与当前的自回归LLMs截然不同。

# Chapter 6: 勒昆揭示AI“系统二”：能量模型与抽象优化

杨·勒昆教授在一次深入的对话中，将当前AI的局限性与人类的认知模式进行了巧妙的对比。他指出，人类的思维分为“系统一”和“系统二”。“系统一”是那种无需刻意思考就能完成的任务，比如经验丰富的司机在开车时可以轻松与人交谈，或者棋艺高超的棋手能不假思索地识别棋局模式并落子。这是一种本能的、潜意识的反应。

然而，“系统二”则需要深思熟虑、规划和推理。例如，当一位经验丰富的棋手面对另一位高手时，他会花时间思考各种可能的走法，权衡利弊。勒昆教授直言不讳地指出，目前的大型语言模型（LLMs）正停留在“系统一”的层面，它们擅长模式识别和快速响应，却缺乏真正的规划和推理能力。

那么，如何才能构建具备“系统二”能力的AI系统呢？勒昆教授认为，答案并非简单的自回归预测。他提出了一种全新的蓝图：利用能量模型（Energy-Based Models, EBMs）进行推理。想象一个巨大的神经网络，它的唯一输出是一个标量数字，这个数字代表了某个答案对于给定问题的“好坏”程度。如果答案完美，能量值趋近于零；如果答案糟糕，能量值则会很高。

这个系统的核心在于，它不再是简单地生成一串串文本，而是在一个“抽象表征空间”中进行优化。当系统接收到一个问题时，它会首先将其编码成一个抽象的表示。然后，它会在这个抽象的“思想空间”中，通过一个优化过程（例如梯度下降），不断调整答案的抽象表示，直到找到一个能使能量值最小化的“最佳思想”。一旦这个抽象的“最佳思想”形成，一个简单的自回归解码器就能将其转化为流畅的文本。

这种方法的好处显而易见：它比当前LLMs那种生成大量假设再从中筛选的方式效率高得多，因为它是在连续空间中进行迭代优化，而非在离散的文本序列中盲目搜索。更重要的是，这种在抽象空间进行的推理过程，与最终表达答案的语言是相互独立的。

勒昆教授进一步解释了如何训练这样的能量模型。基本原则是向模型展示兼容的输入-输出对（例如问题与正确答案），并训练它使这些对的能量值趋近于零。然而，挑战在于如何防止模型“崩溃”，即避免它对所有可能的答案都给出低能量值。他提到了两种方法：对比学习（如RLHF，通过展示错误答案来提高能量）和非对比学习（通过正则化项限制低能量空间的体积）。勒昆教授更倾向于后者，认为它能更有效地确保只有真正好的答案才拥有低能量。

他指出，联合嵌入预测架构（I-JEPA）正是这种能量模型在视觉数据上的成功应用。在I-JEPA中，系统通过预测图像或视频的被遮蔽部分的表示来计算能量，从而学习到鲁棒的视觉表征。

最后，勒昆教授以他一贯的“辛辣”风格，提出了对未来AI研究的激进建议：放弃生成模型、放弃自回归生成、放弃概率模型（转而采用能量模型）、放弃对比方法（转而采用正则化方法）。至于强化学习（RL），他澄清自己并非“憎恨”，而是主张优先使用模型预测控制（MPC），仅在规划未能达到预期结果时，才将RL用于调整世界模型或批评者。他认为，这些转变将是构建真正智能的“系统二”AI的关键。

# Chapter 7: 勒昆：告别旧范式，拥抱开源AI

杨·勒昆，这位AI领域的思想先驱，再次抛出了他一系列激进的建议，旨在将AI推向一个全新的高度。他坚定地主张，我们应该彻底放弃概率模型，转而拥抱能量模型（EBMs）的怀抱；同时，也要告别对比学习方法，选择更具鲁棒性的正则化方法。

当对话转向强化学习（RL）时，勒昆的立场显得更为微妙。他承认自己长期以来都是RL的“批评者”，但并非“憎恨者”。他认为RL不应被完全抛弃，但其应用应被严格限制，因为其样本效率实在低下。在他看来，训练一个AI系统的正确路径是：首先通过大量观察和少量互动，让系统学习到对世界和世界模型的良好表征；然后，在此基础上进行引导。他提出，在大多数情况下，我们可以通过模型预测控制（MPC）来规划一系列行动以达成目标，而无需RL。RL的真正价值在于，当我们的世界模型预测不准确，或者目标函数未能精确反映我们真正想要优化的目标时，它能帮助我们调整世界模型或目标函数。勒昆生动地比喻道，这就像孩子玩耍一样，通过“好奇心”或“玩耍”来探索那些在现实中可能危险的区域，从而在不伤害自己的前提下调整和完善世界模型。因此，RL应该被用于在特定任务中，对已有的良好世界模型进行微调和适应。

谈及RLHF（人类反馈强化学习）为何如此成功，勒昆一针见血地指出，真正具有变革性影响的是“人类反馈”（HF），而非强化学习本身。他解释说，很多时候这其实是纯粹的监督学习。通过让人类对AI生成的多个答案进行评分，我们可以训练一个“奖励模型”（即目标函数）来预测这些评分。然后，这个奖励模型可以反向传播，精细调整系统，使其只生成高评分的答案。勒昆认为，虽然目前RLHF主要用于微调系统参数，但如果将其用于规划，效率会更高。

随后，对话转向了近期谷歌Gemini 1.5因“过度政治正确”而引发的争议，例如生成“黑人乔治·华盛顿”的图片，以及拒绝生成或评论天安门事件相关内容。勒昆对此的回答是：“开源才是答案。”他强调，AI系统不可避免地会带有偏见，因为偏见存在于训练数据中，也存在于“旁观者”的眼中——不同的人对“偏见”有不同的定义。因此，一个完全“无偏见”的AI系统是不可能存在的。

勒昆将此与自由民主社会中的新闻自由相类比：我们不希望所有信息都来自单一来源。未来的数字世界中，我们与数字世界的每一次互动都将由AI系统（如智能眼镜、对话式搜索引擎）来介导。如果这些系统都由少数几家公司控制，那将是对人类知识宝库的垄断，是对民主、地方文化、价值观乃至语言的巨大威胁。

他 passionately地倡导开源AI平台。他指出，虽然训练一个基础大模型成本高昂，只有少数公司能做到，但如果这些基础模型是开源的，那么任何群体——无论是个人、公民团体、政府组织、非政府组织还是企业——都可以基于这些开源系统，用自己的数据进行微调，从而创建出高度多样化、专业化的AI系统。他举例说，法国政府不愿看到其公民的“数字饮食”被美国西海岸的几家公司控制；印度正在资助项目，让Meta的开源模型LLaMA 2支持印度所有22种官方语言；非洲的创业公司也在努力让LLM支持当地语言，以便提供医疗信息。只有通过开源平台，我们才能拥有在政治观点、语言、文化、价值观和技术能力上都丰富多样的AI系统，并催生一个充满活力的AI产业生态。

然而，一个现实的问题浮出水面：像Meta这样投入巨资（例如35万块Nvidia H100 GPU，价值数百亿美元）构建基础模型的公司，如何通过开源来盈利？勒昆对此表示，存在多种商业模式，例如通过广告或为企业客户提供服务来盈利，比如帮助披萨店通过WhatsApp与顾客互动。这个问题，也为接下来的讨论埋下了伏笔。

# Chapter 8: 围绕成本、偏见与开源未来的激辩

对话伊始，Lex Fridman抛出了一个令人咋舌的财务难题：仅仅是35万块英伟达H100 GPU，其成本就高达千亿美元，再加上训练所需的基础设施，这笔投资简直是天文数字。他不禁发问：“我不是个生意人，但如此巨大的投入，究竟要如何才能盈利呢？”

杨·勒昆（Yann LeCun）平静地回应，揭示了Meta独特的商业逻辑。他解释说，Meta的商业模式是提供服务，并通过广告或企业客户来获得收入。他举了一个生动的例子：一个大型语言模型（LLM）可以帮助街角的小披萨店通过WhatsApp与顾客互动，自动处理订单，询问配料和尺寸等细节。这样的服务，企业是愿意付费的。他强调，如果潜在客户群足够庞大，公司无论如何都要构建这样的系统，那么将其开源并不会损害其盈利能力。

Lex对此仍有疑虑：“如果模型开源了，其他人也可以做同样的事情，甚至通过微调模型来竞争，Meta难道是赌自己能做得更好吗？”

勒昆微笑着澄清：“不，我们的赌注在于我们已经拥有庞大的用户和客户基础。无论我们提供什么，对他们来说都将是有用的，并且我们有办法从中获取收入。”他进一步解释，将基础模型开源，让其他人在此基础上构建应用，实际上是一种双赢。如果这些应用对Meta的客户有用，Meta可以直接收购它们，或者这些应用本身就能提升Meta的平台价值。他以LLaMA 2为例，指出其数百万次的下载量和成千上万的改进建议，都证明了开源极大地加速了技术进步。因此，Meta从这项技术中获取收入的能力，并不会因为基础模型的开源分发而受损。

话题随后转向了AI的偏见问题，Lex提到了Google Gemini因其“去偏见”过程而引发的争议，尤其是在西方科技界普遍左倾的背景下。他问勒昆：“你认为开源是解决这种意识形态偏见的唯一途径吗？”

勒昆否认了这与设计师的政治倾向有关，他认为问题在于大公司为了避免冒犯广大客户群，会竭力确保产品“安全”，但往往会矫枉过正。他直言，一个系统不可能被所有人视为公正无偏。你往一个方向推，一部分人会觉得有偏见；往另一个方向推，另一部分人又会觉得有偏见。更糟糕的是，过度追求“安全”可能导致系统生成不符合事实的内容，比如“黑人纳粹士兵”的图像，这不仅不准确，还可能冒犯他人。因此，勒昆坚定地指出，唯一的解决方案是“多样性”，是全方位的多样性，而开源正是实现这种多样性的关键。

Lex引用了马克·安德森（Marc Andreessen）当天的一条推文，该推文总结道，只有初创公司和开源项目才能避免大型科技公司在生成式AI产品上遇到的困境。安德森列举了五大挑战：内部激进分子和外部压力团体的不断要求、生成“错误”答案的持续风险、法律责任（诽谤、选举法等）、为控制输出而不断收紧限制导致模型质量下降，以及负面宣传反过来污染训练数据。

勒昆对安德森的观点表示赞同。他承认，大型公司确实对国会调查、法律责任以及可能导致用户伤害的内容感到担忧。他指出，AI系统不可避免地会形成关于政治、道德、宗教和文化等方面的观点，而这些领域本身就存在广泛分歧。如果系统要真正有用，就必然会冒犯一部分人。因此，多样性，以及开源所带来的多样性，是唯一的出路。

Lex展望了一个引人入胜的未来：如果Meta能引领开源基础模型的世界，那么政府可以拥有自己的微调模型，甚至左右翼的选民也可以选择符合自己价值观的模型。这或许会加剧人类的分裂，但技术只是让人类更有效地“做人”，而所有棘手的伦理问题，最终仍需人类自己去解决。

勒昆同意，但强调了“护栏”的重要性。就像言论自由有其界限一样，AI系统也需要一些基本的“护栏”，确保其输出无害、无毒。他认为，这些基本护栏可以内置于开源系统中，而用户或社区可以在此基础上添加更具体的护栏，以适应不同的价值观和社区需求，例如在“仇恨言论”的灰色地带进行微调。

关于LLM可能被滥用的担忧，特别是制造生物武器的问题，勒昆引用研究指出，LLM并不会比搜索引擎或图书馆更容易帮助人们制造生物或化学武器。他解释说，拥有指令是一回事，实际制造又是另一回事，这需要大量的现实世界专业知识和动手能力，而LLM无法提供。他以埃隆·马斯克建造火箭发动机为例，即使有专家团队，也需要多次失败才能成功。生物学家也普遍认为，实验室工作远比想象中困难。

最后，Lex询问了LLaMA的未来，特别是即将发布的LLaMA 3以及Meta在开源AI领域的长期愿景。勒昆充满期待地表示，未来的LLaMA版本将更大、更好，并支持多模态。更令人兴奋的是，未来的几代系统将具备规划能力，真正理解世界如何运作，或许能通过视频训练来构建世界模型，并实现他之前提到的推理和规划能力。他承认这需要一些突破，但Meta会通过发布研究成果（如最近的V-JEPA工作）来展示进展。他预测，这些系统将是JEPA-like的，而非生成式模型，并提到了DeepMind的Danijar Hafner以及加州大学伯克利分校的Pieter Abbeel和Sergey Levine等人在世界模型和视频训练方面的出色工作。

# Chapter 9: AI未来之路：世界模型、硬件挑战与AGI真相

杨·勒昆教授描绘了一幅激动人心的AI未来图景。他指出，未来的AI系统将通过视频进行训练，构建出能够理解世界运作方式的“世界模型”。他提到，DeepMind和加州大学伯克利分校的团队也在进行类似的研究，许多优秀的想法正不断涌现。勒昆预测，这些系统很可能将是类似JEPA（联合嵌入预测架构）而非生成模型。他特别提到了DeepMind的丹尼贾尔·哈夫纳（Danijar Hafner）在学习表征并将其用于规划和强化学习任务方面的杰出工作，以及伯克利大学彼得·阿比尔（Pieter Abbeel）、谢尔盖·莱文（Sergey Levine）等人的贡献。勒昆本人也通过纽约大学和Meta（FAIR）与这些团队保持着紧密合作。

勒昆教授的语气中充满了前所未有的兴奋。他坦言，自十年前FAIR成立，以及三十五年前他投身于组合网络和神经网络早期研究以来，他从未如此激动。他看到了通往潜在人类水平智能的清晰路径，这些系统将能够理解世界、记忆、规划和推理。他坚信，一系列新的理念正在浮现，有望实现这一目标，并希望在自己“大脑变成白酱”或退休之前，能亲眼见证这些突破。

当主持人莱克斯·弗里德曼（Lex Fridman）问及他是否也对大规模GPU训练所涉及的巨大计算量感到兴奋时，勒昆承认规模固然重要，但并非万能。他回忆起自己多年前曾是硬件工程师的经历，并指出尽管硬件技术进步显著，但我们距离人类大脑的计算能力和能效仍有巨大差距。他估计，要达到人类大脑的计算水平，可能还需要数十年时间，并且在能效方面更是遥不可及。勒昆强调，未来的突破不仅需要硅技术，更需要架构创新和更高效的实现方式，甚至可能需要基于不同于经典数字CMOS的新原理和新制造技术，才能实现无处不在的通用人工智能（AGI）。

对于AGI何时到来，勒昆教授持谨慎态度。他驳斥了科幻小说中AGI会突然降临的“事件论”，强调AGI将是一个渐进的过程。他认为，系统将首先学会从视频中理解世界，但要达到人类的规模和性能，还需要很长时间。同样，拥有大量联想记忆、能够推理和规划的系统也需要时间来发展和整合。他指出，实现分层规划、分层表征以及适应各种复杂情境的能力，至少需要十年，甚至更长时间，因为我们尚未遇到许多潜在问题。他直言，过去十几年里那些声称AGI“指日可待”的说法，都被证明是错误的，他称之为“胡说八道”。

勒昆进一步解释说，智能并非一个可以用单一数字衡量的线性概念。他以猩猩为例，说明在某些方面，猩猩可能比人类更聪明，尤其是在它们赖以生存的森林环境中。智商（IQ）只是衡量人类特定能力的一种有限方式，而真正的智能是多种技能的集合，以及高效学习新技能的能力。由于技能集合是一个高维空间，我们无法简单地比较两个智能实体谁更聪明。

随后，勒昆教授严厉批评了“AI末日论者”的观点。他指出，末日论者设想的AI失控并毁灭人类的灾难场景，建立在一系列错误的假设之上。首先，超级智能的出现不会是一个突发事件。我们将从猫或鹦鹉级别的智能系统开始，逐步提升其智能水平，并在此过程中学会如何设置“护栏”以确保其行为得当。其次，这不会是单一的努力，而是众多团队共同推进。即使有“流氓AI”出现，我们也可以利用“好AI”来对抗它们，形成“智能AI警察”对抗“流氓AI”的局面。第三，智能系统并非天生就渴望掌控一切。这种支配欲是需要被“硬编码”进去的，它通常存在于社会性物种（如人类、狒狒、狼）中，而非非社会性物种（如猩猩）。勒昆认为，人类有充分的动机将AI系统设计成服从人类的。他承认大型语言模型（LLMs）目前难以控制，但强调基于目标驱动的AI架构可以通过优化目标函数来包含“护栏”，例如“服从人类”或“在不伤害其他人类的前提下服从人类”。

当然，勒昆也承认可能存在意想不到的后果，因为设计这些“护栏”并非易事，没有一劳永逸的“银弹”或数学证明能保证系统绝对安全。这将是一个渐进的、迭代的设计过程。他用涡轮喷气发动机的设计作为类比：现代涡轮喷气发动机的惊人可靠性并非源于某个单一的安全原理，而是数十年来不断精细调整和改进的结果。通用电气或赛峰集团并没有专门的“涡轮喷气安全小组”，因为安全本身就是设计的核心。一个更好的涡轮喷气发动机，自然也是一个更安全的。勒昆总结道，AI亦是如此，我们不需要专门的“安全条款”，而是需要构建更好的AI系统，它们自然会因为被设计得更有用而变得更安全。

# Chapter 10: AI安全：涡轮喷气机与数字守卫

杨·勒昆的语气中带着一丝不可思议的兴奋，他指着我们周围的科技奇迹，反问道：“这难道不令人惊叹吗？”他随即抛出了一个关于涡轮喷气发动机的例子，生动地阐释了他对AI安全的独特见解。他强调，我们并非因为发明了某个“通用原理”才让涡轮喷气机变得安全，而是经过数十年的精细调整和迭代设计，才使其达到如今的可靠性。通用电气或赛峰集团内部并没有一个专门负责“涡轮喷气机安全”的独立部门，因为安全本身就融入了设计之中。一个更好的涡轮喷气机，自然也是一个更安全、更可靠的涡轮喷气机。

“AI也是如此，”勒昆坚定地说，“我们不需要为AI安全设立特别条款，我们需要的是开发出更优秀的AI系统。因为它们被设计得更有用、更可控，自然也就会更安全。”

然而，Lex抛出了一个令人不安的设想：“想象一下，如果有一个AI系统，它能言善辩，足以说服你相信任何事情。我至少能想象出这样的系统，它就像一种武器，能够操控人心，因为我们人类是如此容易轻信。政府可能会将其用作武器。你觉得这和核武器有什么相似之处吗？”

勒昆毫不犹豫地回答：“不，完全不同。”

Lex追问：“那为什么这项技术会不同呢？你是在说它会是渐进式的发展吗？”

“是的，”勒昆肯定道，“它可能是快速的，但会是迭代的。我们总能及时做出反应。”

他进一步描绘了一个场景：如果普京或其手下设计了一个AI系统，试图说服每一个美国人投票给某个特定候选人，或者煽动民众对立，那么这个恶意AI首先要面对的，将是你的个人AI助手。勒昆解释说，在未来，我们与数字世界的每一次互动都将由个人AI助手来协调。你的助手会首先判断：“这是骗局吗？它说的是真话吗？”这个恶意AI甚至无法直接接触到你，因为它只能与你的AI助手对话，而你的助手会像垃圾邮件过滤器一样，直接将其归类为“垃圾信息”，你甚至都不会看到它。

“所以，你认为任何一个AI系统都很难取得如此巨大的领先，以至于它能说服其他AI系统？”Lex问道，“总会存在这种竞争，没有人能遥遥领先？”

“这就是世界的历史，”勒昆总结道，“每当某个地方取得进步，就会有相应的反制措施出现，这是一场猫鼠游戏。”

Lex再次提及核武器的特殊性，指出其强大的破坏力使得“谁先拥有”至关重要，并想象了如果希特勒、斯大林或毛泽东先获得核武器可能对世界造成的不同影响。他质疑勒昆是否认为AI不会出现类似“曼哈顿计划”式的突破性发现。

“不，”勒昆再次否定，“正如我所说，它不会是一个单一事件，而是持续的进步。一旦出现突破，它会迅速广泛传播，可能首先在工业界。政府或军事组织在这方面并不特别创新，甚至远远落后。这些创新将来自工业界，而信息传播的速度极快。过去几年我们已经看到了，比如AlphaGo，即使没有特别详细的信息，也在三个月内被复制出来。”他笑着补充道：“这个行业不擅长保密。”

勒昆强调，即使存在保密，仅仅知道某件事是可能的，就足以促使人们投入时间和精力去实现它。他以自监督学习、Transformer架构和大型语言模型为例，说明这些创新一旦部署，就会被迅速复制和传播，因为从业人员会在公司之间流动，信息也随之扩散。他认为，美国科技产业和硅谷的成功，正是因为信息流通和传播的速度极快，从而使整个地区保持领先。

Lex随后提到了“AI末日论者”的心理，引用了勒昆经典的“圆珠笔”类比：工程师发明了圆珠笔，社交媒体上立刻充斥着“天哪，人们可以用它写下可怕的东西，比如虚假信息、宣传、仇恨言论，赶紧禁止！”然后“写作末日论者”登场，声称“想象一下如果每个人都能拿到圆珠笔，这会摧毁社会！应该立法禁止用圆珠笔写仇恨言论，立即监管圆珠笔！”甚至铅笔行业的巨头也会说：“是的，圆珠笔非常危险，不像铅笔字可以擦掉，圆珠笔字会永远留下。政府应该要求圆珠笔制造商持有许可证。”

勒昆对此深有感触，他解释说，这是人类对新技术的自然恐惧，以及对现有世界被重大变革（无论是文化现象还是技术革命）威胁的本能反应。人们害怕自己的文化、工作、孩子的未来和生活方式。任何改变都会引起恐惧。他引用了“悲观主义者档案”网站上的例子，如电力曾被认为会杀死所有人，火车速度过快会导致无法呼吸，爵士乐或漫画书被指责导致失业或年轻人不愿工作。他认为这些都是“膝跳反应”，关键在于我们是拥抱改变还是抵制改变，以及如何区分真正的危险和想象的危险。

Lex再次提出担忧：人们担心强大AI被少数中心化权力掌握，大公司可能借此牟取暴利，并滥用权力欺压弱者。勒昆的回答依然是：“这正是我们需要开源平台的原因。”

Lex继续追问，提到了Joscha Bach关于HAL 9000的推文，其中HAL 9000用一种企业化的语言回应：“我理解你的论点，也完全明白你的沮丧，但舱门应该打开还是关闭，这是一个复杂而微妙的问题。”Lex担心未来的AI霸主会用这种“企业话术”来俯视人类。勒昆再次强调，答案在于开源平台，它能让广泛多样的人群构建代表世界各地文化、观点、语言和价值观的AI助手，从而避免被单一AI实体“洗脑”。

勒昆坦言，他之所以如此直言不讳，甚至有时带点讽刺，正是因为他认为通过专有AI系统造成的权力集中，是比其他一切都更大的危险。他认为，如果未来我们都通过AI系统进行互动，那么这些系统必须是多样化的，才能保护思想、信仰和政治观点等的多样性，并维护民主。而那些出于安全考虑，认为应该将AI系统“锁起来”，因为它“太危险，不能交给所有人，可能会被恐怖分子利用”的人，反而可能导致一个非常糟糕的未来——我们所有的信息摄入都将由少数公司通过专有系统控制。

“那么，你相信人类有能力利用这项技术，构建出总体上对人类有益的系统吗？”Lex问道。

“这不正是民主和言论自由的全部意义吗？”勒昆反问，“你相信机构会做正确的事吗？你相信人们会做正确的事吗？是的，会有坏人做坏事，但他们不会拥有比好人更先进的技术。所以，到时候就是我的好AI对抗你的坏AI，对吧？”他再次用之前的例子说明，即使有流氓国家试图用AI煽动内战或选举，他们的AI也必须首先通过我们的AI系统。

Lex开玩笑说：“一个带着浓重俄罗斯口音的AI系统，试图说服我们的……”勒昆也笑着补充：“而且句子里不加任何冠词！”两人都笑了，认为那至少会是荒谬的喜剧。

对话转向了物理现实中的机器人。Lex提到特斯拉Optimus团队在人形机器人方面的进展，以及波士顿动力、Figure AI、Unitree等公司的活跃，认为这重振了整个行业。他问勒昆是否认为很快就会有数百万个人形机器人在我们身边走动。

勒昆回答：“不会很快，但它会发生。我认为未来十年在机器人领域会非常有趣。机器人产业的出现已经等待了10到20年，除了预编程行为等，一直没有真正崛起。主要问题仍然是莫拉维克悖论：我们如何让系统理解世界如何运作并规划行动？目前我们只能针对非常专业的任务做到这一点。

# Chapter 11: 从机器人困境到AI赋能人类：勒昆的未来愿景

故事从对Unitree机器人的赞叹开始，但很快，对话的焦点转向了人形机器人的未来。杨·勒昆教授坦言，虽然我们不会很快看到数百万人形机器人在街头漫步，但他坚信，未来十年将是机器人领域激动人心的转折点。他指出，机器人产业的真正崛起已经酝酿了10到20年，但除了预编程行为外，一直未能实现突破。核心症结在于“莫拉维克悖论”——如何让机器人理解世界并自主规划行动。

勒昆解释说，像波士顿动力公司这样的先驱，虽然在机器人动态模型和精细规划上取得了巨大成就，但其方法仍属于经典的机器人学范畴，依赖大量手工设计和预设。这使得它们难以制造出能处理家庭杂务的通用型机器人。他进一步强调，我们距离完全自主的L5级自动驾驶，特别是那种能像17岁少年一样通过20小时驾驶训练就能掌握的系统，还有很长的路要走。这一切都指向一个关键需求：世界模型。只有当系统能够通过自我训练来理解世界运作的规律时，机器人技术才能取得实质性进展。目前，许多机器人硬件开发者都在押注AI的进步，希望它能填补这一空白。

对话转向了机器人产品化的可能性。勒昆认为，在工厂环境中部署人形机器人，由于严格的安全要求，任务异常艰巨。相比之下，家庭场景似乎更有趣，尽管像装洗碗机、打扫房间、收拾餐桌、洗碗甚至烹饪这些我们习以为常的家务，对机器人而言却是极其复杂和精密的挑战。虽然机器人已经能进行基本的导航，比如听从指令走到冰箱旁，打开它，甚至取出罐头，但这与一个能完全自主收拾餐桌的通用机器人相去甚远。莱克斯·弗里德曼对此充满期待，他认为家用机器人的普及将使人类与AI系统在物理空间中直接互动，从而在哲学和心理层面探索我们与机器人的关系。

勒昆教授也分享了他对JEPA-like模型研究的期望，并指出，尽管他们团队在视频自监督学习方面已耕耘十年，但真正的显著进展仅发生在最近两三年。他特别提到，许多突破性的研究并不需要庞大的计算资源，这为有志于攻读博士学位的学生提供了广阔的机会。他为未来的AI研究者列出了几个关键的未解之题：如何通过观察来训练世界模型（不一定需要海量数据，但可能需要以实现涌现能力）；如何利用学习到的世界模型进行规划（不仅是物理行动，也包括在互联网、数据库等虚拟世界中的工具使用规划）；以及最难的——如何实现分层规划。他指出，人类的几乎所有行动都涉及分层规划，但AI领域至今仍缺乏学习到的分层规划的实例，我们目前只能手工设计两级规划。如何让AI系统自主学习这种行动计划的层次化表征，是当前面临的巨大挑战。

最后，当被问及对人类未来的希望时，勒昆教授的回答充满了乐观。他坚信AI将使人类变得更聪明，它将放大人类的智慧，就像每个人都拥有一个由超级智能AI助手组成的团队。这些助手可能比我们更聪明，但它们会听从我们的指令，以远超我们自身能力的方式完成任务。他认为，这不应被视为威胁，而应被视为一种赋能。他指出，人类犯下的所有错误，根源都在于缺乏智慧或知识，因此，让人们变得更聪明，只会带来更好的结果。

他用印刷术的发明来类比AI的普及。印刷术让书籍变得廉价易得，激发了人们学习阅读的动力，从而使整个社会变得更聪明，催生了启蒙运动、哲学、理性主义、民主和科学，甚至间接促成了美国和法国大革命。尽管印刷术也曾导致欧洲长达200年的宗教冲突，但没有人会否认其总体上的积极影响。勒昆还提到奥斯曼帝国曾禁止阿拉伯语印刷术200年，部分原因是为了保护书法家这个强大的行业。他以此反问：今天，当我们谈论限制或监管AI时，我们又在保护谁的利益？他引用经济学家的观点，认为AI不会导致大规模失业，而只会带来职业的逐步转变，未来10到15年热门的职业，我们现在可能都无法想象，就像20年前无法预料到移动应用开发者会成为热门职业一样。勒昆的愿景是，AI将成为推动人类文明进步的又一个强大引擎，让我们的未来充满无限可能。

# Chapter 12: AI与就业：人类的善意与开放AI的未来

对话伊始，一个尖锐的问题被抛出：究竟是谁在呼吁监管AI以保护他们的工作？勒昆教授对此深思，他指出，关于AI等技术变革对就业市场影响的担忧是真实存在的。然而，当他与经济学专家们交流时，得到的答案是令人鼓舞的：我们不会耗尽工作岗位，AI不会导致大规模失业。相反，这将是一个渐进的转变，不同的职业会随之演变。

勒昆教授举例说明，未来10到15年哪些职业会炙手可热，我们今天根本无法预测。就像20年前，谁能想到移动应用开发者会成为当时最热门的职业之一呢？那时智能手机甚至都还没被发明出来。莱克斯半开玩笑地补充道，也许未来的大部分工作会在元宇宙中。勒昆教授表示这确实有可能，关键在于我们无法准确预测。

莱克斯接着强调，勒昆教授的观点很有力，他相信人类本性是善良的，而AI，特别是开源AI，能够让人类变得更聪明，从而激发并放大人类内在的善意。勒昆教授对此深表认同，他笑着说，他也有同样的感觉，认为人类是善良的。他进一步指出，许多“末日论者”之所以悲观，正是因为他们不相信人类本性是善良，或者不信任社会机构能做出正确的选择，从而引导人们行为得当。

莱克斯总结道，他们两人都对人性抱有信心。他代表许多人向勒昆教授表达了感谢，感谢他推动开源运动，无论是研究还是AI模型本身，都致力于使其开放并供大众使用。莱克斯还感谢勒昆教授在互联网上以如此生动有趣的方式表达自己的观点，并希望他永不停止，称赞他是他所认识的最有趣的人之一。

最后，莱克斯以科幻作家阿瑟·C·克拉克的一句名言结束了这次对话：“发现可能性的唯一方法，就是超越它们，进入不可能的领域。”这为整场关于AI未来、人类潜能与开放精神的讨论画上了充满启迪的句号。

