# Chapter 1: 从赛道到代码：速度、专注与质量的追求

故事始于一段寻求放松的时光。为了在紧张的工作之余释放压力，我和团队成员们决定尝试一项刺激的活动——赛车。最初的契机是一位微软的同事提出的，他建议我们去拉古纳塞卡（Laguna Seca）的罗素赛车学校（Russell Racing School）体验一番。

大约有八个人一同前往，我们从为期四天的“新手入门课程”开始。驾驶着马自达的开放式赛车，我第一次真正感受到了赛车的魅力。然而，这次经历也让我清醒地认识到，自己并没有想象中那么擅长赛车。新手课程主要教授赛车的基本驾驶技巧、车辆动力学、G力以及过弯等基础知识。

随着对赛车的热情渐浓，我们中的几个人又报名参加了进阶课程。这次课程结束后，我们甚至有机会参加一场真正的比赛。经过几个月的练习，我的驾驶技术有了显著提升，比赛也充满了乐趣。

赛车学校还提供了一种“学校系列赛”，每月举行一次，周六和周日各一场比赛。我的妻子黛比（Debbie）也加入了进来，她先完成了新手和进阶课程，然后我们俩每个月都会前往赛道，从三月一直比到九月，总共参加了大约十场比赛。那段时光真是令人难忘。

每年，赛车学校还会举办一场“职业赛”（Pro Race）。这种比赛的赛车没有限速器，引擎控制单元（ECU）会被重新编程，以提供更强劲的动力。职业赛设有奖金，最高可达一万美元，因此吸引了许多顶尖车手。我曾参加过一次职业赛，最终名列倒数第二，仅仅因为有一位车手比我更糟糕。

这次经历让我决定认真对待赛车。我与一位赛车学校的教练以及一位赛车队老板合作，从新西兰进口了一辆大西洋系列赛车。1995年，我拥有了这辆赛车，并由那位教练驾驶参赛。第二年，我们决定购买一辆新车，于是旧车该如何处理成了问题。我坐进了旧车，惊讶地发现自己的驾驶技术已经相当不错了。

于是，我决定亲自驾驶这辆旧车参加大西洋系列赛。在这项赛事中，我总共参加了56场比赛。我的赛车生涯中也经历了一些惊心动魄的事故，虽然没有受过重伤，但其中一次在拉古纳塞卡赛道发生的事故却颇为“臭名昭著”。那是在第10号弯，一个高速下坡右弯，我的赛车发生了一次“纵向翻滚”，就像桶滚一样，最终车顶朝下着地。由于当时尘土飞扬，巨大的尘雾弥漫开来。ESPN电视台反复播放了这段事故录像长达五分钟，因为它发生在比赛暂停、赛道清理期间，成为了他们填充时间的绝佳素材。

我的最好成绩是在密尔沃基英里赛道（Milwaukee Mile）获得第八名。我还曾在墨西哥蒙特雷（Monterrey Mexico）参赛，那里的赛道安全设施远不如美国。此外，我还去过霍姆斯特德（Homestead）、克利夫兰机场（Cleveland at the airport）、俄亥俄州中部（Mid-Ohio）、蒙特利尔（Montreal）、三河城（Trois-Rivières）、温哥华（Vancouver）、波特兰（Portland）以及长滩（Long Beach）等地比赛。

在罗德阿美利加赛道（Road America），我遭遇了另一次重大事故，甚至可能是最严重的一次。在被称为“Kink”的弯道，一个时速高达140到150英里的高速弯，我失去了对赛车的控制，撞上了石墙。当我从车里爬出来时，发现赛车还在摇晃，但四个车轮却不翼而飞，只剩下我坐在单体式座舱里，引擎还在后面，但车轮却全部脱落了。尽管如此，密尔沃基英里赛道仍然是我最喜欢的赛道。

从赛车经历中，我看到了与软件开发相通之处，那就是“专注”。无论是驾驶赛车还是编写代码，都需要极度的专注。赛车时，你必须专注于前方即将发生的一切，而不是十毫秒前已经过去的事情。这种专注力是成功的关键，它意味着将注意力集中在最重要、最需要实现的目标上。

在编写代码时，我通常不受外界噪音的干扰，尽管偶尔走廊里传来的口哨声会让我有些不悦。我习惯于在运行代码之前，在脑海中对代码进行三到四次迭代的模拟执行，检查所有可能的执行路径和错误路径，以证明其正确性。因此，我的代码通常第一次运行时就能成功运行，即使不完全正确，也不会崩溃。

谈到NT程序中的bug，如何减少错误并使其更容易修复是一个核心问题。我认为减少bug的数量相对更容易实现，因为bug越少，剩下的bug就越难解决。最难解决的bug往往与同步问题有关，而非代码逻辑的正确性，例如缺少正确的屏障、锁或存在竞态条件。多处理器系统（MP系统）的引入，进一步加剧了同步问题的复杂性。

我坚信，质量必须从底层开始，自下而上地构建，而非自上而下地强制推行。你无法通过立法来保证质量，它必须是每个底层环节的工程师内心固有的信念。任何一个底层环节未能产出高质量的组件，都会导致上层系统出现问题。因此，减少bug的关键在于从一开始就更加关注质量，而不是寄希望于事后100%的测试。

我个人非常讨厌有bug被提交给我，一旦发现，我会立刻放下手头的一切去修复它，因为我知道我可以解决它。这种“现在就修复”的心态，就像戒烟一样，最佳时机就是当下。

至于如何让bug更容易修复，除了构建更好的分析工具之外，我没有太多其他方法。我们现在拥有许多工具，但总有人觉得现有工具不够完善，希望开发出更强大的工具。

在NT系统演进的中后期，大约2001年之后，互联网的兴起带来了严峻的安全挑战，病毒和系统攻击层出不穷。为此，我们开发了专门的工具来分析源代码，以发现潜在的漏洞，其中最常见的就是缓冲区溢出。当时发现的缓冲区溢出数量之多令人震惊，这些工具能够准确指出可能发生溢出的位置，极大地降低了bug的数量。甚至像算术溢出这样看似微小的错误，也可能导致bug，例如在计算缓冲区大小时发生溢出，从而分配了错误的内存大小。

# Chapter 2: 从漏洞到卓越：NT的质量之路

随着互联网的兴起，一个全新的威胁也随之而来——病毒与系统攻击。这迫使我们必须开发出能够分析源代码的工具，以抵御这些无形的敌人。我们发现，其中最令人头疼的莫过于“缓冲区溢出”问题，其数量之庞大简直令人难以置信。我们的工具就像一位敏锐的侦探，能够精准地指出代码中潜在的溢出点，这极大地帮助我们降低了错误率。

然而，挑战远不止于此。即使是看似微不足道的“算术溢出”，也可能引发灾难。想象一下，当你计算一个缓冲区所需的大小时，一个溢出可能导致系统分配了一个过小的空间，而后续的数据传输却试图写入更大的数据，瞬间便会引发严重问题。我们的工具能够识别并修复这些潜在的缺陷，就像对待真正的bug一样。

但这些静态分析工具并非完美无缺。它们常常会发出“误报”，指出并非真正问题的代码，同时又可能“漏报”一些隐藏的逻辑错误。因此，我们不得不投入精力去过滤这些误报，确保我们只关注真正的威胁。

谈及如何激励程序员写出高质量的代码，我深信“以身作则”是最好的方式。我所做的每一件事，都力求做到最好，我的代码是公开的，每个人都能看到。他们会发现我的代码几乎没有bug，甚至为零。我希望我的榜样作用能帮助他们成为更优秀的程序员。

更重要的是，我们需要从一开始就向年轻的开发者灌输“质量至上”的理念。对我而言，没有什么比一个无法正常工作的软件更令人沮丧的了。当手机、平板或电脑上的某个功能本应正常却突然失灵时，那简直令人愤怒！因为这根本不应该发生。那些无法正常工作的功能，本应只是微不足道的小瑕疵，比如屏幕上几个像素的偏差，或者一些无关紧要的细节。事实上，许多针对NT系统提交的bug，至今仍是这类外观上的小问题，它们优先级较低，并不会让人感到不安。

真正令人痛心的是，当你发布了一个产品，它在市场上运行了四个月，却突然爆发了一场灾难，比如抹掉了用户的数据。那一刻，你会意识到这才是真正的大问题。因此，我坚信，从年轻一代开始，就必须强调质量的重要性，这是我们软件开发的基石。

# Chapter 3: Windows演进：64位之路与架构之争

故事从Windows 2000的成功开始，它以卓越的可靠性赢得了赞誉。我主要负责内核性能方面的新功能开发，确保系统的稳定与高效。然而，在2000年，一个关键的决策改变了NT的未来走向。负责消费者产品组的领导认为，消费者对系统质量的要求不如服务器用户高，而服务器组为下一个版本准备的功能清单也远超消费者所需。于是，我们做出了一个后来被证明是错误的选择：将代码库一分为二，分别用于Windows XP（面向消费者）和Server 2003（面向服务器）。

结果是，Server 2003的代码库在安全性方面得到了彻底的修复，并集成了大量服务器特性，最终几乎与XP同时发布。然而，XP虽然带来了全新的用户界面（比如新的开始按钮），却也伴随着更多的安全漏洞和错误。我主要投入到Server 2003的开发中。

就在这段时间，大约在2000年之后不久，AMD带着一个激动人心的提议找上门来：他们开发了一个兼容x86架构的64位扩展。在此之前，我们已经在Windows 2000中加入了对Itanium平台的支持。Itanium虽然能运行32位x86程序，但性能不佳，且其自身速度也从未超越最快的x86处理器。因此，AMD的提议显得格外诱人。我们最初持观望态度，让他们回去完善方案。六个月后，AMD带着更成熟的方案卷土重来。我立刻意识到其巨大潜力：x86程序可以在64位处理器上全速运行，只需一个轻量级的封装层；而64位程序则能利用更大的地址空间，同样保持高速。

我坚信这是正确的方向，于是主动向领导Al提出，我们必须着手实施。在我的带领下，一个由四人组成的小团队开始了这项艰巨的任务。幸运的是，由于之前对Itanium和64位Alpha系统的支持尝试，我们的代码库在指针大小方面已经做好了数据类型中立的准备，这为64位移植打下了良好基础。在项目初期，我们与AMD紧密合作，成功地将大约六项关键特性融入了处理器架构，其中一项——32位位移的相对寻址——被我视为绝对不可或缺。这意味着只要二进制文件不超过2GB，相对寻址就能覆盖整个文件，这对于实现位置无关代码（Position Independent Code, PIC）至关重要，而PIC又是地址空间布局随机化（ASLR）等安全机制的基础。

与此同时，消费者端的XP已经发布，而我们正在开发代号为“Longhorn”的下一代消费者系统。然而，“Longhorn”项目很快陷入了泥潭，充斥着无数的bug，难以正常运行。我最终说服Al，放弃这个烂摊子，转而以Windows 2003的代码库为基础重新开始。这个新系统最终演变成了Vista。

我们的x64支持最终以Windows Server 2003 SP1的形式发布，包含了64位的专业桌面版和服务器版。事实证明，这是一个巨大的成功，如今几乎所有系统都已转向64位。然而，在Server 2003发布之前，我们不得不投入巨大精力，将Server 2003中所有的安全增强功能回溯到XP上，这便是XP Service Pack 2的由来，它几乎是对XP的一次全面重新发布。

在NT系统不断演进的历程中，RISC（精简指令集计算机）架构逐渐式微。我认为，RISC阵营过于执着于“简单架构、简单实现”的理念，而Intel和AMD则凭借其庞大的晶体管数量、精湛的设计和验证能力，将复杂性转化为性能优势，例如高度复杂的乱序执行、分支预测和缓存技术。RISC架构未能提供与x86系统相当的性价比和兼容性，尽管我们曾尝试通过二进制重编译等技术来弥补兼容性问题，但这无疑增加了开发和测试的负担。

如今，ARM架构在手机和移动设备领域占据了一席之地，主要得益于其低功耗特性。虽然ARM在性能上仍无法与Intel或AMD的x86处理器匹敌，但其多样化的芯片尺寸组合使其能够适应从智能手表到手机等各种设备的需求，这与Intel和AMD倾向于制造大型、高性能芯片的策略形成了鲜明对比。这场关于架构、性能、功耗和市场定位的较量，仍在继续。

# Chapter 4: 芯片演进、Vista困境与Azure的诞生

在芯片技术飞速发展的浪潮中，ARM公司以其多样化的芯片组合独树一帜，无论是用于智能手表的小巧芯片，还是其他各种尺寸的处理器，都展现了其灵活的市场策略。然而，英特尔和AMD则长期专注于高性能、大尺寸芯片的研发，似乎从未认真考虑过将现有功能集成到更小尺寸芯片中的可能性。当P4处理器达到4GHz的极限，芯片因过热而“白炽化”时，半导体工艺的瓶颈迫使行业走向了多核时代。这对于操作系统而言，无疑是一场巨变，突然间，系统需要驾驭多个处理器核心。

幸运的是，Windows NT从设计之初就具备了多处理器（MP）支持，其早期的RISC原型机便是MP架构，因此，面对多核处理器，NT系统能够从容应对。核心（Core）的概念，实际上分为两种：一种是英特尔的对称多线程（SMT），即在一个物理核心上运行多个线程上下文，通过快速切换来提升并行度，但并非总是能带来显著的性能提升，有时甚至会因缓存未命中等延迟而导致性能波动，效果难以预测。另一种则是AMD的芯片多处理器（CMT），它直接在一个芯片上集成了多个独立的物理处理器。最初的AMD 64位芯片便是如此，拥有两个真正的处理器。尽管后来所有厂商都逐渐转向了多核设计，共享部分逻辑（如L2或L3缓存），但NT系统对这些复杂的缓存层次结构和关联性早已了然于胸，因此多核的出现对NT本身影响不大，真正受益的是那些能够充分利用多线程的应用。

随着单核处理器性能的停滞，应用程序开发者们期待着从3.5GHz向4GHz的飞跃，但由于高电流泄漏导致芯片发热严重，这一飞跃未能实现。于是，芯片制造商们转而投入到“核心数量竞赛”中，不断增加单个芯片上的核心数量，甚至达到了80个之多。然而，对于NT系统而言，核心数量的增加更多是应用程序的福音，而非操作系统本身的挑战。

时间来到Vista发布之际，这是“Longhorn”项目重定向后的产物，代码库终于回归统一。作为x64项目的核心推动者之一，我继续为Vista贡献力量，主要负责x64相关的工作。尽管我们团队只有四人承担了大部分x64的开发，但编译器团队的几位专家也功不可没，整个x64系统大约由百人左右的小团队完成。Vista的发布却面临巨大困境，它与XP乃至之前的所有Windows版本都截然不同，市场反响极差，甚至被认为是Windows历史上最失败的版本之一（除了短暂的Windows Bob）。

2006年11月Vista发布后不久，云计算的概念开始兴起。这是一种将计算和存储重新集中化的趋势，与早期的分时系统有异曲同工之妙，只不过现在是通过庞大的服务器集群提供服务。微软内部启动了一个名为“Azure”的孵化项目，旨在提供一个平台，让用户可以租用计算能力和存储空间。由于需要运行多个操作系统实例，虚拟化成为了核心技术。当时，AMD和英特尔刚刚推出第一代虚拟化扩展，但我们认为现有的影子页表（shadow page tables）方案效率不高，决定采用更直接的翻译方式，尽管会增加翻译层级，但避免了页表的重复拷贝。AMD率先提供了这种技术，英特尔紧随其后。我负责Azure的Hypervisor开发，我们在华盛顿昆西建立了一个价值数千万美元的测试服务器农场。Azure最初的定位是平台即服务（PaaS），我们投入大量精力构建了配置管理、操作系统和应用更新等基础设施，以解决大规模服务器部署的痛点。大约两年半后，史蒂夫·鲍尔默决定将Azure正式产品化，并于2010年作为PaaS平台推出。随着Azure的成熟，我们开始在全球范围内建设数据中心，但这也带来了跨地缘政治区域数据复制的挑战，例如德国政府的数据不能随意存储在美国的服务器上。

Azure首个版本发布后，服务器部门的管理层发生变动，我决定转投Xbox团队。这颇具讽刺意味，因为我曾开玩笑地建议鲍尔默，与其每周在Xbox上亏损数百万美元，不如把钱拿去开篝火晚会。七年后，我却加入了这个“黑暗面”。Xbox的安全性与传统PC截然不同，它不仅要防范网络入侵，更要抵御物理层面的攻击，比如有人用烙铁和逻辑分析仪修改系统。一旦系统被篡改，Xbox会立即检测到并“变砖”，彻底失去功能。构建这样一个系统需要极高的安全细节和硬件支持，包括大量的加密技术。AMD为Xbox定制了SoC芯片，集成了八个核心（分为四个双核组），并特别加入了一个ARM安全处理器，以确保系统的坚不可摧。

# Chapter 5: Xbox的极致安全：从硬件到软件的防盗版堡垒

故事从一个令人警醒的警告开始：一台被篡改的Xbox，瞬间就会变成一块毫无用处的“砖头”。这并非危言耸听，而是微软在设计Xbox时，对安全性投入极致关注的体现。这种严苛的安全机制，从硬件到软件，无不渗透着精密的考量与设计。

Xbox的核心是一颗由AMD定制的系统级芯片（SoC），它拥有八个强大的核心，被巧妙地划分为四个双核组。然而，真正的安全守护者，是其中一颗独立的ARM安全处理器。它确保了安全启动的完整性，任何试图通过焊接、拆卸或重新编程硬件组件来篡改系统的行为，都将无所遁形。系统中的一切数据都经过加密，而安全处理器甚至能“屏蔽”部分硬件，使其对其他组件不可见，甚至能直接停止处理器运行，拥有对系统运行的绝对控制权。

这一切严密的防护，其背后只有一个核心原因：打击盗版。Xbox上运行的都是所谓的“AAA级”游戏，这些游戏制作成本高昂，追求极致的响应速度和高帧率，为玩家带来沉浸式体验。与PC游戏不同，PC游戏虽然也面临盗版问题，但通常可以通过首发销量高峰来弥补部分损失。而AAA级主机游戏，往往售价不菲，开发者和发行商需要一个坚实的保证，即他们的心血不会轻易被盗版侵蚀。微软通过这种“铜墙铁壁”般的安全系统，向游戏开发者承诺，他们的作品将得到最大程度的保护，从而吸引更多优质游戏登陆Xbox平台，形成良性循环。

Xbox的操作系统，被内部称为“九头蛇”（Hydra），它并非一个简单的操作系统，而是一个基于Hypervisor（虚拟机监控程序）的三头六臂的复杂架构。Hypervisor是整个系统的基石，它负责隔离内存、调度资源，并对系统进行物理分区。在其之上，运行着三个独立的虚拟机：
1.  **主机系统（Host System）**：它是可信计算基的一部分，与Hypervisor直接通信，负责管理底层硬件和设备驱动。
2.  **系统操作系统（System OS）**：这是一个共享资源环境，承载着用户界面（UI）以及各种应用，如直播电视、Netflix、Hulu等。玩家通过手柄进行的所有交互，都首先与这个系统OS进行沟通。
3.  **游戏虚拟机（Game VM）**：这是为AAA级游戏量身定制的环境。当游戏在前台运行时，Hypervisor会为其分配六个核心，确保游戏获得专属的计算资源，避免其他系统进程的干扰，从而保证游戏运行的流畅性和稳定性，杜绝音频或视频卡顿。即使游戏在后台运行（例如玩家在看电视），也能获得四个核心的保障，而其他环境则共享剩余的核心。Hypervisor在主机系统的指令下，不断地动态调整核心分配，以优化用户体验。

这种独特的虚拟化设计，使得Xbox能够同时运行游戏、观看直播电视、进行Skype通话甚至浏览网页，而互不干扰。每个虚拟机都有独立的内存空间，不像传统分时系统那样会“偷取”页面，确保了游戏每次运行都能保持一致的性能。

展望未来，Xbox的演进将是渐进式的。关于是否还需要独立游戏主机，或者PC平台能否承载AAA级游戏的讨论仍在继续。然而，PC环境的开放性使其难以达到Xbox那样的安全级别。Xbox用户已经习惯了这种“非我所有”的环境，他们无法随意安装设备驱动或注入代码，这极大地限制了常见的网络攻击。Xbox的Hypervisor通过“嵌套分页”技术，结合安全哈希，确保只有经过识别和验证的代码才能被执行，即使发生缓冲区溢出，攻击者也难以注入恶意代码，只能利用现有代码中的“小工具”（gadgets），但这种攻击的难度和效果都大打折扣。

将目光投向更广阔的操作系统领域，VMS系统在37年后依然活跃，Windows的代码量已达数十亿行，Linux也在不断壮大。然而，像Unix那样因碎片化而逐渐式微的例子，也警示着兼容性的重要性。构建一个像Linux或NT这样规模的新操作系统，并确保其兼容性，将是一项极其艰巨的任务，短期内几乎不可能实现。除非现有代码库的复杂性达到无法维护的地步，人们才可能考虑“推倒重来”，但即便如此，兼容性依然是不可逾越的鸿沟。

至于“大数据”，它并非操作系统本身的问题，而是一种数据分析和应用范式。它利用应用程序对海量数据进行可视化、分析和筛选，以发现隐藏的关系。从杜邦公司早年的“Rammer”项目，到如今的市场营销广告投放，大数据分析早已无处不在。云计算的弹性优势，使得用户可以根据需求动态调配成千上万的处理器进行并行计算，极大地推动了大数据应用的发展。

并行处理的通用分解，一直是计算机科学领域孜孜不倦的追求。从上世纪70年代Ken Kennedy在莱斯大学对Fortran代码进行向量化和并行化的尝试，到今天，这一领域的技术仍在不断进步，为未来的计算模式奠定基础。

# Chapter 6: 并行计算与AI浪潮：从Fortran到智能驾驶的思辨

故事从一位名叫肯·肯尼迪（Ken Kennedy）的传奇人物开始，他以其在“老旧Fortran代码”（dusty deck Fortran）领域的卓越贡献而闻名。肯尼迪教授是Tera Cray初创公司的核心成员，早在上世纪70年代，他就致力于将那些年代久远的Fortran程序进行向量化和并行化处理，试图让它们在当时最先进的并行计算机上焕发新生。这在当时无疑是一项开创性的工作，尽管挑战重重。

随着时间的推移，计算机科学的重心逐渐从“改造旧代码以适应并行”转向了“从一开始就设计新代码以实现并行”。这种转变在微软的操作系统中也得到了体现。例如，Windows 8乃至Windows 10都引入了用户模式调度（user mode scheduling）机制，允许应用程序创建成千上万个线程并自行调度，从而最大限度地减少了系统开销。这背后的驱动力，正是大约五年前业界普遍认识到的一个瓶颈：CPU核心频率的增长遇到了物理极限。曾几何时，英特尔豪言十年内将推出10GHz处理器，但很快便修正了这一预测，承认除非晶体管技术发生革命性突破，否则这一目标难以实现。因此，充分利用多线程和并行计算，成为了提升系统性能的关键路径。

话题随后转向了人工智能（AI）。回溯AI的早期，曾有过一阵狂热的喧嚣，人们一度认为计算机将很快无所不能。然而，随之而来的是一段漫长的沉寂期，让人不禁怀疑AI是否真的能有所作为。但如今，AI领域正涌现出许多令人惊叹的突破性进展。其中最令人难以置信的，莫过于Skype翻译器。想象一下，一位法国人与一位西班牙人可以毫无障碍地实时对话，Skype能够自动将一方的语言翻译成另一方的语言。这简直是奇迹！在此之前，有多少博士论文曾试图解决这一难题，却都未能成功？而现在，它已然成为现实。

AI的进步也体现在日常生活中。Windows系统现在支持指纹识别登录，而一位同事甚至展示了她新换的Windows 10手机，可以通过虹膜识别来解锁。这些都预示着AI正以前所未有的速度融入我们的生活。

然而，当谈到自动驾驶汽车（AI car）时，对话者的态度变得谨慎起来。他承认自动驾驶是AI的一个重要应用方向，但对于“何时能安心地坐在自动驾驶汽车的后座上，让它自行驾驶”这个问题，他认为答案是“很长一段时间内都不能”。他解释说，自动驾驶AI与Skype翻译器这类AI有着本质的区别。翻译中偶尔的“口误”可能只是小问题，但如果像医疗设备（如CT扫描仪）那样的实时控制系统出现故障，则可能危及生命。自动驾驶汽车无疑属于后者。

考虑到Windows和Linux等操作系统的庞大与复杂，其中存在大量软件缺陷的可能性极高。除非自动驾驶系统所使用的软件被极度精简，并能被严格证明其正确性，否则其安全性令人担忧。此外，计算机并非百分之百可靠，偶尔会出现“莫名其妙”的故障。虽然像Tandem这样的公司曾构建过三重冗余逻辑的系统来确保高可用性，但自动驾驶汽车是否也采用了如此高标准的冗余控制器，目前尚不得而知。

更令人担忧的是环境因素。他分享了一个亲身经历：在加利福尼亚的兰卡斯特，他们租用了一辆配备“永不迷路”（NeverLost）导航系统的赫兹汽车，结果却真的迷路了。原因在于当时的大气条件导致GPS信号失效。试想，当自动驾驶汽车在高速公路上以每小时70英里的速度行驶时，如果遇到类似的大气条件导致GPS失灵，虽然近距离传感器可能仍在工作，但汽车将失去方向感。因此，在将自动驾驶汽车大规模投入使用之前，还有许多关键问题需要得到充分的解答。

