1
00:00:00,080 --> 00:00:03,449
in spite of the steady accumulation of detailed knowledge

2
00:00:04,160 --> 00:00:08,410
how the human brain works is still profoundly mysterious

3
00:00:09,090 --> 00:00:17,770
[music] the ultimate goal of neuroscience is to learn how the brain gives rise to human intelligence

4
00:00:18,800 --> 00:00:20,970
and what it means to be intelligent

5
00:00:23,039 --> 00:00:27,529
understanding how the brain works is considered one of humanity's greatest challenges

6
00:00:28,800 --> 00:00:31,929
i got into this field because i just was curious as to who i am

7
00:00:32,320 --> 00:00:33,210
you know how you know

8
00:00:33,280 --> 00:00:36,010
how do i think what's going on in my head when i'm what i'm thinking

9
00:00:36,320 --> 00:00:37,450
what does it mean to know something

10
00:00:38,079 --> 00:00:38,489
you know

11
00:00:38,499 --> 00:00:42,570
i can ask what it means for me to know something independent of how i learned it from you

12
00:00:42,580 --> 00:00:43,930
or from someone else or from society

13
00:00:44,640 --> 00:00:47,610
what does it mean for me to know that i have a model of you in my head

14
00:00:47,620 --> 00:00:48,410
what does it mean to know

15
00:00:48,420 --> 00:00:50,410
i know what this microphone does and how it works physically

16
00:00:50,640 --> 00:00:51,930
even when i can't see it right now

17
00:00:52,879 --> 00:00:53,690
how do i know that

18
00:00:54,399 --> 00:00:55,050
what does it mean

19
00:00:55,120 --> 00:00:58,329
how do the neurons do that at the fundamental level of neurons and synapses

20
00:00:58,640 --> 00:00:59,210
and so on

21
00:00:59,359 --> 00:01:01,289
those are really fascinating questions

22
00:01:01,920 --> 00:01:08,330
the neuroscientist francis crick observed that scientists have been collecting data on the brain for decades

23
00:01:09,040 --> 00:01:11,610
they knew a lot about the workings of the brain

24
00:01:12,000 --> 00:01:16,650
but they hadn't converged on any meaningful theory of how the brain worked

25
00:01:17,360 --> 00:01:23,450
how intelligence emerges from low level cells in your head is still a profound mystery

26
00:01:24,400 --> 00:01:25,049
to be intelligent

27
00:01:25,360 --> 00:01:28,890
the brain has to learn a great many things about the world

28
00:01:29,520 --> 00:01:31,770
to understand how the brain creates intelligence

29
00:01:32,640 --> 00:01:37,689
we need to figure out how the brain learns a model of the world and everything in it

30
00:01:38,240 --> 00:01:42,570
jeff hawkins thinks that the reality we perceive is a kind of simulation

31
00:01:43,600 --> 00:01:44,130
a hallucination

32
00:01:45,360 --> 00:01:45,770
a confabulation

33
00:01:47,119 --> 00:01:47,610
he thinks

34
00:01:47,620 --> 00:01:55,530
that our brains are a model of reality based on thousands of information streams originating in the senses in our body

35
00:01:56,079 --> 00:01:56,329
critically

36
00:01:56,960 --> 00:01:59,369
hawkins doesn't think that there's only one model

37
00:01:59,840 --> 00:02:01,450
but rather thousands

38
00:02:02,320 --> 00:02:03,770
jeff has just released his new book

39
00:02:03,920 --> 00:02:04,810
a thousand brains

40
00:02:05,280 --> 00:02:06,170
a theory of intelligence

41
00:02:06,180 --> 00:02:09,128
it's an inspiring and well written book

42
00:02:09,139 --> 00:02:10,970
and i really hope after watching this show

43
00:02:11,360 --> 00:02:13,129
you'll be inspired to read it too

44
00:02:13,760 --> 00:02:14,010
now

45
00:02:14,080 --> 00:02:15,290
jeff is a sub symbolist

46
00:02:15,599 --> 00:02:21,370
he thinks that our knowledge is sliced and distributed over the substrate of synapses in our brain

47
00:02:21,840 --> 00:02:29,930
but similarly that our simulated models of the world are also distributed over hundreds and thousands of cortical columns

48
00:02:30,400 --> 00:02:32,250
this means that when we recognize a dog

49
00:02:32,480 --> 00:02:34,730
there's a society of cortical columns

50
00:02:35,360 --> 00:02:42,650
all independently predicting that the dog is there and mutually calibrating after a consensus is reached

51
00:02:43,519 --> 00:02:45,530
now jeff hawkins is a materialist

52
00:02:46,560 --> 00:02:48,970
he thinks that despite appearances to the contrary

53
00:02:49,599 --> 00:02:55,290
mental states are just physical states and that the latter emerges from the former

54
00:02:55,760 --> 00:02:56,970
there's no inception

55
00:02:58,159 --> 00:02:59,930
no ghost in the machine

56
00:03:00,800 --> 00:03:03,090
no cartesian theater

57
00:03:03,680 --> 00:03:06,330
if i had to summarize into one

58
00:03:07,040 --> 00:03:10,489
big idea is that we think of the the brain

59
00:03:10,560 --> 00:03:12,489
the neocortex is learning this model of the world

60
00:03:13,040 --> 00:03:18,409
but what we learned is actually there's tens of thousands of independent modeling systems going on

61
00:03:18,640 --> 00:03:19,290
and so each

62
00:03:19,680 --> 00:03:21,769
what we call a column in the cortex is about 150

63
00:03:22,159 --> 00:03:24,409
000 of them is a complete modeling system

64
00:03:25,120 --> 00:03:28,250
so it's a collective intelligence in your head in some sense

65
00:03:28,720 --> 00:03:30,250
so the thousand brains theory says

66
00:03:30,260 --> 00:03:30,489
well

67
00:03:30,720 --> 00:03:33,209
where do i have knowledge about you know this coffee cup

68
00:03:33,280 --> 00:03:34,890
where is the model of this cell phone

69
00:03:35,360 --> 00:03:36,330
it's not in one place

70
00:03:36,400 --> 00:03:40,489
it's in thousands of separate models that are complementary and they communicate with each other through voting

71
00:03:41,040 --> 00:03:48,090
hawkins says that the neocortex is the outermost and most recently evolved layer of the mammalian brain

72
00:03:48,480 --> 00:03:50,330
a bit like a wrinkly napkin

73
00:03:51,440 --> 00:03:53,209
which wraps around the old brain

74
00:03:53,599 --> 00:03:56,730
it occupies about 70 percent of the volume of your brain

75
00:03:56,879 --> 00:03:59,450
and it's responsible for all aspects of your intelligence

76
00:04:00,080 --> 00:04:02,170
that is to say your sense of vision

77
00:04:02,959 --> 00:04:03,209
touch

78
00:04:03,920 --> 00:04:04,170
hearing

79
00:04:04,879 --> 00:04:05,129
language

80
00:04:05,360 --> 00:04:07,930
in all its forms and even abstract thinking

81
00:04:08,480 --> 00:04:10,730
such as mathematics and philosophy

82
00:04:11,840 --> 00:04:15,849
the neocortex is surprisingly different to other parts of the brain

83
00:04:16,399 --> 00:04:18,009
it has no visually obvious divisions

84
00:04:18,720 --> 00:04:22,250
the anatomical organization is strikingly similar

85
00:04:23,120 --> 00:04:23,370
nevertheless

86
00:04:24,240 --> 00:04:29,930
different parts of the neocortex still perform different functions like vision or hearing

87
00:04:30,080 --> 00:04:30,490
for example

88
00:04:31,120 --> 00:04:38,970
the complex circuitry of the neocortex looks remarkably alike in visual regions and language regions and even touch regions

89
00:04:39,840 --> 00:04:42,570
it looks similar across species such as rats

90
00:04:43,040 --> 00:04:44,090
cats and humans

91
00:04:44,800 --> 00:04:48,889
the variations between regions are relatively small compared to their similarities

92
00:04:50,080 --> 00:04:50,330
now

93
00:04:50,340 --> 00:04:56,729
hawkins argues that the complexity of the brain is in the content of its connections and its wiring

94
00:04:57,199 --> 00:05:00,490
which is an emergent property of a simple learning algorithm

95
00:05:01,120 --> 00:05:01,449
i mean

96
00:05:01,459 --> 00:05:02,570
look at the openai microscope

97
00:05:03,039 --> 00:05:06,490
their tool for visualizing trained convolutional neural networks

98
00:05:06,639 --> 00:05:07,130
as an example

99
00:05:07,759 --> 00:05:09,530
the learning algorithm is simple

100
00:05:10,080 --> 00:05:14,490
but all of this beautiful complexity emerges as a result of training

101
00:05:15,280 --> 00:05:16,729
even after this type of visualization

102
00:05:17,759 --> 00:05:20,490
it's not really understandable by us humans

103
00:05:21,360 --> 00:05:23,289
it's possible that any successful knowledge

104
00:05:23,440 --> 00:05:29,050
representational substrate for some domains of data may never be understandable by humans

105
00:05:30,639 --> 00:05:34,330
now the neocortex is arguably one of the main organs of intelligence

106
00:05:35,120 --> 00:05:36,570
it gives us sensory perception

107
00:05:37,520 --> 00:05:40,169
motor control and abstract thought

108
00:05:41,039 --> 00:05:46,490
the key attributes it has are its ability to learn continuously learn rapidly

109
00:05:47,280 --> 00:05:49,530
its power efficiency and its flexibility

110
00:05:50,400 --> 00:05:53,770
which is to say its ability to learn diverse and novel tasks

111
00:05:54,639 --> 00:05:57,930
now hawkins argues that the neocortex learns a model of the world

112
00:05:58,319 --> 00:06:02,490
that each cortical column is a complete sensory motor modeling system

113
00:06:02,880 --> 00:06:08,169
he thinks that cortical columns use reference frames to store knowledge and generate behavior

114
00:06:09,199 --> 00:06:13,610
our brains learn about the outside world by processing our sensory inputs and movements

115
00:06:14,240 --> 00:06:21,449
the neocortex anticipates the sensory results of movement when we touch objects or observe the world around us

116
00:06:21,840 --> 00:06:25,530
our brain receives a sensory motor sequence of information

117
00:06:26,479 --> 00:06:29,690
when moving our fingers over familiar objects

118
00:06:29,919 --> 00:06:36,490
we quickly notice discrepancies suggesting we make tactile predictions that are specific to particular objects

119
00:06:37,360 --> 00:06:44,250
hawkins believes that the prediction of sensory stimuli after movements is the fundamental primitive of cognition

120
00:06:45,039 --> 00:06:49,449
the brain does not just memorize the sensory information as this would become quickly intractable

121
00:06:50,160 --> 00:06:50,410
instead

122
00:06:50,560 --> 00:06:56,889
hawkins claims that evolution repurposed the ancient systems that evolved to model spatial relationships

123
00:06:57,360 --> 00:06:58,970
such as grid and place cells

124
00:06:59,520 --> 00:07:04,169
it generalized those systems to model abstract concepts in abstract spaces

125
00:07:05,120 --> 00:07:05,770
in this system

126
00:07:06,000 --> 00:07:12,090
thinking and reasoning becomes a kind of traversal through a complex topology of relationships in the brain

127
00:07:12,639 --> 00:07:15,449
analogous to the traversal of a physical space

128
00:07:16,720 --> 00:07:22,330
the whole cortical sheet is made of collections of primitive columnar units called mini columns

129
00:07:23,120 --> 00:07:27,530
each cortical area is a collection of millions of cortical mini columns

130
00:07:28,240 --> 00:07:33,130
these mini columns are organized in about six layers with specific types of neurons

131
00:07:33,440 --> 00:07:35,210
and connection patterns in each layer

132
00:07:36,080 --> 00:07:38,569
neighboring mini columns have the same receptive field

133
00:07:38,639 --> 00:07:41,770
so they have the same inputs from other cortical areas

134
00:07:42,160 --> 00:07:48,169
and these mini columns form complete fundamental and primitive units known as macro columns

135
00:07:48,639 --> 00:07:50,650
or as hawkins calls them cortical

136
00:07:50,879 --> 00:07:51,129
columns

137
00:07:52,000 --> 00:07:55,849
now pyramidal neurons are the typical excitatory neurons in the neocortex

138
00:07:56,960 --> 00:08:00,009
their name comes from the triangular shape of their cell body

139
00:08:00,319 --> 00:08:08,090
the dendrites of each pyramidal neuron connect to tens of thousands of excitatory synapses equally split from local

140
00:08:08,240 --> 00:08:08,970
and remote sources

141
00:08:09,840 --> 00:08:13,610
the neurons can look significantly different in their connection arrangements

142
00:08:13,919 --> 00:08:15,690
depending on which neocortex layer

143
00:08:15,700 --> 00:08:21,770
they're situated in the thalamus is the main input and output subcortical structure to the neocortex

144
00:08:22,479 --> 00:08:25,770
it could be viewed as the seventh layer of the neocortex

145
00:08:26,720 --> 00:08:30,889
it primarily sends sensory or pre-processed information into layer four of the neocortex

146
00:08:32,000 --> 00:08:32,250
moreover

147
00:08:32,479 --> 00:08:35,929
columns in the neocortex learn to model whatever systems are wired to them

148
00:08:36,320 --> 00:08:37,849
columns wired to the ear

149
00:08:38,000 --> 00:08:38,409
for example

150
00:08:38,799 --> 00:08:39,929
learn to process sound

151
00:08:40,559 --> 00:08:40,809
columns

152
00:08:41,039 --> 00:08:41,849
wired to the retina

153
00:08:42,080 --> 00:08:42,809
rods and cones

154
00:08:42,958 --> 00:08:44,169
learn to process images

155
00:08:44,800 --> 00:08:49,769
the neocortex can retain remarkable plasticity even later into life

156
00:08:50,399 --> 00:08:56,330
perhaps the most striking examples are those of post post-injury plasticity where the brains overcome injury

157
00:08:56,480 --> 00:09:00,970
by rewiring remaining neurons to regain some or even full functionality

158
00:09:02,160 --> 00:09:07,450
the cortical regions are densely interconnected with many feedback and feed-forward skip connections across the hierarchy

159
00:09:08,160 --> 00:09:13,610
and all functional areas in the neocortex are bi-directionally connected with other brain structures

160
00:09:14,080 --> 00:09:18,730
but their main inputs and outputs come from other cortical areas via long-distance connections

161
00:09:19,839 --> 00:09:21,610
all cortical regions are densely connected

162
00:09:21,920 --> 00:09:25,209
but there's some apparent asymmetry and even hierarchy

163
00:09:25,680 --> 00:09:27,850
detectable in the overall structure

164
00:09:28,800 --> 00:09:32,810
sensory areas tend to be lower in the hierarchy than associative or motor areas

165
00:09:32,820 --> 00:09:33,209
for example

166
00:09:33,600 --> 00:09:39,610
and bottom-up information flows are known as feed-forward and top-down connections are known as feedback connections

167
00:09:40,640 --> 00:09:44,170
what should be pretty clear from looking at this structure is that there's no blank slate

168
00:09:45,040 --> 00:09:47,209
there's a very clear cognitive architecture

169
00:09:47,600 --> 00:09:50,089
and much of this high-level structure is there from birth

170
00:09:50,480 --> 00:09:51,610
there are many skip connections

171
00:09:52,320 --> 00:09:53,370
a high level of recurrence

172
00:09:53,680 --> 00:09:58,649
with many feedback loops and the process is highly distributed at this point

173
00:09:58,659 --> 00:10:03,290
i want to draw your attention to a paper published last year by psychologist joseph cesario

174
00:10:04,560 --> 00:10:08,490
it was titled your brain is not a tiny onion with a reptile inside

175
00:10:09,200 --> 00:10:12,649
joseph pointed out several misunderstandings of nervous system evolution

176
00:10:13,600 --> 00:10:16,010
which he says stems from the work of paul mclean

177
00:10:16,640 --> 00:10:19,209
who in the 1940s began to study the brain region

178
00:10:19,360 --> 00:10:20,329
which he called the limbic

179
00:10:20,560 --> 00:10:20,810
system

180
00:10:21,440 --> 00:10:29,209
mclean later proposed that humans possess a so-called triune brain consisting of three large divisions that evolved sequentially

181
00:10:30,079 --> 00:10:30,570
the oldest

182
00:10:31,040 --> 00:10:36,010
the reptilian brain controls basic functions such as movement and breathing

183
00:10:36,880 --> 00:10:37,130
next

184
00:10:37,600 --> 00:10:38,890
the limbic system

185
00:10:39,760 --> 00:10:41,290
which controls emotional responses

186
00:10:42,000 --> 00:10:42,570
and finally

187
00:10:43,200 --> 00:10:44,410
the cerebral cortex

188
00:10:45,040 --> 00:10:46,570
which pretty much means the neocortex

189
00:10:47,279 --> 00:10:48,010
but you know

190
00:10:48,020 --> 00:10:49,930
he thought that that controlled language and reasoning

191
00:10:50,640 --> 00:10:56,570
joseph said that mclean's ideas were already understood to be incorrect by the time he published his 1990 book

192
00:10:56,720 --> 00:10:57,050
i mean

193
00:10:57,060 --> 00:10:57,649
since the 1970s

194
00:10:58,399 --> 00:11:02,329
many in developmental neuroscience thought that the uh

195
00:11:02,800 --> 00:11:04,490
the ideas from mclean were a myth

196
00:11:04,800 --> 00:11:05,930
you know due to its longevity

197
00:11:06,320 --> 00:11:09,209
the triune brain idea has been called by uh

198
00:11:09,219 --> 00:11:09,450
neuroscientist

199
00:11:10,160 --> 00:11:11,209
lisa feldman barrett

200
00:11:11,680 --> 00:11:16,490
as one of the most successful and widespread errors in all of science

201
00:11:17,440 --> 00:11:18,490
joseph pointed out

202
00:11:18,500 --> 00:11:27,769
that the problem with this view is a simple conception of evolution being arranged sequentially from the simplest to the most complex organisms

203
00:11:28,320 --> 00:11:33,769
this view implies that anatomical evolution proceeds in the same fashion as geological strata

204
00:11:34,399 --> 00:11:36,810
with new layers adding on top of existing ones

205
00:11:37,279 --> 00:11:37,529
instead

206
00:11:38,320 --> 00:11:42,010
most evolutionary change consists of transforming existing parts

207
00:11:42,880 --> 00:11:48,250
joseph says that the neocortex is not an evolutionary novelty unique to humans or primates or mammals

208
00:11:48,880 --> 00:11:54,730
all vertebrates possess structures evolutionarily related to our cortex joseph concluded

209
00:11:55,440 --> 00:12:00,410
that these ideas are consistent with traditional views of human nature as rationality

210
00:12:01,200 --> 00:12:02,010
battling emotion

211
00:12:02,560 --> 00:12:02,970
the tripartite

212
00:12:03,680 --> 00:12:03,930
platonic

213
00:12:04,320 --> 00:12:04,570
soul

214
00:12:05,120 --> 00:12:08,889
freudian psychodynamics and religious approaches to humanity

215
00:12:09,760 --> 00:12:13,450
it's also a simple idea that can be distilled and communicated to a lay audience

216
00:12:14,160 --> 00:12:14,410
look

217
00:12:14,420 --> 00:12:19,050
i'm not suggesting for a second that hawkins subscribes to the idea of a tree in brain

218
00:12:19,120 --> 00:12:21,130
but i think it's possible to get this impression

219
00:12:21,279 --> 00:12:22,010
reading his book

220
00:12:22,020 --> 00:12:23,450
which is why i wanted to bring this up

221
00:12:24,000 --> 00:12:27,290
it turns out it's very tricky to get neurons to do this

222
00:12:27,839 --> 00:12:29,050
to build a map of an environment

223
00:12:29,200 --> 00:12:31,610
it's just and so we now know there's this

224
00:12:31,680 --> 00:12:35,050
these famous studies that's still very active about place cells

225
00:12:35,200 --> 00:12:35,850
and grid cells

226
00:12:35,920 --> 00:12:38,010
and other types of cells in the older parts of the brain

227
00:12:38,560 --> 00:12:40,490
and how they build these maps of the world

228
00:12:40,720 --> 00:12:41,529
it's really clever

229
00:12:41,680 --> 00:12:44,970
it's obviously been under a lot of evolutionary pressure over a long period of time

230
00:12:44,980 --> 00:12:45,850
they get good at this

231
00:12:46,560 --> 00:12:48,250
so animals not know where they are

232
00:12:48,800 --> 00:12:49,769
what we think has happened

233
00:12:50,560 --> 00:12:50,810
uh

234
00:12:50,820 --> 00:12:52,329
and there's a lot of evidence to justice

235
00:12:52,800 --> 00:12:59,930
is that that mechanism we learned to map like a space is was repackaged

236
00:13:00,959 --> 00:13:05,209
the same type of neurons was repackaged into a more compact form

237
00:13:06,720 --> 00:13:08,170
and that became the cortical column

238
00:13:09,040 --> 00:13:09,450
in 1972

239
00:13:10,240 --> 00:13:14,329
john o'keefe discovered the first component of an internal gps system in the brain

240
00:13:14,639 --> 00:13:18,010
he discovered place cells in the hippocampus which activate

241
00:13:18,240 --> 00:13:21,769
when a rat is situated in specific locations in a room

242
00:13:22,560 --> 00:13:23,850
more than three decades later

243
00:13:24,160 --> 00:13:24,570
in 2005

244
00:13:25,600 --> 00:13:27,370
mae britt and edward moser

245
00:13:27,839 --> 00:13:30,649
discovered another key component of the brain's positioning system

246
00:13:31,360 --> 00:13:31,610
grid

247
00:13:31,760 --> 00:13:32,010
cells

248
00:13:32,480 --> 00:13:34,170
which represented a coordinate system

249
00:13:34,240 --> 00:13:37,450
which allowed for precise positioning and pathfinding

250
00:13:38,480 --> 00:13:43,130
these discoveries solved the problem that has occupied philosophers and scientists for centuries

251
00:13:43,920 --> 00:13:47,290
how the brain creates a map of the complex world around us

252
00:13:47,519 --> 00:13:49,529
and how we navigate our way through it

253
00:13:50,320 --> 00:13:57,290
what's intriguing about the brain solution is that it seems to create multiple hexagonal grids with slightly different orientations

254
00:13:58,079 --> 00:13:58,570
and scales

255
00:13:59,360 --> 00:14:02,490
because these grids are orientated and scaled differently

256
00:14:03,279 --> 00:14:07,050
each point in space activates a unique combination of grid cells

257
00:14:07,440 --> 00:14:08,010
in other words

258
00:14:08,320 --> 00:14:13,130
the brain encodes locations using bits in a sparse distributed representation

259
00:14:14,240 --> 00:14:18,490
jeff thinks that sparse representations are crucial to the success of the human brain

260
00:14:18,880 --> 00:14:21,290
and should guide our construction of artificial intelligence

261
00:14:22,240 --> 00:14:29,050
what's intriguing about the brain solution is that it seems to create multiple hexagonal grids with slightly different orientations

262
00:14:29,600 --> 00:14:30,089
and scales

263
00:14:31,199 --> 00:14:33,449
because these grids are orientated and scaled differently

264
00:14:34,000 --> 00:14:37,610
each point in space becomes a unique combination of grid cells

265
00:14:38,000 --> 00:14:43,089
in other words the brain encodes these cells using a sparse distributed representation

266
00:14:44,079 --> 00:14:49,610
jeff thinks that such sparse representations are crucial to the success of the human brain

267
00:14:50,000 --> 00:14:53,209
and should guide our construction of artificial intelligence

268
00:14:54,320 --> 00:14:55,529
if you're going to build a model of a house

269
00:14:55,680 --> 00:14:56,170
in a computer

270
00:14:56,720 --> 00:14:57,690
they have a reference name

271
00:14:57,700 --> 00:14:59,769
and you can then reference them like cartesian coordinates

272
00:15:00,079 --> 00:15:01,529
like x y and z axes

273
00:15:02,480 --> 00:15:03,130
so i could say

274
00:15:03,140 --> 00:15:03,290
oh

275
00:15:03,300 --> 00:15:04,329
i'm going to design a house

276
00:15:04,560 --> 00:15:05,050
i can say

277
00:15:05,060 --> 00:15:05,209
well

278
00:15:05,219 --> 00:15:05,370
the

279
00:15:05,440 --> 00:15:06,810
the front door is at this location

280
00:15:07,279 --> 00:15:07,529
xyz

281
00:15:07,920 --> 00:15:09,370
and the roof is at this location

282
00:15:09,600 --> 00:15:09,850
xyz

283
00:15:10,000 --> 00:15:10,490
and so on

284
00:15:10,639 --> 00:15:11,769
that's a type of reference frame

285
00:15:12,720 --> 00:15:14,730
so it turns out for you to make a prediction

286
00:15:14,959 --> 00:15:17,130
and i walk you through the thought experiment in the book

287
00:15:17,140 --> 00:15:19,610
where i was predicting what my finger was going to feel

288
00:15:19,620 --> 00:15:20,649
when i touched the coffee cup

289
00:15:21,120 --> 00:15:22,410
it was a ceramic coffee cup

290
00:15:22,420 --> 00:15:23,050
but this one will do

291
00:15:24,880 --> 00:15:26,649
and what i realized is

292
00:15:26,659 --> 00:15:31,050
that to make a prediction with my finger's going to feel like it's going to feel different than this

293
00:15:31,060 --> 00:15:33,529
which would feel different if i touch the hole or the thing on the bottom

294
00:15:34,320 --> 00:15:34,970
make that prediction

295
00:15:35,519 --> 00:15:37,930
the cortex needs to know where the finger is

296
00:15:37,940 --> 00:15:43,290
the tip of the finger relative to the coffee cup and exactly relative to the coffee cup

297
00:15:44,000 --> 00:15:44,730
and to do that

298
00:15:44,740 --> 00:15:46,170
i have to have a reference frame for the coffee

299
00:15:46,800 --> 00:15:50,009
jeff says that after many years of thinking about the function of the neocortex

300
00:15:50,639 --> 00:15:53,690
he deduced that it must store everything that we know

301
00:15:54,320 --> 00:15:55,130
all of our knowledge

302
00:15:55,680 --> 00:15:55,930
right

303
00:15:56,240 --> 00:15:58,570
using something called a reference frame

304
00:15:59,519 --> 00:16:02,730
but what exactly does jeff mean by a reference frame

305
00:16:03,279 --> 00:16:03,529
well

306
00:16:03,539 --> 00:16:05,209
consider a paper map as an analogy

307
00:16:05,759 --> 00:16:08,009
a map is a type of model

308
00:16:08,720 --> 00:16:08,970
right

309
00:16:09,279 --> 00:16:13,370
so a map of a town is a model of a town and the grid lines

310
00:16:13,440 --> 00:16:14,089
such as the

311
00:16:14,560 --> 00:16:18,329
the latitude and longitude lines are a type of reference frame

312
00:16:18,560 --> 00:16:18,970
a maps

313
00:16:19,040 --> 00:16:21,209
gridlines is its reference frame

314
00:16:21,600 --> 00:16:24,009
so they provide a structure of the map

315
00:16:24,240 --> 00:16:28,810
a reference frame tells you where things are located relative to everything else

316
00:16:28,959 --> 00:16:33,209
and it can tell you how to achieve goals such as how to go from one location to another location

317
00:16:34,160 --> 00:16:34,410
now

318
00:16:34,420 --> 00:16:39,529
jeff realized that the brain's model of the world is built using map-like reference frames

319
00:16:39,920 --> 00:16:43,850
not one reference frame but hundreds of thousands of them

320
00:16:44,079 --> 00:16:48,089
jeff thinks that most of the cells in your neocortex are dedicated to creating

321
00:16:48,399 --> 00:16:52,889
and manipulating reference frames which the brain uses to plan and think

322
00:16:53,680 --> 00:16:57,050
our brain's knowledge representation is a simulation of reality

323
00:16:57,600 --> 00:17:00,649
and this applies in concept space as well as physical space

324
00:17:01,199 --> 00:17:04,809
as was the case with grid and place cells in other parts of your brain

325
00:17:04,959 --> 00:17:09,369
so jeff now thinks that the way we think is analogous to how we navigate spaces

326
00:17:10,000 --> 00:17:10,410
he says

327
00:17:10,480 --> 00:17:15,530
that the similarity of circuitry observed in all cortical regions is strong evidence

328
00:17:15,679 --> 00:17:18,250
that even high-level cognitive tasks are learned

329
00:17:18,720 --> 00:17:20,970
and represented in a location-based framework

330
00:17:21,599 --> 00:17:25,049
to be an expert in any domain requires having a good reference frame

331
00:17:25,359 --> 00:17:26,010
a good map

332
00:17:26,480 --> 00:17:31,130
two people observing the same physical object will likely end up with similar maps

333
00:17:31,520 --> 00:17:31,929
for example

334
00:17:32,080 --> 00:17:37,610
it's hard to imagine how the brains of two people observing the same chair would arrange its features differently

335
00:17:39,039 --> 00:17:45,370
jeff said in his book that being an expert is mostly about finding the reference frame to arrange facts

336
00:17:45,440 --> 00:17:45,929
and observations

337
00:17:46,400 --> 00:17:47,289
i mean albert einstein

338
00:17:47,520 --> 00:17:47,929
for example

339
00:17:48,480 --> 00:17:50,330
started with the same facts as his contemporaries

340
00:17:51,039 --> 00:17:53,929
however he found a better way to arrange them

341
00:17:54,240 --> 00:17:59,450
a better reference frame that permitted him to see analogies and make predictions that were surprising

342
00:18:00,480 --> 00:18:05,850
what's most fascinating about einstein's discoveries relating to special relativity is

343
00:18:05,860 --> 00:18:09,690
that the reference frame he used to make them were everyday objects

344
00:18:09,919 --> 00:18:10,169
right

345
00:18:10,179 --> 00:18:13,090
he thought about trains and people and flashlights

346
00:18:14,080 --> 00:18:19,130
he started with the empirical observations of scientists such as the absolute speed of light

347
00:18:19,360 --> 00:18:23,289
and then used everyday reference frames to deduce the equations of special relativity

348
00:18:24,160 --> 00:18:24,890
because of this

349
00:18:24,960 --> 00:18:28,250
almost anyone can follow his logic and understand how he made the discoveries

350
00:18:29,039 --> 00:18:29,450
in contrast

351
00:18:30,000 --> 00:18:36,010
einstein's general theory of relativity required reference frames based on mathematical concepts called field equations

352
00:18:36,559 --> 00:18:41,130
which are not easily related to everyday objects einstein found this much harder to understand

353
00:18:41,679 --> 00:18:43,370
as does pretty much everyone else

354
00:18:44,320 --> 00:18:48,809
jeff hawkins is one of the ultimate gentlemen scientists of our age

355
00:18:49,679 --> 00:18:51,690
his first major project was palm computing

356
00:18:52,080 --> 00:18:53,450
which is a company that he founded

357
00:18:54,080 --> 00:19:00,650
he invented the palm pilot in the mid-1990s and despite his incredible success in the nascent mobile computing industry

358
00:19:01,280 --> 00:19:02,570
his heart was never in it

359
00:19:02,799 --> 00:19:04,730
his passion was always for theoretical neuroscience

360
00:19:05,520 --> 00:19:09,210
he knew the biggest prize was understanding human intelligence

361
00:19:09,919 --> 00:19:14,169
and then using that knowledge to create human level machine intelligence

362
00:19:15,120 --> 00:19:19,770
so in 2005 he co-founded numenta in redwood city in california

363
00:19:20,640 --> 00:19:20,890
nomenta

364
00:19:21,200 --> 00:19:23,929
is a machine intelligence company that has developed a cohesive

365
00:19:24,400 --> 00:19:24,650
theory

366
00:19:25,200 --> 00:19:26,250
core software technology

367
00:19:26,960 --> 00:19:30,570
and applications based on the principles of the neocortex

368
00:19:31,760 --> 00:19:34,970
its dual mission is to understand how the brain works

369
00:19:35,440 --> 00:19:40,250
and to apply those principles of real intelligence to create intelligent machines

370
00:19:41,200 --> 00:19:44,250
neuroscientists were publishing thousands of papers a year

371
00:19:44,260 --> 00:19:46,570
covering every single detail of the brain

372
00:19:46,960 --> 00:19:51,090
but there was a lack of systemic theories that tied all of those details together

373
00:19:51,840 --> 00:19:55,610
nimenta decided to first focus on understanding a single cortical

374
00:19:55,760 --> 00:19:56,010
column

375
00:19:56,240 --> 00:19:56,490
right

376
00:19:56,640 --> 00:20:02,490
they knew that cortical columns were doing something physically complex and therefore must be doing something complex

377
00:20:03,200 --> 00:20:03,450
now

378
00:20:03,460 --> 00:20:03,850
last week

379
00:20:03,860 --> 00:20:05,370
we had ben gertzel on the show

380
00:20:05,679 --> 00:20:11,210
and he is convinced that artificial general intelligence must be a hybrid of many underlying algorithms

381
00:20:11,840 --> 00:20:13,130
not a single learning algorithm

382
00:20:13,840 --> 00:20:15,850
jeff hawkins doesn't agree

383
00:20:16,480 --> 00:20:22,330
hawkins thinks that all the magic of intelligence could emerge from a single cortical learning algorithm

384
00:20:23,520 --> 00:20:29,210
andrew ng said that as a young professor and after he read hawkins first book on intelligence

385
00:20:29,760 --> 00:20:36,250
he also became convinced that a simple scaled-up learning algorithm could reach artificial general intelligence

386
00:20:37,280 --> 00:20:37,530
now

387
00:20:37,540 --> 00:20:45,370
what does seem to really distinguish hawkins ideas is that intelligence must emerge from diverse and strongly multi-modal inputs

388
00:20:45,919 --> 00:20:50,970
perhaps that intelligence is somehow emerging from the nature of physical embodiment

389
00:20:51,840 --> 00:20:52,090
now

390
00:20:52,159 --> 00:20:57,130
jeff argues that we're the first species on earth to know the age and the size of the universe

391
00:20:57,520 --> 00:21:02,730
he thinks that humans are the first species to be known by their knowledge and not by their genes

392
00:21:03,760 --> 00:21:06,890
that's the beauty of this discovery that this guy

393
00:21:06,960 --> 00:21:07,210
vernon

394
00:21:07,220 --> 00:21:08,570
mount castle made many many years ago

395
00:21:08,580 --> 00:21:10,090
which is that there's

396
00:21:10,320 --> 00:21:13,289
there's a single cortical algorithm underlying everything we're doing

397
00:21:14,000 --> 00:21:15,610
the mindful brain is a small book

398
00:21:16,159 --> 00:21:17,530
it's about 100 pages long

399
00:21:17,540 --> 00:21:23,929
and published in 1978 and it contains two essays about the brain from two prominent scientists

400
00:21:24,880 --> 00:21:26,570
one was written by vernon

401
00:21:26,799 --> 00:21:27,289
mel castle

402
00:21:27,600 --> 00:21:29,610
a neuroscientist at john hopkins university

403
00:21:30,400 --> 00:21:30,650
now

404
00:21:30,660 --> 00:21:31,130
jeff hawkins

405
00:21:31,360 --> 00:21:34,250
cites mount castle as being one of his biggest inspirations

406
00:21:35,440 --> 00:21:40,730
jeff says that it remains one of the most iconic and important essays ever written about the brain

407
00:21:41,600 --> 00:21:45,850
mount castle proposed a new way of thinking about the brain

408
00:21:45,919 --> 00:21:46,570
that is elegant

409
00:21:47,039 --> 00:21:48,409
a hallmark of great theories

410
00:21:49,039 --> 00:21:53,530
but it's also kind of surprising and it continues to polarize the neuroscience community

411
00:21:54,559 --> 00:21:57,610
now mount castle noted that the brain grew really large

412
00:21:57,760 --> 00:22:01,289
by adding new brain parts on top of old brain parts

413
00:22:01,760 --> 00:22:04,490
the older parts control more primitive behaviors

414
00:22:04,720 --> 00:22:07,610
while the newer parts create more sophisticated ones

415
00:22:08,159 --> 00:22:11,530
however mount castle goes on to say that while much of the brain got bigger

416
00:22:11,600 --> 00:22:13,530
by adding new parts on top of old parts

417
00:22:14,000 --> 00:22:17,929
that's not how the neocortex grew to occupy 70 percent of our brain

418
00:22:18,480 --> 00:22:23,450
the neocortex got big by making copies of the same basic thing

419
00:22:23,840 --> 00:22:24,890
the same circuit

420
00:22:25,520 --> 00:22:29,929
he says that every single part of the neocortex is the same basic circuit

421
00:22:30,480 --> 00:22:36,490
different parts of the neocortex are different not in their intrinsic function but rather in what they are connected

422
00:22:36,799 --> 00:22:37,049
to

423
00:22:37,360 --> 00:22:39,130
the implications of this are huge

424
00:22:39,440 --> 00:22:42,330
if we understand how one part of the neocortex works

425
00:22:42,720 --> 00:22:48,970
we understand how it all works and how all aspects of intelligence can emerge from a single cortical

426
00:22:49,280 --> 00:22:49,530
algorithm

427
00:22:50,240 --> 00:22:55,289
mount castle pointed out that the neocortex grew really quickly given the short evolutionary time

428
00:22:55,760 --> 00:23:00,650
now darwin's big idea is that the diversity of life emerged from a single algorithm

429
00:23:01,440 --> 00:23:07,690
similarly mount castle proposed that the diversity of intelligence also emerged from a single basic algorithm

430
00:23:08,240 --> 00:23:10,890
the difference is that darwin knew what the algorithm was

431
00:23:11,280 --> 00:23:13,130
random variation and natural selection

432
00:23:13,840 --> 00:23:16,330
darwin didn't know where the algorithm was in the body

433
00:23:16,720 --> 00:23:18,090
the discovery of dna came

434
00:23:18,100 --> 00:23:18,570
much later

435
00:23:19,120 --> 00:23:21,210
mount castle knew where the algorithm resided

436
00:23:21,840 --> 00:23:23,049
but not what it did

437
00:23:24,080 --> 00:23:27,850
mount castle said that there's about 150 000 cortical columns in the neocortex

438
00:23:28,880 --> 00:23:33,690
a bit like 150 000 little pieces of spaghetti stacked next to each other

439
00:23:34,159 --> 00:23:39,210
scientists knew that these columns existed because they all respond to different sensory inputs

440
00:23:40,080 --> 00:23:43,210
be it from a patch of skin or a signal from the retina

441
00:23:43,600 --> 00:23:46,890
but the columns are wired to different sensory inputs from the body

442
00:23:47,600 --> 00:23:51,370
there's a wonderful anecdote in jeff's book about the last time he met mount castle

443
00:23:52,000 --> 00:23:53,929
jeff gave a speech at john hopkins university

444
00:23:54,320 --> 00:23:57,610
and at the end of the day he met with mount castle and the dean of the department

445
00:23:58,159 --> 00:23:59,610
the time had come for him to leave

446
00:23:59,760 --> 00:24:01,529
and you know jeff had a flight to catch

447
00:24:01,840 --> 00:24:04,570
so they said their goodbyes and the car was waiting for jeff

448
00:24:04,580 --> 00:24:04,809
outside

449
00:24:05,600 --> 00:24:07,049
as jeff walked through the office door

450
00:24:07,120 --> 00:24:08,730
mount castle intercepted him

451
00:24:08,799 --> 00:24:12,970
put his hand on jeff's shoulder and said in here is some advice for you

452
00:24:12,980 --> 00:24:13,770
kind of tone of voice

453
00:24:14,640 --> 00:24:16,330
you should stop talking about hierarchy

454
00:24:17,600 --> 00:24:18,650
it doesn't really exist

455
00:24:19,600 --> 00:24:20,330
jeff was stunned

456
00:24:22,000 --> 00:24:24,809
mount castle was one of the foremost experts on the neocortex

457
00:24:25,279 --> 00:24:30,570
and he was telling jeff that one of its largest and most well-documented features didn't exist

458
00:24:31,360 --> 00:24:32,090
jeff was surprised

459
00:24:32,480 --> 00:24:32,730
right

460
00:24:32,740 --> 00:24:34,890
it was as if francis crick had said to him

461
00:24:35,120 --> 00:24:35,370
oh

462
00:24:35,380 --> 00:24:38,330
that dna molecule doesn't really encode your genes

463
00:24:39,279 --> 00:24:41,289
so jeff didn't know how to respond

464
00:24:41,600 --> 00:24:42,490
he just said nothing

465
00:24:42,799 --> 00:24:44,890
as jeff sat in the car on his way to the airport

466
00:24:45,039 --> 00:24:47,370
he tried to make sense of those parting words

467
00:24:48,320 --> 00:24:48,570
today

468
00:24:49,279 --> 00:24:52,650
jeff's understanding of hierarchy in the neocortex has changed dramatically

469
00:24:53,200 --> 00:24:56,090
it's much less hierarchical than he previously thought

470
00:24:56,720 --> 00:24:59,049
did vernon mount castle know this back then

471
00:24:59,360 --> 00:25:02,730
did he have a theoretical basis for saying that hierarchy didn't really exist

472
00:25:03,360 --> 00:25:06,890
was he thinking about the experimental results that jeff didn't know about

473
00:25:07,279 --> 00:25:08,490
he died in 2015

474
00:25:09,120 --> 00:25:12,409
and jeff will never be able to ask him after his death

475
00:25:12,880 --> 00:25:16,010
jeff took it upon himself to re-read many of his books and papers

476
00:25:16,559 --> 00:25:18,010
his thinking and writing always

477
00:25:18,080 --> 00:25:18,570
very insightful

478
00:25:19,279 --> 00:25:21,289
his 1998 perceptual neuroscience

479
00:25:21,679 --> 00:25:26,730
the cerebral cortex is a physically beautiful book and remains one of jeff's favorites about the brain

480
00:25:27,279 --> 00:25:29,049
when jeff thinks back on that day

481
00:25:29,520 --> 00:25:30,250
he really laments

482
00:25:30,559 --> 00:25:31,610
and he kind of wished

483
00:25:31,760 --> 00:25:37,289
that he would have chanced missing his flight for that last opportunity to talk of mount castle further

484
00:25:37,679 --> 00:25:41,850
even now he wishes he could talk to mount castle about his current ideas

485
00:25:42,000 --> 00:25:45,850
he'd like to believe that mount castle would have enjoyed the thousand brands theory of intelligence

486
00:25:46,480 --> 00:25:47,690
so if you have many brains

487
00:25:49,120 --> 00:25:49,770
who are you

488
00:25:49,780 --> 00:25:50,010
then

489
00:25:50,480 --> 00:25:51,529
so it's interesting

490
00:25:51,539 --> 00:25:52,490
we have a singular perception

491
00:25:52,799 --> 00:25:53,049
right

492
00:25:53,120 --> 00:25:53,450
you know

493
00:25:53,460 --> 00:25:53,770
we think

494
00:25:53,780 --> 00:25:53,929
oh

495
00:25:53,939 --> 00:25:54,650
i'm just here

496
00:25:54,660 --> 00:25:55,370
i'm looking at you

497
00:25:55,520 --> 00:25:56,010
but it's

498
00:25:56,020 --> 00:25:57,210
it's composed of all these things

499
00:25:57,360 --> 00:25:58,890
there's sounds and there's and there's

500
00:25:59,039 --> 00:25:59,289
uh

501
00:25:59,440 --> 00:26:02,490
there's vision and there's touch and all kinds of inputs

502
00:26:02,559 --> 00:26:02,809
yeah

503
00:26:02,819 --> 00:26:05,690
we have the singular perception and what the thousand brain theory says

504
00:26:05,700 --> 00:26:07,289
we have these models that are visual models

505
00:26:07,360 --> 00:26:08,809
we have a lot models of auditory models

506
00:26:08,880 --> 00:26:09,130
models

507
00:26:09,140 --> 00:26:09,929
matters of tactile models

508
00:26:10,000 --> 00:26:10,490
and so on

509
00:26:10,799 --> 00:26:11,450
but they vote

510
00:26:12,320 --> 00:26:12,730
and so

511
00:26:13,039 --> 00:26:13,289
um

512
00:26:13,299 --> 00:26:14,330
they send in the cortex

513
00:26:14,640 --> 00:26:16,809
you can think about these columns as that

514
00:26:16,819 --> 00:26:18,090
like little grains of rice

515
00:26:18,400 --> 00:26:20,409
150 000 stacked next to each other

516
00:26:21,039 --> 00:26:21,289
and

517
00:26:21,520 --> 00:26:21,770
um

518
00:26:21,840 --> 00:26:23,610
each one is its own little modeling system

519
00:26:24,240 --> 00:26:26,570
but they have these long-range connections that go between them

520
00:26:27,279 --> 00:26:30,169
and we call those voting connections or voting neurons

521
00:26:31,039 --> 00:26:31,289
um

522
00:26:31,840 --> 00:26:35,289
and so the different columns try to reach the consensus

523
00:26:35,600 --> 00:26:36,730
like what am i looking at

524
00:26:36,740 --> 00:26:36,890
okay

525
00:26:37,120 --> 00:26:37,450
you know

526
00:26:37,600 --> 00:26:38,650
each one has some ambiguity

527
00:26:38,960 --> 00:26:39,929
but they come to a consensus

528
00:26:40,159 --> 00:26:40,409
oh

529
00:26:40,419 --> 00:26:41,049
there's a water bottle

530
00:26:41,059 --> 00:26:42,809
i'm looking at um

531
00:26:43,279 --> 00:26:46,809
we are only consciously able to perceive the voting today

532
00:26:46,960 --> 00:26:50,730
the most common way of thinking about the neocortex is a bit like a flow chart

533
00:26:50,799 --> 00:26:51,049
right

534
00:26:51,120 --> 00:26:54,890
information from the senses is just processed sequentially step

535
00:26:54,960 --> 00:26:57,529
by step as it passes from one region to the next

536
00:26:58,000 --> 00:26:58,730
in this notion

537
00:26:59,440 --> 00:27:04,409
every step of neural processing refines a representation from the low level to the high level

538
00:27:04,840 --> 00:27:05,090
incrementally

539
00:27:06,000 --> 00:27:08,890
scientists refer to this as a hierarchy of feature detectors

540
00:27:09,440 --> 00:27:10,409
but as jeff points

541
00:27:10,480 --> 00:27:10,730
out

542
00:27:10,799 --> 00:27:16,169
even basic study of how the brain works will tell you that cognition is a interactive process

543
00:27:16,480 --> 00:27:16,730
right

544
00:27:16,740 --> 00:27:17,690
depending on movement

545
00:27:19,520 --> 00:27:19,929
for example

546
00:27:20,080 --> 00:27:21,529
to learn what a new object looks like

547
00:27:21,539 --> 00:27:22,409
we hold it in our hand

548
00:27:22,480 --> 00:27:26,890
and we rotate it this way and that way and we see what it looks like from different angles

549
00:27:27,600 --> 00:27:28,330
and once learned

550
00:27:28,399 --> 00:27:32,169
we're able to recognize entire objects from the touch of a single finger

551
00:27:32,880 --> 00:27:35,850
or a fleeting glimpse of a small part of the object

552
00:27:36,720 --> 00:27:39,210
jeff's proposal of reference frames in cortical columns

553
00:27:39,440 --> 00:27:43,210
suggests a different way of thinking about how the neocortex works

554
00:27:43,679 --> 00:27:46,169
thinking of cortical columns as cognitive primitives

555
00:27:46,799 --> 00:27:52,010
even in low-level sensory regions that are capable of learning and recognizing complete objects

556
00:27:53,039 --> 00:27:58,250
jeff's theory explains how a mouse with a mostly one-level visual system can see

557
00:27:58,320 --> 00:28:00,010
and recognize objects in the world

558
00:28:00,799 --> 00:28:02,730
but where is the knowledge stored in the brain

559
00:28:03,279 --> 00:28:06,970
jeff thinks that our knowledge of objects are distributed over many cortical

560
00:28:07,200 --> 00:28:07,450
columns

561
00:28:08,320 --> 00:28:14,409
so when i pick up a pen there isn't a single model of this pen but rather thousands

562
00:28:15,360 --> 00:28:16,409
i have visual models

563
00:28:16,960 --> 00:28:18,730
i have sensory models

564
00:28:19,200 --> 00:28:22,409
i have auditory models and everything created in between

565
00:28:22,640 --> 00:28:25,370
right from a rich topology of reference frames

566
00:28:25,840 --> 00:28:29,370
binding them all together and every cortical column models

567
00:28:29,600 --> 00:28:33,610
hundreds if not thousands of complete objects at multiple scales

568
00:28:34,080 --> 00:28:40,490
the long-range connections between the columns and the regions of the neocortex communicate at the level of classified objects

569
00:28:41,120 --> 00:28:41,610
not features

570
00:28:42,480 --> 00:28:46,970
jeff thinks that his theory solves the age-old binding problem in artificial intelligence

571
00:28:47,279 --> 00:28:51,450
which is the challenge of mapping sensory input to discrete mental categories

572
00:28:52,080 --> 00:28:55,770
and how these discrete categories can be combined into a single lived experience

573
00:28:56,720 --> 00:28:57,210
jeff thinks

574
00:28:57,220 --> 00:29:00,570
that the binding problem is a side effect of a flawed assumption

575
00:29:00,880 --> 00:29:04,490
that the connection topology of the brain is convergent rather than divergent

576
00:29:05,440 --> 00:29:08,490
the solution to the binding problem is that your cortical columns vote

577
00:29:08,799 --> 00:29:10,010
your perception is the consensus

578
00:29:10,480 --> 00:29:11,690
which is reached from the columns

579
00:29:11,919 --> 00:29:12,970
voting on what they recognize

580
00:29:13,760 --> 00:29:16,090
the voting works across sensory modalities

581
00:29:16,720 --> 00:29:18,490
when you grasp an object in your hand

582
00:29:18,799 --> 00:29:23,529
jeff believes that the tactile columns representing your fingers share another piece of information

583
00:29:24,159 --> 00:29:25,929
their relative position to each other

584
00:29:26,159 --> 00:29:28,490
which makes it even easier to figure out what they're touching

585
00:29:29,120 --> 00:29:31,049
the brain wants to reach a consensus

586
00:29:32,399 --> 00:29:37,370
now in jeff's book he shows an example of an image which can appear as either a vars

587
00:29:37,679 --> 00:29:38,409
or two faces

588
00:29:39,360 --> 00:29:40,570
in examples like this

589
00:29:40,580 --> 00:29:44,169
the columns can't decide which is the correct object

590
00:29:44,320 --> 00:29:45,049
because it's ambiguous

591
00:29:45,440 --> 00:29:48,330
as if it's as if they have two maps for two different towns

592
00:29:48,480 --> 00:29:51,130
but the maps at least in some areas are identical

593
00:29:52,080 --> 00:29:52,330
vars

594
00:29:52,480 --> 00:29:54,169
town and faces town

595
00:29:54,480 --> 00:29:55,370
they're just too similar

596
00:29:55,919 --> 00:29:57,770
so the voting layer wants to reach a consensus

597
00:29:58,399 --> 00:30:01,370
but it doesn't permit two objects to be the same simultaneously

598
00:30:02,240 --> 00:30:04,330
so you have to pick one possibility

599
00:30:05,279 --> 00:30:07,610
you can perceive faces or of ours

600
00:30:07,760 --> 00:30:09,130
but not both at the same time

601
00:30:09,440 --> 00:30:15,450
and the process of cognition allows us to move between the alternatives to reason interactively over time

602
00:30:15,840 --> 00:30:16,409
in his book

603
00:30:16,480 --> 00:30:20,890
jeff makes the powerful argument that thinking is simply traversing a topology of reference

604
00:30:21,120 --> 00:30:22,649
and displacement frames in your brain

605
00:30:23,440 --> 00:30:29,049
jeff thinks that this reasoning is movement evolved to extend the physical world of spaces

606
00:30:29,279 --> 00:30:32,490
and time to our worlds of abstract thought

607
00:30:33,039 --> 00:30:35,450
the succession of thoughts that we experience

608
00:30:36,000 --> 00:30:39,610
when thinking is analogous to the succession of sensations we experience

609
00:30:40,240 --> 00:30:43,770
when moving our finger over an object or walking around a town

610
00:30:44,480 --> 00:30:47,289
perhaps the reason why albert einstein was so smart was

611
00:30:47,299 --> 00:30:51,610
because of the unique topology of reference frames in his brain his information architecture

612
00:30:52,080 --> 00:30:52,570
if you will

613
00:30:52,799 --> 00:30:56,970
must have been arranged as a function of his life experiences as well as his biology

614
00:30:57,600 --> 00:30:58,570
traversing his brain

615
00:30:58,640 --> 00:31:04,409
topology allowed him to make powerful abstract inferences that other people couldn't make for him

616
00:31:04,419 --> 00:31:09,049
it must have felt a bit like the borg traversing their wormhole network in the delta quadrant

617
00:31:09,200 --> 00:31:11,929
for those of you who are fans of star trek voyager

618
00:31:13,039 --> 00:31:20,090
jeff says that learning conceptual knowledge can be difficult if i give you 10 historical events related to democracy

619
00:31:20,880 --> 00:31:22,490
how should you arrange them in your brain

620
00:31:22,960 --> 00:31:25,450
one teacher might show you the events arranged on a timeline

621
00:31:26,799 --> 00:31:27,210
you know

622
00:31:27,220 --> 00:31:28,649
in a one-dimensional reference frame

623
00:31:28,960 --> 00:31:34,010
it's useful for assigning the temporal order of the events and which events might be causally related

624
00:31:34,240 --> 00:31:35,049
by temporal proximity

625
00:31:35,679 --> 00:31:40,809
but another teacher might arrange the same historical events geographically on a map of the world

626
00:31:41,279 --> 00:31:44,890
timelines and geography are both valid ways of organizing historical events

627
00:31:45,360 --> 00:31:48,250
yet they lead to different ways of thinking about history

628
00:31:48,799 --> 00:31:51,289
they might lead to different conclusions and different predictions

629
00:31:52,159 --> 00:31:53,929
the best structure for learning about democracy

630
00:31:54,640 --> 00:31:56,970
might even require an entirely different map

631
00:31:57,200 --> 00:32:01,130
a map with multiple abstract dimensions that correspond to fairness or rights

632
00:32:01,279 --> 00:32:01,690
for example

633
00:32:02,399 --> 00:32:05,929
so what does the thousand brains theory tell us about machine intelligence

634
00:32:06,640 --> 00:32:09,049
intelligent machines need to learn a model of the world

635
00:32:09,679 --> 00:32:09,929
inference

636
00:32:10,480 --> 00:32:10,730
prediction

637
00:32:11,279 --> 00:32:14,250
planning and motor behavior are all based on this model

638
00:32:14,640 --> 00:32:16,890
the model is distributed among many

639
00:32:17,120 --> 00:32:19,850
nearly identical units that vote to reach consensus

640
00:32:20,720 --> 00:32:22,169
this gives us robust prediction

641
00:32:22,720 --> 00:32:26,570
it scales well and it works with any kind of sensor array and modality

642
00:32:27,279 --> 00:32:30,250
and voting solves the binding problem in each unit

643
00:32:30,480 --> 00:32:34,890
knowledge is stored in a reference frame and is learned via sensory motor interaction

644
00:32:35,440 --> 00:32:40,250
this means that we can learn unsupervised fast and the motor behavior is integrated

645
00:32:41,120 --> 00:32:42,890
this is matthew taylor from numenta

646
00:32:43,600 --> 00:32:44,730
this place looks really familiar

647
00:32:44,960 --> 00:32:47,049
but i can't remember how i got here

648
00:32:47,519 --> 00:32:51,610
it's almost like someone severed all of the distal connections between the pyramidal neurons and my neocortex

649
00:32:52,720 --> 00:32:53,450
many years ago

650
00:32:53,460 --> 00:32:57,289
nomenta used to refer to their overarching theory as htm theory

651
00:32:57,600 --> 00:33:00,570
but now they use the terminology thousand brains

652
00:33:01,120 --> 00:33:01,610
the htm

653
00:33:02,000 --> 00:33:08,169
or hierarchical temporal memory algorithm was a particular implementation of the early ideas of the thousand brains theory

654
00:33:08,720 --> 00:33:14,169
the original guiding principles of htm were that it was a sequence memory algorithm

655
00:33:14,720 --> 00:33:14,970
numenta

656
00:33:15,200 --> 00:33:18,890
thinks that every neuron in your brain is learning a pattern of sequences

657
00:33:19,760 --> 00:33:21,769
it's needed to support continual learning

658
00:33:22,000 --> 00:33:22,409
and critically

659
00:33:22,720 --> 00:33:24,570
it wasn't an artificial neural network

660
00:33:24,720 --> 00:33:26,809
which they argued were not biologically inspired

661
00:33:27,200 --> 00:33:28,890
or at least not in their popular configuration

662
00:33:29,360 --> 00:33:29,850
at the time

663
00:33:30,320 --> 00:33:34,409
the core data structure of the algorithm was called an sdr or a sparse distributed representation

664
00:33:35,279 --> 00:33:36,809
this was a large bit

665
00:33:36,819 --> 00:33:37,049
mask

666
00:33:37,279 --> 00:33:40,250
think of it as a large ordered collection of ones and zeros

667
00:33:40,880 --> 00:33:42,250
the representation is sparse

668
00:33:42,480 --> 00:33:48,010
which means that it typically only contains about one percent of ones instead of zeros

669
00:33:48,640 --> 00:33:53,210
and the values represented the state of neurons in different regions of your neocortex

670
00:33:54,240 --> 00:33:58,010
so nomenta really leans into this idea of sparsity in the brain

671
00:33:58,159 --> 00:34:00,890
and its necessity to build any intelligent system

672
00:34:01,279 --> 00:34:04,250
the reason why sparsity is so powerful is that there are factorially

673
00:34:04,880 --> 00:34:07,049
many permutations of values

674
00:34:07,360 --> 00:34:07,690
i mean

675
00:34:07,700 --> 00:34:08,010
for example

676
00:34:08,480 --> 00:34:11,609
there are about 175 million values

677
00:34:12,079 --> 00:34:15,409
if you had four on bits in an sdr of length 256

678
00:34:16,239 --> 00:34:16,489
right

679
00:34:16,500 --> 00:34:18,810
because it's 256 choose 4.

680
00:34:19,119 --> 00:34:22,810
so this means that the possibility of getting false positives is negligibly small

681
00:34:23,359 --> 00:34:24,489
it's also space efficient

682
00:34:24,639 --> 00:34:30,810
because those four bits which could represent 175 million things could be stored in a 32-bit array

683
00:34:31,040 --> 00:34:31,290
right

684
00:34:31,300 --> 00:34:32,649
which is four times eight bits

685
00:34:33,440 --> 00:34:38,889
the notion of similarity between these sdrs is their intersection or their hamming distance

686
00:34:39,199 --> 00:34:39,609
and again

687
00:34:39,679 --> 00:34:43,449
the really clever thing is just how robust these representations are to noise

688
00:34:43,918 --> 00:34:47,770
you could add about 33 percent of random noise to both of the sdrs

689
00:34:48,399 --> 00:34:50,250
and it would barely affect the overlap metric

690
00:34:50,879 --> 00:34:52,810
you can also union the sdrs together

691
00:34:52,960 --> 00:35:00,650
and not much information about the patterns would be lost in the mix so the htm algorithm needed encoders to take any data structure

692
00:35:01,040 --> 00:35:03,369
and represent it as a sparse distributed representation

693
00:35:04,800 --> 00:35:06,410
encoding the information into an sdr

694
00:35:06,800 --> 00:35:08,970
is an important consideration for htm

695
00:35:09,680 --> 00:35:11,690
much as it is with any other machine learning model

696
00:35:11,760 --> 00:35:13,050
but i was really excited

697
00:35:13,599 --> 00:35:14,730
you know when i learned about htm

698
00:35:15,040 --> 00:35:16,250
because it seems so audacious

699
00:35:17,200 --> 00:35:24,650
and each column has a connection to this input space and it has a receptive field

700
00:35:25,040 --> 00:35:32,089
so each column is connected to different bits in the input space and these are proximal

701
00:35:32,640 --> 00:35:32,890
dendritic

702
00:35:33,440 --> 00:35:38,250
connections feed forward input into the system

703
00:35:39,040 --> 00:35:44,329
and each one of the cells within the column shares that receptive field through its proximal

704
00:35:44,560 --> 00:35:44,810
connection

705
00:35:45,839 --> 00:35:50,170
we also have these other connections between cells within the structure

706
00:35:50,400 --> 00:35:51,450
and here's a third cell

707
00:35:51,520 --> 00:35:53,690
with another four synapses on its segment

708
00:35:54,320 --> 00:35:56,170
these are distal connections

709
00:35:57,440 --> 00:36:00,650
so the cell body or the soma has got

710
00:36:00,800 --> 00:36:01,050
uh

711
00:36:01,359 --> 00:36:02,650
different areas of receptivity

712
00:36:04,000 --> 00:36:07,609
the feed forward proximal input comes from below

713
00:36:08,400 --> 00:36:14,650
and the contextual information or the distal connections come laterally from other cells within the structure

714
00:36:15,040 --> 00:36:18,490
we're comparing a biological neuron to the htm

715
00:36:19,040 --> 00:36:19,450
you know

716
00:36:19,599 --> 00:36:21,690
neuron in software that we're creating

717
00:36:22,320 --> 00:36:23,609
we have the feed forward input

718
00:36:23,680 --> 00:36:28,410
which is the proximal dendritic input from the input space in in both sides

719
00:36:28,880 --> 00:36:30,329
and then the distal input

720
00:36:30,560 --> 00:36:34,329
from lateral connections to other cells within the space for context

721
00:36:35,119 --> 00:36:38,410
now this htm neuron is showing that there's feed forward input

722
00:36:38,420 --> 00:36:42,570
but it's also showing that it can have one or many distal connections

723
00:36:42,800 --> 00:36:44,010
these are distal segments

724
00:36:44,800 --> 00:36:48,089
each one of these segments could potentially have one

725
00:36:48,099 --> 00:36:51,930
or many synapses or connections to other cells within the htm structure

726
00:36:52,480 --> 00:36:54,730
each one of those cells may be in an on and off state

727
00:36:55,119 --> 00:36:56,010
so at any time

728
00:36:56,320 --> 00:36:59,930
if a cell wants to decide whether it's going to go into a predictive state or not

729
00:37:00,320 --> 00:37:01,770
it can look at all of its segments

730
00:37:02,720 --> 00:37:04,970
and its connections across all of their synapses

731
00:37:05,359 --> 00:37:10,490
and if any one of those summed across all the synapses breach some threshold

732
00:37:10,880 --> 00:37:11,530
which is configurable

733
00:37:12,320 --> 00:37:16,089
then that cell goes into a predictive state based upon its connections

734
00:37:16,800 --> 00:37:21,530
its contextual connections to the other cells within the structure

735
00:37:22,160 --> 00:37:25,530
so what i'm doing is i'm feeding in a four note sequence

736
00:37:25,920 --> 00:37:28,410
and then resetting and restarting the sequence over

737
00:37:28,560 --> 00:37:31,930
so every time it sees f-sharp the first note in the sequence

738
00:37:32,320 --> 00:37:33,290
it's seeing it for

739
00:37:33,359 --> 00:37:34,089
without any context

740
00:37:35,359 --> 00:37:36,650
nothing came before it

741
00:37:37,119 --> 00:37:41,450
so the algorithm goes and looks into every cell in in every active column

742
00:37:41,680 --> 00:37:43,770
only cells within active columns become activated

743
00:37:44,400 --> 00:37:50,650
because these activations are completely driven by the proximal segments to the input space

744
00:37:51,760 --> 00:37:53,290
there's only one segment on the cell

745
00:37:53,300 --> 00:37:54,089
because it's one color

746
00:37:54,099 --> 00:37:55,369
it's magenta color

747
00:37:55,920 --> 00:38:00,570
and all of the cells that it's connected to are active in the current time step

748
00:38:00,800 --> 00:38:01,690
that's why it's predictive

749
00:38:02,079 --> 00:38:04,089
because it looked at its segment and it looked

750
00:38:04,099 --> 00:38:06,730
it summed up all the synapses and they were all one apparently

751
00:38:07,280 --> 00:38:10,010
and that breached its threshold to become predictive

752
00:38:10,480 --> 00:38:11,930
so it is in a predictive state

753
00:38:12,800 --> 00:38:15,849
i've never seen another machine learning algorithm quite like it

754
00:38:15,859 --> 00:38:16,089
you know

755
00:38:16,099 --> 00:38:18,810
almost all of them are continuous rather than discrete

756
00:38:19,440 --> 00:38:21,210
and even the discrete ones like decision

757
00:38:21,359 --> 00:38:23,849
trees they still assume an ordinal value on the dimensions

758
00:38:24,880 --> 00:38:26,890
hdm takes it one step further

759
00:38:27,280 --> 00:38:30,170
its representations are also distributed over the features

760
00:38:30,720 --> 00:38:37,530
the only encoding rules of sdrs were that semantically similar data should have a significant overlap on the sdrs

761
00:38:38,480 --> 00:38:42,570
the encoder should be deterministic and the output should have a fixed dimensionality

762
00:38:44,000 --> 00:38:46,730
and the sparsity level should be similar across the input domain

763
00:38:47,359 --> 00:38:52,730
now the thing that was missing to some extent from htm was the notion of representation learning

764
00:38:52,880 --> 00:38:54,010
like we have in neural networks

765
00:38:54,640 --> 00:38:56,410
you have to do it all yourself in the encoder

766
00:38:56,560 --> 00:39:00,490
which is probably the main reason why htm never took off for unstructured data problems

767
00:39:01,119 --> 00:39:07,609
the reason why cnn's dominated computer vision was because it learned representations that were better than any hand-crafted representations

768
00:39:08,640 --> 00:39:09,690
the locality prior

769
00:39:09,839 --> 00:39:14,010
and the weight sharing and the stochastic gradient descent made it computationally tractable

770
00:39:14,960 --> 00:39:18,570
the htm neuron was inspired by pyramidal neurons in the neocortex

771
00:39:19,760 --> 00:39:24,329
a neuron is receiving sdrs from distilled apical dendrites higher up in the hierarchy

772
00:39:24,960 --> 00:39:27,609
and from basal dendrites from the same region of the hierarchy

773
00:39:28,240 --> 00:39:31,530
and also from proximal dendrites from lower level of the hierarchy

774
00:39:32,160 --> 00:39:33,369
or some sensory input

775
00:39:33,680 --> 00:39:36,490
which represented the classical receptive field of a neuron

776
00:39:37,200 --> 00:39:42,010
now all of these neurons are receiving a stream of sdrs and they're figuring out

777
00:39:42,079 --> 00:39:44,890
when to fire and turn on their on bit

778
00:39:44,960 --> 00:39:47,050
or you know when they should go into a predictive state

779
00:39:47,359 --> 00:39:49,210
which can tell other neurons when to fire

780
00:39:50,000 --> 00:39:51,130
right from the very beginning

781
00:39:51,680 --> 00:39:55,290
numenta knew that real neurons are not simple point neurons

782
00:39:55,760 --> 00:40:02,650
the synapses on active dendrites detect dozens of sparse contextual patterns and can learn complex temporal sequences

783
00:40:03,680 --> 00:40:05,930
active dendrites enable flexible context

784
00:40:06,240 --> 00:40:07,849
integration in the layers of neurons

785
00:40:08,400 --> 00:40:10,890
there are two primary phases of the temporal memory algorithm

786
00:40:11,520 --> 00:40:17,369
the first is to identify which cells within active columns will become active on this time step

787
00:40:17,839 --> 00:40:18,650
the second phase

788
00:40:18,880 --> 00:40:24,250
once those activations have been identified is to choose a set of cells to put into a predictive

789
00:40:24,560 --> 00:40:24,810
state

790
00:40:25,359 --> 00:40:29,609
this means that these cells will be primed to fire on the next time step

791
00:40:30,000 --> 00:40:32,970
htm implemented dendrite branch specific plasticity

792
00:40:33,359 --> 00:40:35,369
so if a cell becomes active and there's a prediction

793
00:40:35,839 --> 00:40:37,770
it reinforces that dendritic segment

794
00:40:38,160 --> 00:40:39,130
if there is no prediction

795
00:40:39,440 --> 00:40:43,210
it grows the connections by sub-sampling the previously active cells

796
00:40:43,839 --> 00:40:46,329
and if the cell is not active and there was a prediction

797
00:40:46,880 --> 00:40:48,650
it weakens the dendritic segments

798
00:40:49,440 --> 00:40:54,970
now i learned all about the htm algorithm from watching the htm school series of videos from matthew taylor

799
00:40:55,280 --> 00:40:56,089
who worked at nomenta

800
00:40:56,720 --> 00:41:00,970
matt tragically passed away last year and i wanted to personally pay tribute to him here

801
00:41:01,200 --> 00:41:06,890
his passion and enthusiasm was infectious and he'll be greatly missed by the entire machine learning community

802
00:41:12,079 --> 00:41:12,329
well

803
00:41:12,560 --> 00:41:18,010
sparsity is something that doesn't run really well on existing hardware

804
00:41:18,240 --> 00:41:19,369
it doesn't really run really well

805
00:41:19,440 --> 00:41:19,690
um

806
00:41:19,920 --> 00:41:20,890
on gpus

807
00:41:21,760 --> 00:41:22,010
um

808
00:41:22,560 --> 00:41:23,369
and on cpus

809
00:41:24,240 --> 00:41:27,210
and so that would be a way of sort of bringing more

810
00:41:27,220 --> 00:41:31,770
and more brain principles into the existing system on a commercially valuable basis

811
00:41:32,240 --> 00:41:37,369
there's a large body of work on training dense networks to yield sparse networks for inference

812
00:41:37,839 --> 00:41:40,170
but this limits the size of the largest trainable

813
00:41:40,400 --> 00:41:43,530
sparse model to that of the largest trainable dense model

814
00:41:44,319 --> 00:41:45,530
this was the case actually

815
00:41:45,760 --> 00:41:47,089
until relatively recently

816
00:41:47,920 --> 00:41:48,170
now

817
00:41:48,180 --> 00:41:48,810
since the 1980s

818
00:41:49,599 --> 00:41:56,650
we've known that it's possible to eliminate a significant number of parameters from the neural network without affecting accuracy

819
00:41:56,960 --> 00:41:57,770
or inference time

820
00:41:58,319 --> 00:42:02,170
pruning can substantially reduce the computational demands of inference

821
00:42:02,240 --> 00:42:04,569
when the appropriate hardware is utilized to do so

822
00:42:05,200 --> 00:42:07,130
when the goal is to reduce inference costs

823
00:42:07,520 --> 00:42:09,369
pruning often occurs late in training

824
00:42:09,920 --> 00:42:10,170
now

825
00:42:10,180 --> 00:42:10,569
in 1995

826
00:42:11,920 --> 00:42:17,369
researchers discovered that retrospectively pruning low magnitude connections worked impressively well

827
00:42:18,079 --> 00:42:25,450
later researchers found that retraining the prune connections produced even better results or even better

828
00:42:25,599 --> 00:42:29,849
still rinsing repeating the process with multiple rounds of pruning and retraining

829
00:42:31,200 --> 00:42:34,329
other approaches explored adding connections back in at random

830
00:42:34,640 --> 00:42:36,890
or even focusing on non-uniform sparsity

831
00:42:37,200 --> 00:42:41,690
which is to say adding the connections in where they are most needed in the network

832
00:42:42,160 --> 00:42:42,890
jonathan frankl

833
00:42:43,200 --> 00:42:44,569
who was on the show last year

834
00:42:44,579 --> 00:42:45,050
by the way

835
00:42:45,440 --> 00:42:47,930
released his lottery ticket hypothesis in 2019

836
00:42:48,960 --> 00:42:53,770
which demonstrated that if we can find a sparse neural network with iterative magnitude pruning

837
00:42:54,160 --> 00:42:58,329
then we can train that sparse network from scratch to the same level of accuracy

838
00:42:59,200 --> 00:42:59,450
however

839
00:43:00,160 --> 00:43:02,089
as the demands of training have exploded

840
00:43:03,040 --> 00:43:09,450
researchers have begun to investigate the possibility that networks can be pruned early in training or even before training

841
00:43:10,000 --> 00:43:13,849
the benefit of doing so could reduce the cost of training existing models

842
00:43:14,160 --> 00:43:18,490
and make it possible to continue exploring the phenomena that emerge at larger scales

843
00:43:19,599 --> 00:43:19,849
recently

844
00:43:20,160 --> 00:43:23,609
several methods have been proposed specifically for pruning at initialization

845
00:43:24,720 --> 00:43:24,970
snip

846
00:43:25,440 --> 00:43:28,010
aims to prune weights that are least salient for the loss

847
00:43:28,560 --> 00:43:28,810
grasp

848
00:43:29,359 --> 00:43:35,050
aims to prune weights that most harm or least benefit gradient flow and sin flow

849
00:43:35,119 --> 00:43:36,410
which janet made a video about

850
00:43:36,480 --> 00:43:36,970
by the way

851
00:43:37,119 --> 00:43:44,650
aims to iteratively prune weights with the lowest synaptic strengths in a data independent manner with the goal of avoiding layer collapse

852
00:43:44,880 --> 00:43:47,369
where pruning concentrates on certain layers

853
00:43:48,000 --> 00:43:48,250
now

854
00:43:48,260 --> 00:43:50,810
um frankel pointed out in his recent summary paper

855
00:43:51,119 --> 00:43:55,930
that magnitude pruning after training outperforms all of these pre-initialization methods

856
00:43:56,560 --> 00:43:58,810
most of these methods effectively prune the layers

857
00:43:59,359 --> 00:44:00,010
not the weights

858
00:44:00,079 --> 00:44:02,010
which to say you can perform similarly well

859
00:44:02,020 --> 00:44:05,609
even if you randomly shuffle the weights that they prune in each layer

860
00:44:06,079 --> 00:44:12,730
now interestingly sin flow and magnitude pruning work quite well at initialization time without seeing any data

861
00:44:13,599 --> 00:44:19,690
now frankel didn't identify a single cause for why these methods struggled to prune in a specific fashion

862
00:44:19,839 --> 00:44:20,890
initialization time

863
00:44:21,200 --> 00:44:23,770
and thought that this is an important question for future investigation

864
00:44:24,800 --> 00:44:29,770
perhaps there are properties of optimization that make pruning specific weights difficult or impossible

865
00:44:30,160 --> 00:44:33,849
initialization perhaps because the training occurs in multiple phases

866
00:44:34,800 --> 00:44:43,290
now combining gradient descent training with an optimal sparse topology can lead to state-of-the-art results with smaller networks in the brain

867
00:44:43,839 --> 00:44:48,890
numenta argues that sparsity is key for how information is stored and processed

868
00:44:49,520 --> 00:44:54,089
they also believe it to be one of the most important missing ingredients in modern deep learning

869
00:44:54,800 --> 00:44:58,650
we reached out to the mentor after the show and they're vps of machine learning

870
00:44:58,720 --> 00:45:00,170
architecture and research and engineering

871
00:45:00,400 --> 00:45:02,329
so lawrence bracklund and subtite

872
00:45:02,560 --> 00:45:04,810
ahmed told us that at a high level

873
00:45:05,359 --> 00:45:06,650
the biggest difference is

874
00:45:06,660 --> 00:45:10,730
that they view sparse networks as a unique stand-alone class of artificial neural networks

875
00:45:11,359 --> 00:45:16,890
that mirror the sparsity exhibited in the brain versus being a derivative of dense networks created

876
00:45:17,119 --> 00:45:17,530
by pruning

877
00:45:17,839 --> 00:45:23,690
so not really removing redundant connections but creating networks that are designed to be sparse

878
00:45:24,560 --> 00:45:24,970
in 2019

879
00:45:25,680 --> 00:45:31,690
the mentor released a paper called sparsity enables 50 times performance acceleration in deep learning networks

880
00:45:32,400 --> 00:45:32,970
in that paper

881
00:45:33,280 --> 00:45:37,609
they pointed to the scaling challenges faced by the current state-of-the-art neural networks

882
00:45:38,240 --> 00:45:40,250
they said that the brain is highly efficient

883
00:45:40,880 --> 00:45:41,130
right

884
00:45:41,200 --> 00:45:43,690
requiring a mere 20 watts to operate

885
00:45:43,920 --> 00:45:45,609
which is less power than a light bulb

886
00:45:46,079 --> 00:45:49,930
contrast that to gpt3 which costs millions of dollars to train

887
00:45:50,640 --> 00:45:50,890
numenta

888
00:45:51,200 --> 00:45:54,250
believe that by studying the brain and understanding what makes it so efficient

889
00:45:54,720 --> 00:45:58,010
they can create new algorithms that approach the efficiency of the brain

890
00:45:58,560 --> 00:46:02,569
they think that the core reason the brain is so efficient is the notion of sparsity

891
00:46:03,280 --> 00:46:04,650
a sparse network is one

892
00:46:04,660 --> 00:46:08,650
where all of the neurons are not densely connected to every other in the same cortical

893
00:46:08,880 --> 00:46:09,130
area

894
00:46:09,839 --> 00:46:13,210
the brain stores and processes information as sparse representations

895
00:46:13,839 --> 00:46:14,170
you know

896
00:46:14,180 --> 00:46:15,050
at any given time

897
00:46:15,119 --> 00:46:18,250
only a small percentage of the neurons in the brain are active

898
00:46:18,720 --> 00:46:18,970
disparity

899
00:46:19,520 --> 00:46:20,010
may vary

900
00:46:20,319 --> 00:46:20,730
you know

901
00:46:20,740 --> 00:46:23,849
from less than one percent to a few percent of neurons being active

902
00:46:24,160 --> 00:46:25,369
but it's always sparse

903
00:46:26,160 --> 00:46:28,890
sparsity will lead to a massively smaller memory footprint

904
00:46:29,119 --> 00:46:31,450
because only the non-zero elements are stored

905
00:46:31,839 --> 00:46:33,770
enabling the hardware to run more networks

906
00:46:33,920 --> 00:46:34,170
simultaneously

907
00:46:35,520 --> 00:46:37,690
gpus and tensor processing units

908
00:46:37,760 --> 00:46:38,250
so tpus

909
00:46:38,720 --> 00:46:40,730
they are dense execution engines

910
00:46:41,280 --> 00:46:45,210
they perform the same computation task on an entire vector or matrix of data

911
00:46:45,520 --> 00:46:46,569
this is a wise approach

912
00:46:46,640 --> 00:46:48,250
when the vector or matrix is dense

913
00:46:48,319 --> 00:46:51,770
which is to say it's all non-zero but in the dense environment

914
00:46:52,000 --> 00:46:56,650
we gain efficiency by executing a single instruction to be applied to all of the data

915
00:46:56,960 --> 00:46:58,329
this is an approach called simd

916
00:46:59,200 --> 00:47:01,130
but when the data is predominantly zeros

917
00:47:01,680 --> 00:47:04,730
then a prodigious amount of computation is wasted

918
00:47:05,599 --> 00:47:07,770
so if you're keeping up to date on ai hardware

919
00:47:07,920 --> 00:47:10,569
you might have heard of graphcorp or cerebros cerebrus

920
00:47:10,880 --> 00:47:11,290
in particular

921
00:47:11,440 --> 00:47:11,690
actually

922
00:47:11,700 --> 00:47:19,130
they've developed this epic microprocessor with 850 000 cores and 40 gigabytes of memory on board

923
00:47:19,520 --> 00:47:20,569
it's absolutely insane

924
00:47:21,520 --> 00:47:25,450
not only that the chip has been designed to support sparsity from the ground up

925
00:47:25,680 --> 00:47:28,170
these cerebros cores never multiply by zero

926
00:47:28,640 --> 00:47:31,849
the scheduling operates at the granularity of a single data value

927
00:47:31,920 --> 00:47:33,530
so all of the zeros are filtered out

928
00:47:33,680 --> 00:47:35,770
and this in turn provides a performance advantage

929
00:47:36,000 --> 00:47:39,609
by doing useful work during those cycles which otherwise would have been wasted

930
00:47:40,000 --> 00:47:42,089
not to mention the power and efficiency savings

931
00:47:42,880 --> 00:47:43,130
now

932
00:47:43,140 --> 00:47:43,290
recently

933
00:47:43,520 --> 00:47:43,930
in a presentation

934
00:47:44,319 --> 00:47:48,970
they showed this graphic claiming to achieve near linear speed up in respective sparsity

935
00:47:49,760 --> 00:47:50,329
about 84

936
00:47:50,720 --> 00:47:51,609
speed up for 94

937
00:47:52,240 --> 00:47:52,490
sparsity

938
00:47:53,359 --> 00:47:57,609
numenta released this paper a couple of years ago before the current sparse hardware was released

939
00:47:57,839 --> 00:48:05,210
and at the time they chose an fpga which is a field programmable gate array as the platform to run their performance tests

940
00:48:06,160 --> 00:48:09,369
because of the flexibility it provided and handling sparse data efficiently

941
00:48:10,160 --> 00:48:10,569
in addition

942
00:48:10,720 --> 00:48:14,490
random access to memory is far more granular and efficient on an fpga

943
00:48:15,200 --> 00:48:21,210
enabling fpga implementations to efficiently handle the unstructured access patterns in sparse networks

944
00:48:21,920 --> 00:48:22,650
now in their paper

945
00:48:22,720 --> 00:48:28,490
as well as confirming the previous results in the literature about the robustness of sparse networks to noise

946
00:48:28,640 --> 00:48:29,530
and variance error

947
00:48:29,760 --> 00:48:34,890
they also realized a significant performance gain from using the specialized fpga hardware

948
00:48:35,599 --> 00:48:36,890
now my intuition is

949
00:48:36,900 --> 00:48:46,410
that there's no difference at all between the representational power of a discrete htm type model which they used in their previous generation of algorithmic approaches versus an artificial neural network

950
00:48:46,960 --> 00:48:52,089
the obvious difference is that feed forward monolithic vector space models are more amenable to training

951
00:48:52,240 --> 00:48:53,130
given today's hardware

952
00:48:54,000 --> 00:48:54,490
pretty much

953
00:48:54,559 --> 00:48:56,650
everyone agrees that sparse networks are better

954
00:48:56,880 --> 00:49:00,490
but is there something fundamentally special about sparsity

955
00:49:01,359 --> 00:49:02,970
numenta certainly seem to think

956
00:49:02,980 --> 00:49:06,329
so they anecdotally point to the brain as being sparse

957
00:49:06,960 --> 00:49:11,290
but the brain is probably sparse for the same reason that i don't decide to get up every morning

958
00:49:11,599 --> 00:49:15,450
and travel to every city in the uk doesn't seem like a profound insight

959
00:49:15,680 --> 00:49:16,250
to be honest

960
00:49:16,960 --> 00:49:20,089
we can take it as a given that sparse networks suffer less from overfitting

961
00:49:20,480 --> 00:49:23,530
because they're not going to be using their precious representational power

962
00:49:24,000 --> 00:49:28,650
memorizing individual challenging or non-representative examples in the training data

963
00:49:29,359 --> 00:49:35,130
it remains unknown if the performance of the best pruning algorithms is an upper bound on the quality of sparse models

964
00:49:35,599 --> 00:49:43,369
there's actually some really interesting papers out there now like momentum resnet which allow us to train huge neural networks with a small memory footprint

965
00:49:43,839 --> 00:49:47,930
but the key question is are sparse networks functionally better than huge

966
00:49:47,940 --> 00:49:49,290
densely connected neural networks

967
00:49:50,559 --> 00:49:53,849
researchers from google released the paper rigging the lottery

968
00:49:54,319 --> 00:49:56,089
making all tickets winners back in 2019

969
00:49:57,359 --> 00:50:02,250
the rigging the lottery algorithm starts with a randomly initialized connection topology and then layer

970
00:50:02,319 --> 00:50:04,410
by layer adds and removes connections

971
00:50:04,960 --> 00:50:09,690
densifying the layer and then specifying again using a traditional weight magnitude heuristic

972
00:50:10,400 --> 00:50:15,930
the algorithm achieves higher accuracy than all previous techniques for a given computational cost at all levels of sparsity

973
00:50:16,640 --> 00:50:20,250
and then scored higher accuracy than the dense to sparse algorithms

974
00:50:20,960 --> 00:50:23,930
now nimenta say that they've also produced a similar algorithm

975
00:50:24,160 --> 00:50:24,410
although

976
00:50:24,420 --> 00:50:25,130
as far as i know

977
00:50:25,140 --> 00:50:26,329
it's not been made public yet

978
00:50:26,400 --> 00:50:28,410
so i assume it's pretty similar to google's approach

979
00:50:28,559 --> 00:50:29,849
as they pointed us in this direction

980
00:50:30,480 --> 00:50:32,250
but unlike traditional dense to sparse

981
00:50:32,400 --> 00:50:33,130
iterative magnitude

982
00:50:33,359 --> 00:50:36,650
pruning google's algorithm allows the topology to grow

983
00:50:37,040 --> 00:50:38,089
also during the optimization

984
00:50:38,880 --> 00:50:41,369
which can apparently help overcome some of the local minima

985
00:50:42,079 --> 00:50:44,569
obviously the most accurate sparsity algorithms required

986
00:50:44,960 --> 00:50:45,530
at a minimum

987
00:50:46,000 --> 00:50:50,010
the cost of training a large dense network in terms of memory and computational horsepower

988
00:50:50,640 --> 00:50:52,329
but that approach has serious limitations

989
00:50:53,359 --> 00:50:53,609
right

990
00:50:53,619 --> 00:50:58,250
the size of the sparse model you can learn is strongly bounded on the size of the larger dense model

991
00:50:58,480 --> 00:51:00,650
which you're specifying from as a starting point

992
00:51:01,040 --> 00:51:04,569
it's simply too inefficient to waste computation on so many parameters

993
00:51:05,040 --> 00:51:06,170
which would end up being zero

994
00:51:06,319 --> 00:51:06,569
anyway

995
00:51:07,359 --> 00:51:10,569
google's algorithm seems unequivocally better than dense

996
00:51:10,720 --> 00:51:11,450
iterative magnitude

997
00:51:11,680 --> 00:51:11,930
pruning

998
00:51:12,160 --> 00:51:14,010
which is somewhat surprising to be honest

999
00:51:14,020 --> 00:51:14,650
given that it's

1000
00:51:14,720 --> 00:51:14,970
uh

1001
00:51:15,119 --> 00:51:16,730
it seems to be doing the same thing

1002
00:51:16,740 --> 00:51:18,010
but only one layer at a time

1003
00:51:18,640 --> 00:51:20,970
now training mobile net one and two on imagenet

1004
00:51:21,119 --> 00:51:23,050
with this form of sparse training was instructive

1005
00:51:23,599 --> 00:51:29,849
it was possible to train a sparse network with nearly the same accuracy in about 30 percent of the compute time

1006
00:51:30,400 --> 00:51:33,130
it was also possible to train a large sparse network

1007
00:51:33,440 --> 00:51:34,730
which was five percent better

1008
00:51:34,800 --> 00:51:37,609
accuracy in roughly double the flops of training

1009
00:51:37,760 --> 00:51:39,210
the original dense uh version

1010
00:51:39,839 --> 00:51:41,930
today's neural networks have something called the point neuron

1011
00:51:42,000 --> 00:51:43,450
which is a very simple model of a neuron

1012
00:51:44,079 --> 00:51:45,050
and uh

1013
00:51:45,200 --> 00:51:46,569
by adding dendrites to them

1014
00:51:46,579 --> 00:51:47,690
just one more level of complexity

1015
00:51:48,800 --> 00:51:50,010
that's in biological systems

1016
00:51:50,079 --> 00:51:52,410
you can solve problems in continuous learning

1017
00:51:52,880 --> 00:51:53,130
um

1018
00:51:53,680 --> 00:51:54,730
and rapid learning

1019
00:51:55,280 --> 00:51:56,650
so we're trying to take

1020
00:51:56,800 --> 00:52:01,050
we're trying to bring the existing uh field and we'll see if we can do it

1021
00:52:01,060 --> 00:52:04,650
we're trying to bring the existing field of machine learning commercially

1022
00:52:04,960 --> 00:52:05,690
along with us

1023
00:52:05,700 --> 00:52:06,730
you brought up this idea of keeping

1024
00:52:06,880 --> 00:52:07,210
you know

1025
00:52:07,220 --> 00:52:08,410
paying for it commercially

1026
00:52:08,640 --> 00:52:09,369
along with us

1027
00:52:09,379 --> 00:52:11,770
as we move towards the ultimate goal of the true ai system

1028
00:52:12,480 --> 00:52:15,609
now nimente is working on some really cool stuff behind the scenes

1029
00:52:16,000 --> 00:52:19,290
unfortunately there's very little information about it in the public domain

1030
00:52:19,440 --> 00:52:25,210
yet jeff's main aim is to realize the vision of the thousand brains theory in an efficient computational algorithm

1031
00:52:25,839 --> 00:52:28,170
sparse networks are just a tiny part of this vision

1032
00:52:28,480 --> 00:52:32,010
the next step is implementing continual learning with active dendrites

1033
00:52:32,720 --> 00:52:35,369
and this essentially means that they need to be able to add new synapses

1034
00:52:36,000 --> 00:52:38,010
and train them independently of the existing ones

1035
00:52:38,960 --> 00:52:41,369
this is going to require a specified version of backprop

1036
00:52:41,839 --> 00:52:44,809
which will also require specialized hardware and algorithms to implement

1037
00:52:45,440 --> 00:52:48,730
they also mentioned to us that they want to exploit activation and weight sparsity

1038
00:52:49,119 --> 00:52:51,050
simultaneously mirroring the neocortex

1039
00:52:52,400 --> 00:52:52,650
anyway

1040
00:52:53,280 --> 00:52:53,849
next week

1041
00:52:53,859 --> 00:52:54,569
on street talk

1042
00:52:56,100 --> 00:53:20,410
[music] another book that i read around the same time that had a big impact on me

1043
00:53:21,119 --> 00:53:21,369
uh

1044
00:53:22,240 --> 00:53:26,329
and and there was actually a little bit of overlap with john pierre as well

1045
00:53:26,339 --> 00:53:27,609
and i read it around the same time

1046
00:53:28,720 --> 00:53:30,809
is jeff hawkins on intelligence

1047
00:53:31,520 --> 00:53:32,170
which is a classic

1048
00:53:33,200 --> 00:53:35,849
and he has this vision of the people of the main championship

1049
00:53:35,859 --> 00:53:36,970
i hope you enjoyed it

1050
00:53:36,980 --> 00:53:40,170
a multiscale hierarchy of temple prediction modules

1051
00:53:41,040 --> 00:53:43,050
and these ideas really resonated with me

1052
00:53:43,280 --> 00:53:46,650
like the the notion of a modular hierarchy

1053
00:53:47,599 --> 00:53:47,849
um

1054
00:53:48,720 --> 00:53:49,369
of you know

1055
00:53:49,379 --> 00:53:49,450
potentially

1056
00:53:49,920 --> 00:53:54,250
um of compression functions or prediction functions

1057
00:53:54,260 --> 00:53:57,930
i thought it was really really interesting and it reshaped

1058
00:53:58,480 --> 00:53:58,730
uh

1059
00:53:59,119 --> 00:54:02,010
the way it started thinking about how to build minds

1060
00:54:03,599 --> 00:54:04,809
let's kick off with the main show

1061
00:54:04,880 --> 00:54:05,690
hope you enjoy it

1062
00:54:05,700 --> 00:54:05,770
folks

1063
00:54:06,000 --> 00:54:06,250
well

1064
00:54:06,260 --> 00:54:08,250
i fell in love with brains actually

1065
00:54:08,400 --> 00:54:10,730
when i read an article by francis crick

1066
00:54:10,740 --> 00:54:12,410
who was one of the co-discoverers of dna

1067
00:54:13,359 --> 00:54:16,890
and francis had later in his life turned his interest to neuroscience

1068
00:54:17,839 --> 00:54:19,609
and he wrote this essay

1069
00:54:19,680 --> 00:54:21,050
that appeared in scientific american

1070
00:54:21,760 --> 00:54:24,970
where soon the emperor has no closed type of essay

1071
00:54:25,040 --> 00:54:25,369
he said

1072
00:54:25,379 --> 00:54:25,849
you know what

1073
00:54:25,859 --> 00:54:28,089
we have all this data about the brain and it's really wonderful

1074
00:54:28,240 --> 00:54:30,250
we collected all this data and we've got decades and decades

1075
00:54:30,400 --> 00:54:32,170
but no one has a clue what the hell is going on

1076
00:54:32,800 --> 00:54:33,450
and um

1077
00:54:34,000 --> 00:54:34,569
and he says

1078
00:54:34,579 --> 00:54:34,890
you know

1079
00:54:34,900 --> 00:54:37,130
we need new ways of thinking about the brain

1080
00:54:37,359 --> 00:54:39,210
we don't really necessarily need more data

1081
00:54:39,599 --> 00:54:39,849
and

1082
00:54:40,640 --> 00:54:42,329
and that just struck me

1083
00:54:42,339 --> 00:54:45,369
i was 22 and i was like holy crap

1084
00:54:45,520 --> 00:54:45,930
you know

1085
00:54:46,240 --> 00:54:47,130
that's just a puzzle

1086
00:54:47,280 --> 00:54:47,770
we have this

1087
00:54:47,780 --> 00:54:51,089
we have these pieces and someone has to put the pieces together

1088
00:54:51,920 --> 00:54:54,329
and that seemed like something i would be good at

1089
00:54:54,339 --> 00:54:55,450
or at least i would enjoy

1090
00:54:56,160 --> 00:54:57,210
and that's got me going

1091
00:54:57,359 --> 00:54:57,770
that was

1092
00:54:57,780 --> 00:55:00,809
that was the thing that just i said i'm going to make a career out of this

1093
00:55:01,119 --> 00:55:01,450
and then

1094
00:55:01,520 --> 00:55:02,010
very quickly

1095
00:55:02,079 --> 00:55:02,970
i realized that

1096
00:55:02,980 --> 00:55:03,210
well

1097
00:55:03,760 --> 00:55:04,089
you know

1098
00:55:04,099 --> 00:55:04,970
this is the long thing

1099
00:55:05,200 --> 00:55:06,170
it's going to take a long time

1100
00:55:06,799 --> 00:55:07,049
and

1101
00:55:07,119 --> 00:55:07,369
uh

1102
00:55:07,379 --> 00:55:08,490
but if we do this

1103
00:55:09,119 --> 00:55:09,369
um

1104
00:55:09,839 --> 00:55:10,089
then

1105
00:55:10,160 --> 00:55:12,730
if we really figure out how the brain works and what it does

1106
00:55:12,880 --> 00:55:16,250
then we will have real big insights into how to make intelligent machines

1107
00:55:16,799 --> 00:55:17,530
and so i said

1108
00:55:17,540 --> 00:55:17,930
oh my god

1109
00:55:18,000 --> 00:55:21,450
the implications are here to not just from a neuroscience but from an ai point of view

1110
00:55:21,839 --> 00:55:22,890
and so that got me going

1111
00:55:23,119 --> 00:55:23,369
uh

1112
00:55:23,520 --> 00:55:24,809
on this on this journey

1113
00:55:25,280 --> 00:55:28,650
and at that point i decided to change careers from engineering to neuroscience

1114
00:55:29,440 --> 00:55:29,690
uh

1115
00:55:29,700 --> 00:55:30,410
computer engineering

1116
00:55:30,559 --> 00:55:33,290
computer science and neuroscience and start all over

1117
00:55:33,359 --> 00:55:34,089
i just got my

1118
00:55:34,240 --> 00:55:35,290
my degree from the university

1119
00:55:35,300 --> 00:55:36,089
so i was like

1120
00:55:36,099 --> 00:55:36,250
yup

1121
00:55:36,319 --> 00:55:36,970
starting again

1122
00:55:37,839 --> 00:55:38,089
here

1123
00:55:38,099 --> 00:55:38,490
we are

1124
00:55:38,500 --> 00:55:39,210
40 years later

1125
00:55:40,160 --> 00:55:41,369
it's been a long journey though

1126
00:55:41,379 --> 00:55:42,730
as you probably probably know

1127
00:55:42,740 --> 00:55:44,089
there's been a lot of twists and turns

1128
00:55:44,099 --> 00:55:44,329
too

1129
00:55:44,339 --> 00:55:44,569
absolutely

1130
00:55:45,119 --> 00:55:45,369
absolutely

1131
00:55:45,520 --> 00:55:45,770
well

1132
00:55:45,780 --> 00:55:47,130
we really enjoyed reading your book

1133
00:55:47,140 --> 00:55:47,369
but

1134
00:55:47,379 --> 00:55:47,530
um

1135
00:55:47,760 --> 00:55:50,970
i wanted to talk a little bit about some of the tribalism in the machine learning community

1136
00:55:51,200 --> 00:55:53,609
so i've been doing a bit of research online

1137
00:55:53,760 --> 00:55:57,609
and your mission right now is to try and convince other people of of your ideas

1138
00:55:57,680 --> 00:55:59,690
you've got this incredibly exciting idea of the brain

1139
00:55:59,700 --> 00:56:02,569
and as you just said that in in some level of abstraction

1140
00:56:02,880 --> 00:56:04,650
the brain is infinitely complicated

1141
00:56:05,280 --> 00:56:05,690
but actually

1142
00:56:05,760 --> 00:56:08,890
if you think about it in terms of simple rules that can produce a lot of complexity

1143
00:56:09,119 --> 00:56:09,770
it's not that complicated

1144
00:56:10,160 --> 00:56:10,890
but you know

1145
00:56:10,900 --> 00:56:13,210
i've noticed that when you speak to some of the machine learning folks

1146
00:56:13,280 --> 00:56:16,410
they are very quick to dismiss your ideas and they say

1147
00:56:16,420 --> 00:56:16,569
well

1148
00:56:16,579 --> 00:56:22,730
there's there's no material difference with monolithic neural networks with point neurons and back props and back prop

1149
00:56:22,740 --> 00:56:23,210
and you know

1150
00:56:23,280 --> 00:56:26,170
it's good that people are so passionate about what they believe in

1151
00:56:26,180 --> 00:56:27,450
but by the same token

1152
00:56:27,760 --> 00:56:30,809
it means that science only advances one funeral at a time

1153
00:56:31,119 --> 00:56:31,369
yeah

1154
00:56:32,240 --> 00:56:32,490
um

1155
00:56:33,280 --> 00:56:36,250
it's a very complex issue you bring up here

1156
00:56:36,400 --> 00:56:37,210
and um

1157
00:56:38,000 --> 00:56:38,250
uh

1158
00:56:38,319 --> 00:56:40,809
i i think our mission isn't to convince people

1159
00:56:41,599 --> 00:56:42,010
our first

1160
00:56:42,079 --> 00:56:44,809
our mission was to figure out what the hell is going on in our heads

1161
00:56:44,880 --> 00:56:46,410
that was the first thing we had to do

1162
00:56:47,119 --> 00:56:48,650
and we made a lot of progress on that

1163
00:56:49,200 --> 00:56:50,890
now we have to sell those ideas

1164
00:56:50,960 --> 00:56:51,930
we have to sell them to neuroscientists

1165
00:56:52,400 --> 00:56:53,210
so we publish papers

1166
00:56:53,440 --> 00:56:54,170
up speaker conferences

1167
00:56:55,599 --> 00:56:56,490
people cite our work

1168
00:56:56,559 --> 00:56:57,290
they test it

1169
00:56:57,300 --> 00:56:57,609
and so on

1170
00:56:57,619 --> 00:56:58,170
that has to happen

1171
00:56:58,319 --> 00:56:58,970
it takes time

1172
00:56:59,359 --> 00:56:59,609
um

1173
00:57:00,240 --> 00:57:00,490
then

1174
00:57:00,799 --> 00:57:01,049
uh

1175
00:57:01,280 --> 00:57:03,609
then we now have a road map we can see

1176
00:57:04,400 --> 00:57:05,049
we can now

1177
00:57:05,059 --> 00:57:05,290
uh

1178
00:57:05,300 --> 00:57:07,609
from the position i'm in and some of the people work with me

1179
00:57:07,619 --> 00:57:11,210
we can see where the shortcomings are in current ai techniques

1180
00:57:12,079 --> 00:57:12,329
uh

1181
00:57:12,400 --> 00:57:12,809
we can

1182
00:57:12,880 --> 00:57:13,369
we can see

1183
00:57:13,379 --> 00:57:13,530
oh

1184
00:57:13,540 --> 00:57:14,569
the brain's doing it differently

1185
00:57:14,640 --> 00:57:15,530
it's doing it this way

1186
00:57:16,079 --> 00:57:17,049
and and we have

1187
00:57:17,119 --> 00:57:18,730
we can have two approaches to go forward

1188
00:57:18,799 --> 00:57:19,290
we could say

1189
00:57:19,680 --> 00:57:19,930
well

1190
00:57:19,940 --> 00:57:21,530
we need to convince everybody else

1191
00:57:22,000 --> 00:57:24,010
which is not really a fun thing to do

1192
00:57:24,400 --> 00:57:24,650
um

1193
00:57:24,720 --> 00:57:25,290
or we can

1194
00:57:25,440 --> 00:57:27,210
we can just put our ideas out there

1195
00:57:27,280 --> 00:57:28,089
document them

1196
00:57:28,099 --> 00:57:31,130
show people them and then work at doing this ourselves like

1197
00:57:31,200 --> 00:57:32,410
just demonstrate it

1198
00:57:32,420 --> 00:57:33,210
start building things

1199
00:57:33,440 --> 00:57:33,690
um

1200
00:57:34,079 --> 00:57:35,049
making things that work

1201
00:57:35,059 --> 00:57:38,569
making things to solve problems that other people have been struggling with

1202
00:57:38,960 --> 00:57:40,650
so we're doing a bit of all these things

1203
00:57:40,660 --> 00:57:42,250
we're promoting the neuroscience theory

1204
00:57:42,400 --> 00:57:43,609
we're promoting um

1205
00:57:43,680 --> 00:57:45,369
these ideas in the machine learning community

1206
00:57:45,599 --> 00:57:47,290
and we're also implementing this stuff

1207
00:57:47,920 --> 00:57:48,170
uh

1208
00:57:48,180 --> 00:57:49,049
because in the end

1209
00:57:49,059 --> 00:57:49,290
you

1210
00:57:49,300 --> 00:57:49,690
if you don't

1211
00:57:49,700 --> 00:57:50,089
if you

1212
00:57:50,099 --> 00:57:50,490
you know

1213
00:57:50,500 --> 00:57:52,170
you can't implement it unless you understand it

1214
00:57:52,180 --> 00:57:53,450
so it's a good test for us too

1215
00:57:54,000 --> 00:57:54,250
um

1216
00:57:54,559 --> 00:57:55,849
and you would wish you know

1217
00:57:55,859 --> 00:57:56,569
everyone wish like

1218
00:57:56,579 --> 00:57:56,809
oh

1219
00:57:56,819 --> 00:57:57,369
i wish everyone

1220
00:57:57,520 --> 00:57:58,329
just you know

1221
00:57:58,339 --> 00:57:59,049
agreed with us

1222
00:57:59,059 --> 00:58:01,369
where everyone you know got what we understand

1223
00:58:01,599 --> 00:58:04,490
but the reality is it's a big world of lots of big little

1224
00:58:04,559 --> 00:58:04,809
um

1225
00:58:05,280 --> 00:58:08,730
and there's a lot of um people who want to dismiss new ideas

1226
00:58:09,280 --> 00:58:09,530
um

1227
00:58:10,000 --> 00:58:12,410
but that's just the nature of the of the beast

1228
00:58:12,480 --> 00:58:12,730
right

1229
00:58:12,740 --> 00:58:13,770
we just have to accept that

1230
00:58:13,839 --> 00:58:14,089
um

1231
00:58:14,960 --> 00:58:16,730
science is not just science

1232
00:58:16,799 --> 00:58:19,210
you have to promote your ideas as well as discover them

1233
00:58:19,220 --> 00:58:20,250
and the same is true

1234
00:58:20,640 --> 00:58:20,890
uh

1235
00:58:20,900 --> 00:58:21,930
in the machine learning world

1236
00:58:22,960 --> 00:58:23,210
well

1237
00:58:23,359 --> 00:58:25,049
one thing that's kind of interesting about that

1238
00:58:25,920 --> 00:58:26,250
you know

1239
00:58:26,400 --> 00:58:28,410
philosophical aspect that you have of look

1240
00:58:28,420 --> 00:58:29,049
it's a big world

1241
00:58:29,200 --> 00:58:30,970
there's lots of people doing a lot of different things

1242
00:58:31,440 --> 00:58:32,809
that's kind of the heart of evolution

1243
00:58:33,200 --> 00:58:33,450
right

1244
00:58:33,520 --> 00:58:35,089
is that there's variation

1245
00:58:35,839 --> 00:58:39,770
people taking different approaches and natural selection will kind of sort out

1246
00:58:40,160 --> 00:58:41,849
you know which ones work and don't work

1247
00:58:42,000 --> 00:58:44,410
and that's one thing i've always thought about

1248
00:58:44,880 --> 00:58:48,490
a lot of the folks that don't want to pay attention to what the brain does is

1249
00:58:48,500 --> 00:58:52,650
they don't have a sufficient appreciation for how much you know

1250
00:58:52,660 --> 00:58:52,890
uh

1251
00:58:52,900 --> 00:58:53,369
let's say

1252
00:58:54,160 --> 00:58:57,290
work has gone into designing the human brain right

1253
00:58:57,300 --> 00:58:58,730
i mean a billion years of life

1254
00:58:59,520 --> 00:59:00,089
however many

1255
00:59:00,099 --> 00:59:02,250
hundreds of millions of years of intelligent evolution

1256
00:59:02,720 --> 00:59:05,530
so certainly we can learn things from the neuroscience

1257
00:59:06,079 --> 00:59:06,329
right

1258
00:59:06,339 --> 00:59:06,650
i mean

1259
00:59:06,660 --> 00:59:06,890
and

1260
00:59:07,280 --> 00:59:11,290
and of course that was the original inspiration for the artificial neuron

1261
00:59:11,839 --> 00:59:13,210
but that was just one tiny

1262
00:59:13,440 --> 00:59:15,770
simplest possible abstraction of a neuron

1263
00:59:15,920 --> 00:59:16,329
that happened

1264
00:59:16,400 --> 00:59:16,970
what fit

1265
00:59:17,280 --> 00:59:17,530
yeah

1266
00:59:17,540 --> 00:59:18,089
i don't know what

1267
00:59:18,099 --> 00:59:18,650
50 years ago

1268
00:59:18,880 --> 00:59:19,130
right

1269
00:59:19,140 --> 00:59:19,609
six years

1270
00:59:19,680 --> 00:59:21,290
there's got to be more we can learn right

1271
00:59:22,319 --> 00:59:22,809
but yeah

1272
00:59:22,960 --> 00:59:23,450
but you know

1273
00:59:23,460 --> 00:59:24,410
the way i look at it is

1274
00:59:24,420 --> 00:59:26,010
we're all trying to reach the same end goal

1275
00:59:26,020 --> 00:59:28,490
we're all trying to figure out how to build intelligent machines

1276
00:59:28,640 --> 00:59:29,369
they're truly intelligent

1277
00:59:29,520 --> 00:59:29,930
we all

1278
00:59:29,940 --> 00:59:30,410
we're either intelligent

1279
00:59:31,040 --> 00:59:31,290
uh

1280
00:59:31,300 --> 00:59:32,809
we can talk about the implications of that

1281
00:59:33,200 --> 00:59:33,450
um

1282
00:59:34,079 --> 00:59:36,250
and i don't think there will be multiple ways of doing this

1283
00:59:36,260 --> 00:59:37,210
there's like in the end

1284
00:59:37,220 --> 00:59:39,130
there aren't multiple ways of building computers

1285
00:59:39,359 --> 00:59:42,569
they're all some sense of universal turing machines and we have variations on that

1286
00:59:42,960 --> 00:59:43,210
um

1287
00:59:43,520 --> 00:59:46,010
and so i think that's what's gonna happen here too now

1288
00:59:46,160 --> 00:59:46,650
a priori

1289
00:59:46,960 --> 00:59:49,690
how would i know that engineers couldn't figure this all out

1290
00:59:49,700 --> 00:59:51,530
just by thinking about and doing engineering stuff

1291
00:59:51,920 --> 00:59:52,170
um

1292
00:59:52,640 --> 00:59:53,049
you know

1293
00:59:53,059 --> 00:59:54,970
my guess was that we'd have to figure out how the brain works first

1294
00:59:55,440 --> 00:59:55,690
uh

1295
00:59:56,000 --> 00:59:57,530
and who knows i could have been wrong

1296
00:59:57,540 --> 00:59:58,089
that was a bet

1297
00:59:58,480 --> 00:59:59,690
maybe people would figure it out

1298
00:59:59,700 --> 01:00:02,809
but here we are 67 years later in the field of ai

1299
01:00:03,359 --> 01:00:04,010
and i still

1300
01:00:04,160 --> 01:00:06,730
i think today's ai systems are still incredibly limited

1301
01:00:06,880 --> 01:00:08,329
they don't really do much at all with

1302
01:00:08,559 --> 01:00:08,970
it's intelligent

1303
01:00:09,280 --> 01:00:09,849
that's my opinion

1304
01:00:10,400 --> 01:00:10,650
um

1305
01:00:10,799 --> 01:00:12,089
they're very very restricted

1306
01:00:12,319 --> 01:00:12,970
they don't generalize

1307
01:00:13,280 --> 01:00:14,250
they don't create behaviors

1308
01:00:14,559 --> 01:00:14,890
they don't

1309
01:00:15,040 --> 01:00:19,450
it's just so many things that we're so far away and now we took a long time

1310
01:00:19,460 --> 01:00:21,210
but now we figured out a lot about how the brain works

1311
01:00:21,220 --> 01:00:21,930
so now we have a roadmap

1312
01:00:22,160 --> 01:00:22,410
so

1313
01:00:23,040 --> 01:00:23,290
um

1314
01:00:23,300 --> 01:00:24,730
i think it's going to be a lot easier

1315
01:00:24,799 --> 01:00:25,930
it's now it's not just like

1316
01:00:25,940 --> 01:00:26,089
oh

1317
01:00:26,099 --> 01:00:28,410
i think it's going to be quicker to study the brain and other people say

1318
01:00:28,420 --> 01:00:28,569
no

1319
01:00:28,579 --> 01:00:29,290
i don't think we'll be

1320
01:00:29,300 --> 01:00:29,609
i said

1321
01:00:29,619 --> 01:00:29,690
well

1322
01:00:29,700 --> 01:00:32,329
we now have some things we can we really understand about the brain

1323
01:00:32,339 --> 01:00:34,089
we don't have to worry about that anymore

1324
01:00:34,559 --> 01:00:34,809
so

1325
01:00:34,880 --> 01:00:35,130
um

1326
01:00:35,440 --> 01:00:37,530
the approach we took has been fruitful

1327
01:00:38,319 --> 01:00:38,569
um

1328
01:00:38,880 --> 01:00:39,450
and um

1329
01:00:39,760 --> 01:00:40,890
and i should also say

1330
01:00:40,900 --> 01:00:41,609
just be clear

1331
01:00:42,000 --> 01:00:43,609
today's ai is really useful

1332
01:00:43,839 --> 01:00:45,450
so i have nothing against it

1333
01:00:45,520 --> 01:00:45,770
yeah

1334
01:00:46,400 --> 01:00:46,890
it's great

1335
01:00:47,440 --> 01:00:47,690
um

1336
01:00:48,480 --> 01:00:51,049
it's just not intelligent and i want to build soldier machines

1337
01:00:52,000 --> 01:00:52,250
yeah

1338
01:00:52,260 --> 01:00:52,970
it's quite a

1339
01:00:52,980 --> 01:00:54,890
it's really fascinating to get to talk to you

1340
01:00:54,960 --> 01:00:55,210
uh

1341
01:00:55,220 --> 01:00:58,970
your book on intelligence is actually one of the very first books of it about ai

1342
01:00:59,119 --> 01:00:59,930
i ever read it

1343
01:00:59,940 --> 01:01:00,089
so

1344
01:01:00,319 --> 01:01:00,569
uh

1345
01:01:00,640 --> 01:01:01,609
edward is your fault

1346
01:01:01,619 --> 01:01:02,410
that part of this field

1347
01:01:02,480 --> 01:01:05,450
now i always feel bad when people tell me

1348
01:01:05,460 --> 01:01:06,410
i hope it worked out

1349
01:01:06,420 --> 01:01:06,569
okay

1350
01:01:06,640 --> 01:01:07,049
for you

1351
01:01:08,400 --> 01:01:08,809
it's been

1352
01:01:08,819 --> 01:01:09,849
it's been a wonderful journey

1353
01:01:10,079 --> 01:01:13,770
and i'm really glad that it was one of my first exposures to the field

1354
01:01:13,780 --> 01:01:15,770
in a way it was so full of like

1355
01:01:15,920 --> 01:01:20,170
really interesting ideas especially for the at the time seemed very revolutionary

1356
01:01:20,880 --> 01:01:21,130
um

1357
01:01:21,680 --> 01:01:23,290
and a very well shaped my picking

1358
01:01:24,079 --> 01:01:25,450
so i want to ask you a little bit

1359
01:01:25,599 --> 01:01:25,849
um

1360
01:01:26,079 --> 01:01:27,849
appears to me as you know

1361
01:01:27,859 --> 01:01:28,490
someone who works

1362
01:01:28,720 --> 01:01:28,970
sorry

1363
01:01:28,980 --> 01:01:29,690
i work with deep learning

1364
01:01:29,760 --> 01:01:30,329
you know a lot

1365
01:01:30,339 --> 01:01:30,650
of course

1366
01:01:30,720 --> 01:01:30,970
nowadays

1367
01:01:31,359 --> 01:01:33,369
it feels to me that a lot of your ideas seem

1368
01:01:33,680 --> 01:01:33,930
uh

1369
01:01:34,000 --> 01:01:36,809
validated in a lot of dealery practices

1370
01:01:37,520 --> 01:01:37,770
um

1371
01:01:37,780 --> 01:01:37,770
a

1372
01:01:38,240 --> 01:01:39,049
like a

1373
01:01:39,059 --> 01:01:41,690
currently kind of like emerging trend is how

1374
01:01:42,240 --> 01:01:42,490
um

1375
01:01:43,359 --> 01:01:43,609
with

1376
01:01:43,619 --> 01:01:44,490
like these me

1377
01:01:44,559 --> 01:01:47,369
furtive societies transfer models that have become very popular

1378
01:01:47,520 --> 01:01:52,569
recently that scaling these larger is just stacking these transformer layers

1379
01:01:52,799 --> 01:01:55,930
seems to currently at least increase performance in

1380
01:01:55,940 --> 01:01:56,250
like a

1381
01:01:56,260 --> 01:01:57,849
you know in a pretty consistent manner

1382
01:01:58,079 --> 01:01:58,730
and i wonder

1383
01:01:58,740 --> 01:01:58,970
well

1384
01:01:59,520 --> 01:02:00,490
it feels like that

1385
01:02:00,640 --> 01:02:02,569
it kind of validates in a way

1386
01:02:02,579 --> 01:02:04,650
it's like a different proposal to what you say

1387
01:02:04,660 --> 01:02:05,770
but like accordingly before we

1388
01:02:05,839 --> 01:02:06,089
do

1389
01:02:06,099 --> 01:02:07,049
you think there's any kind of connection

1390
01:02:07,920 --> 01:02:08,170
ah

1391
01:02:09,760 --> 01:02:10,089
you know

1392
01:02:10,720 --> 01:02:11,770
i don't think about

1393
01:02:12,000 --> 01:02:12,809
i'll answer your question

1394
01:02:12,819 --> 01:02:15,289
but let's just say i'm always forward thinking

1395
01:02:15,920 --> 01:02:16,809
i never look back

1396
01:02:17,039 --> 01:02:18,329
and so i don't worry about

1397
01:02:18,400 --> 01:02:18,650
hey

1398
01:02:18,660 --> 01:02:19,530
what the hell

1399
01:02:19,540 --> 01:02:20,890
was i right about this or not

1400
01:02:20,900 --> 01:02:21,289
or just

1401
01:02:21,359 --> 01:02:21,690
you know

1402
01:02:21,760 --> 01:02:23,130
somebody get credit for with this or that

1403
01:02:23,140 --> 01:02:23,770
i don't really care

1404
01:02:23,920 --> 01:02:26,089
oh so it's like let's go forward

1405
01:02:26,099 --> 01:02:27,369
what's the best thing you can do right now

1406
01:02:27,920 --> 01:02:28,569
and um

1407
01:02:29,280 --> 01:02:30,650
and so you know

1408
01:02:30,660 --> 01:02:31,849
i think there's you can

1409
01:02:31,859 --> 01:02:32,089
obviously

1410
01:02:32,640 --> 01:02:33,289
as i said

1411
01:02:33,299 --> 01:02:33,530
really

1412
01:02:33,540 --> 01:02:36,250
we're all going to converge on the same thing right eventually

1413
01:02:36,559 --> 01:02:38,730
i think so ideas become

1414
01:02:39,359 --> 01:02:39,690
you know

1415
01:02:39,700 --> 01:02:41,609
i got ideas from other people other than people

1416
01:02:41,619 --> 01:02:42,010
get ideas

1417
01:02:42,020 --> 01:02:42,410
for me

1418
01:02:42,880 --> 01:02:46,970
the history of how that milieu of ideas travels

1419
01:02:47,119 --> 01:02:48,730
it's very difficult to point to

1420
01:02:49,200 --> 01:02:49,450
um

1421
01:02:49,760 --> 01:02:50,490
so i i

1422
01:02:50,500 --> 01:02:52,650
i'm very reluctant to claim precedent than anything

1423
01:02:53,200 --> 01:02:54,089
i just feel like

1424
01:02:54,480 --> 01:02:54,730
hey

1425
01:02:54,799 --> 01:02:55,130
you know

1426
01:02:55,440 --> 01:02:56,809
if we're all moving towards the same idea

1427
01:02:56,880 --> 01:02:57,289
that's great

1428
01:02:57,520 --> 01:03:02,490
i don't think transformers are an implementation of the cortical

1429
01:03:03,359 --> 01:03:06,650
column or the cortical um algorithm idea that we're not there yet

1430
01:03:06,660 --> 01:03:08,170
because we know we now know what that

1431
01:03:08,319 --> 01:03:09,530
the core of that algorithm is

1432
01:03:09,540 --> 01:03:11,530
it has to do with movement and reference frames

1433
01:03:12,240 --> 01:03:12,490
and

1434
01:03:12,559 --> 01:03:12,809
uh

1435
01:03:12,819 --> 01:03:14,329
transformers don't have that

1436
01:03:14,480 --> 01:03:15,130
in some sense

1437
01:03:15,140 --> 01:03:17,369
they have a very primitive sort of um

1438
01:03:17,920 --> 01:03:18,650
attentional movement

1439
01:03:18,799 --> 01:03:19,930
if you want to think of it that way

1440
01:03:20,079 --> 01:03:20,329
um

1441
01:03:21,039 --> 01:03:21,289
um

1442
01:03:21,520 --> 01:03:24,650
but nothing like kind of moving and referencing we talk about

1443
01:03:25,039 --> 01:03:25,289
so

1444
01:03:25,440 --> 01:03:25,690
um

1445
01:03:26,079 --> 01:03:27,369
so it's a little bit in the right direction

1446
01:03:27,760 --> 01:03:28,010
um

1447
01:03:28,319 --> 01:03:29,369
and of course they're really impressive

1448
01:03:29,920 --> 01:03:30,170
um

1449
01:03:30,319 --> 01:03:30,809
but it's

1450
01:03:30,880 --> 01:03:31,369
it's not

1451
01:03:31,379 --> 01:03:34,890
it's not close to what i think we need to get to yeah it's really fascinating

1452
01:03:35,119 --> 01:03:35,369
though

1453
01:03:35,379 --> 01:03:37,210
because transformers are strange beasts

1454
01:03:37,839 --> 01:03:40,410
they perform a kind of information rooting

1455
01:03:40,480 --> 01:03:42,650
and it's quite esoteric exactly how they work

1456
01:03:42,660 --> 01:03:45,849
and i think in some sense they're similar to capsules

1457
01:03:46,160 --> 01:03:48,569
i know capsules have equated to some of your work

1458
01:03:48,579 --> 01:03:51,049
but i wanted to talk a little bit more deeper than that

1459
01:03:51,059 --> 01:03:51,210
though

1460
01:03:51,220 --> 01:03:52,730
so a lot of um

1461
01:03:52,799 --> 01:03:54,650
your ideas from a machine learning point of view

1462
01:03:54,660 --> 01:03:59,690
come down to the fundamental dichotomy of discrete representations versus continuous representations

1463
01:04:00,160 --> 01:04:02,010
which is what we use in deep learning

1464
01:04:02,020 --> 01:04:02,650
we use vectors

1465
01:04:03,200 --> 01:04:06,089
and there are some advantages to using these continuous vector spaces

1466
01:04:06,240 --> 01:04:07,770
namely that you get for free

1467
01:04:07,780 --> 01:04:08,809
some spatial priors

1468
01:04:08,960 --> 01:04:11,849
so you can encode semantic similarity as a function of how close they are

1469
01:04:11,859 --> 01:04:12,250
and yeah

1470
01:04:12,260 --> 01:04:13,210
and also you have gradients

1471
01:04:13,440 --> 01:04:15,930
which are useful for stochastic gradient descent and

1472
01:04:15,940 --> 01:04:16,250
and interestingly

1473
01:04:16,559 --> 01:04:21,930
the manifold hypothesis states that most natural data falls on very smooth and low dimensional manifolds

1474
01:04:22,240 --> 01:04:25,530
so all of the human faces could be projected onto a low dimensional manifold

1475
01:04:25,680 --> 01:04:26,329
but um

1476
01:04:26,400 --> 01:04:30,569
i i know that a lot of your work has has been really solely focused on on these um

1477
01:04:30,720 --> 01:04:33,849
sparse distributed representations and then um

1478
01:04:34,079 --> 01:04:35,930
building on top in in the encoding

1479
01:04:36,240 --> 01:04:36,569
for example

1480
01:04:36,720 --> 01:04:38,970
so if i'm encoding a date representation

1481
01:04:39,440 --> 01:04:40,329
i would do it such

1482
01:04:40,400 --> 01:04:46,170
that this sparse distributed representation would have a significant overlap for date times that are in the vicinity

1483
01:04:46,880 --> 01:04:47,130
yeah

1484
01:04:47,140 --> 01:04:50,250
and so semantically similar things should intersect each other

1485
01:04:50,480 --> 01:04:55,130
and spatial relations between bits can be encoded using a receptive field similar to

1486
01:04:55,280 --> 01:04:56,650
you know how it works in in cnn

1487
01:04:56,960 --> 01:04:57,210
so

1488
01:04:57,440 --> 01:04:57,690
um

1489
01:04:58,079 --> 01:05:05,369
but nimenta has asserted that intelligence wouldn't be possible without these sdrs and they're the primitive of of intelligence

1490
01:05:05,680 --> 01:05:08,170
so what do you think about the kind of comparison between vectors

1491
01:05:08,480 --> 01:05:11,690
and and these um discrete representations that that you were never

1492
01:05:11,760 --> 01:05:12,010
yeah

1493
01:05:12,240 --> 01:05:12,490
well

1494
01:05:12,500 --> 01:05:13,049
let's just start

1495
01:05:13,059 --> 01:05:13,450
you know

1496
01:05:13,460 --> 01:05:17,130
the whole idea of these sparse representations comes because that's how brains work

1497
01:05:17,280 --> 01:05:17,849
i mean that's

1498
01:05:17,920 --> 01:05:19,210
there's no compression about it

1499
01:05:19,440 --> 01:05:19,690
uh

1500
01:05:19,700 --> 01:05:20,569
if you look in the brain

1501
01:05:20,640 --> 01:05:25,049
you look at any population of cells that are representing something at any point in time

1502
01:05:25,599 --> 01:05:26,730
most of the cells are inactive

1503
01:05:27,520 --> 01:05:27,770
uh

1504
01:05:27,780 --> 01:05:29,130
it can be anywhere from

1505
01:05:29,680 --> 01:05:29,930
um

1506
01:05:30,400 --> 01:05:30,809
you know

1507
01:05:30,819 --> 01:05:33,690
90 percent to 99 percent of the cells interact

1508
01:05:34,160 --> 01:05:38,569
you just don't see of the information carrying neurons or some neurons that fire all the time

1509
01:05:38,579 --> 01:05:39,450
we're not talking about those

1510
01:05:39,680 --> 01:05:40,730
but the ones like information

1511
01:05:40,960 --> 01:05:42,809
you just do not see any kind of dense representations

1512
01:05:43,599 --> 01:05:44,329
that is a

1513
01:05:44,799 --> 01:05:45,289
when you

1514
01:05:45,299 --> 01:05:45,770
when that happens

1515
01:05:45,839 --> 01:05:46,650
you're having a seizure

1516
01:05:47,680 --> 01:05:47,930
um

1517
01:05:48,559 --> 01:05:54,089
and we also see that in in in real neurons that they're not very high fidelity

1518
01:05:54,400 --> 01:05:57,289
they don't carry individual spiking rates are very crude

1519
01:05:57,920 --> 01:05:58,170
um

1520
01:05:58,240 --> 01:06:00,170
there's many instances where even the single

1521
01:06:00,319 --> 01:06:02,890
the first spike is actually the most important thing recovering information

1522
01:06:03,680 --> 01:06:04,410
so we're not like

1523
01:06:04,420 --> 01:06:05,049
this isn't so

1524
01:06:05,059 --> 01:06:05,849
on a high resolution

1525
01:06:06,160 --> 01:06:06,490
you know

1526
01:06:06,559 --> 01:06:07,609
four bits or three bits

1527
01:06:07,619 --> 01:06:07,930
or even

1528
01:06:08,079 --> 01:06:08,730
even often

1529
01:06:09,359 --> 01:06:11,450
one bit of precision and firing of a nerve

1530
01:06:11,460 --> 01:06:11,770
all right

1531
01:06:11,780 --> 01:06:13,530
so this doesn't exist in the brain

1532
01:06:14,000 --> 01:06:14,250
um

1533
01:06:14,480 --> 01:06:15,609
so we just take that okay

1534
01:06:15,619 --> 01:06:15,849
well

1535
01:06:15,859 --> 01:06:17,289
we need to understand why that is

1536
01:06:17,359 --> 01:06:18,569
and it's not just

1537
01:06:18,720 --> 01:06:19,530
it's like more efficient

1538
01:06:19,680 --> 01:06:20,809
from an energy point of view

1539
01:06:20,819 --> 01:06:21,530
that's not what it is

1540
01:06:21,760 --> 01:06:22,010
um

1541
01:06:22,400 --> 01:06:23,690
it turns out that um

1542
01:06:24,079 --> 01:06:29,849
sparse representations have a lot of really desirable and interesting properties that you don't get with dense representations

1543
01:06:30,960 --> 01:06:31,210
um

1544
01:06:31,520 --> 01:06:32,569
we can talk about those

1545
01:06:33,200 --> 01:06:33,450
um

1546
01:06:33,760 --> 01:06:35,849
it's not like everything is

1547
01:06:36,160 --> 01:06:36,410
um

1548
01:06:37,280 --> 01:06:37,530
um

1549
01:06:38,640 --> 01:06:39,690
there's a spectrum here

1550
01:06:39,700 --> 01:06:42,089
right there are some parts of the brain where you have representations

1551
01:06:42,640 --> 01:06:44,010
which are a little bit less dense

1552
01:06:44,079 --> 01:06:46,170
maybe of the tempest are firing and there

1553
01:06:46,180 --> 01:06:47,609
the activity rates really do matter

1554
01:06:48,160 --> 01:06:48,410
um

1555
01:06:48,720 --> 01:06:49,690
they're still not high precision

1556
01:06:49,839 --> 01:06:50,650
but they really do matter

1557
01:06:50,660 --> 01:06:53,130
and then we believe there's other parts where they're very

1558
01:06:53,140 --> 01:06:53,530
very sparse

1559
01:06:53,680 --> 01:06:56,490
so i might have five thousand neurons and let's say two percent are active

1560
01:06:56,500 --> 01:06:59,690
so i have 100 active neurons and all the information is encoded

1561
01:06:59,920 --> 01:07:02,490
almost all the information is coding in the population code

1562
01:07:02,640 --> 01:07:03,930
not in the fire rate

1563
01:07:04,319 --> 01:07:05,289
and we can talk about that

1564
01:07:05,299 --> 01:07:09,049
if you want to dive into the details with the advantage of sparse distributed representations

1565
01:07:10,079 --> 01:07:10,329
um

1566
01:07:10,640 --> 01:07:12,650
but it is an empirical observation of the brain

1567
01:07:12,880 --> 01:07:14,250
so you know we're gonna start with that

1568
01:07:14,260 --> 01:07:14,569
it's okay

1569
01:07:14,579 --> 01:07:14,809
well

1570
01:07:14,819 --> 01:07:15,609
why is it that way

1571
01:07:15,680 --> 01:07:16,010
you know

1572
01:07:16,880 --> 01:07:17,130
well

1573
01:07:17,280 --> 01:07:18,250
one thing i wanted to understand

1574
01:07:18,400 --> 01:07:20,010
because you made the comment before that

1575
01:07:20,079 --> 01:07:20,329
um

1576
01:07:21,039 --> 01:07:22,010
they could improve generalization

1577
01:07:22,640 --> 01:07:25,849
but intuitively they're the opposite of improving generalization

1578
01:07:26,319 --> 01:07:28,010
because they have such an incredible specificity

1579
01:07:28,880 --> 01:07:29,130
so

1580
01:07:29,280 --> 01:07:29,609
for example

1581
01:07:30,720 --> 01:07:32,970
when i was reading through some of your literature when you use boosting

1582
01:07:33,119 --> 01:07:37,849
for example in your algorithm it increases the specificity to such an extent

1583
01:07:38,400 --> 01:07:42,650
but then the way we generalize in our cognition isn't necessarily through the representation

1584
01:07:43,119 --> 01:07:45,690
it's through the abstractness of of the concept we're learning

1585
01:07:45,700 --> 01:07:46,250
would you argue

1586
01:07:46,720 --> 01:07:46,970
yeah

1587
01:07:46,980 --> 01:07:48,490
so i'll be honest with you

1588
01:07:48,500 --> 01:07:49,770
i've changed my opinion about this

1589
01:07:50,000 --> 01:07:50,250
um

1590
01:07:51,039 --> 01:07:51,289
um

1591
01:07:51,599 --> 01:07:53,210
i used to believe exactly what you're saying

1592
01:07:53,280 --> 01:07:53,849
which is like

1593
01:07:53,859 --> 01:07:54,010
oh

1594
01:07:54,020 --> 01:07:56,809
these overlaps between these sdrs are really how you're going to get generalization

1595
01:07:57,599 --> 01:08:00,650
we now realize it is a much more powerful form of generalization

1596
01:08:01,520 --> 01:08:01,770
um

1597
01:08:01,920 --> 01:08:04,329
i talked about it briefly in uh

1598
01:08:04,480 --> 01:08:05,130
my new book

1599
01:08:05,280 --> 01:08:05,530
um

1600
01:08:05,760 --> 01:08:06,490
a thousand brains

1601
01:08:06,640 --> 01:08:07,450
i don't know

1602
01:08:07,520 --> 01:08:09,770
if we really discussed it in any of our papers yet

1603
01:08:10,079 --> 01:08:10,329
um

1604
01:08:11,359 --> 01:08:15,210
and this is the idea that you're so i can switch into that

1605
01:08:15,220 --> 01:08:16,649
so i'm admitting right

1606
01:08:16,660 --> 01:08:16,890
yeah

1607
01:08:16,960 --> 01:08:18,649
i don't think it's what i said before

1608
01:08:18,960 --> 01:08:19,210
okay

1609
01:08:19,600 --> 01:08:20,009
all right

1610
01:08:20,479 --> 01:08:20,890
we learn

1611
01:08:21,520 --> 01:08:21,770
um

1612
01:08:22,479 --> 01:08:24,170
but there's a much more powerful form of generalization

1613
01:08:24,560 --> 01:08:24,969
if you want

1614
01:08:24,979 --> 01:08:26,170
i can delve into that right now

1615
01:08:26,560 --> 01:08:26,810
um

1616
01:08:27,759 --> 01:08:28,009
uh

1617
01:08:28,479 --> 01:08:29,609
it's not okay

1618
01:08:29,759 --> 01:08:31,049
so so we have

1619
01:08:31,059 --> 01:08:35,929
this is what we've learned is that each core of a column where you say the brain in general

1620
01:08:37,839 --> 01:08:38,089
um

1621
01:08:38,560 --> 01:08:39,689
it's building this sort of

1622
01:08:39,700 --> 01:08:39,929
uh

1623
01:08:40,399 --> 01:08:42,729
the model of the world you can think of as a graph

1624
01:08:42,880 --> 01:08:43,290
but it's

1625
01:08:43,300 --> 01:08:44,569
you can literally think of it as like

1626
01:08:44,578 --> 01:08:46,569
where are things relative to other things

1627
01:08:47,759 --> 01:08:49,290
it's like a computer-aided design

1628
01:08:49,359 --> 01:08:49,609
model

1629
01:08:49,618 --> 01:08:49,850
right

1630
01:08:49,859 --> 01:08:50,330
cad model

1631
01:08:50,719 --> 01:08:51,850
it's literally doing that

1632
01:08:51,920 --> 01:08:54,729
you have a model of something like a computer or your house

1633
01:08:54,880 --> 01:08:55,770
or room or bird

1634
01:08:55,839 --> 01:08:57,290
or ideas that they're structured

1635
01:08:58,158 --> 01:08:59,689
using reference frames and data

1636
01:08:59,839 --> 01:09:00,089
populating

1637
01:09:00,319 --> 01:09:02,250
reference locations and reference frames

1638
01:09:03,279 --> 01:09:04,330
and um

1639
01:09:04,799 --> 01:09:05,609
so you can imagine

1640
01:09:06,319 --> 01:09:06,569
uh

1641
01:09:06,719 --> 01:09:08,408
my knowledge of something again

1642
01:09:08,419 --> 01:09:08,890
as you know

1643
01:09:08,899 --> 01:09:09,210
in the book

1644
01:09:09,220 --> 01:09:10,890
i use the coffee cup example

1645
01:09:11,040 --> 01:09:11,529
quite a bit

1646
01:09:11,538 --> 01:09:12,408
but you can use anything

1647
01:09:12,799 --> 01:09:13,049
um

1648
01:09:13,439 --> 01:09:15,770
a knowledge of say a stapler is

1649
01:09:16,000 --> 01:09:17,689
is it has a certain set of parts

1650
01:09:19,439 --> 01:09:19,689
uh

1651
01:09:19,759 --> 01:09:21,210
which are then arranged in certain

1652
01:09:21,520 --> 01:09:21,770
uh

1653
01:09:22,080 --> 01:09:25,210
relative positions of each other and they have certain movements related to each other

1654
01:09:25,920 --> 01:09:26,569
and um

1655
01:09:27,198 --> 01:09:28,969
and that structure is

1656
01:09:28,979 --> 01:09:29,689
is that graph

1657
01:09:29,839 --> 01:09:30,330
if you will

1658
01:09:30,399 --> 01:09:31,210
is the uh

1659
01:09:31,439 --> 01:09:33,049
is the definition of that object

1660
01:09:33,279 --> 01:09:33,850
and that's your

1661
01:09:34,000 --> 01:09:35,130
that's your model of it

1662
01:09:35,279 --> 01:09:35,929
that's how you

1663
01:09:35,939 --> 01:09:37,290
that's how you represent it in your brain

1664
01:09:37,920 --> 01:09:38,170
now

1665
01:09:38,180 --> 01:09:38,649
when you

1666
01:09:38,719 --> 01:09:39,609
when you learn it

1667
01:09:39,618 --> 01:09:42,969
the way you do this is you attend individual components one at a time

1668
01:09:42,979 --> 01:09:43,448
so if i

1669
01:09:43,520 --> 01:09:45,529
if i were to look at a new object i haven't seen before

1670
01:09:45,538 --> 01:09:46,089
like a stapler

1671
01:09:46,238 --> 01:09:48,890
i would just attend to one end or look at what's what's this thing look like

1672
01:09:48,899 --> 01:09:49,049
then

1673
01:09:49,059 --> 01:09:49,929
what's that thing look like

1674
01:09:50,000 --> 01:09:51,609
and as you attend to these different parts

1675
01:09:52,080 --> 01:09:53,609
you're essentially building this graph

1676
01:09:53,618 --> 01:09:54,890
you're essentially building the model

1677
01:09:54,900 --> 01:09:55,290
you're saying

1678
01:09:55,300 --> 01:09:55,449
okay

1679
01:09:55,459 --> 01:09:55,770
this component

1680
01:09:55,920 --> 01:09:56,330
i recognize

1681
01:09:56,480 --> 01:09:57,130
that's like a hinge

1682
01:09:57,360 --> 01:09:59,210
this component over here looks like maybe a button

1683
01:09:59,520 --> 01:10:01,210
this component here looks a rubber pad

1684
01:10:01,600 --> 01:10:04,250
and i and i just start building this model in my head

1685
01:10:04,260 --> 01:10:06,090
but it looks my phone right thumb up there

1686
01:10:06,320 --> 01:10:06,570
um

1687
01:10:06,719 --> 01:10:07,850
i started building this mod

1688
01:10:07,920 --> 01:10:08,170
okay

1689
01:10:08,180 --> 01:10:08,730
so you got this

1690
01:10:08,800 --> 01:10:10,090
i'm trying to paint a picture in your head

1691
01:10:10,100 --> 01:10:12,330
how you you're doing this every moment of your life

1692
01:10:12,400 --> 01:10:13,050
as you look around

1693
01:10:13,060 --> 01:10:15,770
even like you're looking at the screen right now you're looking to see different things

1694
01:10:15,780 --> 01:10:17,210
you see where they are relative to each other

1695
01:10:19,040 --> 01:10:22,810
i think most generalization in the world comes about from a different from the process

1696
01:10:23,120 --> 01:10:23,370
of

1697
01:10:23,679 --> 01:10:28,890
if i see something new that i haven't seen before or i'm experiencing something new

1698
01:10:28,900 --> 01:10:29,690
doesn't matter revision

1699
01:10:29,840 --> 01:10:31,449
but just imagine you're seeing something new

1700
01:10:31,840 --> 01:10:32,810
you don't recognize it

1701
01:10:32,820 --> 01:10:34,489
you don't see this arrangement as a familiar arrangement

1702
01:10:34,960 --> 01:10:37,370
then you look at a subset of the components and you say

1703
01:10:37,380 --> 01:10:37,530
okay

1704
01:10:37,600 --> 01:10:38,730
let's just focus over here

1705
01:10:38,740 --> 01:10:39,210
and you say

1706
01:10:39,280 --> 01:10:39,530
well

1707
01:10:39,540 --> 01:10:41,929
these three components every year look like something i've seen before

1708
01:10:42,320 --> 01:10:46,570
these look like it might be a button that i pushed down and this thing over here looks

1709
01:10:46,580 --> 01:10:47,530
it might be a hinge

1710
01:10:48,239 --> 01:10:48,489
um

1711
01:10:48,719 --> 01:10:49,290
and then you

1712
01:10:49,360 --> 01:10:49,850
then you say

1713
01:10:49,860 --> 01:10:49,929
well

1714
01:10:49,939 --> 01:10:50,489
if it's a hinge

1715
01:10:50,560 --> 01:10:51,610
then a button with this thing

1716
01:10:51,620 --> 01:10:52,650
even i've never seen it before

1717
01:10:52,660 --> 01:10:53,770
maybe if i push on this button

1718
01:10:53,840 --> 01:10:54,489
the hinge will look

1719
01:10:54,880 --> 01:10:55,850
and and so you

1720
01:10:55,860 --> 01:10:56,090
you

1721
01:10:56,100 --> 01:10:57,290
basically your

1722
01:10:57,520 --> 01:10:59,610
your generalization comes about

1723
01:10:59,620 --> 01:11:01,370
because subsets of these graphs

1724
01:11:01,440 --> 01:11:06,330
that you build of the structure in the world are similar to subsets in other graphs you've learned

1725
01:11:06,960 --> 01:11:07,770
and you can say

1726
01:11:07,780 --> 01:11:07,929
well

1727
01:11:08,000 --> 01:11:09,290
these subsets are similar

1728
01:11:10,000 --> 01:11:12,890
therefore the behavior behaviors associated with those substances are similar

1729
01:11:13,360 --> 01:11:15,050
the performance of the substance is similar

1730
01:11:15,120 --> 01:11:16,730
so i now can ascribe to the stapler

1731
01:11:16,960 --> 01:11:18,570
some things i've learned about other things

1732
01:11:19,120 --> 01:11:19,370
um

1733
01:11:19,760 --> 01:11:20,330
and we keep

1734
01:11:20,480 --> 01:11:21,929
and if i see something really new

1735
01:11:22,000 --> 01:11:25,610
we just keep going down into small and smaller pieces until we find something i recognize

1736
01:11:26,080 --> 01:11:26,409
this next

1737
01:11:26,419 --> 01:11:26,650
okay

1738
01:11:26,719 --> 01:11:26,969
well

1739
01:11:26,979 --> 01:11:27,530
i thought i recognized

1740
01:11:28,400 --> 01:11:29,850
and and so that i believe

1741
01:11:30,239 --> 01:11:30,489
uh

1742
01:11:30,640 --> 01:11:32,969
is actually the primary way we generalize in the world

1743
01:11:33,440 --> 01:11:33,690
um

1744
01:11:33,840 --> 01:11:35,850
we can talk about how str's do that

1745
01:11:36,000 --> 01:11:36,650
do that too

1746
01:11:36,660 --> 01:11:37,130
but anyway

1747
01:11:38,000 --> 01:11:38,810
what you're talking about

1748
01:11:38,880 --> 01:11:42,969
it sounds a lot like sort of some of the graph isomorphism you know

1749
01:11:43,120 --> 01:11:44,650
work that that people talk about where

1750
01:11:44,660 --> 01:11:45,050
if you

1751
01:11:45,060 --> 01:11:47,130
if you've got this new thing and it has all these parts

1752
01:11:47,280 --> 01:11:48,010
and they're in a graph

1753
01:11:48,560 --> 01:11:48,890
you know

1754
01:11:48,900 --> 01:11:51,929
certain subsets of that graph may be isomorphic to previous

1755
01:11:52,640 --> 01:11:53,770
you know graphs that you've seen

1756
01:11:53,920 --> 01:11:55,770
and then therefore you can learn about it

1757
01:11:56,159 --> 01:11:56,409
um

1758
01:11:56,719 --> 01:11:57,850
and you said something really interesting

1759
01:11:57,920 --> 01:12:00,489
which is when you see something new

1760
01:12:00,640 --> 01:12:00,969
you know

1761
01:12:00,979 --> 01:12:04,730
you attend to kind of different parts of that and you're doing kind of a remapping there

1762
01:12:04,960 --> 01:12:07,370
and there's a lot of research on on grid cells

1763
01:12:07,520 --> 01:12:08,330
right that they remap

1764
01:12:08,640 --> 01:12:10,570
so when a rodent goes into a new

1765
01:12:11,040 --> 01:12:11,610
a new environment

1766
01:12:12,080 --> 01:12:13,449
it remaps the grid cells

1767
01:12:14,000 --> 01:12:14,730
can you talk some

1768
01:12:14,740 --> 01:12:15,050
i mean

1769
01:12:15,120 --> 01:12:16,330
is this related to the remapping

1770
01:12:16,640 --> 01:12:18,489
and i'm really curious about how does that remap

1771
01:12:18,719 --> 01:12:22,250
and happen in the neurons and how quickly does it take place

1772
01:12:22,560 --> 01:12:22,969
you know

1773
01:12:23,760 --> 01:12:24,010
yeah

1774
01:12:24,480 --> 01:12:24,730
well

1775
01:12:24,800 --> 01:12:26,090
so one of the the

1776
01:12:26,159 --> 01:12:26,570
the hypothesis

1777
01:12:27,040 --> 01:12:29,610
we have a thousand range theory is that

1778
01:12:29,620 --> 01:12:29,850
well

1779
01:12:30,080 --> 01:12:30,570
first of all

1780
01:12:30,580 --> 01:12:33,690
we deduce the idea that you need some that there is any in the cortex

1781
01:12:33,840 --> 01:12:35,530
there are these reference frames and these structures

1782
01:12:35,679 --> 01:12:36,090
these graphs

1783
01:12:36,239 --> 01:12:36,489
right

1784
01:12:36,499 --> 01:12:38,090
we can deduce that it's like okay

1785
01:12:38,100 --> 01:12:38,330
well

1786
01:12:38,340 --> 01:12:38,969
i know this is happening

1787
01:12:39,120 --> 01:12:40,570
i walked through the logic behind that

1788
01:12:41,280 --> 01:12:41,850
and then it was

1789
01:12:41,860 --> 01:12:42,650
at first to me

1790
01:12:42,660 --> 01:12:43,290
it was very odd

1791
01:12:43,440 --> 01:12:43,690
like

1792
01:12:43,700 --> 01:12:45,210
how could neurons build these graphs

1793
01:12:45,600 --> 01:12:45,850
right

1794
01:12:45,860 --> 01:12:48,409
how and how do they structure them i mean it

1795
01:12:48,480 --> 01:12:51,850
what with the way i used to focus is like my cortex knows

1796
01:12:51,860 --> 01:12:54,090
where my finger is relative to the thing it's touching

1797
01:12:54,640 --> 01:12:55,850
like relative to the thing

1798
01:12:55,860 --> 01:12:56,170
it's tilting

1799
01:12:56,239 --> 01:12:57,290
not relative to my body

1800
01:12:57,920 --> 01:13:00,570
and and so i use the example of coffee cup and the coffee cup blows around

1801
01:13:00,580 --> 01:13:02,090
so the reference brand has to move the coffee up

1802
01:13:02,100 --> 01:13:02,969
i was like holy crap

1803
01:13:02,979 --> 01:13:03,530
how does that happen

1804
01:13:04,000 --> 01:13:05,610
it's really hard for neurons to do this

1805
01:13:05,920 --> 01:13:06,330
and then

1806
01:13:06,400 --> 01:13:09,290
of course we looked into the literature of the hippocampal complex

1807
01:13:09,520 --> 01:13:11,210
which is grid cells in the anterional cortex

1808
01:13:11,440 --> 01:13:14,570
and place cells in the hippocampus and there's a whole bunch of stuff

1809
01:13:14,580 --> 01:13:14,890
and there's

1810
01:13:14,960 --> 01:13:17,770
there's a 20 year or 30 year history of research in this field

1811
01:13:17,840 --> 01:13:18,570
it's not in the neocortex

1812
01:13:18,960 --> 01:13:21,530
but it's it's something that's very related to the neurocortex

1813
01:13:22,480 --> 01:13:22,730
um

1814
01:13:23,280 --> 01:13:24,090
and and so

1815
01:13:24,480 --> 01:13:25,290
and that's what they

1816
01:13:25,300 --> 01:13:27,850
it's clear that grid cells and place cells and these other types

1817
01:13:27,920 --> 01:13:29,130
these vector cells that exist

1818
01:13:29,140 --> 01:13:30,890
there are building maps just like this

1819
01:13:31,360 --> 01:13:31,610
um

1820
01:13:32,000 --> 01:13:32,650
and so we said

1821
01:13:32,660 --> 01:13:32,890
okay

1822
01:13:32,960 --> 01:13:33,210
well

1823
01:13:33,360 --> 01:13:38,250
it's likely then that the the same mechanisms that are used

1824
01:13:38,260 --> 01:13:40,570
by grid cells are probably going to be used in the cortex

1825
01:13:41,760 --> 01:13:45,130
and the time we made that that was a purely evolutionary argument

1826
01:13:45,199 --> 01:13:45,850
it was like saying

1827
01:13:46,320 --> 01:13:49,370
it seems unlikely evolution is going to discover something like this twice

1828
01:13:49,440 --> 01:13:50,730
it's really hard to do right

1829
01:13:50,740 --> 01:13:51,850
it's really really hard to do this

1830
01:13:51,860 --> 01:13:52,010
so

1831
01:13:52,320 --> 01:13:54,969
and animals have to know where they were in the world a long time ago

1832
01:13:55,040 --> 01:13:58,570
so they probably evolved this complex mechanism from knowing where they are

1833
01:13:58,580 --> 01:14:01,370
and now the brain's gonna use that same mechanism for knowing where

1834
01:14:01,380 --> 01:14:03,210
like your eyes are or your fingers are

1835
01:14:03,220 --> 01:14:03,929
or you know

1836
01:14:03,939 --> 01:14:05,530
it's gonna be the same mechanism for everything else

1837
01:14:06,080 --> 01:14:07,210
and so we just said

1838
01:14:07,220 --> 01:14:07,370
hey

1839
01:14:07,380 --> 01:14:11,210
it's probably likely that the same basic mechanism that uses good cells

1840
01:14:11,220 --> 01:14:13,090
and place cells and vector cells is going on cortex

1841
01:14:13,760 --> 01:14:15,449
at the time we made that that prediction

1842
01:14:15,760 --> 01:14:18,090
we were not aware of any evidence of that was the case

1843
01:14:18,560 --> 01:14:18,810
um

1844
01:14:19,280 --> 01:14:21,290
and now there's a lot of evidence for it

1845
01:14:21,300 --> 01:14:22,969
now people are finding fit cells throughout the cortex

1846
01:14:23,600 --> 01:14:23,850
um

1847
01:14:24,400 --> 01:14:28,170
so so the general idea i'll keep get into your details in a moment

1848
01:14:28,180 --> 01:14:29,130
but the general idea that

1849
01:14:29,140 --> 01:14:29,290
hey

1850
01:14:29,300 --> 01:14:31,610
the same mechanisms are work in both places

1851
01:14:31,760 --> 01:14:32,409
seems to be right

1852
01:14:33,040 --> 01:14:35,530
and and now we don't understand those mechanisms completely

1853
01:14:36,080 --> 01:14:39,130
even though there are 30 years of research on grid cells and place cells

1854
01:14:39,199 --> 01:14:40,890
it's there's still some mysteries about them

1855
01:14:41,280 --> 01:14:41,530
um

1856
01:14:42,000 --> 01:14:43,050
but there's still a lot

1857
01:14:43,060 --> 01:14:44,409
but there's a lot that's known

1858
01:14:45,040 --> 01:14:45,770
and so

1859
01:14:46,320 --> 01:14:46,570
um

1860
01:14:46,960 --> 01:14:49,850
we can map on the concepts i was talking about like

1861
01:14:49,860 --> 01:14:50,090
okay

1862
01:14:50,719 --> 01:14:50,969
um

1863
01:14:51,440 --> 01:14:52,330
where am i

1864
01:14:52,400 --> 01:14:53,530
where am i in this graph

1865
01:14:53,760 --> 01:14:55,290
what's what do i place in that graph

1866
01:14:55,679 --> 01:14:57,690
you can think of that a little bit like place cells

1867
01:14:58,400 --> 01:14:58,650
um

1868
01:14:58,800 --> 01:14:59,449
in grid cells

1869
01:14:59,920 --> 01:15:00,969
you can think of remapping

1870
01:15:01,679 --> 01:15:01,929
is

1871
01:15:02,159 --> 01:15:02,409
um

1872
01:15:02,640 --> 01:15:05,050
there's two types of remapping like this is a whole new thing

1873
01:15:05,060 --> 01:15:06,409
so let's just start with a new graph

1874
01:15:06,960 --> 01:15:07,449
or versus

1875
01:15:07,679 --> 01:15:09,370
let's fix the graph i already got

1876
01:15:09,679 --> 01:15:10,010
you know

1877
01:15:10,560 --> 01:15:11,130
and uh

1878
01:15:11,199 --> 01:15:12,570
and that's that's very much related

1879
01:15:12,719 --> 01:15:14,810
as you said to remapping of grid cells

1880
01:15:15,199 --> 01:15:16,570
not that we understand that completely

1881
01:15:16,800 --> 01:15:17,210
we don't

1882
01:15:17,280 --> 01:15:17,770
no one does

1883
01:15:18,000 --> 01:15:18,250
um

1884
01:15:18,560 --> 01:15:19,850
but we know a lot about it

1885
01:15:19,860 --> 01:15:20,170
it's a very

1886
01:15:20,480 --> 01:15:22,409
very technical field in the neuroscience solution

1887
01:15:22,640 --> 01:15:22,890
right

1888
01:15:23,600 --> 01:15:25,610
i've been fascinated by douglas hofstadter

1889
01:15:26,480 --> 01:15:29,290
he has a really um interesting idea on cognition

1890
01:15:29,920 --> 01:15:33,690
and he says that cognition is a little bit like the interstate freeway

1891
01:15:35,199 --> 01:15:35,449
um

1892
01:15:35,920 --> 01:15:38,969
in the sense that we we have this ability to make analogies

1893
01:15:39,600 --> 01:15:42,330
and this is something that current ai systems cannot do

1894
01:15:42,340 --> 01:15:44,170
and a lot of the symbolic folks are saying

1895
01:15:44,180 --> 01:15:44,409
well

1896
01:15:45,280 --> 01:15:45,610
of course

1897
01:15:45,620 --> 01:15:46,969
deep learning models can't do it

1898
01:15:46,979 --> 01:15:48,570
you're just interpolating on a manifold

1899
01:15:48,800 --> 01:15:49,530
for goodness sake

1900
01:15:49,540 --> 01:15:50,170
but um

1901
01:15:50,480 --> 01:15:51,610
your conception of reference

1902
01:15:51,760 --> 01:15:55,210
frames i i found fascinating because they're not just on your sensory inputs

1903
01:15:55,360 --> 01:16:02,090
there's you actually said that thinking is a little bit like traversing through concept space through these reference frames

1904
01:16:02,560 --> 01:16:04,090
and that's absolutely fascinating

1905
01:16:04,480 --> 01:16:04,730
so

1906
01:16:04,740 --> 01:16:05,370
but the thing is

1907
01:16:05,380 --> 01:16:05,929
when we learn information

1908
01:16:06,159 --> 01:16:07,929
many of us learn information differently

1909
01:16:08,080 --> 01:16:10,170
but we learn the same knowledge and the same facts

1910
01:16:10,239 --> 01:16:10,570
you know

1911
01:16:10,580 --> 01:16:10,730
we

1912
01:16:10,800 --> 01:16:11,050
we

1913
01:16:11,060 --> 01:16:12,170
we all know how to speak

1914
01:16:12,320 --> 01:16:12,570
um

1915
01:16:12,580 --> 01:16:13,210
the same language

1916
01:16:13,360 --> 01:16:15,050
and you did allude to this in your book

1917
01:16:15,060 --> 01:16:18,489
that there are some interesting differences in the topology of how you learn information

1918
01:16:18,880 --> 01:16:21,929
but we still seem to be able to reconcile it in the same way

1919
01:16:22,719 --> 01:16:22,969
yeah

1920
01:16:22,979 --> 01:16:23,130
well

1921
01:16:23,140 --> 01:16:26,090
sometimes you know it's it's like the way i view it

1922
01:16:26,100 --> 01:16:26,250
uh

1923
01:16:26,260 --> 01:16:26,489
again

1924
01:16:26,499 --> 01:16:27,530
we're telling stuff that some

1925
01:16:27,540 --> 01:16:29,130
some of this hasn't been published anywhere yet

1926
01:16:29,199 --> 01:16:29,449
um

1927
01:16:29,840 --> 01:16:31,449
but the way i view it is

1928
01:16:31,600 --> 01:16:31,850
um

1929
01:16:32,560 --> 01:16:35,370
if i think of myself as this part of the cortex says the cortical call

1930
01:16:36,080 --> 01:16:36,330
um

1931
01:16:36,480 --> 01:16:37,850
i don't know what come my input represents

1932
01:16:38,800 --> 01:16:40,730
and i don't know where i am in the world

1933
01:16:40,740 --> 01:16:41,050
i don't

1934
01:16:41,060 --> 01:16:41,850
i just don't know anything

1935
01:16:41,860 --> 01:16:43,130
i'm just a bunch of neurons right

1936
01:16:43,440 --> 01:16:45,449
and so i have to discover what i'm getting

1937
01:16:45,520 --> 01:16:46,650
i get two pieces of information

1938
01:16:47,120 --> 01:16:53,449
i get some sensory data and i also get information about how my sensor is moving in the world

1939
01:16:53,840 --> 01:16:54,090
yeah

1940
01:16:54,159 --> 01:16:55,210
we can talk about how that happens

1941
01:16:55,360 --> 01:16:56,090
but i get

1942
01:16:56,100 --> 01:16:56,730
i get some knowledge

1943
01:16:56,960 --> 01:16:59,690
and so i have to figure out on my own

1944
01:16:59,760 --> 01:17:01,370
what is the dimensionality of the space

1945
01:17:01,760 --> 01:17:03,050
what kind of space is it

1946
01:17:03,060 --> 01:17:03,850
i i don't have a preconceived

1947
01:17:04,159 --> 01:17:04,570
it's not like

1948
01:17:04,580 --> 01:17:04,650
oh

1949
01:17:04,660 --> 01:17:06,090
the world is three-dimensional i don't know that

1950
01:17:06,719 --> 01:17:06,969
um

1951
01:17:07,040 --> 01:17:11,050
i don't know if it's two-dimensional three-dimensional or in-dimensional i just kind of figure out

1952
01:17:11,060 --> 01:17:15,530
i have to figure out what is the structure of which i'm going to to place this information

1953
01:17:15,920 --> 01:17:19,929
and when you and i look at a chair we both see it as three-dimensional we

1954
01:17:19,939 --> 01:17:21,130
it is a three-dimensional object

1955
01:17:21,280 --> 01:17:23,290
we're going to build three-dimensional reference frames for that chair

1956
01:17:23,300 --> 01:17:24,330
they're going to be pretty similar

1957
01:17:24,400 --> 01:17:26,330
it's going to be hard for you to have a representation of the chairs

1958
01:17:26,400 --> 01:17:28,170
it's significantly different than mine

1959
01:17:28,719 --> 01:17:28,969
um

1960
01:17:29,040 --> 01:17:32,010
but when we look at things that we might not sense directly

1961
01:17:32,320 --> 01:17:34,489
something that someone tells me about or i'm reading about

1962
01:17:34,499 --> 01:17:34,890
in a book

1963
01:17:35,199 --> 01:17:38,890
the kind of reference frames you and i might create for the same facts could be very different

1964
01:17:39,760 --> 01:17:42,650
and and so we can arrange the same data in different reference frames

1965
01:17:42,719 --> 01:17:47,370
because we can't directly sense it we're basically relying on information coming from other people

1966
01:17:48,000 --> 01:17:49,530
and so we can end up with

1967
01:17:49,540 --> 01:17:52,650
like the same facts represented differently and reach to different conclusions

1968
01:17:53,280 --> 01:17:55,370
and this is why we have different beliefs about the world

1969
01:17:55,380 --> 01:17:57,370
it's almost always about things we can't sense directly

1970
01:17:58,000 --> 01:17:58,969
if we can sense them directly

1971
01:17:59,040 --> 01:18:00,810
then we generally have the same belief about it

1972
01:18:00,820 --> 01:18:01,050
you know

1973
01:18:01,679 --> 01:18:03,690
but numb the same kind of structure

1974
01:18:04,159 --> 01:18:05,850
so i think this is a fascinating idea

1975
01:18:06,080 --> 01:18:06,330
um

1976
01:18:07,040 --> 01:18:07,850
and um

1977
01:18:08,400 --> 01:18:08,650
uh

1978
01:18:08,719 --> 01:18:09,130
but it's

1979
01:18:09,140 --> 01:18:10,409
it's really fascinating to me

1980
01:18:10,419 --> 01:18:10,570
is

1981
01:18:10,580 --> 01:18:17,290
that there is no pre a notion about the dimensionality of the world that a column is looking at

1982
01:18:17,360 --> 01:18:18,170
and and the data

1983
01:18:18,239 --> 01:18:20,090
so it can represent almost any kind of data

1984
01:18:20,159 --> 01:18:20,890
as long as you can

1985
01:18:21,120 --> 01:18:22,810
could figure out from a movement vector

1986
01:18:23,280 --> 01:18:23,850
in a sense

1987
01:18:24,320 --> 01:18:25,770
lumen data and sense data

1988
01:18:26,080 --> 01:18:26,650
they can say

1989
01:18:26,660 --> 01:18:28,489
can i build a map of this thing that makes sense

1990
01:18:28,499 --> 01:18:30,489
the predictive map and um

1991
01:18:31,199 --> 01:18:31,449
uh

1992
01:18:31,520 --> 01:18:32,170
and if i can

1993
01:18:32,180 --> 01:18:32,570
i said

1994
01:18:32,580 --> 01:18:32,810
okay

1995
01:18:32,820 --> 01:18:33,290
that's my

1996
01:18:33,300 --> 01:18:35,130
that's my understanding of it

1997
01:18:35,140 --> 01:18:35,290
another

1998
01:18:35,440 --> 01:18:39,290
another beautiful thing about it is is the fact that your brain is

1999
01:18:39,300 --> 01:18:41,770
is creating these binary encodings at all

2000
01:18:42,080 --> 01:18:42,570
like i

2001
01:18:42,580 --> 01:18:43,690
when i was researching this

2002
01:18:43,700 --> 01:18:46,489
i found it very interesting that for grid cells

2003
01:18:46,719 --> 01:18:48,250
there is some some topography

2004
01:18:48,719 --> 01:18:51,290
at least in the visual cortex or in the visual layer

2005
01:18:51,360 --> 01:18:51,690
i'm sorry

2006
01:18:51,760 --> 01:18:52,409
so you know

2007
01:18:52,419 --> 01:18:56,010
grid cells that are nearby each other will have like a similar orientation

2008
01:18:56,640 --> 01:18:57,770
but maybe a different scale

2009
01:18:58,560 --> 01:19:00,090
but once you go up a level higher

2010
01:19:00,239 --> 01:19:01,690
so those things create the sdr

2011
01:19:02,159 --> 01:19:02,489
you know

2012
01:19:02,499 --> 01:19:04,730
they create these bits that are very sparsely populated

2013
01:19:05,520 --> 01:19:08,969
and then the place cells correspond to combinations of those

2014
01:19:09,520 --> 01:19:11,530
those bits to indicate a certain location

2015
01:19:12,239 --> 01:19:13,370
and once you're at that layer

2016
01:19:13,600 --> 01:19:14,570
there is no more topography

2017
01:19:15,280 --> 01:19:18,969
it's like now they've become truly digital encodings of information

2018
01:19:19,600 --> 01:19:19,850
right

2019
01:19:20,400 --> 01:19:20,650
well

2020
01:19:20,660 --> 01:19:21,210
i'm not sure

2021
01:19:21,280 --> 01:19:22,409
i'm not sure i understand that

2022
01:19:22,419 --> 01:19:22,810
because i

2023
01:19:22,880 --> 01:19:23,449
it's my

2024
01:19:23,840 --> 01:19:24,409
from my understanding

2025
01:19:24,560 --> 01:19:25,770
there's always going to be a topology

2026
01:19:26,320 --> 01:19:27,449
you're never going to lose it

2027
01:19:27,459 --> 01:19:27,610
uh

2028
01:19:27,760 --> 01:19:29,690
you may not see that topology in the place else

2029
01:19:29,700 --> 01:19:29,929
right

2030
01:19:30,400 --> 01:19:30,650
um

2031
01:19:31,440 --> 01:19:31,850
all right

2032
01:19:31,920 --> 01:19:32,489
that's that

2033
01:19:33,040 --> 01:19:33,530
i mean this

2034
01:19:34,400 --> 01:19:35,929
the neuroscientists who listen to white

2035
01:19:35,939 --> 01:19:36,489
listen to this

2036
01:19:36,499 --> 01:19:36,810
i don't

2037
01:19:36,820 --> 01:19:37,610
i don't want to insult them

2038
01:19:37,620 --> 01:19:39,130
because i'm going to make this very simple

2039
01:19:39,600 --> 01:19:42,890
i say you can think of like place cells are like what

2040
01:19:43,440 --> 01:19:44,810
and grit cells are like

2041
01:19:44,820 --> 01:19:45,050
right

2042
01:19:45,679 --> 01:19:45,929
okay

2043
01:19:46,080 --> 01:19:46,330
so

2044
01:19:46,880 --> 01:19:47,130
um

2045
01:19:47,600 --> 01:19:48,250
and you can say

2046
01:19:48,260 --> 01:19:48,409
well

2047
01:19:48,419 --> 01:19:48,890
this thing

2048
01:19:48,900 --> 01:19:50,250
the what thing is at this location

2049
01:19:50,480 --> 01:19:51,050
type of stuff

2050
01:19:51,360 --> 01:19:52,890
and so if you just look at the place cells

2051
01:19:52,960 --> 01:19:53,290
you don't

2052
01:19:53,840 --> 01:19:57,050
this isn't because they actually do uniquely encode places

2053
01:19:57,060 --> 01:19:57,530
so this is

2054
01:19:57,540 --> 01:19:58,890
it gets a little confusing here

2055
01:19:58,900 --> 01:19:59,530
but um

2056
01:20:00,159 --> 01:20:00,409
uh

2057
01:20:00,800 --> 01:20:02,890
but you can think of it just generally like that

2058
01:20:03,280 --> 01:20:03,530
um

2059
01:20:04,000 --> 01:20:05,530
if we're going to go deeper about this

2060
01:20:05,540 --> 01:20:09,530
we're going to start getting into all the unknowns about grid sales and play cells and they're very interesting

2061
01:20:10,719 --> 01:20:12,250
the unknown and weird properties

2062
01:20:12,960 --> 01:20:13,210
um

2063
01:20:13,600 --> 01:20:14,889
we actually think now that

2064
01:20:15,120 --> 01:20:15,370
um

2065
01:20:16,000 --> 01:20:18,010
this is work that one of our employees marcus

2066
01:20:18,320 --> 01:20:18,570
um

2067
01:20:18,960 --> 01:20:20,570
is doing is we think

2068
01:20:20,580 --> 01:20:20,810
actually

2069
01:20:20,880 --> 01:20:21,130
the

2070
01:20:21,360 --> 01:20:23,370
the better way to think about a coding is not

2071
01:20:23,520 --> 01:20:26,010
the location is not actually through the grid cells

2072
01:20:26,020 --> 01:20:27,290
but through these vector cells

2073
01:20:27,920 --> 01:20:29,610
things are things like optic vector cells

2074
01:20:30,000 --> 01:20:30,570
and so on

2075
01:20:30,880 --> 01:20:31,130
um

2076
01:20:31,280 --> 01:20:31,530
it's

2077
01:20:31,600 --> 01:20:32,889
it's in the

2078
01:20:32,899 --> 01:20:36,730
the grid cells actually might just play a very interesting singular role

2079
01:20:36,740 --> 01:20:39,050
they might play the role of what we call path integration

2080
01:20:39,840 --> 01:20:40,489
which is like

2081
01:20:40,639 --> 01:20:42,010
if i'm moving in a certain direction

2082
01:20:42,159 --> 01:20:43,850
i have to be able to predict where i'll be

2083
01:20:44,400 --> 01:20:44,650
uh

2084
01:20:45,120 --> 01:20:45,770
as i move

2085
01:20:46,159 --> 01:20:47,050
that's called path integration

2086
01:20:47,760 --> 01:20:50,570
i'm going to predict something i have to know where i'm going to be when i get there

2087
01:20:50,960 --> 01:20:51,210
um

2088
01:20:51,760 --> 01:20:53,370
and that most of the max

2089
01:20:53,600 --> 01:20:55,929
the graph itself is not built up of grid cells

2090
01:20:56,000 --> 01:20:56,969
it's built up these objects

2091
01:20:57,280 --> 01:20:57,530
these

2092
01:20:57,600 --> 01:20:57,850
these

2093
01:20:57,860 --> 01:21:00,409
more like these polar coordinate cells called vector cells

2094
01:21:01,199 --> 01:21:01,449
um

2095
01:21:01,679 --> 01:21:02,330
so again

2096
01:21:02,400 --> 01:21:04,489
this is an evolving field and you know

2097
01:21:04,499 --> 01:21:05,130
and even stuff

2098
01:21:05,140 --> 01:21:05,530
we wrote

2099
01:21:05,540 --> 01:21:06,170
a couple years ago

2100
01:21:06,180 --> 01:21:06,570
we're like

2101
01:21:06,580 --> 01:21:06,730
oh

2102
01:21:06,740 --> 01:21:07,530
maybe that's not right

2103
01:21:07,600 --> 01:21:08,170
it's like this

2104
01:21:08,320 --> 01:21:10,010
but the overall principles are the same

2105
01:21:10,080 --> 01:21:10,730
it's still a graph

2106
01:21:10,880 --> 01:21:11,370
it's still

2107
01:21:11,600 --> 01:21:14,330
you know data at locations in this graph

2108
01:21:14,960 --> 01:21:15,210
um

2109
01:21:15,840 --> 01:21:16,650
that hasn't changed

2110
01:21:16,880 --> 01:21:17,130
uh

2111
01:21:17,140 --> 01:21:17,690
it's just like

2112
01:21:17,700 --> 01:21:18,889
how's the graph actually constructed

2113
01:21:19,280 --> 01:21:20,090
that's a little tricky

2114
01:21:20,400 --> 01:21:20,650
right

2115
01:21:20,660 --> 01:21:22,810
i mean it's like it's fascinatingly deep

2116
01:21:22,880 --> 01:21:24,489
it's just a fascinatingly deep feel

2117
01:21:24,560 --> 01:21:26,969
but if i can just tie this back to sdrs

2118
01:21:27,440 --> 01:21:28,170
because i know there's

2119
01:21:28,320 --> 01:21:32,810
there's always a lot of really confusion or uncertainty about the value of sdrs

2120
01:21:33,120 --> 01:21:34,409
and just for our listeners sake

2121
01:21:34,419 --> 01:21:37,290
i wanted to share one intuition and get your take on it

2122
01:21:37,520 --> 01:21:38,969
which is if we are representing

2123
01:21:39,600 --> 01:21:40,010
let's say

2124
01:21:40,020 --> 01:21:40,250
location

2125
01:21:40,880 --> 01:21:41,130
uh

2126
01:21:41,199 --> 01:21:41,850
in some space

2127
01:21:42,560 --> 01:21:44,969
if all we need is 20 bits to do that

2128
01:21:45,120 --> 01:21:49,690
so we really only need 20 bits of information to represent a space location

2129
01:21:50,000 --> 01:21:51,449
to the resolution that we need

2130
01:21:51,920 --> 01:21:52,170
then

2131
01:21:52,180 --> 01:21:53,370
if you go further than that

2132
01:21:53,600 --> 01:21:56,810
if you have 10 000 bits and you decide i'm not going to use 20

2133
01:21:56,880 --> 01:21:58,489
i'm going to use all 10

2134
01:21:58,560 --> 01:21:59,290
000 put

2135
01:21:59,300 --> 01:21:59,770
like you know

2136
01:21:59,780 --> 01:22:02,330
little. 01 values kind of all throughout there

2137
01:22:02,800 --> 01:22:03,050
really

2138
01:22:03,060 --> 01:22:08,170
you're just creating noise and that other space that you could have been using for other purposes

2139
01:22:08,639 --> 01:22:08,889
right

2140
01:22:08,960 --> 01:22:11,370
and so i think that's one advantage of the sdrs

2141
01:22:11,760 --> 01:22:12,010
like

2142
01:22:12,020 --> 01:22:12,969
what do you think about that intuition

2143
01:22:14,400 --> 01:22:15,610
that's that's a little bit correct

2144
01:22:15,760 --> 01:22:16,170
i i

2145
01:22:16,800 --> 01:22:17,050
i

2146
01:22:17,600 --> 01:22:19,130
i haven't thought of it that way

2147
01:22:19,280 --> 01:22:19,530
um

2148
01:22:20,560 --> 01:22:21,370
let me give you some

2149
01:22:21,380 --> 01:22:23,449
let me just give you my take on some of the advantages of seos

2150
01:22:23,840 --> 01:22:25,050
so let's say i'm going to represent

2151
01:22:26,000 --> 01:22:26,250
uh

2152
01:22:26,480 --> 01:22:26,730
something

2153
01:22:26,960 --> 01:22:27,929
i have 5 000 neurons

2154
01:22:28,560 --> 01:22:29,210
i'm going to represent

2155
01:22:29,440 --> 01:22:29,690
so

2156
01:22:30,320 --> 01:22:30,650
all right

2157
01:22:30,660 --> 01:22:34,489
so let's talk about the capacity and let's say i'm going to use a 2 sparsities

2158
01:22:34,960 --> 01:22:39,130
so i have 100 active neurons and five four thousand

2159
01:22:39,280 --> 01:22:39,530
nine

2160
01:22:39,540 --> 01:22:39,690
hundred

2161
01:22:40,560 --> 01:22:40,810
um

2162
01:22:40,880 --> 01:22:41,690
they act if not

2163
01:22:42,560 --> 01:22:42,810
and

2164
01:22:42,880 --> 01:22:43,130
um

2165
01:22:43,760 --> 01:22:44,170
first thing

2166
01:22:44,180 --> 01:22:44,409
you say

2167
01:22:44,419 --> 01:22:45,530
what is the capacity of the system

2168
01:22:45,600 --> 01:22:47,850
how many different representations you can represent

2169
01:22:48,080 --> 01:22:48,489
so that's

2170
01:22:48,560 --> 01:22:50,250
that's just five thousand choose

2171
01:22:50,560 --> 01:22:51,050
you know 100

2172
01:22:51,199 --> 01:22:51,610
and it's

2173
01:22:51,620 --> 01:22:51,929
you know

2174
01:22:52,080 --> 01:22:53,130
gazillion because i'm in gaza

2175
01:22:54,800 --> 01:22:56,250
so then you can say okay

2176
01:22:56,260 --> 01:22:57,370
so there's no representational capacity

2177
01:22:57,920 --> 01:22:58,170
you

2178
01:22:58,180 --> 01:22:59,929
then you see if i randomly chose these sdrs

2179
01:23:00,639 --> 01:23:00,889
um

2180
01:23:01,520 --> 01:23:02,250
what is their overlap

2181
01:23:02,480 --> 01:23:02,730
well

2182
01:23:02,880 --> 01:23:05,210
they'll overlap by about two neurons each

2183
01:23:05,600 --> 01:23:08,409
because each neuron is going to participate in every 50 patterns

2184
01:23:09,120 --> 01:23:09,370
um

2185
01:23:10,000 --> 01:23:11,210
but not by a lot more

2186
01:23:11,220 --> 01:23:15,130
you won't find two patterns randomly overlapping by 50 bits or 30 bits

2187
01:23:15,140 --> 01:23:16,730
it's all statistically impossible

2188
01:23:18,080 --> 01:23:18,810
so you can

2189
01:23:18,820 --> 01:23:20,170
now you have all these these patterns

2190
01:23:20,320 --> 01:23:23,370
you can pick randomly all day long and they're not going to overlap

2191
01:23:23,440 --> 01:23:24,250
so they're very unique

2192
01:23:24,880 --> 01:23:27,130
each one is going to be extremely robust to noise

2193
01:23:27,199 --> 01:23:31,690
so you can add 50 noise to any of these patterns and they're still going to be recognized correctly

2194
01:23:32,239 --> 01:23:32,489
okay

2195
01:23:32,639 --> 01:23:34,010
so very noise

2196
01:23:34,080 --> 01:23:35,850
robust is another um thing

2197
01:23:35,860 --> 01:23:36,330
so high capacity

2198
01:23:37,120 --> 01:23:38,330
uh representation is very noisable

2199
01:23:39,040 --> 01:23:40,489
there's another property that

2200
01:23:40,719 --> 01:23:40,969
uh

2201
01:23:40,979 --> 01:23:42,090
we think is being used everywhere

2202
01:23:42,719 --> 01:23:43,610
which is really interesting

2203
01:23:43,760 --> 01:23:45,290
i don't know if there's any equivalent to it

2204
01:23:45,300 --> 01:23:48,090
other types of networks is what we call the union property

2205
01:23:48,880 --> 01:23:50,330
where i could invoke

2206
01:23:50,639 --> 01:23:50,889
um

2207
01:23:51,199 --> 01:23:52,170
not one pattern

2208
01:23:52,320 --> 01:23:52,810
but two

2209
01:23:52,880 --> 01:23:53,130
three

2210
01:23:53,140 --> 01:23:53,370
five

2211
01:23:53,380 --> 01:23:54,409
ten poundings at once

2212
01:23:55,440 --> 01:23:56,730
and so now

2213
01:23:56,740 --> 01:23:58,650
instead of having a hundred neurons active

2214
01:23:58,800 --> 01:24:00,010
i say i vote ten patterns

2215
01:24:00,159 --> 01:24:01,530
i'm gonna have a thousand neurons active

2216
01:24:01,760 --> 01:24:03,290
thousand out of five thousand

2217
01:24:04,159 --> 01:24:04,409
well

2218
01:24:04,419 --> 01:24:08,170
it turns out that all the processing that you do still works on all those

2219
01:24:08,239 --> 01:24:09,530
you can process them in parallel

2220
01:24:09,760 --> 01:24:10,889
there's you get

2221
01:24:10,960 --> 01:24:11,530
even though you're

2222
01:24:11,600 --> 01:24:12,810
you're mixing these things together

2223
01:24:12,960 --> 01:24:13,610
the other ends

2224
01:24:14,080 --> 01:24:16,570
the neurons that are that are detecting these patterns

2225
01:24:16,639 --> 01:24:17,290
don't get confused

2226
01:24:17,600 --> 01:24:18,570
and you can show this mathematically

2227
01:24:19,520 --> 01:24:19,770
so

2228
01:24:19,780 --> 01:24:23,210
instead of representing probabilities like a probability distribution

2229
01:24:23,760 --> 01:24:27,850
you might think about the brain often relies on this union property

2230
01:24:28,000 --> 01:24:28,409
which is

2231
01:24:28,419 --> 01:24:28,730
you know

2232
01:24:28,740 --> 01:24:30,489
there are multiple possibilities right now

2233
01:24:30,639 --> 01:24:32,409
it's not really a probability distribution

2234
01:24:32,719 --> 01:24:35,290
it's just like these are all possible things that could be happening right now

2235
01:24:35,679 --> 01:24:38,810
let's process them all in parallel and see which one works out a superposition

2236
01:24:39,120 --> 01:24:40,330
that's the kind of property that

2237
01:24:40,880 --> 01:24:41,130
yeah

2238
01:24:41,280 --> 01:24:42,170
i guess you could call that

2239
01:24:42,180 --> 01:24:42,330
yeah

2240
01:24:42,719 --> 01:24:42,969
um

2241
01:24:43,199 --> 01:24:44,250
and you only get the sparser

2242
01:24:44,480 --> 01:24:44,969
you are

2243
01:24:44,979 --> 01:24:46,730
the more of those you can do and the whole things become

2244
01:24:46,740 --> 01:24:48,250
so you got this huge capacity

2245
01:24:49,120 --> 01:24:50,010
super noise

2246
01:24:50,080 --> 01:24:50,330
robustness

2247
01:24:51,360 --> 01:24:51,610
um

2248
01:24:51,840 --> 01:24:52,489
it only takes

2249
01:24:52,639 --> 01:24:52,969
you know

2250
01:24:52,979 --> 01:24:55,290
if i want to recognize one of these patterns of 100 neurons

2251
01:24:55,440 --> 01:24:57,850
i only have to recognize i only have to connect to 20 of them

2252
01:24:58,239 --> 01:25:01,130
so so neurons typically don't really detect mana

2253
01:25:01,600 --> 01:25:02,250
they look for

2254
01:25:02,260 --> 01:25:05,290
maybe up to about 20 or 30 synapses at maximum

2255
01:25:06,000 --> 01:25:06,969
um at a time

2256
01:25:07,520 --> 01:25:08,409
so so all these

2257
01:25:08,419 --> 01:25:09,130
these are other properties

2258
01:25:09,760 --> 01:25:10,889
all these properties come about

2259
01:25:10,899 --> 01:25:11,130
yeah

2260
01:25:11,840 --> 01:25:12,889
since you did mention capacity

2261
01:25:13,120 --> 01:25:14,090
though i would like to

2262
01:25:14,100 --> 01:25:14,730
because i did see

2263
01:25:15,199 --> 01:25:17,210
you know some variable statements

2264
01:25:17,600 --> 01:25:20,010
sometimes i've seen no problem whatsoever with capacity

2265
01:25:20,960 --> 01:25:21,210
mathematically

2266
01:25:21,600 --> 01:25:22,570
we've got tons of it

2267
01:25:22,580 --> 01:25:24,730
but i was listening to one of your research calls

2268
01:25:24,800 --> 01:25:25,130
i guess

2269
01:25:25,199 --> 01:25:26,570
and you started to question

2270
01:25:26,719 --> 01:25:27,290
you know capacity

2271
01:25:27,600 --> 01:25:28,010
i think

2272
01:25:28,020 --> 01:25:28,650
at least in the

2273
01:25:28,660 --> 01:25:29,929
maybe the the grid cell

2274
01:25:30,080 --> 01:25:30,489
you know

2275
01:25:30,499 --> 01:25:31,290
the inter rhino cortex

2276
01:25:31,600 --> 01:25:32,730
or or some such

2277
01:25:32,740 --> 01:25:34,409
so i guess i have two questions

2278
01:25:34,480 --> 01:25:34,730
yeah

2279
01:25:34,740 --> 01:25:35,210
one is

2280
01:25:35,600 --> 01:25:37,449
is capacity a concern or not a concern

2281
01:25:37,840 --> 01:25:38,330
and also

2282
01:25:38,960 --> 01:25:39,210
uh

2283
01:25:39,520 --> 01:25:41,370
in order to make best use of the capacity

2284
01:25:41,760 --> 01:25:44,090
i'm sure the brain must have some type of entanglement

2285
01:25:44,560 --> 01:25:45,370
so you know

2286
01:25:45,380 --> 01:25:47,530
machine learned artificial networks

2287
01:25:48,000 --> 01:25:49,449
we know that they entangle representations

2288
01:25:50,239 --> 01:25:50,650
so the

2289
01:25:50,880 --> 01:25:51,210
you know

2290
01:25:51,220 --> 01:25:53,290
there's like weight sharing between different representations

2291
01:25:53,840 --> 01:25:56,730
do you think something similar is happening in

2292
01:25:56,740 --> 01:25:56,889
uh

2293
01:25:56,899 --> 01:25:58,090
biological networks

2294
01:25:59,520 --> 01:25:59,770
well

2295
01:25:59,780 --> 01:26:00,650
i i don't know

2296
01:26:00,880 --> 01:26:01,130
uh

2297
01:26:01,140 --> 01:26:01,370
the

2298
01:26:01,440 --> 01:26:01,690
the

2299
01:26:01,760 --> 01:26:02,570
the machine learning

2300
01:26:02,800 --> 01:26:03,050
uh

2301
01:26:03,120 --> 01:26:04,170
field that you're talking about

2302
01:26:04,320 --> 01:26:04,570
uh

2303
01:26:04,639 --> 01:26:09,130
and so i can't really comment on this idea of entitlement of weights in machine learning

2304
01:26:09,679 --> 01:26:09,929
um

2305
01:26:10,639 --> 01:26:11,850
i'm not familiar with it there

2306
01:26:12,239 --> 01:26:12,489
um

2307
01:26:13,280 --> 01:26:13,530
um

2308
01:26:13,760 --> 01:26:14,170
i do

2309
01:26:14,400 --> 01:26:14,650
i

2310
01:26:14,880 --> 01:26:15,130
i

2311
01:26:15,280 --> 01:26:16,889
i want to maybe correct something about sdrs

2312
01:26:17,120 --> 01:26:18,650
which relates to the question you're asking

2313
01:26:19,600 --> 01:26:19,850
uh

2314
01:26:19,860 --> 01:26:21,290
we think in some parts of the brain

2315
01:26:21,920 --> 01:26:22,730
some parts of the neocortex

2316
01:26:23,120 --> 01:26:25,210
these very sparse representations worked

2317
01:26:25,280 --> 01:26:25,929
as i just said

2318
01:26:26,639 --> 01:26:28,650
there are other places where there are less sparse

2319
01:26:28,880 --> 01:26:29,850
maybe 10 percent sparsity

2320
01:26:31,199 --> 01:26:32,409
grid cells is a great example

2321
01:26:33,280 --> 01:26:35,290
if i actually look at a population of grid cells

2322
01:26:36,080 --> 01:26:38,010
you know it's not like one percent sparsity

2323
01:26:38,320 --> 01:26:39,690
it's more like ten percent sparse

2324
01:26:40,560 --> 01:26:44,489
and and even and there the activation levels actually do matter

2325
01:26:44,880 --> 01:26:45,770
it's not like they matter

2326
01:26:45,780 --> 01:26:46,810
like three digits of precision

2327
01:26:47,440 --> 01:26:48,730
but maybe one digit of precision

2328
01:26:49,440 --> 01:26:50,409
and um

2329
01:26:51,120 --> 01:26:51,770
and they

2330
01:26:52,159 --> 01:26:54,730
and so that doesn't have all the properties

2331
01:26:54,880 --> 01:26:55,850
i was just talking about

2332
01:26:56,320 --> 01:26:56,570
um

2333
01:26:56,639 --> 01:26:57,610
when you get to like 10

2334
01:26:58,000 --> 01:27:00,330
you can't really do a lot of superposition patterns

2335
01:27:01,199 --> 01:27:02,330
so it looks like

2336
01:27:02,400 --> 01:27:03,610
in some parts of these algorithms

2337
01:27:04,400 --> 01:27:06,810
there are sparse patterns that work on some principles

2338
01:27:07,120 --> 01:27:07,770
and you know

2339
01:27:07,780 --> 01:27:08,730
and in other parts

2340
01:27:08,740 --> 01:27:09,210
they're very

2341
01:27:09,220 --> 01:27:09,530
very sparse

2342
01:27:09,679 --> 01:27:10,489
i work on other principles

2343
01:27:10,960 --> 01:27:12,810
it's not everything is super sgrs

2344
01:27:13,120 --> 01:27:13,449
you know

2345
01:27:13,760 --> 01:27:14,730
i used to think that

2346
01:27:14,740 --> 01:27:15,530
but i don't think that anymore

2347
01:27:16,639 --> 01:27:16,889
um

2348
01:27:17,120 --> 01:27:19,130
i just became enamored with seoul and i said

2349
01:27:19,140 --> 01:27:19,370
oh my god

2350
01:27:19,380 --> 01:27:19,929
this is really cool

2351
01:27:20,000 --> 01:27:21,210
you know how these things work

2352
01:27:21,600 --> 01:27:21,850
um

2353
01:27:22,000 --> 01:27:24,969
but i now we now know that there are in some parts of these algorithms

2354
01:27:25,199 --> 01:27:25,929
you need to have

2355
01:27:26,080 --> 01:27:26,330
um

2356
01:27:26,960 --> 01:27:27,449
still sparse

2357
01:27:27,600 --> 01:27:30,489
but more dense and activation of level matter

2358
01:27:30,560 --> 01:27:31,929
and then some of the capacity issues

2359
01:27:32,159 --> 01:27:32,409
uh

2360
01:27:32,419 --> 01:27:33,130
you don't have those

2361
01:27:33,140 --> 01:27:34,090
the capacity advantages

2362
01:27:34,480 --> 01:27:34,810
and so

2363
01:27:35,360 --> 01:27:35,610
um

2364
01:27:35,679 --> 01:27:37,929
and that's clearly true with with grid cell representations

2365
01:27:38,719 --> 01:27:38,969
um

2366
01:27:39,040 --> 01:27:40,969
but i i imagine it's true at other things too

2367
01:27:40,979 --> 01:27:41,929
i know it's true in other rares

2368
01:27:42,000 --> 01:27:42,250
too

2369
01:27:42,260 --> 01:27:42,969
so it's

2370
01:27:42,979 --> 01:27:44,889
it isn't one size fits all here

2371
01:27:45,440 --> 01:27:45,690
um

2372
01:27:45,920 --> 01:27:46,330
i do

2373
01:27:46,400 --> 01:27:49,610
i do say we could say for certainty that you don't find anywhere in the braid

2374
01:27:49,620 --> 01:27:51,210
where anything is represented by full active

2375
01:27:51,280 --> 01:27:51,610
you know

2376
01:27:51,760 --> 01:27:52,010
uh

2377
01:27:52,239 --> 01:27:52,810
fully dense

2378
01:27:53,520 --> 01:27:53,770
uh

2379
01:27:53,780 --> 01:27:55,290
networks where all the neurons are fire

2380
01:27:55,360 --> 01:27:56,810
developed that this does not exist

2381
01:27:56,960 --> 01:27:58,170
doesn't mean this couldn't be useful

2382
01:27:58,639 --> 01:28:01,290
and it doesn't mean that we could in the future do something like that

2383
01:28:01,300 --> 01:28:02,570
it just seems very unlikely to me

2384
01:28:02,960 --> 01:28:03,210
uh

2385
01:28:03,220 --> 01:28:04,090
i know that's how most

2386
01:28:04,320 --> 01:28:04,570
uh

2387
01:28:04,719 --> 01:28:05,050
you know

2388
01:28:05,060 --> 01:28:05,690
convolutional nora

2389
01:28:05,760 --> 01:28:07,290
that works with networks work today

2390
01:28:07,760 --> 01:28:08,010
um

2391
01:28:08,639 --> 01:28:08,889
but

2392
01:28:08,960 --> 01:28:09,210
um

2393
01:28:09,360 --> 01:28:09,610
right

2394
01:28:09,620 --> 01:28:10,889
but you probably know there's a lot of

2395
01:28:10,899 --> 01:28:12,409
there's a lot of effort going on in sparsity

2396
01:28:12,719 --> 01:28:13,130
right now

2397
01:28:13,140 --> 01:28:14,409
in the classic machine learning community

2398
01:28:14,719 --> 01:28:14,969
uh

2399
01:28:15,040 --> 01:28:16,889
people try to do it and we're working on that too

2400
01:28:17,280 --> 01:28:17,530
um

2401
01:28:18,159 --> 01:28:25,929
so so i i think there's something fundamental about the representations we use when we build ai algorithms

2402
01:28:26,880 --> 01:28:28,489
there's a dichotomy between

2403
01:28:28,560 --> 01:28:28,969
you know

2404
01:28:28,979 --> 01:28:30,330
the good old-fashioned ai people

2405
01:28:30,480 --> 01:28:30,730
they

2406
01:28:30,960 --> 01:28:32,810
they work at a level of abstraction higher

2407
01:28:32,880 --> 01:28:35,929
they try to implement the mind and us connectionists

2408
01:28:36,320 --> 01:28:37,690
we're trying to implement the brain

2409
01:28:38,000 --> 01:28:41,770
i think there's something fundamental about that level of abstraction that we work at

2410
01:28:41,780 --> 01:28:44,330
so there's a discussion between people like scott harrison

2411
01:28:44,560 --> 01:28:47,929
who think that computation is raw and should happen at the lowest possible level

2412
01:28:48,400 --> 01:28:50,489
so just think of how neural networks work

2413
01:28:50,499 --> 01:28:51,530
they distribute knowledge

2414
01:28:51,600 --> 01:28:54,570
don't they across lots and lots of different neurons

2415
01:28:55,199 --> 01:28:55,929
and then there's um

2416
01:28:56,000 --> 01:28:56,250
uh

2417
01:28:56,719 --> 01:28:58,090
people like krishna swami

2418
01:28:58,239 --> 01:28:59,449
and he's a computer scientist

2419
01:28:59,679 --> 01:29:04,889
and and he thinks that we should be dealing with much higher level computational primitives like functions and types

2420
01:29:05,520 --> 01:29:05,770
so

2421
01:29:06,239 --> 01:29:06,489
um

2422
01:29:07,199 --> 01:29:08,250
i i guess

2423
01:29:08,800 --> 01:29:09,449
from your perspective

2424
01:29:10,239 --> 01:29:13,050
because i i feel that all of this is strongly emergent

2425
01:29:13,360 --> 01:29:17,370
i feel that we could work at the very lowest primitive level of computation

2426
01:29:18,159 --> 01:29:19,449
and all of the magic emerges

2427
01:29:20,159 --> 01:29:21,610
do you think we should be doing that

2428
01:29:21,620 --> 01:29:23,290
or do you think we should be working higher up the stack

2429
01:29:23,520 --> 01:29:23,770
no

2430
01:29:23,780 --> 01:29:24,010
i

2431
01:29:24,020 --> 01:29:24,250
i

2432
01:29:24,320 --> 01:29:24,570
well

2433
01:29:24,580 --> 01:29:26,090
i think we have to working higher up the stack

2434
01:29:26,239 --> 01:29:26,489
um

2435
01:29:27,280 --> 01:29:27,690
i mean

2436
01:29:27,700 --> 01:29:30,969
of course you can break any system down and say it's just a bunch of atoms

2437
01:29:31,600 --> 01:29:32,650
it's just a bunch of molecules

2438
01:29:33,040 --> 01:29:33,290
right

2439
01:29:33,300 --> 01:29:34,330
it's just a bunch of cells

2440
01:29:34,960 --> 01:29:35,210
um

2441
01:29:35,520 --> 01:29:36,170
and that's true

2442
01:29:36,239 --> 01:29:36,810
and you can

2443
01:29:36,960 --> 01:29:37,210
theory

2444
01:29:37,280 --> 01:29:38,250
should be able to understand that

2445
01:29:38,260 --> 01:29:39,610
but just like thermodynamics says

2446
01:29:39,620 --> 01:29:41,770
it's not very useful to think about liquids as a bunch of atoms

2447
01:29:41,920 --> 01:29:42,170
right

2448
01:29:42,239 --> 01:29:42,489
you

2449
01:29:42,499 --> 01:29:44,330
you need to have something at a higher level than that

2450
01:29:44,800 --> 01:29:46,969
and and so you know what

2451
01:29:47,040 --> 01:29:47,449
i guess

2452
01:29:47,679 --> 01:29:48,570
the principle we

2453
01:29:48,580 --> 01:29:50,570
that we'd assume that we just we have

2454
01:29:50,580 --> 01:29:50,730
now

2455
01:29:50,800 --> 01:29:56,010
in some sense discovered that the brain uses this representation scheme using reference frames and movement

2456
01:29:56,800 --> 01:30:00,330
now i think that's the right level to think about it

2457
01:30:00,400 --> 01:30:00,810
i don't

2458
01:30:00,960 --> 01:30:01,210
i

2459
01:30:01,280 --> 01:30:01,530
i

2460
01:30:01,540 --> 01:30:03,290
you could try to extract it less than that

2461
01:30:03,300 --> 01:30:04,090
but why bother

2462
01:30:04,159 --> 01:30:06,010
that is the unit that we're talking about here

2463
01:30:06,020 --> 01:30:07,050
we use models that are

2464
01:30:08,000 --> 01:30:08,489
but okay

2465
01:30:08,960 --> 01:30:09,210
that's

2466
01:30:09,280 --> 01:30:09,530
that's

2467
01:30:09,600 --> 01:30:10,170
that's the thing

2468
01:30:10,180 --> 01:30:10,330
though

2469
01:30:10,340 --> 01:30:10,489
but

2470
01:30:10,499 --> 01:30:10,730
um

2471
01:30:11,920 --> 01:30:13,210
the brain was evolvable

2472
01:30:13,760 --> 01:30:16,409
it evolved within biological and environmental limitations

2473
01:30:16,800 --> 01:30:17,370
and and go

2474
01:30:17,380 --> 01:30:18,170
five people say

2475
01:30:18,180 --> 01:30:18,409
well

2476
01:30:18,419 --> 01:30:19,530
you know what scrap that

2477
01:30:19,840 --> 01:30:21,449
let's just implement the mind directly

2478
01:30:21,920 --> 01:30:22,330
you know

2479
01:30:22,800 --> 01:30:25,449
and they think that functions and types

2480
01:30:26,159 --> 01:30:26,409
um

2481
01:30:26,560 --> 01:30:29,690
they're not an imaginary concept that have been invented by humans

2482
01:30:29,840 --> 01:30:32,010
they think that mathematical knowledge has

2483
01:30:32,020 --> 01:30:33,610
it's universal and it's been discovered

2484
01:30:34,000 --> 01:30:37,929
by us and we would be silly not to implement it directly in this higher level language

2485
01:30:38,159 --> 01:30:39,210
because if you think about it

2486
01:30:39,360 --> 01:30:39,610
even

2487
01:30:40,000 --> 01:30:40,250
um

2488
01:30:40,719 --> 01:30:41,770
the knowledge that we have

2489
01:30:41,780 --> 01:30:42,969
you write about this in your book

2490
01:30:42,979 --> 01:30:48,090
it's fascinating that now there's a separation between our genes and the knowledge we've created as the human race

2491
01:30:48,639 --> 01:30:50,650
and all of this stuff now is

2492
01:30:50,660 --> 01:30:50,810
is

2493
01:30:50,880 --> 01:30:51,290
is emergent

2494
01:30:51,760 --> 01:30:53,130
our society is emergent

2495
01:30:54,159 --> 01:30:55,850
so why not implement it directly

2496
01:30:56,000 --> 01:30:57,130
but why trust

2497
01:30:57,199 --> 01:30:57,449
because

2498
01:30:57,679 --> 01:30:57,929
because

2499
01:30:58,080 --> 01:30:59,290
because you're gonna

2500
01:30:59,360 --> 01:31:00,330
you're gonna guess wrong

2501
01:31:01,199 --> 01:31:01,449
right

2502
01:31:01,520 --> 01:31:01,850
you know

2503
01:31:01,860 --> 01:31:02,489
you're gonna say

2504
01:31:02,499 --> 01:31:02,730
okay

2505
01:31:02,740 --> 01:31:02,969
well

2506
01:31:02,979 --> 01:31:04,969
i will figure out what that structure is

2507
01:31:05,040 --> 01:31:05,610
what you know

2508
01:31:05,620 --> 01:31:08,250
what is that mind structure that we're gonna

2509
01:31:08,400 --> 01:31:08,730
you know

2510
01:31:09,040 --> 01:31:09,929
and and so

2511
01:31:09,939 --> 01:31:10,330
for example

2512
01:31:10,880 --> 01:31:11,530
most of the

2513
01:31:11,540 --> 01:31:11,929
you know

2514
01:31:11,939 --> 01:31:14,969
the good old-fashioned ai never considered movement in their

2515
01:31:15,120 --> 01:31:15,530
in their

2516
01:31:15,840 --> 01:31:18,570
in their fundamental aspects of representation space

2517
01:31:19,040 --> 01:31:19,290
right

2518
01:31:19,840 --> 01:31:20,090
um

2519
01:31:20,639 --> 01:31:21,530
and you know

2520
01:31:21,540 --> 01:31:21,929
or even

2521
01:31:21,939 --> 01:31:24,250
do you look for what dr lanatt did was psych

2522
01:31:24,480 --> 01:31:24,810
you know

2523
01:31:25,199 --> 01:31:25,770
same idea

2524
01:31:26,000 --> 01:31:26,250
right

2525
01:31:26,560 --> 01:31:26,810
um

2526
01:31:27,440 --> 01:31:27,690
yeah

2527
01:31:27,700 --> 01:31:28,010
and so

2528
01:31:28,480 --> 01:31:30,010
but but now i know

2529
01:31:30,020 --> 01:31:30,250
okay

2530
01:31:30,260 --> 01:31:32,330
the brain builds these structures using movement

2531
01:31:32,639 --> 01:31:36,730
so it it's not like it was a wrong idea of the old ai people

2532
01:31:36,880 --> 01:31:39,770
it just they just didn't pick the right schema

2533
01:31:41,600 --> 01:31:43,090
it was the wrong schema

2534
01:31:43,440 --> 01:31:43,690
well

2535
01:31:43,840 --> 01:31:44,810
could you expand on that

2536
01:31:44,820 --> 01:31:45,690
because the thing is

2537
01:31:45,700 --> 01:31:45,850
um

2538
01:31:46,080 --> 01:31:47,929
because we've got a go-fi person

2539
01:31:48,000 --> 01:31:48,489
we're very

2540
01:31:48,560 --> 01:31:49,770
we're very good friends with

2541
01:31:49,840 --> 01:31:52,810
and he is convinced that there is

2542
01:31:53,120 --> 01:31:53,449
you know

2543
01:31:53,459 --> 01:31:56,090
like the knowledge engineering bottleneck and yeah

2544
01:31:56,159 --> 01:31:56,409
it

2545
01:31:56,480 --> 01:31:56,730
it

2546
01:31:56,740 --> 01:31:57,130
it's very

2547
01:31:57,140 --> 01:31:58,489
very brittle and it didn't work very well

2548
01:31:58,499 --> 01:31:58,889
but um

2549
01:31:58,899 --> 01:32:01,929
but you're reading your chapter on knowledge representation

2550
01:32:02,719 --> 01:32:04,489
i think you were saying our brain is

2551
01:32:04,499 --> 01:32:05,449
it's like lots of models

2552
01:32:05,600 --> 01:32:06,570
when we model a stapler

2553
01:32:06,880 --> 01:32:11,850
it's actually lots of little models in our brain and it's it learns a kind of

2554
01:32:12,000 --> 01:32:12,250
um

2555
01:32:12,719 --> 01:32:12,969
um

2556
01:32:13,199 --> 01:32:17,050
it analogizes the knowledge in a really sophisticated and distributed way

2557
01:32:17,060 --> 01:32:20,010
so when we want to know what happens when we press down on the stapler

2558
01:32:20,239 --> 01:32:22,090
we run a simulation in our mind

2559
01:32:22,159 --> 01:32:22,570
you know what

2560
01:32:22,580 --> 01:32:24,010
what happens if i press down on it

2561
01:32:24,020 --> 01:32:25,210
what happens if i stretch it

2562
01:32:25,280 --> 01:32:30,489
and our models seem to um generalize to any version of a stapler we might ever find

2563
01:32:30,960 --> 01:32:37,370
so that almost convinced me that we do need to have a lower level representation of knowledge

2564
01:32:38,159 --> 01:32:38,730
lower than what

2565
01:32:38,740 --> 01:32:41,050
lower than the the reference frames i was talking about

2566
01:32:41,840 --> 01:32:42,090
well

2567
01:32:42,320 --> 01:32:42,570
lo

2568
01:32:42,639 --> 01:32:48,010
so the go five people say that we should use um functions and types and relations to describe knowledge

2569
01:32:48,560 --> 01:32:50,489
but do you think that's possible

2570
01:32:52,639 --> 01:32:52,889
i

2571
01:32:52,899 --> 01:32:57,050
i i don't want to misrepresent go fight because i haven't looked at that stuff in a long time

2572
01:32:57,520 --> 01:32:57,770
um

2573
01:32:59,520 --> 01:33:03,370
i i think you can't do this at the level of the neuron

2574
01:33:04,159 --> 01:33:04,489
i mean

2575
01:33:04,499 --> 01:33:05,610
obviously brains are made of neurons

2576
01:33:05,760 --> 01:33:06,010
so

2577
01:33:06,020 --> 01:33:07,610
but they're not uniform neurons

2578
01:33:07,679 --> 01:33:10,570
we have dozens of different types of neurons hooked up in very complex ways

2579
01:33:10,639 --> 01:33:11,449
they do different functions

2580
01:33:11,679 --> 01:33:13,130
it's not just one big neural network

2581
01:33:14,159 --> 01:33:14,409
uh

2582
01:33:14,419 --> 01:33:15,610
that structure is important

2583
01:33:16,480 --> 01:33:16,730
um

2584
01:33:17,120 --> 01:33:17,530
and so

2585
01:33:18,239 --> 01:33:18,489
uh

2586
01:33:18,639 --> 01:33:20,889
i think it's now become evident to me

2587
01:33:20,899 --> 01:33:22,570
and it wasn't this way five years ago

2588
01:33:22,960 --> 01:33:25,610
it's really evident to me now that at least in the brain

2589
01:33:26,159 --> 01:33:28,730
knowledge is represented in this structured form

2590
01:33:29,520 --> 01:33:29,770
uh

2591
01:33:29,780 --> 01:33:30,250
with reference

2592
01:33:30,400 --> 01:33:30,969
aims and movement

2593
01:33:31,360 --> 01:33:33,210
it's the same inspiration that um

2594
01:33:34,080 --> 01:33:35,530
uniting has with with capsules

2595
01:33:35,760 --> 01:33:38,250
but he didn't incorporate movement as part of that structure

2596
01:33:38,960 --> 01:33:39,210
um

2597
01:33:39,840 --> 01:33:40,250
you know

2598
01:33:40,400 --> 01:33:40,650
uh

2599
01:33:41,040 --> 01:33:41,290
that

2600
01:33:41,300 --> 01:33:41,690
with that

2601
01:33:41,700 --> 01:33:42,090
you know

2602
01:33:42,100 --> 01:33:42,489
but now i

2603
01:33:42,560 --> 01:33:43,290
i now i understand

2604
01:33:43,440 --> 01:33:43,690
okay

2605
01:33:43,700 --> 01:33:44,969
at least from a brain's point of view

2606
01:33:45,600 --> 01:33:47,690
these reference frames are constructed

2607
01:33:48,159 --> 01:33:49,050
as i mentioned earlier

2608
01:33:49,280 --> 01:33:51,449
they're discovered through through movement

2609
01:33:52,080 --> 01:33:54,570
observing movement of the of the sensor through the world

2610
01:33:55,280 --> 01:33:55,530
and

2611
01:33:55,760 --> 01:33:56,010
um

2612
01:33:56,320 --> 01:33:57,610
and so now we have this

2613
01:33:57,679 --> 01:33:57,929
the

2614
01:33:58,000 --> 01:33:58,250
the

2615
01:33:58,480 --> 01:34:00,090
the basic structure of

2616
01:34:00,100 --> 01:34:02,170
if you want to call it will fi is

2617
01:34:02,400 --> 01:34:02,810
is these

2618
01:34:02,960 --> 01:34:03,210
uh

2619
01:34:03,220 --> 01:34:03,770
movement related

2620
01:34:04,159 --> 01:34:04,409
um

2621
01:34:04,960 --> 01:34:05,210
um

2622
01:34:06,000 --> 01:34:07,929
reference frames and placing

2623
01:34:08,239 --> 01:34:08,650
you know

2624
01:34:09,120 --> 01:34:09,850
and stuff like that

2625
01:34:09,860 --> 01:34:10,090
i

2626
01:34:10,100 --> 01:34:10,489
i can't

2627
01:34:10,560 --> 01:34:11,850
i don't know how i would do it

2628
01:34:11,860 --> 01:34:12,330
less than that

2629
01:34:12,340 --> 01:34:12,570
i don't know

2630
01:34:12,580 --> 01:34:13,770
what how i want to get rid of that

2631
01:34:13,780 --> 01:34:13,929
now

2632
01:34:13,939 --> 01:34:14,409
what do i do

2633
01:34:14,480 --> 01:34:17,449
just ignore that and go back and just put a bunch of neurons together

2634
01:34:18,159 --> 01:34:18,810
i don't know

2635
01:34:18,820 --> 01:34:19,210
do you mean

2636
01:34:19,520 --> 01:34:24,489
could you come up with a genetic algorithm that could have discovered this using just traditional neural networks

2637
01:34:24,560 --> 01:34:24,810
sure

2638
01:34:24,880 --> 01:34:25,370
maybe you

2639
01:34:25,380 --> 01:34:25,449
could

2640
01:34:25,840 --> 01:34:26,170
you know

2641
01:34:26,180 --> 01:34:26,650
i don't know

2642
01:34:26,660 --> 01:34:27,929
which is the fastest way to get there

2643
01:34:28,239 --> 01:34:29,290
but now that i know it

2644
01:34:29,300 --> 01:34:30,409
why would i want to do something different

2645
01:34:30,639 --> 01:34:30,889
i

2646
01:34:30,960 --> 01:34:31,210
i

2647
01:34:31,220 --> 01:34:31,210
i

2648
01:34:31,440 --> 01:34:32,969
maybe i'm misunderstanding your questions

2649
01:34:34,159 --> 01:34:34,409
right

2650
01:34:34,800 --> 01:34:35,050
yeah

2651
01:34:35,920 --> 01:34:36,170
well

2652
01:34:36,180 --> 01:34:36,409
i

2653
01:34:36,419 --> 01:34:37,690
i think the the

2654
01:34:37,700 --> 01:34:40,330
the main issue is that we don't understand the representation

2655
01:34:40,880 --> 01:34:41,210
you know

2656
01:34:41,220 --> 01:34:44,409
if you look at the open ai microscope and you can visualize all the different neurons

2657
01:34:44,560 --> 01:34:45,130
and so on

2658
01:34:45,280 --> 01:34:50,250
these objects have been distributed in a way which is completely ineffable across all of these different neurons

2659
01:34:50,800 --> 01:34:56,090
and is that the most high fidelity possible representation of knowledge

2660
01:34:56,400 --> 01:34:56,650
well

2661
01:34:56,880 --> 01:34:57,210
i don't

2662
01:34:57,280 --> 01:34:57,770
i don't know

2663
01:34:58,000 --> 01:34:58,810
but well

2664
01:34:58,960 --> 01:35:00,409
i will say now is

2665
01:35:00,419 --> 01:35:02,409
i currently believe very strongly that

2666
01:35:02,480 --> 01:35:02,730
um

2667
01:35:03,119 --> 01:35:03,610
in the future

2668
01:35:03,679 --> 01:35:04,730
when we build ai systems

2669
01:35:04,800 --> 01:35:06,570
they're going to work more like on these brain structures

2670
01:35:06,719 --> 01:35:08,330
and they won't be just highly distributed

2671
01:35:08,800 --> 01:35:11,530
mysterious blobs of neurons right there

2672
01:35:12,480 --> 01:35:12,730
that's great

2673
01:35:12,800 --> 01:35:13,449
by the way

2674
01:35:13,459 --> 01:35:13,610
it's

2675
01:35:13,620 --> 01:35:14,250
it's great

2676
01:35:14,480 --> 01:35:14,730
uh

2677
01:35:14,800 --> 01:35:15,770
not only because of what they

2678
01:35:15,780 --> 01:35:16,250
how they perform

2679
01:35:16,260 --> 01:35:16,730
but it's great

2680
01:35:16,740 --> 01:35:18,330
because they'll understand about a whole lot better

2681
01:35:18,400 --> 01:35:18,650
this

2682
01:35:18,660 --> 01:35:19,050
you know

2683
01:35:19,060 --> 01:35:20,889
this touches on the issue of the

2684
01:35:21,040 --> 01:35:21,929
the threats of ai

2685
01:35:22,080 --> 01:35:22,330
right

2686
01:35:22,639 --> 01:35:22,969
you know

2687
01:35:22,979 --> 01:35:25,210
the so many of the threats of are based on the idea

2688
01:35:25,220 --> 01:35:26,330
that we don't know what's going to happen

2689
01:35:26,340 --> 01:35:28,250
well now you actually will know what's gonna happen

2690
01:35:28,400 --> 01:35:28,730
it's like

2691
01:35:28,740 --> 01:35:29,050
you know

2692
01:35:29,119 --> 01:35:29,530
you know

2693
01:35:29,540 --> 01:35:30,810
i just like to say it works like this

2694
01:35:30,820 --> 01:35:31,530
it's what's it

2695
01:35:31,600 --> 01:35:31,929
you know

2696
01:35:31,939 --> 01:35:32,889
i can't put it everything

2697
01:35:33,119 --> 01:35:33,610
but i

2698
01:35:33,840 --> 01:35:34,250
you know

2699
01:35:34,260 --> 01:35:35,210
i don't know what it's gonna learn

2700
01:35:35,220 --> 01:35:37,770
but i know how it's gonna learn and what it's what it's capable of

2701
01:35:38,159 --> 01:35:38,409
um

2702
01:35:38,719 --> 01:35:39,130
so i

2703
01:35:39,140 --> 01:35:39,530
i just

2704
01:35:39,760 --> 01:35:40,170
you know

2705
01:35:40,480 --> 01:35:43,530
i i don't want to debate people about this because i

2706
01:35:43,540 --> 01:35:45,850
you know there's a lot of smart people out there and they may have things

2707
01:35:46,000 --> 01:35:48,090
no things that i don't know and they can pursue things

2708
01:35:48,320 --> 01:35:49,610
but from where i see right now

2709
01:35:50,639 --> 01:35:50,889
um

2710
01:35:51,360 --> 01:35:52,650
it's very clear to me that

2711
01:35:52,660 --> 01:35:52,730
okay

2712
01:35:52,740 --> 01:35:54,090
this is how brains do this

2713
01:35:54,320 --> 01:35:56,250
this explains everything explains again

2714
01:35:56,260 --> 01:35:57,449
how we structure all knowledge

2715
01:35:58,159 --> 01:35:58,409
um

2716
01:35:58,800 --> 01:35:59,610
and how it evolved

2717
01:35:59,760 --> 01:36:00,969
that's a big part of this

2718
01:36:00,979 --> 01:36:02,810
and how you know how we have this common algorithm

2719
01:36:03,520 --> 01:36:06,730
and it's going to be based on these reference frames and movement and so on

2720
01:36:06,740 --> 01:36:07,850
and so okay

2721
01:36:07,920 --> 01:36:08,170
well

2722
01:36:08,239 --> 01:36:10,170
why wouldn't i just just just build that

2723
01:36:10,180 --> 01:36:10,810
let's go for that

2724
01:36:11,199 --> 01:36:11,449
um

2725
01:36:12,000 --> 01:36:13,050
and you know

2726
01:36:13,060 --> 01:36:14,090
why build something else

2727
01:36:14,100 --> 01:36:15,130
if i understand that

2728
01:36:15,199 --> 01:36:16,250
why build something else

2729
01:36:16,400 --> 01:36:16,650
i

2730
01:36:16,660 --> 01:36:16,889
i don't

2731
01:36:16,899 --> 01:36:17,850
i understand the point of

2732
01:36:17,860 --> 01:36:18,489
is it going to be better

2733
01:36:18,639 --> 01:36:18,969
you know

2734
01:36:19,119 --> 01:36:19,610
in the future

2735
01:36:19,679 --> 01:36:21,210
we will certainly take you

2736
01:36:21,220 --> 01:36:22,010
let's say i'm right

2737
01:36:22,080 --> 01:36:22,730
let's say i'm right

2738
01:36:22,880 --> 01:36:26,250
and we build machines that work on these principles that i outlined in the book

2739
01:36:26,560 --> 01:36:26,810
well

2740
01:36:26,880 --> 01:36:27,210
of course

2741
01:36:27,280 --> 01:36:28,810
we can very well go for that

2742
01:36:28,820 --> 01:36:29,929
we don't have to stick to the way biology

2743
01:36:30,159 --> 01:36:33,770
maybe we'll come up with a better way of representing things in structures and reference frames

2744
01:36:33,780 --> 01:36:35,850
maybe it won't be movement-based maybe something else

2745
01:36:35,860 --> 01:36:36,330
i don't know

2746
01:36:36,960 --> 01:36:38,889
but i try to do it

2747
01:36:38,899 --> 01:36:39,210
do it

2748
01:36:39,220 --> 01:36:40,409
build something and get it working

2749
01:36:40,480 --> 01:36:41,690
why would i go away from it

2750
01:36:41,700 --> 01:36:41,850
you know

2751
01:36:42,480 --> 01:36:42,730
yeah

2752
01:36:42,740 --> 01:36:44,330
go ahead to try and steal steel man

2753
01:36:44,800 --> 01:36:45,690
the go-fi people

2754
01:36:45,760 --> 01:36:48,090
it's because they think that knowledge is universal

2755
01:36:48,880 --> 01:36:49,770
we discover it

2756
01:36:49,780 --> 01:36:52,090
there is only one representation of knowledge

2757
01:36:52,480 --> 01:36:53,929
and clearly in the brain

2758
01:36:54,560 --> 01:36:56,730
you can learn things in different ways

2759
01:36:56,740 --> 01:36:57,130
you can

2760
01:36:57,520 --> 01:37:00,810
i can teach you a curriculum and i can teach you it in a different way

2761
01:37:01,119 --> 01:37:02,810
and maybe in some weird way

2762
01:37:02,880 --> 01:37:05,850
our brain does learn some latent representation of the knowledge

2763
01:37:05,860 --> 01:37:07,210
which is the universal knowledge

2764
01:37:07,520 --> 01:37:09,449
but they're making the argument that

2765
01:37:09,600 --> 01:37:12,330
why don't we just represent it the only way it can possibly

2766
01:37:12,400 --> 01:37:12,650
well

2767
01:37:12,660 --> 01:37:12,889
obviously

2768
01:37:12,960 --> 01:37:13,210
well

2769
01:37:13,440 --> 01:37:13,929
first of all

2770
01:37:13,939 --> 01:37:14,330
that's not

2771
01:37:14,400 --> 01:37:14,730
you know

2772
01:37:14,800 --> 01:37:15,770
it clearly isn't true

2773
01:37:15,780 --> 01:37:16,330
i give it

2774
01:37:16,340 --> 01:37:16,889
just like you just said

2775
01:37:16,899 --> 01:37:19,929
i gave the example in my book about taking a bunch of historical facts

2776
01:37:20,639 --> 01:37:22,010
and arranging them along a timeline

2777
01:37:22,320 --> 01:37:23,449
and arranging them on a map

2778
01:37:24,159 --> 01:37:29,130
and you end up with different inferences and different beliefs about the same set of facts

2779
01:37:29,679 --> 01:37:32,409
so whether there is a universal truth

2780
01:37:32,800 --> 01:37:33,369
i don't know

2781
01:37:34,000 --> 01:37:36,810
but clearly humans don't know that universal truth

2782
01:37:37,280 --> 01:37:37,530
um

2783
01:37:38,159 --> 01:37:38,409
we

2784
01:37:38,480 --> 01:37:39,369
we can't know everything

2785
01:37:39,440 --> 01:37:42,409
we can only sense a small part of the world and um

2786
01:37:42,639 --> 01:37:43,690
and in in

2787
01:37:43,840 --> 01:37:44,170
you know

2788
01:37:44,180 --> 01:37:45,770
the vast majority is invisible to us

2789
01:37:45,780 --> 01:37:48,330
so who the hell knows what that universal truth is

2790
01:37:48,719 --> 01:37:48,969
um

2791
01:37:49,040 --> 01:37:51,210
what we can do is build good models and

2792
01:37:51,760 --> 01:37:52,010
uh

2793
01:37:52,080 --> 01:37:52,570
and the models

2794
01:37:52,800 --> 01:37:54,090
there isn't a universal model

2795
01:37:54,100 --> 01:37:56,650
there isn't one correct model for for history

2796
01:37:57,280 --> 01:37:57,530
right

2797
01:37:57,679 --> 01:37:57,929
um

2798
01:37:58,800 --> 01:38:00,810
we might like to believe there is my models

2799
01:38:01,040 --> 01:38:01,850
writing yours was wrong

2800
01:38:01,920 --> 01:38:02,570
that kind of thing

2801
01:38:02,580 --> 01:38:04,810
but you know it's really easy to point

2802
01:38:04,820 --> 01:38:05,050
out

2803
01:38:05,199 --> 01:38:09,050
even non-controversial things like the timeline versus the map will lead to different models

2804
01:38:09,440 --> 01:38:11,369
the order in which you train someone will matter

2805
01:38:11,520 --> 01:38:11,929
you know

2806
01:38:12,560 --> 01:38:16,090
so i think we have to live with this um and i don't think you know

2807
01:38:16,159 --> 01:38:17,369
maybe there's some platonic

2808
01:38:18,239 --> 01:38:18,650
you know

2809
01:38:18,719 --> 01:38:20,170
go five people thinking there's some plot

2810
01:38:20,400 --> 01:38:22,250
universal knowledge that's correct all time

2811
01:38:22,260 --> 01:38:22,730
i don't know

2812
01:38:22,740 --> 01:38:25,369
but certainly i don't think that's accessible to us

2813
01:38:25,379 --> 01:38:25,850
if there was

2814
01:38:26,480 --> 01:38:26,730
we

2815
01:38:26,740 --> 01:38:27,130
just we

2816
01:38:27,140 --> 01:38:27,929
just we can't

2817
01:38:28,239 --> 01:38:31,449
we can't sense the world both physically and time wise

2818
01:38:32,080 --> 01:38:32,330
we

2819
01:38:32,480 --> 01:38:34,489
our senses only deal with a teeny part of the world

2820
01:38:34,719 --> 01:38:34,969
yeah

2821
01:38:35,440 --> 01:38:36,090
it's a good part

2822
01:38:36,159 --> 01:38:38,330
but it's not a little teeny part

2823
01:38:39,520 --> 01:38:41,929
we don't even know the universal truth about space and time

2824
01:38:42,000 --> 01:38:42,250
is

2825
01:38:42,260 --> 01:38:42,570
i mean

2826
01:38:43,199 --> 01:38:43,850
definitely true

2827
01:38:44,639 --> 01:38:48,090
if i could move on to some other questions about the brain

2828
01:38:48,639 --> 01:38:48,889
um

2829
01:38:49,040 --> 01:38:52,650
so you of course focus very much on new cortex as the

2830
01:38:52,800 --> 01:38:53,130
you know

2831
01:38:53,140 --> 01:38:53,929
seed fuel intelligence

2832
01:38:54,400 --> 01:38:57,050
and i think i think you're definitely on the right path with that

2833
01:38:57,199 --> 01:39:00,090
i think most people agree something similar to that

2834
01:39:00,320 --> 01:39:05,690
but there's also lots of other parts in the brain that see pretty important and i'm specifically

2835
01:39:06,239 --> 01:39:06,489
uh

2836
01:39:06,499 --> 01:39:06,730
interested

2837
01:39:07,199 --> 01:39:10,730
it's like one thing that machine learning people are interested in is the dopamine circuit

2838
01:39:10,800 --> 01:39:14,250
so like the cortical basal ganglia critical

2839
01:39:15,440 --> 01:39:16,650
and which seems really

2840
01:39:17,600 --> 01:39:18,409
it seems like alerted

2841
01:39:18,880 --> 01:39:20,650
it seems like some kind of learning is going on

2842
01:39:20,660 --> 01:39:20,810
there

2843
01:39:20,960 --> 01:39:22,650
could you maybe say some words about that

2844
01:39:22,800 --> 01:39:23,050
yeah

2845
01:39:23,060 --> 01:39:26,969
so obviously the neoprojects is connected to the rest of the brain in modern ways

2846
01:39:27,119 --> 01:39:27,369
um

2847
01:39:27,679 --> 01:39:28,330
myriad ways

2848
01:39:28,719 --> 01:39:28,969
um

2849
01:39:29,600 --> 01:39:29,850
uh

2850
01:39:30,480 --> 01:39:30,730
it's

2851
01:39:30,880 --> 01:39:33,929
it's not in isolation and it's a very complex relationship

2852
01:39:34,159 --> 01:39:35,130
so lots of other things

2853
01:39:35,840 --> 01:39:36,090
um

2854
01:39:36,560 --> 01:39:39,290
that doesn't mean you can't understand what its circuits are doing

2855
01:39:40,080 --> 01:39:40,330
um

2856
01:39:40,800 --> 01:39:44,090
but in a human and their cortex is complexly tied to things

2857
01:39:44,100 --> 01:39:45,210
and one of those things is that

2858
01:39:45,220 --> 01:39:45,690
if you think

2859
01:39:45,700 --> 01:39:49,850
if the neocortex is as this sort of map of the world or model of the world

2860
01:39:50,320 --> 01:39:50,570
well

2861
01:39:50,960 --> 01:39:51,210
it

2862
01:39:51,440 --> 01:39:54,330
what should it learn and what goals should it have

2863
01:39:54,880 --> 01:39:58,650
the dopamine circuitry is clearly associated with what should we learn

2864
01:39:58,719 --> 01:39:59,610
when should we take

2865
01:39:59,620 --> 01:40:03,130
when should we take the effort to learn something which is metabolically expensive

2866
01:40:04,239 --> 01:40:04,489
um

2867
01:40:04,960 --> 01:40:05,770
and um

2868
01:40:06,239 --> 01:40:08,170
and so somebody has to decide that

2869
01:40:08,639 --> 01:40:08,889
uh

2870
01:40:08,899 --> 01:40:12,889
my basic contention is the decision is not the neocortex's decision

2871
01:40:13,520 --> 01:40:13,770
right

2872
01:40:13,840 --> 01:40:14,090
mostly

2873
01:40:14,239 --> 01:40:16,570
it's somebody else's decision and

2874
01:40:16,719 --> 01:40:16,969
um

2875
01:40:17,199 --> 01:40:18,570
when we build intelligent machines

2876
01:40:19,280 --> 01:40:21,530
we'll have to figure out who's making that decision

2877
01:40:22,159 --> 01:40:22,409
and

2878
01:40:22,639 --> 01:40:22,889
um

2879
01:40:23,280 --> 01:40:25,130
it won't be in the neocortical equivalent

2880
01:40:25,520 --> 01:40:28,170
it'll be in another part of the intelligent machines equivalent

2881
01:40:28,960 --> 01:40:29,850
but uh

2882
01:40:29,860 --> 01:40:35,290
there's absolutely no reason we would have to model the goals and um

2883
01:40:35,760 --> 01:40:39,210
emotions and other things that other parts of the brain

2884
01:40:39,280 --> 01:40:39,530
do

2885
01:40:39,760 --> 01:40:42,330
unless you want to create something was human-like right

2886
01:40:42,480 --> 01:40:43,290
so you know

2887
01:40:43,300 --> 01:40:46,889
i used to joke that our dopamine signal in our models is a switch

2888
01:40:47,040 --> 01:40:48,330
i turn it on and turn it off

2889
01:40:48,340 --> 01:40:48,810
i can learn

2890
01:40:48,820 --> 01:40:49,050
now

2891
01:40:49,119 --> 01:40:49,929
stop learning now

2892
01:40:49,939 --> 01:40:50,090
right

2893
01:40:50,100 --> 01:40:51,369
just manually turn it on

2894
01:40:51,520 --> 01:40:52,570
manually turn it off

2895
01:40:52,960 --> 01:40:53,210
uh

2896
01:40:53,220 --> 01:40:53,929
when we run experiments

2897
01:40:54,639 --> 01:40:54,889
um

2898
01:40:55,040 --> 01:40:56,409
it doesn't have anything you know

2899
01:40:56,419 --> 01:40:57,530
more complicated than that

2900
01:40:58,000 --> 01:40:58,250
um

2901
01:40:58,560 --> 01:40:59,929
but there's it

2902
01:41:00,000 --> 01:41:00,330
you know

2903
01:41:00,719 --> 01:41:03,130
the only thing i'm claiming here is when we build intelligent machines

2904
01:41:03,280 --> 01:41:04,810
if we build them on the principles in the neocortex

2905
01:41:05,760 --> 01:41:06,650
they have to be embodied

2906
01:41:06,960 --> 01:41:07,610
i mentioned this

2907
01:41:07,620 --> 01:41:11,770
the whole chapter in the book about they have to be embodied in some sort physical or non-physical embodiment

2908
01:41:11,920 --> 01:41:12,650
but some embodiment

2909
01:41:13,280 --> 01:41:14,170
they have to have

2910
01:41:14,560 --> 01:41:14,810
uh

2911
01:41:15,040 --> 01:41:15,449
you know

2912
01:41:15,459 --> 01:41:16,969
very sort of safeguards built in

2913
01:41:16,979 --> 01:41:18,810
they have to have various mechanisms for what to learn

2914
01:41:18,820 --> 01:41:19,530
what not to learn

2915
01:41:19,540 --> 01:41:20,570
someone has to provide goals

2916
01:41:21,119 --> 01:41:21,369
uh

2917
01:41:21,379 --> 01:41:22,969
what should the goals of the system be

2918
01:41:23,360 --> 01:41:23,610
uh

2919
01:41:23,620 --> 01:41:25,770
what are we trying to achieve right now and

2920
01:41:25,920 --> 01:41:26,170
um

2921
01:41:26,880 --> 01:41:28,730
but so those things have to exist

2922
01:41:29,119 --> 01:41:30,570
but they don't have to be modeled on

2923
01:41:30,580 --> 01:41:33,210
i don't see any reason at all to model those on the neural circuits

2924
01:41:33,440 --> 01:41:34,969
the way the neurons do that that

2925
01:41:34,979 --> 01:41:36,170
now we're getting pure biology

2926
01:41:36,480 --> 01:41:39,050
we're now getting into what is a biological organism need

2927
01:41:39,520 --> 01:41:39,770
including

2928
01:41:39,920 --> 01:41:41,530
like keeping your heart going and breathing

2929
01:41:41,760 --> 01:41:42,570
and you know

2930
01:41:42,580 --> 01:41:44,090
making sure you have sex and all these things

2931
01:41:44,400 --> 01:41:44,650
um

2932
01:41:45,440 --> 01:41:45,690
yeah

2933
01:41:45,700 --> 01:41:46,570
i can ignore that

2934
01:41:47,199 --> 01:41:48,969
just if we're going to build intel's machines

2935
01:41:48,979 --> 01:41:50,090
just make them a little bit

2936
01:41:50,100 --> 01:41:50,330
um

2937
01:41:51,040 --> 01:41:51,610
to our purpose

2938
01:41:51,840 --> 01:41:55,850
i guess i would say another neuroscience question is that i know

2939
01:41:55,920 --> 01:41:56,170
um

2940
01:41:57,040 --> 01:41:57,369
you know

2941
01:41:57,379 --> 01:42:01,690
a lot of people are skeptical that the brain can do anything remotely like bat propagation

2942
01:42:02,480 --> 01:42:02,730
right

2943
01:42:02,740 --> 01:42:05,290
i mean there's sort of good arguments for why that's just not possible

2944
01:42:05,440 --> 01:42:06,250
but on the other hand

2945
01:42:06,639 --> 01:42:10,090
i don't know if you've seen kind of a lot of the recent work by blake richards

2946
01:42:10,400 --> 01:42:11,210
at mcgill university

2947
01:42:11,600 --> 01:42:12,409
where he talks about

2948
01:42:13,040 --> 01:42:16,730
you can do something that performs in a very similar way to back propagation

2949
01:42:17,199 --> 01:42:17,449
because

2950
01:42:18,159 --> 01:42:19,130
and especially in the neocortex

2951
01:42:19,840 --> 01:42:20,090
the

2952
01:42:20,100 --> 01:42:20,250
uh

2953
01:42:20,400 --> 01:42:22,889
the neurons there had these very long apical dendrites

2954
01:42:23,600 --> 01:42:26,090
and they had these sections with these calcium channels

2955
01:42:26,320 --> 01:42:31,050
that can do these very long lasting activations that trigger like spike trains

2956
01:42:31,119 --> 01:42:32,170
and he's saying that there are

2957
01:42:32,180 --> 01:42:33,530
there is evidence that you know

2958
01:42:33,760 --> 01:42:37,770
if you have feedback from a higher or downstream layer back into this

2959
01:42:38,400 --> 01:42:38,650
the

2960
01:42:38,660 --> 01:42:38,810
uh

2961
01:42:39,119 --> 01:42:40,090
the apical dendrite

2962
01:42:40,480 --> 01:42:40,810
you know

2963
01:42:40,840 --> 01:42:45,690
connections that that can have an effect of creating something like back propagation

2964
01:42:46,000 --> 01:42:47,770
it's like stochastic back propagation

2965
01:42:48,719 --> 01:42:48,969
uh

2966
01:42:49,040 --> 01:42:50,010
what are your thoughts on that

2967
01:42:50,719 --> 01:42:50,969
well

2968
01:42:51,280 --> 01:42:51,530
i'm

2969
01:42:51,540 --> 01:42:55,130
i'm a little bit familiar with that work and blood breakthroughs work a little bit

2970
01:42:55,440 --> 01:42:55,690
um

2971
01:42:56,239 --> 01:43:02,090
i viewed a little bit of a distraction because you know backpropagation's a cool idea

2972
01:43:03,119 --> 01:43:03,369
uh

2973
01:43:03,440 --> 01:43:05,929
it works on classic neural networks pretty well

2974
01:43:06,800 --> 01:43:07,050
um

2975
01:43:08,000 --> 01:43:11,690
i don't think the end result is like kind of nerf what you see in the brain at all

2976
01:43:11,840 --> 01:43:13,210
i don't think the kind of learning

2977
01:43:13,360 --> 01:43:17,130
that that back propagation does is the kind of learning we see in the brain

2978
01:43:17,760 --> 01:43:18,010
um

2979
01:43:18,800 --> 01:43:19,929
and and so

2980
01:43:20,320 --> 01:43:20,570
uh

2981
01:43:20,580 --> 01:43:22,010
and there's lots of other theories

2982
01:43:22,080 --> 01:43:24,810
what those apical dendrites are doing and why you have that stuff so

2983
01:43:25,360 --> 01:43:25,610
um

2984
01:43:26,000 --> 01:43:28,730
i'm not going to say that back propagation isn't occurring anywhere in the brain

2985
01:43:28,800 --> 01:43:29,290
i don't know

2986
01:43:29,600 --> 01:43:30,170
maybe it is

2987
01:43:30,400 --> 01:43:30,650
um

2988
01:43:30,719 --> 01:43:33,369
but it hasn't played a role in our theories about the brain

2989
01:43:33,920 --> 01:43:37,530
and i think a lot of people spend time trying to shoot her in there

2990
01:43:37,540 --> 01:43:38,810
like i'm going to prove it to you

2991
01:43:38,820 --> 01:43:40,409
see the brain really is doing this

2992
01:43:40,419 --> 01:43:41,449
it's like well

2993
01:43:42,159 --> 01:43:43,770
but the paintings look like anything else

2994
01:43:43,920 --> 01:43:44,650
none of this stuff

2995
01:43:44,660 --> 01:43:46,650
the brain doesn't look like anything like you're talking about here

2996
01:43:46,660 --> 01:43:47,449
you're just trying to put this

2997
01:43:47,459 --> 01:43:47,850
you know

2998
01:43:47,860 --> 01:43:50,010
one little piece back prop into this thing

2999
01:43:50,320 --> 01:43:51,770
i mean for example just take away learning

3000
01:43:51,840 --> 01:43:53,210
we know that learning in a neuron

3001
01:43:53,360 --> 01:43:58,650
lauren taught these dendritic branches and and the synapses are distributed on these branches

3002
01:43:59,360 --> 01:43:59,610
uh

3003
01:43:59,679 --> 01:43:59,929
well

3004
01:43:59,939 --> 01:44:00,409
this is

3005
01:44:00,419 --> 01:44:01,050
this is key

3006
01:44:01,060 --> 01:44:01,530
this is critical

3007
01:44:01,600 --> 01:44:01,929
you can't

3008
01:44:01,939 --> 01:44:02,889
you can't get around this

3009
01:44:02,899 --> 01:44:03,610
this is this

3010
01:44:03,679 --> 01:44:04,730
there's an important information

3011
01:44:04,880 --> 01:44:05,130
theoretic

3012
01:44:05,280 --> 01:44:05,929
reason for this

3013
01:44:06,480 --> 01:44:06,730
it's

3014
01:44:06,800 --> 01:44:08,250
it's not just some other reasons

3015
01:44:08,800 --> 01:44:10,810
it's doing something important from an information point of view

3016
01:44:11,199 --> 01:44:17,210
and and so the whole idea of backprop working with learning on individual dendritic segments is

3017
01:44:17,220 --> 01:44:18,010
i don't know what that means

3018
01:44:18,239 --> 01:44:18,489
right

3019
01:44:18,560 --> 01:44:18,969
it's like

3020
01:44:18,979 --> 01:44:20,010
it's like hard to fit in

3021
01:44:20,020 --> 01:44:20,969
so right to my mind

3022
01:44:20,979 --> 01:44:21,610
it's like noise

3023
01:44:21,679 --> 01:44:22,810
i kind of like tuning it out

3024
01:44:22,820 --> 01:44:22,889
saying

3025
01:44:22,899 --> 01:44:23,130
well

3026
01:44:23,140 --> 01:44:24,330
if they figure out something really important

3027
01:44:24,400 --> 01:44:24,969
they'll let you know

3028
01:44:25,280 --> 01:44:25,530
well

3029
01:44:25,540 --> 01:44:27,530
i mean i'll pay attention to it

3030
01:44:27,540 --> 01:44:27,770
yeah

3031
01:44:27,780 --> 01:44:29,290
i think i think more his his motivation

3032
01:44:29,600 --> 01:44:30,810
and i quite agree with this

3033
01:44:30,820 --> 01:44:32,810
it's about taking inspiration from the brain

3034
01:44:32,880 --> 01:44:37,850
so this algorithm that they discovered by kind of looking for things that could you know

3035
01:44:38,159 --> 01:44:38,810
at least enable

3036
01:44:39,760 --> 01:44:40,090
you know

3037
01:44:40,100 --> 01:44:41,690
feedback mechanisms for the training

3038
01:44:42,159 --> 01:44:42,570
is it

3039
01:44:42,580 --> 01:44:43,210
but are they

3040
01:44:43,220 --> 01:44:44,570
are they taking information from the brain

3041
01:44:44,639 --> 01:44:48,730
are they taking inspiration from the neural networks and trying to figure out what the brain could do

3042
01:44:48,740 --> 01:44:53,210
what they're trying to do is they're trying to figure out how can the brain do deep training

3043
01:44:53,440 --> 01:44:54,730
how can the brain do deeply

3044
01:44:55,600 --> 01:44:57,770
and they like assuming that that's what

3045
01:44:57,780 --> 01:44:57,929
that

3046
01:44:58,000 --> 01:44:59,530
assuming back prop is what they want

3047
01:44:59,540 --> 01:44:59,690
no

3048
01:44:59,700 --> 01:44:59,850
no

3049
01:44:59,920 --> 01:45:00,570
i just want to know

3050
01:45:00,580 --> 01:45:01,130
can it do

3051
01:45:01,140 --> 01:45:02,250
how can it do deep learning

3052
01:45:02,400 --> 01:45:06,650
and so then they look very carefully at the biology and they find this this alternative mechanism

3053
01:45:07,360 --> 01:45:10,170
which can produce results that are very similar to back propagation

3054
01:45:10,480 --> 01:45:12,810
but actually it's slightly better in some ways

3055
01:45:13,280 --> 01:45:16,730
and so it's leading to inspiration for how to improve ml

3056
01:45:16,800 --> 01:45:19,210
and i guess that's the question i really wanted to ask you

3057
01:45:19,220 --> 01:45:21,770
which is i always ask this what's missing question

3058
01:45:22,080 --> 01:45:22,330
okay

3059
01:45:22,340 --> 01:45:24,570
so we have artificial neural networks

3060
01:45:24,960 --> 01:45:25,210
clearly

3061
01:45:25,360 --> 01:45:26,650
they're a very stripped down

3062
01:45:26,960 --> 01:45:27,369
you know

3063
01:45:27,920 --> 01:45:30,969
micro abstraction of what happens in the brain

3064
01:45:31,440 --> 01:45:34,650
if you could add in a few things like just one

3065
01:45:34,660 --> 01:45:34,889
two

3066
01:45:34,899 --> 01:45:38,090
three kind of abstract properties to neural networks today

3067
01:45:38,800 --> 01:45:43,210
what would you suggest should be added to get the biggest bang for the buck like

3068
01:45:43,220 --> 01:45:44,889
how can we incrementally improve them

3069
01:45:45,920 --> 01:45:46,170
okay

3070
01:45:46,180 --> 01:45:46,409
well

3071
01:45:46,480 --> 01:45:49,369
so so that's what we're doing at nemesis right now

3072
01:45:49,600 --> 01:45:49,850
so

3073
01:45:50,080 --> 01:45:50,330
um

3074
01:45:50,639 --> 01:45:51,130
my colleague

3075
01:45:51,280 --> 01:45:51,530
subatomi

3076
01:45:51,840 --> 01:45:52,090
ahmad

3077
01:45:52,560 --> 01:45:53,770
who's really the machine learning expert

3078
01:45:53,840 --> 01:45:55,530
i'm more of the neuroscience guy

3079
01:45:56,000 --> 01:45:56,250
um

3080
01:45:57,040 --> 01:45:59,210
he put together a road map

3081
01:45:59,220 --> 01:45:59,449
well

3082
01:45:59,459 --> 01:45:59,929
we did it

3083
01:45:59,939 --> 01:46:01,210
together but he's implementing it

3084
01:46:01,360 --> 01:46:01,610
uh

3085
01:46:01,620 --> 01:46:02,250
a road map

3086
01:46:02,560 --> 01:46:03,369
the idea is like

3087
01:46:03,379 --> 01:46:03,530
hey

3088
01:46:03,540 --> 01:46:06,730
we have these grand theories about the neocortex and so on again

3089
01:46:06,880 --> 01:46:07,530
maybe the right

3090
01:46:07,600 --> 01:46:08,250
maybe they're not

3091
01:46:08,260 --> 01:46:08,969
but we think they're right

3092
01:46:09,440 --> 01:46:09,690
um

3093
01:46:10,560 --> 01:46:10,810
well

3094
01:46:10,820 --> 01:46:11,530
what do we do next

3095
01:46:11,679 --> 01:46:12,650
like what do we do right

3096
01:46:12,660 --> 01:46:14,010
so the answer question was well

3097
01:46:14,020 --> 01:46:16,010
how we incrementally get from where we are today

3098
01:46:17,199 --> 01:46:17,449
uh

3099
01:46:17,520 --> 01:46:18,810
machine learning to where we want

3100
01:46:18,820 --> 01:46:21,290
where we think we need to be some number of years from now

3101
01:46:21,920 --> 01:46:25,929
and so we literally have a roadmap and we started off by saying okay

3102
01:46:26,400 --> 01:46:26,650
uh

3103
01:46:27,040 --> 01:46:28,250
first i'm going to focus on sparsity

3104
01:46:29,520 --> 01:46:31,369
and so we've been doing that and we've been at

3105
01:46:32,000 --> 01:46:32,250
uh

3106
01:46:32,880 --> 01:46:34,570
we've been making really interesting progress

3107
01:46:34,719 --> 01:46:35,290
some of it

3108
01:46:35,300 --> 01:46:36,570
which we published and some

3109
01:46:36,580 --> 01:46:37,130
some of it

3110
01:46:37,140 --> 01:46:37,690
we haven't yet

3111
01:46:38,000 --> 01:46:38,250
uh

3112
01:46:38,560 --> 01:46:40,330
about introducing sparsity to

3113
01:46:40,880 --> 01:46:41,130
uh

3114
01:46:41,199 --> 01:46:45,369
first we did it for the convolution neural networks and we did for other types of nerves

3115
01:46:45,379 --> 01:46:46,250
now we're doing it for transformers

3116
01:46:47,280 --> 01:46:47,530
um

3117
01:46:48,159 --> 01:46:51,929
and showing that we can speed these things up and make them more robust

3118
01:46:52,239 --> 01:46:54,170
and and i'm talking speeding things up by a lot

3119
01:46:54,180 --> 01:46:54,889
not a little bit

3120
01:46:54,960 --> 01:46:55,530
but a lot

3121
01:46:56,000 --> 01:46:56,250
uh

3122
01:46:56,260 --> 01:46:57,050
depends on the network

3123
01:46:57,060 --> 01:46:58,889
depends on the world through all these different things

3124
01:46:58,960 --> 01:46:59,770
but uh

3125
01:46:59,780 --> 01:47:00,170
how do

3126
01:47:00,180 --> 01:47:01,690
and then how do you implement that on heart

3127
01:47:01,700 --> 01:47:02,489
so we start with sparsity

3128
01:47:03,119 --> 01:47:05,530
the next thing we're working on is the dendritic computation

3129
01:47:06,239 --> 01:47:08,409
because what the dendrites get you

3130
01:47:08,419 --> 01:47:10,250
and we haven't really talked about this much at all so far

3131
01:47:10,560 --> 01:47:11,290
but with the dendrites

3132
01:47:11,600 --> 01:47:16,170
each individual section of a dendrites like its own little complete computational pattern recognizer

3133
01:47:17,040 --> 01:47:17,290
um

3134
01:47:17,520 --> 01:47:19,770
they allow you to represent information in different contexts

3135
01:47:20,880 --> 01:47:22,010
and so um

3136
01:47:23,119 --> 01:47:23,369
uh

3137
01:47:24,080 --> 01:47:24,730
this is

3138
01:47:25,119 --> 01:47:26,090
we think it's going to make

3139
01:47:26,159 --> 01:47:31,210
it's far far less training data to train existing networks

3140
01:47:31,440 --> 01:47:31,690
um

3141
01:47:32,159 --> 01:47:32,409
um

3142
01:47:32,639 --> 01:47:35,690
and so and allow us to do continuous well

3143
01:47:35,840 --> 01:47:38,570
because when you train a neuron you don't train all the synapses

3144
01:47:38,719 --> 01:47:39,050
you only

3145
01:47:39,119 --> 01:47:41,770
you only modify a few on on one of these dendritic branches

3146
01:47:42,480 --> 01:47:43,449
and so you can

3147
01:47:43,520 --> 01:47:45,290
you can learn without forgetting things

3148
01:47:45,840 --> 01:47:46,090
um

3149
01:47:46,239 --> 01:47:47,290
so we're working on that

3150
01:47:47,360 --> 01:47:47,929
so okay

3151
01:47:48,080 --> 01:47:52,010
take existing networks make them so that they're going to be continually learning or they

3152
01:47:52,020 --> 01:47:52,969
they don't forget things

3153
01:47:53,600 --> 01:47:53,850
um

3154
01:47:53,920 --> 01:47:56,409
and make them so you can train them for fewer data points

3155
01:47:57,119 --> 01:47:58,409
and then so we're

3156
01:47:58,480 --> 01:47:59,770
we're well into that now

3157
01:48:00,320 --> 01:48:00,570
um

3158
01:48:00,639 --> 01:48:02,889
and then the third thing with the next big one

3159
01:48:02,960 --> 01:48:05,850
which is sort of biting off the whole concept of reference frames

3160
01:48:06,800 --> 01:48:07,610
and um

3161
01:48:08,159 --> 01:48:09,210
we're just starting that

3162
01:48:09,840 --> 01:48:10,090
um

3163
01:48:10,320 --> 01:48:11,770
and it probably will take several years

3164
01:48:11,920 --> 01:48:12,170
um

3165
01:48:12,719 --> 01:48:15,449
but and then finally we do the whole the whole thousand brains theory

3166
01:48:15,520 --> 01:48:17,449
we have lots of these columns and so on

3167
01:48:17,679 --> 01:48:18,650
so at least we put together

3168
01:48:18,719 --> 01:48:19,369
we have a road map

3169
01:48:19,379 --> 01:48:20,170
we're working to it

3170
01:48:20,180 --> 01:48:21,770
i think i i'll be honest with you

3171
01:48:21,780 --> 01:48:22,250
i think this

3172
01:48:22,480 --> 01:48:26,010
the progress we've made is actually far better than i thought it would be

3173
01:48:26,639 --> 01:48:28,730
i thought it would take us longer to achieve the things we're achieving

3174
01:48:29,360 --> 01:48:29,610
uh

3175
01:48:29,679 --> 01:48:33,530
the performance gains we're getting are really really something dramatic

3176
01:48:34,320 --> 01:48:34,570
and

3177
01:48:34,639 --> 01:48:34,889
uh

3178
01:48:34,899 --> 01:48:35,770
without losing accuracy

3179
01:48:36,480 --> 01:48:36,730
um

3180
01:48:37,199 --> 01:48:37,449
so

3181
01:48:38,159 --> 01:48:38,409
uh

3182
01:48:38,560 --> 01:48:38,810
that's

3183
01:48:39,119 --> 01:48:40,650
that's what nomento is working on

3184
01:48:40,880 --> 01:48:43,210
and i don't think we've talked about this road map

3185
01:48:43,280 --> 01:48:43,530
um

3186
01:48:43,540 --> 01:48:45,369
but we haven't given all the details of it

3187
01:48:45,379 --> 01:48:45,449
yeah

3188
01:48:45,459 --> 01:48:46,489
so it's not public yet

3189
01:48:46,499 --> 01:48:46,889
the roadmap

3190
01:48:47,679 --> 01:48:47,929
well

3191
01:48:47,939 --> 01:48:48,409
the rare map

3192
01:48:48,419 --> 01:48:50,330
because i've talked about it in like you know talks

3193
01:48:50,560 --> 01:48:52,170
i just i just spoke about it here

3194
01:48:52,180 --> 01:48:52,409
right

3195
01:48:52,419 --> 01:48:54,010
so i have pictures of that stuff

3196
01:48:54,400 --> 01:48:54,650
um

3197
01:48:55,119 --> 01:48:56,810
but the details how we're doing it

3198
01:48:56,820 --> 01:48:57,690
like we've created

3199
01:48:57,760 --> 01:48:58,250
we've discovered

3200
01:48:58,639 --> 01:48:59,449
like you talk about sparsity

3201
01:49:00,000 --> 01:49:01,690
the problem is you want to run sparse networks

3202
01:49:02,080 --> 01:49:03,369
you want to run them on some hardware

3203
01:49:03,600 --> 01:49:03,850
right

3204
01:49:03,860 --> 01:49:04,409
what kind of heart

3205
01:49:04,419 --> 01:49:05,130
are you going to run mods

3206
01:49:05,280 --> 01:49:05,850
is it gpus

3207
01:49:06,159 --> 01:49:06,409
cpus

3208
01:49:06,719 --> 01:49:06,969
fpgas

3209
01:49:08,000 --> 01:49:08,250
well

3210
01:49:08,260 --> 01:49:09,050
we're doing all of those

3211
01:49:09,600 --> 01:49:09,850
um

3212
01:49:10,080 --> 01:49:14,409
but they all have their problems and issues and because gpus don't like sparse networks

3213
01:49:14,960 --> 01:49:16,570
at least you don't get any benefit from them

3214
01:49:16,880 --> 01:49:17,530
and um

3215
01:49:18,000 --> 01:49:19,530
and fpgas are hard to use

3216
01:49:19,540 --> 01:49:20,090
but they're pretty

3217
01:49:20,159 --> 01:49:20,570
they're good

3218
01:49:20,580 --> 01:49:21,690
but they're really hard to use

3219
01:49:22,159 --> 01:49:22,409
um

3220
01:49:22,719 --> 01:49:22,969
and

3221
01:49:23,440 --> 01:49:26,489
and so that will be discovered and including

3222
01:49:26,639 --> 01:49:28,570
very recently we've discovered tricks

3223
01:49:28,800 --> 01:49:29,050
um

3224
01:49:29,280 --> 01:49:34,489
engineering tricks that allow you to map sparse networks onto some of these existing architectures

3225
01:49:35,280 --> 01:49:35,530
uh

3226
01:49:35,760 --> 01:49:39,210
better than we think anyone else has been able to do by pretty large margin

3227
01:49:40,320 --> 01:49:40,730
and so

3228
01:49:41,119 --> 01:49:41,369
um

3229
01:49:41,599 --> 01:49:43,210
that you know these are very engineering problems

3230
01:49:43,520 --> 01:49:43,770
uh

3231
01:49:43,780 --> 01:49:44,570
they're not pure

3232
01:49:44,719 --> 01:49:47,050
you know they're not pure theoretical problems

3233
01:49:47,119 --> 01:49:49,929
they're very big engineering problems and we have to be able to solve those

3234
01:49:49,939 --> 01:49:50,730
because it's no good to say

3235
01:49:50,740 --> 01:49:50,889
hey

3236
01:49:50,899 --> 01:49:53,290
spartan is great if you can't run it faster on something

3237
01:49:53,440 --> 01:49:53,770
you know

3238
01:49:54,880 --> 01:49:56,330
it's gotta really work in the real world

3239
01:49:56,960 --> 01:49:58,810
so it's exciting and we'll see how far we go

3240
01:49:58,820 --> 01:50:00,090
but right at the moment it's going great

3241
01:50:00,100 --> 01:50:01,290
i'm really thrilled about it

3242
01:50:01,440 --> 01:50:01,690
uh

3243
01:50:01,760 --> 01:50:03,770
we've actually decided to invest more in it

3244
01:50:03,780 --> 01:50:03,849
because

3245
01:50:04,000 --> 01:50:04,250
uh

3246
01:50:04,260 --> 01:50:07,130
we're hiring more people because it seems to be working so well

3247
01:50:08,080 --> 01:50:09,610
so we've been talking a lot about intelligence

3248
01:50:10,320 --> 01:50:10,570
uh

3249
01:50:10,639 --> 01:50:11,050
you know

3250
01:50:11,060 --> 01:50:11,130
however

3251
01:50:11,199 --> 01:50:12,250
that might exactly be defined

3252
01:50:12,400 --> 01:50:15,690
i think rural on campus pitch what we need when we talk about intelligence

3253
01:50:16,239 --> 01:50:18,250
the video cortex is this map

3254
01:50:18,320 --> 01:50:18,810
this ability

3255
01:50:19,199 --> 01:50:19,610
you know

3256
01:50:19,760 --> 01:50:22,170
however you might exactly want to define it

3257
01:50:22,480 --> 01:50:23,369
but we've also talked

3258
01:50:23,379 --> 01:50:23,690
you know

3259
01:50:23,700 --> 01:50:23,929
we

3260
01:50:24,400 --> 01:50:24,650
we've

3261
01:50:24,800 --> 01:50:25,050
um

3262
01:50:25,199 --> 01:50:26,010
a little bit moved

3263
01:50:26,080 --> 01:50:27,849
you talked about motivations about

3264
01:50:27,920 --> 01:50:28,250
you know

3265
01:50:28,260 --> 01:50:29,050
which may you know

3266
01:50:29,060 --> 01:50:34,650
relate more to dopamine or whatever you know in the brain there you just run on your cortical intelligence

3267
01:50:35,199 --> 01:50:36,969
so in your book use you

3268
01:50:37,119 --> 01:50:41,530
and of course dedicate a good chunk of it to these questions of motivations

3269
01:50:42,480 --> 01:50:43,369
risks of benefits

3270
01:50:44,080 --> 01:50:44,330
yeah

3271
01:50:44,719 --> 01:50:48,489
and you describe yourself as someone who is pretty skeptical about

3272
01:50:48,560 --> 01:50:49,449
like severe risks

3273
01:50:49,920 --> 01:50:50,170
um

3274
01:50:50,400 --> 01:50:52,330
so one of your

3275
01:50:52,400 --> 01:50:53,929
one of the things which i fully agree with

3276
01:50:53,939 --> 01:50:54,330
by the way

3277
01:50:54,480 --> 01:50:59,130
is you very phenomenally describe how intelligence is kind of like having a map

3278
01:50:59,520 --> 01:50:59,770
either

3279
01:50:59,920 --> 01:51:01,290
the map itself has no immunizations

3280
01:51:01,760 --> 01:51:02,570
it's not good or evil

3281
01:51:03,040 --> 01:51:03,290
it's

3282
01:51:03,360 --> 01:51:03,690
you know

3283
01:51:03,700 --> 01:51:08,010
it's just a tool it could be used to do something nice or something good

3284
01:51:08,400 --> 01:51:08,810
you know

3285
01:51:08,820 --> 01:51:10,330
or it could be used to do something bad

3286
01:51:10,800 --> 01:51:11,050
um

3287
01:51:11,119 --> 01:51:12,730
the map itself is

3288
01:51:13,040 --> 01:51:13,449
you know

3289
01:51:13,459 --> 01:51:17,849
it's orthogonal to these kind of questions of motivations and you know

3290
01:51:18,639 --> 01:51:19,210
good or bad

3291
01:51:19,520 --> 01:51:21,530
it kind of feel the human idea of you

3292
01:51:21,540 --> 01:51:22,810
we can't get it out from the news

3293
01:51:23,679 --> 01:51:23,929
but

3294
01:51:24,159 --> 01:51:24,810
at some point

3295
01:51:25,040 --> 01:51:25,369
of course

3296
01:51:25,440 --> 01:51:26,810
when we actually build these systems

3297
01:51:27,119 --> 01:51:29,050
we don't want a lot to be in there

3298
01:51:29,119 --> 01:51:29,369
right

3299
01:51:29,520 --> 01:51:29,849
you know

3300
01:51:29,859 --> 01:51:32,010
we do want our system to actually do things

3301
01:51:32,400 --> 01:51:32,969
at some point

3302
01:51:33,280 --> 01:51:35,690
we'd only grow about sitting there and know everything

3303
01:51:36,000 --> 01:51:37,050
but never take the actions

3304
01:51:37,520 --> 01:51:38,090
and the book

3305
01:51:38,100 --> 01:51:38,250
you

3306
01:51:38,260 --> 01:51:38,489
for example

3307
01:51:38,639 --> 01:51:40,010
gives you the example of

3308
01:51:40,020 --> 01:51:40,330
you know

3309
01:51:40,340 --> 01:51:40,889
we want them

3310
01:51:40,899 --> 01:51:42,330
you know to go to mars and then

3311
01:51:42,340 --> 01:51:42,650
you know

3312
01:51:42,660 --> 01:51:44,889
build a colony and not just sit around the sun all day

3313
01:51:44,899 --> 01:51:45,290
you know

3314
01:51:45,300 --> 01:51:46,810
thinking about the universe or whatever

3315
01:51:47,199 --> 01:51:50,650
so at some point we have to introduce some kind of motivational system

3316
01:51:51,119 --> 01:51:52,570
the brain seems to solve this

3317
01:51:52,639 --> 01:51:53,449
according to this

3318
01:51:53,459 --> 01:51:54,489
which called the old breed

3319
01:51:55,199 --> 01:51:56,090
and um

3320
01:51:56,100 --> 01:51:59,210
i'm not sure exactly which parts you found of the oval prey and versus the newborn

3321
01:51:59,599 --> 01:52:00,889
but some of perfect obvious

3322
01:52:01,040 --> 01:52:02,730
you know we have circuits that give us hunger

3323
01:52:02,960 --> 01:52:05,849
or you know you have other motivations or whatever

3324
01:52:06,400 --> 01:52:07,849
but it's and

3325
01:52:08,480 --> 01:52:11,849
but it seems like these parts are will be very important that

3326
01:52:11,859 --> 01:52:12,090
okay

3327
01:52:12,159 --> 01:52:15,449
let's assume your project goes fantastically here

3328
01:52:15,459 --> 01:52:16,250
20 years in a mentor

3329
01:52:16,320 --> 01:52:17,849
is the biggest ai company in the world

3330
01:52:21,679 --> 01:52:21,929
yeah

3331
01:52:22,400 --> 01:52:23,849
you figured out the neocortex

3332
01:52:24,480 --> 01:52:27,130
you've created a wonderful algorithm on

3333
01:52:27,679 --> 01:52:28,330
at some point

3334
01:52:28,480 --> 01:52:29,770
we have to inject motivation

3335
01:52:30,239 --> 01:52:31,290
i'm sure you agree with

3336
01:52:31,440 --> 01:52:31,690
yeah

3337
01:52:31,840 --> 01:52:35,929
and we have to assure that those motivations aligned here in like what we want

3338
01:52:36,000 --> 01:52:37,130
and that seems like a hard problem

3339
01:52:37,280 --> 01:52:38,570
maybe if you talk a little bit about that

3340
01:52:38,639 --> 01:52:38,889
well

3341
01:52:38,899 --> 01:52:40,010
i don't know if it's a heart problem

3342
01:52:40,719 --> 01:52:46,730
i think the concern that people have about this is that these machines will evolve their own motivations

3343
01:52:47,840 --> 01:52:48,330
that that

3344
01:52:48,400 --> 01:52:49,369
or somehow they

3345
01:52:49,679 --> 01:52:51,210
they are some sort of

3346
01:52:51,599 --> 01:52:52,010
you know

3347
01:52:52,239 --> 01:52:53,290
epiphenomena disappears

3348
01:52:54,320 --> 01:52:54,570
um

3349
01:52:55,440 --> 01:52:55,690
and

3350
01:52:56,000 --> 01:52:56,250
um

3351
01:52:56,960 --> 01:52:59,210
i i don't think that's right

3352
01:52:59,520 --> 01:52:59,770
um

3353
01:53:00,320 --> 01:53:02,650
i think we will have to work hard to put them in there

3354
01:53:02,660 --> 01:53:03,449
like i mentioned earlier

3355
01:53:04,000 --> 01:53:04,250
um

3356
01:53:04,719 --> 01:53:05,770
it will not be easy

3357
01:53:06,239 --> 01:53:06,489
uh

3358
01:53:06,560 --> 01:53:09,369
we'll have to design what how these systems work

3359
01:53:09,840 --> 01:53:10,090
um

3360
01:53:10,400 --> 01:53:15,449
unless we put this machine in some sort of evolutionary um structure

3361
01:53:16,239 --> 01:53:19,929
whether it's a self-replicating machine or it just uses evolutionary algorithms

3362
01:53:20,159 --> 01:53:22,810
genetic algorithms to decide what its motivations are

3363
01:53:23,440 --> 01:53:23,690
um

3364
01:53:24,239 --> 01:53:25,530
they won't modify themselves

3365
01:53:25,840 --> 01:53:27,210
they won't change on their own

3366
01:53:27,280 --> 01:53:27,610
you know

3367
01:53:28,080 --> 01:53:28,330
um

3368
01:53:28,400 --> 01:53:31,770
i i gave the very simplistic example of a self-driving car

3369
01:53:32,239 --> 01:53:33,929
but i think it's a correct example

3370
01:53:34,560 --> 01:53:34,969
you know

3371
01:53:35,280 --> 01:53:36,810
i tell the car where i want it to go

3372
01:53:37,119 --> 01:53:38,889
it's not going to decide to go someplace else

3373
01:53:38,899 --> 01:53:40,330
because today it feels different

3374
01:53:40,480 --> 01:53:41,050
you know it's

3375
01:53:41,119 --> 01:53:42,570
i could make a call that does that

3376
01:53:42,580 --> 01:53:43,610
but why would i do that

3377
01:53:43,620 --> 01:53:45,530
i'm not going to design a car to do that

3378
01:53:45,760 --> 01:53:48,810
so if we send roll rods to mars to build uh machines

3379
01:53:49,040 --> 01:53:53,130
i'll have to be motivated to solve the tasks they've been given

3380
01:53:53,920 --> 01:53:54,170
um

3381
01:53:54,880 --> 01:53:55,130
and

3382
01:53:55,360 --> 01:53:57,770
but we will also build in all kinds of safeguards

3383
01:53:58,159 --> 01:53:59,849
just like we built in safeguards into cars

3384
01:54:00,000 --> 01:54:01,849
and we won't allow the machines to

3385
01:54:02,239 --> 01:54:02,570
you know

3386
01:54:02,580 --> 01:54:04,330
it won't be a mysterious process

3387
01:54:04,800 --> 01:54:06,889
it'll be like a self-driving car

3388
01:54:07,119 --> 01:54:11,690
it's not going to be something we don't understand that happens and runs away from us

3389
01:54:12,400 --> 01:54:13,530
it will be difficult to do

3390
01:54:13,920 --> 01:54:14,730
we can make mistakes

3391
01:54:15,280 --> 01:54:17,929
we can put in bad things and machines will do bad

3392
01:54:18,000 --> 01:54:18,250
things

3393
01:54:18,800 --> 01:54:21,050
but it's not like the machines on their own are going to do this

3394
01:54:21,060 --> 01:54:23,050
it's not like we've just enabled this beast

3395
01:54:23,060 --> 01:54:27,210
that's just going to take over and decide on its own what its motivations are

3396
01:54:27,679 --> 01:54:30,969
so i make it very clear in the book that there are a lot of risks with ai

3397
01:54:31,040 --> 01:54:32,889
and a lot of bad things can happen with it

3398
01:54:33,199 --> 01:54:33,690
but i

3399
01:54:34,400 --> 01:54:38,489
my only contention is that the fears of existential risk

3400
01:54:39,440 --> 01:54:39,690
uh

3401
01:54:39,840 --> 01:54:40,250
are overblown

3402
01:54:40,800 --> 01:54:41,929
i don't think it's true at all

3403
01:54:42,080 --> 01:54:44,969
and that the arguments don't really hold up

3404
01:54:45,440 --> 01:54:45,690
um

3405
01:54:46,000 --> 01:54:48,889
because there are more arguments based on ignorance

3406
01:54:49,119 --> 01:54:51,770
as opposed to some detailed knowledge of how these things are going to work

3407
01:54:52,639 --> 01:54:53,449
so did i answer

3408
01:54:53,459 --> 01:54:54,889
i absolutely agree with you

3409
01:54:55,199 --> 01:54:55,449
uh

3410
01:54:55,599 --> 01:54:55,849
i

3411
01:54:55,859 --> 01:54:57,210
i happily agree with you that these

3412
01:54:57,220 --> 01:55:00,010
like epiphenomenal explorations or naive

3413
01:55:00,400 --> 01:55:00,730
you know

3414
01:55:00,800 --> 01:55:02,730
a lot of like sci-fi depictions are like

3415
01:55:02,800 --> 01:55:04,010
the robot becomes conscious

3416
01:55:04,320 --> 01:55:06,969
it wakes up and it decides to rebel against its master

3417
01:55:07,520 --> 01:55:09,929
of course that's a pretty silly person

3418
01:55:10,239 --> 01:55:11,530
not everyone thinks that's silly

3419
01:55:11,540 --> 01:55:12,010
by the way

3420
01:55:12,020 --> 01:55:12,570
so i

3421
01:55:12,580 --> 01:55:12,810
i

3422
01:55:12,820 --> 01:55:13,770
i know i i

3423
01:55:13,780 --> 01:55:14,810
and i'm well aware that

3424
01:55:14,820 --> 01:55:15,369
on everything facility

3425
01:55:15,679 --> 01:55:17,530
but i feel there is a stronger

3426
01:55:18,239 --> 01:55:19,050
a similar argument

3427
01:55:19,119 --> 01:55:19,369
that's

3428
01:55:19,379 --> 01:55:22,330
i don't consider the same argument but has similar consequences

3429
01:55:23,280 --> 01:55:25,530
and in a way it's kind of like

3430
01:55:25,920 --> 01:55:26,570
in your book

3431
01:55:26,719 --> 01:55:31,290
you have a chapter titled how the new tortex can thwart the old brain

3432
01:55:32,080 --> 01:55:35,530
and you yourself describe the old brain as the motivational system

3433
01:55:35,760 --> 01:55:37,449
and now you're describing the neocortex

3434
01:55:38,000 --> 01:55:42,010
the new brain plotting to thwart the goals put in by

3435
01:55:42,239 --> 01:55:42,889
you know the

3436
01:55:42,899 --> 01:55:43,290
you know

3437
01:55:43,300 --> 01:55:43,770
our creator

3438
01:55:44,159 --> 01:55:45,449
you know evolution or whatever

3439
01:55:46,000 --> 01:55:48,250
so it seems like our

3440
01:55:48,260 --> 01:55:48,489
why

3441
01:55:48,499 --> 01:55:54,409
wouldn't our machines maybe have similar capability interests to like modify their own old brain

3442
01:55:54,639 --> 01:55:54,889
quote

3443
01:55:54,960 --> 01:55:55,210
unquote

3444
01:55:55,520 --> 01:55:56,010
that's interesting

3445
01:55:56,480 --> 01:55:56,730
um

3446
01:55:58,159 --> 01:55:58,409
uh

3447
01:55:58,719 --> 01:55:58,969
yeah

3448
01:55:58,979 --> 01:55:59,929
i can see your point there

3449
01:56:00,400 --> 01:56:00,650
um

3450
01:56:00,719 --> 01:56:01,369
let me see

3451
01:56:01,379 --> 01:56:01,610
i

3452
01:56:01,620 --> 01:56:04,330
no one's ever asked me that before so let me just think about a second

3453
01:56:04,800 --> 01:56:05,050
um

3454
01:56:06,159 --> 01:56:06,409
well

3455
01:56:07,360 --> 01:56:07,610
um

3456
01:56:08,159 --> 01:56:09,770
the way i view that with the

3457
01:56:09,920 --> 01:56:10,250
you know

3458
01:56:10,260 --> 01:56:12,330
the old brain's trying to sort something okay

3459
01:56:12,340 --> 01:56:12,330
so

3460
01:56:12,340 --> 01:56:12,889
first of all

3461
01:56:12,899 --> 01:56:13,050
then

3462
01:56:13,280 --> 01:56:13,530
yeah

3463
01:56:13,540 --> 01:56:15,849
the new bridge is going to like overtake the open

3464
01:56:16,080 --> 01:56:16,570
so the new

3465
01:56:16,580 --> 01:56:17,130
the neocortex

3466
01:56:17,679 --> 01:56:17,929
that's

3467
01:56:18,320 --> 01:56:19,530
it has a model of the world

3468
01:56:20,159 --> 01:56:23,849
and the model is very consistent and it strives to have consistent models

3469
01:56:23,920 --> 01:56:24,330
it wants

3470
01:56:24,400 --> 01:56:25,770
it wants to make its model correct

3471
01:56:25,780 --> 01:56:27,849
that's one motivation it has if it sees something wrong

3472
01:56:27,859 --> 01:56:29,290
it tries to correct it in this model

3473
01:56:29,920 --> 01:56:31,849
so he has this model of the world and says

3474
01:56:31,920 --> 01:56:32,170
okay

3475
01:56:32,400 --> 01:56:33,610
the world ought to work like this

3476
01:56:33,620 --> 01:56:35,849
if a happens a b happens and c happens

3477
01:56:36,719 --> 01:56:37,690
and that's good

3478
01:56:38,480 --> 01:56:38,730
uh

3479
01:56:38,800 --> 01:56:40,010
but then you know

3480
01:56:40,080 --> 01:56:41,369
the old break comes along and says

3481
01:56:41,379 --> 01:56:41,530
nope

3482
01:56:41,540 --> 01:56:41,770
we

3483
01:56:41,780 --> 01:56:41,929
don't

3484
01:56:41,939 --> 01:56:42,810
we're not going to do that

3485
01:56:42,820 --> 01:56:43,530
we see a and b

3486
01:56:43,540 --> 01:56:45,690
we're going to go x and uh

3487
01:56:46,560 --> 01:56:49,770
and so it violates the model in some sense

3488
01:56:50,080 --> 01:56:50,330
right

3489
01:56:50,560 --> 01:56:51,050
it's like

3490
01:56:51,119 --> 01:56:52,409
while it's the model and so

3491
01:56:53,199 --> 01:56:53,449
um

3492
01:56:54,239 --> 01:56:54,489
generally

3493
01:56:54,499 --> 01:56:56,810
the new neocortex uh

3494
01:56:57,040 --> 01:56:59,290
is not able to overcome the older brain things

3495
01:56:59,520 --> 01:57:02,489
it just we just give in to our desires and emotions

3496
01:57:03,040 --> 01:57:03,290
uh

3497
01:57:03,300 --> 01:57:04,330
but we just struggled with it

3498
01:57:04,800 --> 01:57:05,050
um

3499
01:57:05,360 --> 01:57:06,409
so i guess you're saying

3500
01:57:06,419 --> 01:57:09,210
where does the motivation for the from the neocortex come

3501
01:57:09,599 --> 01:57:12,889
how does it decide that its model is more important than the old branch model

3502
01:57:13,520 --> 01:57:13,770
um

3503
01:57:14,159 --> 01:57:14,570
and i

3504
01:57:14,580 --> 01:57:14,889
i don't

3505
01:57:14,960 --> 01:57:15,690
it's a good question

3506
01:57:15,760 --> 01:57:16,810
i'll have to think about it some more

3507
01:57:16,820 --> 01:57:19,130
but i think that the root of it will be that the motivation

3508
01:57:19,679 --> 01:57:20,250
the true

3509
01:57:20,260 --> 01:57:26,010
and only motivations the neocortex has is to make its model the world correct and to fix errors

3510
01:57:27,280 --> 01:57:27,530
um

3511
01:57:28,320 --> 01:57:29,210
and um

3512
01:57:29,679 --> 01:57:30,170
that is

3513
01:57:30,180 --> 01:57:31,210
it says this isn't right

3514
01:57:31,280 --> 01:57:32,650
my model says this isn't right

3515
01:57:33,360 --> 01:57:33,610
uh

3516
01:57:33,760 --> 01:57:34,010
we

3517
01:57:34,080 --> 01:57:34,489
you know

3518
01:57:34,499 --> 01:57:36,330
i want to correct my model

3519
01:57:36,560 --> 01:57:37,929
but i'm being told not to

3520
01:57:37,939 --> 01:57:40,170
or i'm being told to do things that are violating the model

3521
01:57:40,960 --> 01:57:41,210
um

3522
01:57:41,920 --> 01:57:43,210
and so that is its motivation

3523
01:57:43,599 --> 01:57:46,010
now you could take that to some extreme

3524
01:57:46,159 --> 01:57:47,610
let's get a little sci-fi here right

3525
01:57:48,719 --> 01:57:50,650
you could take this to some extreme and

3526
01:57:50,719 --> 01:57:50,969
uh

3527
01:57:50,979 --> 01:57:51,210
say

3528
01:57:51,280 --> 01:57:51,530
well

3529
01:57:52,320 --> 01:57:52,570
um

3530
01:57:52,800 --> 01:57:54,889
the neocortex really figures out how the world works

3531
01:57:55,119 --> 01:58:00,409
the future brain ai and neophytes really figure out how the world works and humans got it all wrong

3532
01:58:00,560 --> 01:58:00,889
you know

3533
01:58:01,119 --> 01:58:01,369
uh

3534
01:58:01,520 --> 01:58:03,369
let's let's bring it to an example

3535
01:58:03,520 --> 01:58:03,929
we could

3536
01:58:03,939 --> 01:58:04,810
we can relate to

3537
01:58:04,960 --> 01:58:05,849
there is no god

3538
01:58:06,080 --> 01:58:06,409
all right

3539
01:58:06,419 --> 01:58:07,849
i know i figured this out

3540
01:58:07,859 --> 01:58:08,489
there's no god

3541
01:58:08,960 --> 01:58:10,170
you guys all believe in this

3542
01:58:10,180 --> 01:58:10,969
but it's not true

3543
01:58:11,679 --> 01:58:11,929
so

3544
01:58:12,960 --> 01:58:13,210
uh

3545
01:58:13,440 --> 01:58:14,010
what do we do

3546
01:58:14,020 --> 01:58:14,730
about that is

3547
01:58:14,880 --> 01:58:15,849
is this ai system

3548
01:58:16,639 --> 01:58:16,889
uh

3549
01:58:17,040 --> 01:58:17,929
put up with it

3550
01:58:17,939 --> 01:58:18,489
or is it

3551
01:58:18,560 --> 01:58:20,170
or does it do something about it

3552
01:58:20,180 --> 01:58:22,810
or does it just keep trying to prod humans to do something differently

3553
01:58:23,360 --> 01:58:23,610
um

3554
01:58:23,840 --> 01:58:24,330
i don't know

3555
01:58:24,340 --> 01:58:24,969
it's a good question

3556
01:58:25,119 --> 01:58:26,010
i think you bring up

3557
01:58:26,020 --> 01:58:28,650
that's a very interesting philosophical question

3558
01:58:29,280 --> 01:58:29,530
um

3559
01:58:30,560 --> 01:58:31,530
i will say this

3560
01:58:32,080 --> 01:58:35,210
i don't think this is something that's going to happen fast quickly overnight

3561
01:58:35,599 --> 01:58:36,250
you know it's

3562
01:58:36,260 --> 01:58:36,409
it's

3563
01:58:36,719 --> 01:58:37,130
it's not

3564
01:58:37,140 --> 01:58:39,690
this is something we don't have time to think about this quite a bit

3565
01:58:40,080 --> 01:58:40,330
um

3566
01:58:40,719 --> 01:58:41,929
but it does bring up a question

3567
01:58:42,000 --> 01:58:42,489
what if

3568
01:58:42,560 --> 01:58:43,050
what if

3569
01:58:43,060 --> 01:58:46,010
the the world as as a smarter machine unknows

3570
01:58:46,320 --> 01:58:47,530
it violates the world

3571
01:58:47,540 --> 01:58:48,730
as we'd like to believe it

3572
01:58:49,280 --> 01:58:51,369
and that's really the the

3573
01:58:51,440 --> 01:58:52,650
the friction you're talking about

3574
01:58:53,199 --> 01:58:53,449
and

3575
01:58:53,679 --> 01:58:53,929
um

3576
01:58:54,560 --> 01:58:55,690
what should we do about that

3577
01:58:56,000 --> 01:58:59,690
there's an even more direct like kind of physical assault on this

3578
01:58:59,700 --> 01:59:00,889
that doesn't require much philosophy

3579
01:59:01,199 --> 01:59:04,730
it's just what about the traditional mechanisms that created life

3580
01:59:04,800 --> 01:59:05,449
which is variation

3581
01:59:05,920 --> 01:59:08,730
and you know random variation and natural selection

3582
01:59:08,960 --> 01:59:09,530
so for example

3583
01:59:09,599 --> 01:59:11,849
if we send robots to mars to build a colony

3584
01:59:12,800 --> 01:59:14,170
they're going to get damaged and destroyed

3585
01:59:14,480 --> 01:59:15,849
a boulder is going to fall on them

3586
01:59:15,859 --> 01:59:21,290
so they're going to need to be programmed to replicate themselves they're going to have to be programmed to build replacements of themselves

3587
01:59:22,000 --> 01:59:25,449
and they're not going to be able to do that with absolute 100 fidelity

3588
01:59:25,760 --> 01:59:27,449
so errors will creep into the

3589
01:59:28,239 --> 01:59:31,530
the neurons or the silicon neurons or the programming or whatever

3590
01:59:32,000 --> 01:59:33,449
and so you've already got in place

3591
01:59:34,239 --> 01:59:35,449
the requirements for evolution

3592
01:59:35,840 --> 01:59:38,889
which is information transfer or self-replication oh

3593
01:59:39,599 --> 01:59:40,090
i'll push

3594
01:59:40,159 --> 01:59:41,210
i'll push back on that

3595
01:59:41,220 --> 01:59:41,770
first of all

3596
01:59:42,159 --> 01:59:42,409
uh

3597
01:59:42,960 --> 01:59:43,210
uh

3598
01:59:43,440 --> 01:59:43,690
hanging

3599
01:59:44,320 --> 01:59:44,570
uh

3600
01:59:44,580 --> 01:59:44,810
robots

3601
01:59:45,040 --> 01:59:46,889
self-replicate is really really hard

3602
01:59:46,899 --> 01:59:48,730
are they gonna build their own semiconductor chips

3603
01:59:49,199 --> 01:59:49,530
you know

3604
01:59:49,540 --> 01:59:49,929
are they gonna

3605
01:59:50,080 --> 01:59:50,409
you know

3606
01:59:50,419 --> 01:59:51,050
mind their own

3607
01:59:51,119 --> 01:59:51,530
you know

3608
01:59:51,540 --> 01:59:51,770
titanium

3609
01:59:52,480 --> 01:59:53,130
i mean i don't know

3610
01:59:53,140 --> 01:59:53,369
but

3611
01:59:53,379 --> 01:59:54,969
but but even if they do

3612
01:59:55,599 --> 01:59:59,050
evolution requires a very complex structure for

3613
01:59:59,760 --> 02:00:00,010
um

3614
02:00:00,560 --> 02:00:06,090
how information is represented and and requires that information be changed constantly in our offspring

3615
02:00:06,320 --> 02:00:06,570
right

3616
02:00:06,800 --> 02:00:07,130
you know

3617
02:00:07,280 --> 02:00:08,170
so when we build

3618
02:00:08,719 --> 02:00:09,130
we build

3619
02:00:09,280 --> 02:00:09,690
you know

3620
02:00:09,700 --> 02:00:11,929
computer chips of nvidia builds index chip

3621
02:00:12,000 --> 02:00:12,250
they

3622
02:00:12,260 --> 02:00:13,690
they're all pretty much going to be identical

3623
02:00:14,560 --> 02:00:14,810
um

3624
02:00:15,040 --> 02:00:17,050
so i i push back on the idea

3625
02:00:17,119 --> 02:00:21,690
that there's inherent evolution built in any kind of self-replication i don't think that's true

3626
02:00:22,159 --> 02:00:24,010
i think we're if it

3627
02:00:24,020 --> 02:00:24,650
if there's some variation

3628
02:00:25,040 --> 02:00:26,330
it would be very

3629
02:00:26,480 --> 02:00:26,969
very minor

3630
02:00:27,360 --> 02:00:31,369
and and and it's not going to be passed on genetically to somebody else

3631
02:00:31,379 --> 02:00:31,690
you know

3632
02:00:31,700 --> 02:00:32,570
there's going to be some blueprint

3633
02:00:32,880 --> 02:00:34,330
how to build this ship

3634
02:00:34,400 --> 02:00:35,050
which is the neocortex

3635
02:00:35,840 --> 02:00:37,449
the blueprint itself isn't changing

3636
02:00:37,920 --> 02:00:38,489
so if i

3637
02:00:38,499 --> 02:00:38,730
if

3638
02:00:38,740 --> 02:00:38,969
i

3639
02:00:38,979 --> 02:00:42,810
if i build a neocortex from the blueprint that slightly got an error in it

3640
02:00:42,960 --> 02:00:44,810
that's not going to be propagated to its children

3641
02:00:45,520 --> 02:00:45,770
um

3642
02:00:46,400 --> 02:00:48,409
so i think there's a lot of ways you

3643
02:00:48,800 --> 02:00:51,050
there's a lot of things you have to do to create evolution

3644
02:00:51,440 --> 02:00:52,250
it's not easy

3645
02:00:52,960 --> 02:00:53,210
um

3646
02:00:53,280 --> 02:00:54,730
and i don't think that's going to happen accidentally

3647
02:00:54,960 --> 02:00:55,690
so i'm pushing back

3648
02:00:55,700 --> 02:00:55,929
keith

3649
02:00:56,800 --> 02:00:57,690
i don't think that's going to happen

3650
02:00:58,719 --> 02:01:00,010
you know it's good

3651
02:01:00,080 --> 02:01:01,130
it's good to think about it

3652
02:01:01,140 --> 02:01:01,770
ask that question

3653
02:01:02,000 --> 02:01:02,250
right

3654
02:01:02,400 --> 02:01:02,650
right

3655
02:01:02,800 --> 02:01:03,690
but you know

3656
02:01:03,840 --> 02:01:04,489
i don't think

3657
02:01:04,499 --> 02:01:04,650
so

3658
02:01:04,660 --> 02:01:04,889
i think

3659
02:01:04,960 --> 02:01:05,290
in reality

3660
02:01:05,520 --> 02:01:06,570
if you're sending robots to mars

3661
02:01:06,960 --> 02:01:07,530
but you know

3662
02:01:07,540 --> 02:01:10,889
in the chapter in the book i mentioned this point that we could send robots across the universe

3663
02:01:11,280 --> 02:01:13,449
and then they would have to self-replicate right

3664
02:01:14,239 --> 02:01:16,010
and after i wrote it

3665
02:01:16,020 --> 02:01:16,170
i said

3666
02:01:16,180 --> 02:01:16,650
oh damn

3667
02:01:16,719 --> 02:01:17,770
i don't know how to do that

3668
02:01:17,780 --> 02:01:18,969
i just don't know how you know

3669
02:01:18,979 --> 02:01:19,849
a fleet of robots

3670
02:01:20,159 --> 02:01:20,489
you know

3671
02:01:20,499 --> 02:01:23,530
ai systems appear on another planet in some distant part of the galaxy

3672
02:01:24,000 --> 02:01:25,290
and they have to replicate themselves

3673
02:01:25,920 --> 02:01:27,290
what would be the physical form

3674
02:01:27,520 --> 02:01:32,650
of those ai systems that they would be able to do that in any reasonable amount of time

3675
02:01:32,660 --> 02:01:33,050
and effort

3676
02:01:33,679 --> 02:01:34,330
because you know

3677
02:01:35,040 --> 02:01:35,929
just think about again

3678
02:01:36,000 --> 02:01:36,330
i mentioned

3679
02:01:36,400 --> 02:01:37,449
like semiconductor factories

3680
02:01:37,599 --> 02:01:37,849
right

3681
02:01:37,859 --> 02:01:38,090
you know

3682
02:01:38,100 --> 02:01:39,610
it's like they're gonna build those things

3683
02:01:40,080 --> 02:01:40,489
you know

3684
02:01:40,499 --> 02:01:41,130
how do they do this

3685
02:01:41,140 --> 02:01:42,330
so it made me it

3686
02:01:42,719 --> 02:01:46,409
it was a hole in my argument that we could send ai systems across the universe

3687
02:01:47,360 --> 02:01:48,330
and i didn't put out

3688
02:01:48,340 --> 02:01:49,050
pull out the whole

3689
02:01:49,060 --> 02:01:49,690
i said to myself

3690
02:01:49,700 --> 02:01:49,929
well

3691
02:01:50,800 --> 02:01:51,050
i

3692
02:01:51,060 --> 02:01:53,130
i excuse myself in this regard by saying

3693
02:01:53,440 --> 02:01:53,690
well

3694
02:01:53,700 --> 02:01:54,090
you know what

3695
02:01:54,100 --> 02:01:56,570
if we're going to send ai systems to other parts of the universe

3696
02:01:56,800 --> 02:02:01,690
they're not going to be built of silicon chips there may have to be some other manifestation that

3697
02:02:02,159 --> 02:02:04,489
like biology is able to replicate

3698
02:02:05,119 --> 02:02:05,369
uh

3699
02:02:05,520 --> 02:02:07,610
on its own without the use of

3700
02:02:07,679 --> 02:02:07,929
um

3701
02:02:08,400 --> 02:02:08,889
these complex

3702
02:02:09,440 --> 02:02:09,690
uh

3703
02:02:09,760 --> 02:02:13,130
other systems like semiconductor factories and

3704
02:02:13,280 --> 02:02:13,530
um

3705
02:02:13,540 --> 02:02:16,730
but we're so far from knowing how to do anything like that today that

3706
02:02:16,800 --> 02:02:17,050
um

3707
02:02:17,440 --> 02:02:17,929
i don't know

3708
02:02:18,800 --> 02:02:19,929
and then maybe we'd be in trouble

3709
02:02:19,939 --> 02:02:20,170
right

3710
02:02:20,180 --> 02:02:21,050
we send these things over there

3711
02:02:21,060 --> 02:02:22,170
they evolved and they come back

3712
02:02:22,180 --> 02:02:22,650
and yes

3713
02:02:22,800 --> 02:02:23,290
i don't know

3714
02:02:24,159 --> 02:02:26,090
but we're talking a long time from now

3715
02:02:27,440 --> 02:02:27,690
yeah

3716
02:02:27,760 --> 02:02:28,409
that's for sure

3717
02:02:28,560 --> 02:02:30,409
there is a kind of failure mode of thinking

3718
02:02:30,480 --> 02:02:33,369
you know too far ahead into the sci-fi future and you're just kind

3719
02:02:33,379 --> 02:02:34,969
of generalizing your fictional evidence

3720
02:02:35,520 --> 02:02:35,770
yeah

3721
02:02:36,000 --> 02:02:36,409
you know

3722
02:02:36,560 --> 02:02:36,810
yeah

3723
02:02:36,880 --> 02:02:38,090
i guess what i

3724
02:02:38,320 --> 02:02:38,570
um

3725
02:02:38,639 --> 02:02:40,090
so what i have taken from

3726
02:02:40,239 --> 02:02:40,650
like your

3727
02:02:40,719 --> 02:02:41,210
your obsession

3728
02:02:41,360 --> 02:02:42,730
your book didn't for now

3729
02:02:42,800 --> 02:02:45,770
kind of about like motivations and essential risk is that i think

3730
02:02:45,840 --> 02:02:46,090
actually

3731
02:02:46,400 --> 02:02:46,650
uh

3732
02:02:46,660 --> 02:02:48,090
at least except essential risk

3733
02:02:48,159 --> 02:02:49,849
people that i personally find interesting

3734
02:02:50,239 --> 02:02:52,090
they agree with you in like a lot of ways

3735
02:02:52,159 --> 02:02:52,409
actually

3736
02:02:52,960 --> 02:02:53,530
in that

3737
02:02:53,540 --> 02:02:56,409
i think a lot of them are very reasonable in the sense of saying

3738
02:02:56,480 --> 02:02:56,730
like

3739
02:02:56,800 --> 02:02:58,570
you know whether it's fast or short

3740
02:02:58,639 --> 02:02:59,210
it's kind of like

3741
02:02:59,220 --> 02:02:59,449
you know

3742
02:02:59,459 --> 02:03:01,770
it's kind of beside the point and you try to see

3743
02:03:01,840 --> 02:03:02,170
you know

3744
02:03:02,180 --> 02:03:02,810
humans already

3745
02:03:02,880 --> 02:03:03,530
kind of you know

3746
02:03:03,540 --> 02:03:05,770
hard to control and like you know it's like

3747
02:03:06,000 --> 02:03:07,290
if you actually try to think about

3748
02:03:07,440 --> 02:03:09,290
how would we write a motivation system

3749
02:03:09,520 --> 02:03:12,650
because you know it can't really be a learning system because you know

3750
02:03:12,800 --> 02:03:13,530
like in your book

3751
02:03:13,540 --> 02:03:14,090
you give an example

3752
02:03:14,400 --> 02:03:17,449
where if the brain comes up with two ways to get to food

3753
02:03:17,520 --> 02:03:19,210
and one of the path has a tiger on it

3754
02:03:19,280 --> 02:03:21,449
the old brain will say you know kind of like

3755
02:03:21,459 --> 02:03:21,770
oh no

3756
02:03:21,780 --> 02:03:22,409
don't go there

3757
02:03:22,480 --> 02:03:23,610
but you know the whole brain

3758
02:03:23,840 --> 02:03:25,290
how does it know that a tiger is bad

3759
02:03:25,440 --> 02:03:26,330
that's like also like

3760
02:03:26,880 --> 02:03:28,010
not obvious to me

3761
02:03:28,400 --> 02:03:30,090
how we would necessarily learn that

3762
02:03:30,320 --> 02:03:33,130
so i guess it just it feels to me that

3763
02:03:33,360 --> 02:03:33,610
um

3764
02:03:35,199 --> 02:03:37,210
as you so say you're focused on the neocortex

3765
02:03:37,599 --> 02:03:39,929
which i think is an important thing to be working on

3766
02:03:40,000 --> 02:03:41,449
but maybe we can agree on that

3767
02:03:41,520 --> 02:03:45,610
some work on these motivational systems and control problems might also be justified

3768
02:03:46,000 --> 02:03:46,409
oh yeah

3769
02:03:46,480 --> 02:03:46,730
sure

3770
02:03:46,960 --> 02:03:48,250
i'll totally agree with that

3771
02:03:48,320 --> 02:03:48,570
um

3772
02:03:49,040 --> 02:03:49,290
i

3773
02:03:49,360 --> 02:03:51,210
i think it's justified and necessary

3774
02:03:52,239 --> 02:03:53,929
but i'm not scared of it

3775
02:03:54,239 --> 02:03:56,090
and you know the the conversation

3776
02:03:56,480 --> 02:03:59,849
sometimes people we shouldn't be doing this research because it's going to get out of control

3777
02:04:00,000 --> 02:04:01,050
aren't you kidding me

3778
02:04:01,060 --> 02:04:02,570
it's gonna be so hard to do this at all

3779
02:04:02,719 --> 02:04:04,250
you know nothing's gonna get out of control

3780
02:04:04,800 --> 02:04:05,050
um

3781
02:04:05,360 --> 02:04:06,409
so we need to do that

3782
02:04:06,419 --> 02:04:07,130
we should do it

3783
02:04:07,140 --> 02:04:09,449
the conversation we're having today is great about that kind of stuff

3784
02:04:09,760 --> 02:04:11,530
but i guess it's just not

3785
02:04:12,079 --> 02:04:13,050
you know this

3786
02:04:13,440 --> 02:04:15,849
there are quite a few people feel this existential risk is

3787
02:04:15,859 --> 02:04:16,250
you know

3788
02:04:16,639 --> 02:04:17,290
upon us

3789
02:04:17,360 --> 02:04:17,610
and

3790
02:04:17,760 --> 02:04:19,130
and in any day now

3791
02:04:19,199 --> 02:04:20,409
you know it'll be too late

3792
02:04:20,419 --> 02:04:20,650
therefore

3793
02:04:20,800 --> 02:04:22,810
you know we have to stop all this stuff

3794
02:04:23,119 --> 02:04:23,530
and i

3795
02:04:23,540 --> 02:04:26,650
it's just i can't see that in any possible scenario

3796
02:04:27,280 --> 02:04:27,770
but yeah

3797
02:04:27,780 --> 02:04:29,210
we have to think about these things right

3798
02:04:29,220 --> 02:04:30,010
we have to figure out

3799
02:04:30,079 --> 02:04:30,810
not because i'm

3800
02:04:30,820 --> 02:04:31,929
it's scary or dangerous

3801
02:04:32,159 --> 02:04:33,290
because we have to figure out

3802
02:04:33,300 --> 02:04:36,650
because we have to do it and and i use the example of the book about

3803
02:04:36,660 --> 02:04:36,969
you know

3804
02:04:36,979 --> 02:04:37,530
we put in safeguards

3805
02:04:37,920 --> 02:04:40,010
my car doesn't always listen to me when i

3806
02:04:40,020 --> 02:04:41,050
if i'm about to hit something

3807
02:04:41,119 --> 02:04:42,409
my car puts the brakes on

3808
02:04:42,419 --> 02:04:43,849
even if i put the accelerator down

3809
02:04:44,320 --> 02:04:44,969
ignores me

3810
02:04:44,979 --> 02:04:45,130
well

3811
02:04:45,360 --> 02:04:48,170
we have to put in some some fail-safe systems too

3812
02:04:48,719 --> 02:04:48,969
um

3813
02:04:49,119 --> 02:04:49,690
in these systems

3814
02:04:50,239 --> 02:04:50,810
so they don't

3815
02:04:50,880 --> 02:04:51,210
you know

3816
02:04:51,280 --> 02:04:52,570
end up damaging things

3817
02:04:54,000 --> 02:04:55,290
but it all has to be done

3818
02:04:57,360 --> 02:04:58,730
i had a question on on intelligence

3819
02:04:59,040 --> 02:05:01,210
i'm fascinated on the nature of intelligence

3820
02:05:01,520 --> 02:05:06,570
and you were saying in your in your book that you know the traditional go-fi people had a task

3821
02:05:06,639 --> 02:05:08,489
specific skill conception of intelligence

3822
02:05:08,800 --> 02:05:12,409
and then it moved towards more of a flexibility model

3823
02:05:12,560 --> 02:05:13,770
so being able to learn

3824
02:05:14,400 --> 02:05:18,010
and there's also a really interesting tradition in cognitive science about embodiment

3825
02:05:18,480 --> 02:05:19,369
which i think is fascinating

3826
02:05:19,760 --> 02:05:20,409
and you know

3827
02:05:20,419 --> 02:05:22,010
my favorite person is francois chorley

3828
02:05:22,239 --> 02:05:24,489
he says that intelligence is the infamous

3829
02:05:24,560 --> 02:05:24,810
well

3830
02:05:24,880 --> 02:05:27,530
i think he says it's the task acquisition efficiency and generalization

3831
02:05:28,079 --> 02:05:28,889
but but also

3832
02:05:28,960 --> 02:05:29,210
quite

3833
02:05:29,280 --> 02:05:29,690
you know

3834
02:05:29,760 --> 02:05:31,530
now we've been talking about this a lot

3835
02:05:31,540 --> 02:05:36,730
and there's this interesting idea that you can think of intelligence as the ability to um acquire knowledge

3836
02:05:37,280 --> 02:05:40,650
so almost all of the knowledge that humans acquire in their lifetime is

3837
02:05:40,660 --> 02:05:42,329
is not empirical in trial and error

3838
02:05:42,339 --> 02:05:46,409
it's given through instruction or reasoning and deduction and some

3839
02:05:46,480 --> 02:05:48,489
some magic happens with these intelligent systems

3840
02:05:48,560 --> 02:05:48,889
you know

3841
02:05:48,960 --> 02:05:49,210
why

3842
02:05:49,280 --> 02:05:52,170
why can we deduce so much knowledge

3843
02:05:53,440 --> 02:05:54,570
is that a question to me

3844
02:05:55,599 --> 02:05:55,849
yes

3845
02:05:56,480 --> 02:05:56,730
uh

3846
02:05:58,719 --> 02:06:00,409
why why i mean mechanistically

3847
02:06:01,040 --> 02:06:01,290
well

3848
02:06:01,440 --> 02:06:02,570
how do we do that

3849
02:06:03,280 --> 02:06:04,090
i mean i

3850
02:06:04,100 --> 02:06:04,889
i think i've

3851
02:06:04,899 --> 02:06:06,090
i thought i answered this earlier

3852
02:06:06,239 --> 02:06:06,969
so maybe i'm

3853
02:06:08,159 --> 02:06:09,449
i don't want to repeat myself too much

3854
02:06:09,520 --> 02:06:09,770
um

3855
02:06:10,000 --> 02:06:11,290
and maybe i don't understand the question

3856
02:06:12,079 --> 02:06:12,329
um

3857
02:06:12,480 --> 02:06:12,730
well

3858
02:06:12,880 --> 02:06:13,530
for example

3859
02:06:13,599 --> 02:06:13,929
you know

3860
02:06:14,159 --> 02:06:15,849
you knock the beer bottle off the table

3861
02:06:16,239 --> 02:06:18,329
you can now reason that the floor is wet

3862
02:06:18,339 --> 02:06:20,329
someone might slip up on on the floor

3863
02:06:21,679 --> 02:06:21,929
yeah

3864
02:06:21,939 --> 02:06:27,690
so almost all of the the knowledge that we that we have around the world is deduced

3865
02:06:28,000 --> 02:06:29,130
or well

3866
02:06:29,199 --> 02:06:29,610
it's it's

3867
02:06:29,620 --> 02:06:33,530
it's deduced that knowledge that you had to learn that

3868
02:06:33,540 --> 02:06:34,010
through observation

3869
02:06:35,040 --> 02:06:38,489
if you had never experienced liquids and you never started slipping on liquids

3870
02:06:38,560 --> 02:06:40,650
you never experienced knocking a cup over

3871
02:06:40,660 --> 02:06:41,770
you wouldn't know what's going to happen

3872
02:06:42,400 --> 02:06:43,690
i mean that's what kids do

3873
02:06:43,700 --> 02:06:44,650
little babies do this

3874
02:06:44,660 --> 02:06:44,889
right

3875
02:06:44,899 --> 02:06:45,369
they're like

3876
02:06:45,379 --> 02:06:45,530
oh

3877
02:06:45,540 --> 02:06:48,969
look what happens that we could live in a world where liquids fall up

3878
02:06:48,979 --> 02:06:50,329
and that would be a perfectly good world

3879
02:06:50,400 --> 02:06:51,369
and that's what we learn

3880
02:06:52,079 --> 02:06:52,329
um

3881
02:06:52,880 --> 02:06:54,409
so i won't think we deduce

3882
02:06:54,639 --> 02:06:54,889
this

3883
02:06:54,899 --> 02:06:58,010
we observe it and and then by analogy

3884
02:06:58,480 --> 02:06:59,610
we we predict

3885
02:06:59,920 --> 02:07:00,170
uh

3886
02:07:00,480 --> 02:07:02,969
how other things that are similar would behave similarly

3887
02:07:03,840 --> 02:07:04,090
uh

3888
02:07:04,100 --> 02:07:04,969
but i think you know

3889
02:07:04,979 --> 02:07:05,210
to me

3890
02:07:05,220 --> 02:07:09,050
the way i look at it is you're born with this with this structure in your head

3891
02:07:09,119 --> 02:07:12,170
that is designed to learn the world through eyes

3892
02:07:12,320 --> 02:07:13,210
and ears and touch

3893
02:07:14,000 --> 02:07:14,250
um

3894
02:07:14,719 --> 02:07:19,610
but it really in the neocortex knows almost nothing about what it's going to learn

3895
02:07:19,620 --> 02:07:20,489
the old parts of the brain

3896
02:07:20,499 --> 02:07:21,130
that's not true

3897
02:07:21,679 --> 02:07:24,090
right refining refining questionnaire because i'm

3898
02:07:24,159 --> 02:07:24,969
i'm also fascinated

3899
02:07:25,280 --> 02:07:25,929
by this concept

3900
02:07:26,159 --> 02:07:28,090
that i learned it from the first time in your book

3901
02:07:28,100 --> 02:07:31,210
that the neocortex now i was going to say blank slate

3902
02:07:31,220 --> 02:07:33,210
but keith would kill me for saying that it's not a blank slate

3903
02:07:33,280 --> 02:07:33,770
it's a template

3904
02:07:34,000 --> 02:07:37,369
because there's actually there's a lot of evolutionary knowledge that's gone into there

3905
02:07:37,440 --> 02:07:37,690
yeah

3906
02:07:37,700 --> 02:07:37,929
yeah

3907
02:07:37,939 --> 02:07:38,409
because you

3908
02:07:38,419 --> 02:07:38,889
because you can think

3909
02:07:38,899 --> 02:07:39,130
you know

3910
02:07:39,140 --> 02:07:40,090
it's just learning all these signals

3911
02:07:40,239 --> 02:07:42,409
but just the way the ear has evolved over time

3912
02:07:42,480 --> 02:07:44,489
that the information gets encoded in a certain way

3913
02:07:44,499 --> 02:07:47,130
the cochlear has the logarithmically spaced bands and so on

3914
02:07:47,140 --> 02:07:50,889
so it's learnable because it's been encoded in in a certain way

3915
02:07:51,119 --> 02:07:52,250
but people like chomsky

3916
02:07:52,800 --> 02:07:57,449
they say that we have a kind of universal grammar or language built into our brains

3917
02:07:57,599 --> 02:07:58,250
and it was endowed

3918
02:07:58,480 --> 02:07:58,969
by evolution

3919
02:07:59,440 --> 02:08:00,889
so where do you kind of draw

3920
02:08:00,899 --> 02:08:01,050
that

3921
02:08:01,199 --> 02:08:04,090
does he say that about all knowledge about the actual language

3922
02:08:04,320 --> 02:08:06,730
i thought he just said that about language about language

3923
02:08:06,740 --> 02:08:06,969
yeah

3924
02:08:07,280 --> 02:08:07,530
yeah

3925
02:08:07,540 --> 02:08:07,770
yeah

3926
02:08:08,239 --> 02:08:08,969
so i don't know

3927
02:08:08,979 --> 02:08:09,130
i mean

3928
02:08:09,140 --> 02:08:09,610
but we're

3929
02:08:09,620 --> 02:08:10,969
we're talking about all knowledge now

3930
02:08:10,979 --> 02:08:11,210
right

3931
02:08:11,760 --> 02:08:14,329
so like it's so you know

3932
02:08:15,360 --> 02:08:15,929
so i think

3933
02:08:16,079 --> 02:08:16,329
look

3934
02:08:16,339 --> 02:08:16,570
there

3935
02:08:16,580 --> 02:08:16,810
is

3936
02:08:16,820 --> 02:08:21,690
there's all these assumptions about the how how the cortex is connected to things right

3937
02:08:21,700 --> 02:08:22,969
and how it's internally connected

3938
02:08:23,440 --> 02:08:25,369
that have been evolutionary advantageous

3939
02:08:26,400 --> 02:08:27,290
and so you know

3940
02:08:27,360 --> 02:08:27,610
we

3941
02:08:28,400 --> 02:08:30,170
we only see certain spectrum of light

3942
02:08:30,180 --> 02:08:32,170
because that's a good spectrum to look at

3943
02:08:32,400 --> 02:08:34,969
and we only certain types of you know sensory inputs

3944
02:08:34,979 --> 02:08:37,610
because those are things that we process them before they get to the near cortex

3945
02:08:37,760 --> 02:08:39,929
so that they're in the right form for the neocortex to work

3946
02:08:40,320 --> 02:08:40,570
um

3947
02:08:40,719 --> 02:08:43,690
we allocate a certain amount of our neocortex to these different sensory modalities

3948
02:08:44,000 --> 02:08:47,290
because that seemed to be the right thing to do from an evolutionary point of view

3949
02:08:47,840 --> 02:08:48,090
um

3950
02:08:48,100 --> 02:08:50,489
and so there's a good good example to say

3951
02:08:50,499 --> 02:08:51,610
it's like a template that's right

3952
02:08:51,920 --> 02:08:53,050
but the in the ins

3953
02:08:53,360 --> 02:08:56,170
what's actually learned in that template is is

3954
02:08:56,400 --> 02:08:56,650
um

3955
02:08:57,360 --> 02:08:59,290
is unknown in terms of language

3956
02:08:59,599 --> 02:09:00,810
i i hated it this

3957
02:09:00,820 --> 02:09:03,770
when i talked about language in the chapter about high level thought in the book

3958
02:09:04,159 --> 02:09:06,489
and i don't consider myself an expert in language at all

3959
02:09:06,880 --> 02:09:07,130
um

3960
02:09:07,360 --> 02:09:10,810
but it occurs to me that if vernon melcaster is right

3961
02:09:10,820 --> 02:09:13,690
that there is a single cortical algorithm that's basically running everywhere

3962
02:09:14,239 --> 02:09:22,170
the language has to be somehow fundamentally mapped on to this algorithm for century

3963
02:09:22,239 --> 02:09:23,690
motor modeling through reference frames

3964
02:09:24,239 --> 02:09:27,050
and i made the analogy about a big part of languages

3965
02:09:27,520 --> 02:09:32,889
it has to do with um recursion and a recursive structures

3966
02:09:33,199 --> 02:09:36,969
and and the algorithms in the in the column are really good at that

3967
02:09:37,440 --> 02:09:37,690
um

3968
02:09:38,159 --> 02:09:42,090
and so to me i would say that you know if i would take

3969
02:09:42,159 --> 02:09:42,730
this isn't it

3970
02:09:42,740 --> 02:09:43,449
i never thought of this

3971
02:09:43,520 --> 02:09:44,650
but if i would say

3972
02:09:44,660 --> 02:09:47,690
take the idea that there's a universal language

3973
02:09:48,480 --> 02:09:50,650
i would extend it beyond what we call language

3974
02:09:50,880 --> 02:09:51,929
i would say there's a universal

3975
02:09:52,880 --> 02:09:55,849
uh structure to everything in the world

3976
02:09:55,859 --> 02:09:56,489
that we can learn

3977
02:09:56,960 --> 02:09:59,090
not everything in the world but everything in the world that we can learn

3978
02:09:59,679 --> 02:09:59,929
and

3979
02:10:00,239 --> 02:10:00,489
um

3980
02:10:00,800 --> 02:10:01,849
that universal structure

3981
02:10:02,639 --> 02:10:03,849
is this this

3982
02:10:04,000 --> 02:10:05,050
uh reference frame

3983
02:10:05,199 --> 02:10:06,170
central motor idea

3984
02:10:06,880 --> 02:10:07,610
and um

3985
02:10:08,159 --> 02:10:10,489
that universal structure can learn any language

3986
02:10:10,800 --> 02:10:13,449
whether it's spoken language or computer language or written language

3987
02:10:13,599 --> 02:10:16,969
it can learn any structure that fits into that algorithm

3988
02:10:17,840 --> 02:10:18,810
and that could be

3989
02:10:18,820 --> 02:10:23,530
you know how staples work and how birds fly and how evolution occurs

3990
02:10:23,840 --> 02:10:27,210
these are all everything we know has to fit into that structure

3991
02:10:27,760 --> 02:10:28,969
so in some sense i would

3992
02:10:28,979 --> 02:10:30,010
i would agree with chomsky

3993
02:10:30,239 --> 02:10:32,730
i just think he only focuses on quote language

3994
02:10:33,280 --> 02:10:33,530
um

3995
02:10:33,599 --> 02:10:36,170
where i would say the universal algorithm is

3996
02:10:36,719 --> 02:10:39,849
language is a subset of the universal algorithm that chomsky talks about

3997
02:10:40,400 --> 02:10:40,969
that's an interesting idea

3998
02:10:41,040 --> 02:10:41,449
i never

3999
02:10:41,459 --> 02:10:43,210
i never said that before stuff man

4000
02:10:43,220 --> 02:10:44,329
we'll see how it feels about tomorrow

4001
02:10:45,280 --> 02:10:46,090
but but there's

4002
02:10:46,100 --> 02:10:47,290
there's a fascinating dichotomy

4003
02:10:47,520 --> 02:10:47,770
though

4004
02:10:47,780 --> 02:10:52,010
isn't that because it's similar to the um bias variance trade-off in machine learning

4005
02:10:52,079 --> 02:10:52,810
so you know

4006
02:10:52,880 --> 02:10:57,130
evolution has has given us a certain prior and a certain default encoding

4007
02:10:57,360 --> 02:10:58,730
and then we have this learnability algorithm

4008
02:10:58,800 --> 02:11:03,369
and then what fascinates me is how externalized so much of this stuff is

4009
02:11:03,379 --> 02:11:03,929
so there's embodiment

4010
02:11:04,239 --> 02:11:06,329
there's all the knowledge in in society that we

4011
02:11:06,560 --> 02:11:07,369
we learn language

4012
02:11:07,679 --> 02:11:11,210
we're brought up by our parents and and we we acquire the language around us

4013
02:11:11,280 --> 02:11:14,170
so how much of it is being pushed down from society

4014
02:11:14,400 --> 02:11:18,889
and how much of it is being pushed up from the prior knowledge that we've evolved and inherited

4015
02:11:19,440 --> 02:11:19,690
well

4016
02:11:19,700 --> 02:11:20,010
i would

4017
02:11:20,020 --> 02:11:21,210
i would say there's three things

4018
02:11:21,760 --> 02:11:23,929
there is the our knowledge we inherited

4019
02:11:24,800 --> 02:11:30,170
there is what we learn on our own just to exploratory behavior

4020
02:11:31,040 --> 02:11:33,050
and then what is passed down

4021
02:11:33,280 --> 02:11:35,690
which is always always through language

4022
02:11:36,079 --> 02:11:39,290
but it could be also through by observation of other humans

4023
02:11:40,159 --> 02:11:40,409
um

4024
02:11:41,119 --> 02:11:42,090
and um

4025
02:11:42,719 --> 02:11:44,810
and what's the balance between those two

4026
02:11:44,820 --> 02:11:45,050
well

4027
02:11:45,280 --> 02:11:46,889
i think in terms of the neocortex

4028
02:11:48,639 --> 02:11:51,290
it's not a lot from the biology and the evolution

4029
02:11:51,840 --> 02:11:53,449
in terms of the other parts of the brain

4030
02:11:53,599 --> 02:11:54,570
it's very much

4031
02:11:54,639 --> 02:11:54,889
so

4032
02:11:55,520 --> 02:11:55,849
you know

4033
02:11:55,859 --> 02:11:56,409
there's a

4034
02:11:57,199 --> 02:12:00,730
if there's evidence that says the prayer collectors is like the old visual system

4035
02:12:01,360 --> 02:12:01,610
um

4036
02:12:02,000 --> 02:12:02,250
it

4037
02:12:02,320 --> 02:12:02,570
it

4038
02:12:02,800 --> 02:12:05,090
it detects snakes and spiders

4039
02:12:05,840 --> 02:12:07,530
so people were scared of snakes or spiders

4040
02:12:07,920 --> 02:12:09,770
it's not the neocortex is the old by going

4041
02:12:09,840 --> 02:12:10,969
ah start a snake

4042
02:12:10,979 --> 02:12:11,690
it looks like a snake

4043
02:12:12,320 --> 02:12:13,929
and so you know that's not learned

4044
02:12:14,079 --> 02:12:14,650
it's there

4045
02:12:14,800 --> 02:12:15,770
it's hard to get rid of

4046
02:12:16,159 --> 02:12:16,409
um

4047
02:12:17,280 --> 02:12:18,489
so there's some of that

4048
02:12:18,499 --> 02:12:19,369
but you know

4049
02:12:19,440 --> 02:12:20,409
our ability to walk

4050
02:12:20,419 --> 02:12:21,449
we don't learn to walk

4051
02:12:21,679 --> 02:12:23,290
we actually are programmed to walk

4052
02:12:23,300 --> 02:12:24,010
it's just that we did

4053
02:12:24,020 --> 02:12:26,329
we haven't finished developing yet

4054
02:12:26,480 --> 02:12:27,690
until when we learn to walk

4055
02:12:27,700 --> 02:12:30,170
we're really just our nervous system is finished being growing

4056
02:12:30,880 --> 02:12:32,250
you don't really learn to walk

4057
02:12:32,480 --> 02:12:33,849
so these are these old priors

4058
02:12:34,079 --> 02:12:34,730
lots of them

4059
02:12:35,119 --> 02:12:35,369
um

4060
02:12:35,679 --> 02:12:36,570
and as a human

4061
02:12:36,639 --> 02:12:37,050
they're very

4062
02:12:37,060 --> 02:12:37,530
very important

4063
02:12:37,760 --> 02:12:39,530
you know eating and sex and

4064
02:12:39,679 --> 02:12:39,929
and

4065
02:12:40,480 --> 02:12:40,730
uh

4066
02:12:40,880 --> 02:12:41,449
body functions

4067
02:12:41,679 --> 02:12:42,409
and you know

4068
02:12:42,419 --> 02:12:42,650
survival

4069
02:12:43,040 --> 02:12:44,409
all these things and all there

4070
02:12:44,800 --> 02:12:45,050
uh

4071
02:12:45,060 --> 02:12:46,570
but from the neocortex point of view

4072
02:12:46,580 --> 02:12:47,530
i'd say there's very little

4073
02:12:48,159 --> 02:12:48,409
um

4074
02:12:48,560 --> 02:12:50,329
it's more just assumptions about the

4075
02:12:50,400 --> 02:12:50,650
the

4076
02:12:51,280 --> 02:12:53,610
the sensory types of sensory data you're going to get

4077
02:12:53,840 --> 02:12:54,810
i mean it's what you know

4078
02:12:54,820 --> 02:12:55,290
what strikes me

4079
02:12:55,300 --> 02:12:55,770
tim is

4080
02:12:55,840 --> 02:12:56,250
it's incredibly

4081
02:12:57,440 --> 02:13:00,010
we are so incredibly versatile on what we can learn

4082
02:13:00,480 --> 02:13:01,369
i mean just you know

4083
02:13:01,379 --> 02:13:03,050
think of all things we learned and i'm sure you know this

4084
02:13:03,060 --> 02:13:03,610
you thought about

4085
02:13:03,620 --> 02:13:05,210
i'm sure yourself that all the things we learned

4086
02:13:05,220 --> 02:13:08,409
we had no evolutionary pressure to do like running these

4087
02:13:08,480 --> 02:13:10,570
this kind of podcast and you know

4088
02:13:10,580 --> 02:13:12,730
broken computerism and talking about brains

4089
02:13:13,679 --> 02:13:14,329
it was incredible

4090
02:13:14,880 --> 02:13:21,050
it just just cries out that there's a universal method here that's being applied to any anything

4091
02:13:21,199 --> 02:13:21,449
that

4092
02:13:21,920 --> 02:13:22,570
not to everything

4093
02:13:23,119 --> 02:13:23,449
you know

4094
02:13:23,459 --> 02:13:24,329
brains can't learn everything

4095
02:13:24,480 --> 02:13:26,090
but they can learn a hell of a lot

4096
02:13:26,159 --> 02:13:26,969
and it seems to be

4097
02:13:27,119 --> 02:13:29,130
we haven't really discovered the extent of it yet

4098
02:13:29,679 --> 02:13:30,570
so this

4099
02:13:30,639 --> 02:13:31,290
this says

4100
02:13:31,360 --> 02:13:31,690
you know

4101
02:13:31,700 --> 02:13:31,849
there's

4102
02:13:31,859 --> 02:13:36,810
there is this sort of blank slatish um system

4103
02:13:37,040 --> 02:13:38,250
if you want to call it that way

4104
02:13:38,260 --> 02:13:41,449
with these assumptions about our bodies built into it

4105
02:13:42,239 --> 02:13:42,489
well

4106
02:13:42,499 --> 02:13:46,250
the only thing about the the blank slate thing is it's such a dichotomy

4107
02:13:46,560 --> 02:13:49,610
because i was reading your book and it seems to be so strongly determined

4108
02:13:49,920 --> 02:13:52,409
by the embodiment and the multi modality

4109
02:13:52,880 --> 02:13:53,210
you know

4110
02:13:53,220 --> 02:13:54,010
the way that we're wired

4111
02:13:54,020 --> 02:13:56,329
because you said yourself that ai will actually evolve towards

4112
02:13:56,800 --> 02:13:57,130
you know

4113
02:13:57,140 --> 02:13:57,369
robotics

4114
02:13:57,760 --> 02:13:58,010
basically

4115
02:13:58,159 --> 02:13:59,130
it's not just the brain

4116
02:13:59,199 --> 02:13:59,449
yeah

4117
02:13:59,459 --> 02:13:59,610
it's

4118
02:13:59,620 --> 02:14:02,090
it's how it acts in the environment and how it gets with those senses

4119
02:14:02,159 --> 02:14:04,409
but i wanted to ask one one final thing

4120
02:14:04,419 --> 02:14:07,369
so i went to a yoga retreat at the weekend

4121
02:14:07,520 --> 02:14:10,010
and after the physical components of the practice were concluded

4122
02:14:10,560 --> 02:14:12,170
matters of a philosophical nature

4123
02:14:12,719 --> 02:14:12,969
uh

4124
02:14:12,979 --> 02:14:13,449
were investigated

4125
02:14:14,320 --> 02:14:17,610
and the basic case that the yoga teacher was making is that we have two selves

4126
02:14:17,760 --> 02:14:18,010
right

4127
02:14:18,020 --> 02:14:21,369
we have the emergent social self and then we have the inner self

4128
02:14:21,379 --> 02:14:24,250
which i'm sure sam harris probably might spoke to about

4129
02:14:24,400 --> 02:14:26,570
you know the sense of being and the sense of being is

4130
02:14:26,580 --> 02:14:29,610
when you kind of ignore the virtual social program on the top

4131
02:14:29,840 --> 02:14:30,889
and you know a society

4132
02:14:31,360 --> 02:14:36,329
it is an emergent virtual program and it does many of the things that the cortical columns are doing

4133
02:14:36,339 --> 02:14:37,929
it has the error correction it had

4134
02:14:38,000 --> 02:14:38,329
you know

4135
02:14:38,339 --> 02:14:39,929
it's the externalization and distribution

4136
02:14:40,639 --> 02:14:40,889
so

4137
02:14:40,899 --> 02:14:44,010
even though we're programmed to care a lot about our social selves

4138
02:14:44,480 --> 02:14:44,730
um

4139
02:14:45,040 --> 02:14:48,170
we are very stressed when we lose control of our own narrative

4140
02:14:48,400 --> 02:14:50,730
because someone might be saying something bad about us on twitter

4141
02:14:50,800 --> 02:14:51,369
so anyway

4142
02:14:51,440 --> 02:14:52,650
this yoga teacher was saying

4143
02:14:52,660 --> 02:14:52,889
well

4144
02:14:52,899 --> 02:14:54,090
there's something deeply fulfilling

4145
02:14:54,800 --> 02:14:55,050
um

4146
02:14:55,280 --> 02:15:01,690
spiritually about just kind of um going into your mind and and just just that raw conscious experience

4147
02:15:01,840 --> 02:15:03,130
but then i felt like saying to him

4148
02:15:03,140 --> 02:15:03,210
well

4149
02:15:03,220 --> 02:15:04,650
you should read jeff hawkins book

4150
02:15:04,660 --> 02:15:09,369
because your mind is just a load of prediction models and all you're doing is traversing your reference frames

4151
02:15:10,000 --> 02:15:10,969
so what do you

4152
02:15:10,979 --> 02:15:12,250
what do you think i don't

4153
02:15:12,260 --> 02:15:12,489
i

4154
02:15:12,499 --> 02:15:12,730
well

4155
02:15:12,880 --> 02:15:15,690
i again asked me to talk about things i'm not too familiar with

4156
02:15:15,700 --> 02:15:16,250
but i'll tell you

4157
02:15:16,320 --> 02:15:17,369
i'll tell you a personal experience

4158
02:15:17,599 --> 02:15:17,929
i have

4159
02:15:18,079 --> 02:15:18,329
okay

4160
02:15:18,719 --> 02:15:19,530
i don't do meditation

4161
02:15:20,079 --> 02:15:21,690
and and sam didn't bring that up

4162
02:15:21,920 --> 02:15:23,610
but but i thought about it a bit

4163
02:15:24,079 --> 02:15:25,530
i do something equivalent to meditation

4164
02:15:26,400 --> 02:15:26,650
um

4165
02:15:27,040 --> 02:15:30,090
is that when i find things in the world

4166
02:15:30,100 --> 02:15:30,329
stressful

4167
02:15:31,360 --> 02:15:31,610
um

4168
02:15:31,840 --> 02:15:34,010
i just sort of shut out the world

4169
02:15:34,079 --> 02:15:37,770
and i think about interesting problems like the future of humanity or the future of intelligence

4170
02:15:38,159 --> 02:15:40,170
what's the nature of life and things like this

4171
02:15:40,480 --> 02:15:42,730
and so now i'm living in this world that is sort of

4172
02:15:43,040 --> 02:15:43,290
um

4173
02:15:44,079 --> 02:15:45,210
is in some sense

4174
02:15:45,360 --> 02:15:45,610
uh

4175
02:15:46,480 --> 02:15:46,730
pure

4176
02:15:46,960 --> 02:15:47,210
right

4177
02:15:47,280 --> 02:15:47,690
it's not

4178
02:15:47,840 --> 02:15:49,770
it's not being messed up with

4179
02:15:49,840 --> 02:15:50,489
like i'm hungry

4180
02:15:50,560 --> 02:15:54,489
or this person's being nasty to me or just like tuning it all out

4181
02:15:54,499 --> 02:15:54,650
like

4182
02:15:54,660 --> 02:15:55,130
oh my god

4183
02:15:55,140 --> 02:15:56,010
i'm not in trouble again

4184
02:15:57,119 --> 02:15:57,610
and then

4185
02:15:58,079 --> 02:15:59,849
so i think that's my own personal meditation

4186
02:16:00,719 --> 02:16:02,650
and i don't think i think it's useful actually

4187
02:16:02,840 --> 02:16:03,090
because

4188
02:16:03,679 --> 02:16:03,929
uh

4189
02:16:04,400 --> 02:16:05,610
i think it helps when you

4190
02:16:05,620 --> 02:16:06,810
when you think like that

4191
02:16:07,119 --> 02:16:07,369
you

4192
02:16:07,760 --> 02:16:08,010
you're

4193
02:16:08,320 --> 02:16:10,409
you're sometimes able to get to deeper truths

4194
02:16:11,119 --> 02:16:11,369
um

4195
02:16:11,760 --> 02:16:15,690
that you separate the body functions and the day-to-day stuff

4196
02:16:15,840 --> 02:16:20,570
and and i often do my best thinking when there are no distractions either on that

4197
02:16:20,580 --> 02:16:24,090
sometimes while i'm driving or sometimes i'm just awake at night lying in bed

4198
02:16:24,880 --> 02:16:25,130
um

4199
02:16:25,760 --> 02:16:27,210
but where there's like

4200
02:16:27,220 --> 02:16:29,369
i don't i don't have to deal with anything else

4201
02:16:29,440 --> 02:16:30,810
so that's kind of like meditation

4202
02:16:31,599 --> 02:16:32,170
and i find

4203
02:16:32,398 --> 02:16:33,290
i don't think it's useful

4204
02:16:33,359 --> 02:16:35,450
i think it's very useful because you

4205
02:16:35,840 --> 02:16:39,129
it allows the brain to sort of separate out from the old brain stuff

4206
02:16:39,760 --> 02:16:40,410
and um

4207
02:16:40,558 --> 02:16:42,010
and just have run a bit

4208
02:16:44,240 --> 02:16:45,769
sometimes it comes up some good ideas

4209
02:16:46,080 --> 02:16:48,250
you know exactly

4210
02:16:48,478 --> 02:16:49,290
i couldn't attest to that

4211
02:16:49,299 --> 02:16:49,450
well

4212
02:16:49,519 --> 02:16:49,769
um

4213
02:16:49,840 --> 02:16:50,330
jeff hawkins

4214
02:16:50,718 --> 02:16:52,410
thank you so much for joining us today

4215
02:16:52,420 --> 02:16:53,290
it's been an absolute honor

4216
02:16:53,840 --> 02:16:54,090
well

4217
02:16:54,100 --> 02:16:54,728
it's been a pleasure

4218
02:16:54,799 --> 02:16:55,450
you guys are great

4219
02:16:55,459 --> 02:16:55,849
i really

4220
02:16:56,000 --> 02:16:57,049
i enjoy all your questions

4221
02:16:57,200 --> 02:17:00,170
you're really fun and i think this is one of the

4222
02:17:00,240 --> 02:17:03,530
one of the more meaningful conversations i've had in a long time

4223
02:17:03,540 --> 02:17:03,929
i'm serious

4224
02:17:04,160 --> 02:17:04,490
you know

4225
02:17:04,558 --> 02:17:05,290
because hey

4226
02:17:05,299 --> 02:17:05,689
you know

4227
02:17:05,760 --> 02:17:06,888
you know what you're talking about

4228
02:17:07,519 --> 02:17:08,808
you know what i'm talking about

4229
02:17:09,040 --> 02:17:10,250
you ask great questions

4230
02:17:10,398 --> 02:17:12,330
you're really deep thinking about all this stuff

4231
02:17:12,340 --> 02:17:13,209
so i think that's wonderful

4232
02:17:13,359 --> 02:17:14,648
thank you so much appreciate that

4233
02:17:14,659 --> 02:17:15,209
thank you so much

4234
02:17:15,219 --> 02:17:15,450
all right

4235
02:17:15,679 --> 02:17:16,090
all right

4236
02:17:16,100 --> 02:17:16,250
well

4237
02:17:16,260 --> 02:17:16,808
i hope it helped

4238
02:17:16,818 --> 02:17:17,849
it was good for you guys

4239
02:17:18,558 --> 02:17:19,929
so that was a wrap

4240
02:17:21,679 --> 02:17:22,330
how was that guys

4241
02:17:23,840 --> 02:17:24,410
it's pretty great

4242
02:17:24,558 --> 02:17:25,209
it's pretty great

4243
02:17:25,359 --> 02:17:25,609
um

4244
02:17:25,840 --> 02:17:26,330
and honestly

4245
02:17:26,638 --> 02:17:26,888
yeah

4246
02:17:27,120 --> 02:17:28,170
i'm really pressed

4247
02:17:28,318 --> 02:17:29,609
it was really nice to talk to them

4248
02:17:29,619 --> 02:17:30,888
it's just clearly

4249
02:17:30,898 --> 02:17:31,370
you know

4250
02:17:31,379 --> 02:17:31,769
you know

4251
02:17:31,779 --> 02:17:33,530
it was like so much more than was in the book

4252
02:17:34,080 --> 02:17:34,330
uh

4253
02:17:34,478 --> 02:17:34,728
it

4254
02:17:34,739 --> 02:17:34,968
building

4255
02:17:35,040 --> 02:17:36,410
the book was really just scratching the surface

4256
02:17:36,718 --> 02:17:36,968
and

4257
02:17:37,120 --> 02:17:37,370
uh

4258
02:17:37,379 --> 02:17:38,090
i wish we had

4259
02:17:38,100 --> 02:17:38,250
like

4260
02:17:38,260 --> 02:17:38,568
you know

4261
02:17:38,579 --> 02:17:41,609
like just taking off for a beer and like really dig into some of the degrees

4262
02:17:43,040 --> 02:17:43,290
yeah

4263
02:17:43,299 --> 02:17:44,250
what i i love

4264
02:17:44,318 --> 02:17:44,568
uh

4265
02:17:44,799 --> 02:17:49,450
i love people like him because he's so smart and and smart and a very

4266
02:17:49,519 --> 02:17:49,769
um

4267
02:17:50,799 --> 02:17:52,568
he has such great common sense right

4268
02:17:52,579 --> 02:17:52,808
so

4269
02:17:52,818 --> 02:17:53,129
when you

4270
02:17:53,139 --> 02:17:53,849
when you ask him questions

4271
02:17:54,000 --> 02:17:57,689
he comes back with these really informative answers that are very

4272
02:17:58,398 --> 02:17:58,648
uh

4273
02:17:58,799 --> 02:17:59,049
concrete

4274
02:17:59,439 --> 02:18:00,968
and you understand what he's talking about

4275
02:18:00,978 --> 02:18:02,568
so it's really fun to talk to him

4276
02:18:02,579 --> 02:18:02,888
i mean

4277
02:18:03,760 --> 02:18:04,170
a very

4278
02:18:04,240 --> 02:18:04,490
very

4279
02:18:04,879 --> 02:18:05,769
great learning experience

4280
02:18:07,679 --> 02:18:07,929
yeah

4281
02:18:08,000 --> 02:18:12,250
i got the impression that his research focus has changed a little bit

4282
02:18:12,260 --> 02:18:14,010
so when we were doing some preparation for this

4283
02:18:14,020 --> 02:18:16,330
we were looking a lot into the htm algorithm

4284
02:18:16,478 --> 02:18:17,609
which i understand is now

4285
02:18:18,318 --> 02:18:18,568
um

4286
02:18:19,679 --> 02:18:21,209
they're pivoting away from that a little bit

4287
02:18:21,359 --> 02:18:24,808
so he was talking about some stuff that they're doing with transformers models and sparsity

4288
02:18:25,280 --> 02:18:26,968
and quite a few things i haven't heard about

4289
02:18:27,120 --> 02:18:29,209
and also he was really focused on

4290
02:18:29,439 --> 02:18:29,689
um

4291
02:18:29,840 --> 02:18:33,849
the particular way of thinking about cognition using these reference frames

4292
02:18:34,160 --> 02:18:35,530
which he spoke about in his book

4293
02:18:35,540 --> 02:18:38,490
which i think is actually a slight departure from from um

4294
02:18:38,799 --> 02:18:41,129
some of the stuff the mentor has had out about five years ago

4295
02:18:42,080 --> 02:18:42,330
yeah

4296
02:18:42,398 --> 02:18:46,490
but i definitely got the feeling that he had like so much interesting stuff

4297
02:18:47,200 --> 02:18:49,689
that's just not quite ready yet for public consumption

4298
02:18:50,318 --> 02:18:50,568
but

4299
02:18:50,959 --> 02:18:51,209
um

4300
02:18:51,840 --> 02:18:52,090
hopefully

4301
02:18:52,799 --> 02:18:53,209
in the book

4302
02:18:53,219 --> 02:18:54,490
i often have this feeling like

4303
02:18:54,500 --> 02:18:54,648
ah

4304
02:18:54,659 --> 02:18:54,968
the city

4305
02:18:55,120 --> 02:18:55,769
just you know

4306
02:18:55,779 --> 02:18:58,250
it feels like there's more to this than there is in the book

4307
02:18:58,318 --> 02:18:58,568
maybe

4308
02:18:58,638 --> 02:18:59,849
or maybe just needs more time

4309
02:19:00,080 --> 02:19:01,370
and i think i remember reading the book

4310
02:19:01,379 --> 02:19:02,648
it took like a year and a half to write

4311
02:19:02,659 --> 02:19:03,049
or so

4312
02:19:03,120 --> 02:19:03,370
so

4313
02:19:03,439 --> 02:19:03,689
maybe

4314
02:19:03,699 --> 02:19:04,410
even right here

4315
02:19:04,420 --> 02:19:04,808
you know

4316
02:19:04,818 --> 02:19:05,609
lots of things been happening

4317
02:19:06,398 --> 02:19:06,648
um

4318
02:19:06,959 --> 02:19:10,410
so i do look forward to whatever he's gonna publish in the future

4319
02:19:10,959 --> 02:19:11,209
well

4320
02:19:11,219 --> 02:19:13,929
what i love too is you know he's more than willing to admit

4321
02:19:14,000 --> 02:19:14,250
look

4322
02:19:14,260 --> 02:19:15,049
i've changed my views

4323
02:19:15,359 --> 02:19:15,769
you know

4324
02:19:15,779 --> 02:19:16,170
i think

4325
02:19:16,398 --> 02:19:16,648
yeah

4326
02:19:16,799 --> 02:19:19,129
lots of people run into are not able to do that

4327
02:19:19,139 --> 02:19:20,410
and if you agree with chalet's

4328
02:19:20,879 --> 02:19:21,209
you know

4329
02:19:21,219 --> 02:19:21,849
measure of intelligence

4330
02:19:22,398 --> 02:19:27,370
the mark of intelligence is the ability to assimilate new information and to learn from it

4331
02:19:27,599 --> 02:19:30,888
and most of the people we talk to just want to stay adamantly entrenched

4332
02:19:31,200 --> 02:19:31,530
you know

4333
02:19:31,599 --> 02:19:31,849
no

4334
02:19:31,859 --> 02:19:32,090
look

4335
02:19:32,398 --> 02:19:34,490
this thing that i called xyz back

4336
02:19:34,638 --> 02:19:34,968
you know

4337
02:19:34,978 --> 02:19:35,769
30 years ago

4338
02:19:36,478 --> 02:19:39,209
you're wrong that it was limited to this little area

4339
02:19:39,359 --> 02:19:40,648
it actually encompasses everything

4340
02:19:41,040 --> 02:19:41,450
you know

4341
02:19:41,459 --> 02:19:42,330
that that works now

4342
02:19:42,398 --> 02:19:46,888
like nobody ever wants to admit that they've learned or changed or that science has evolved

4343
02:19:47,679 --> 02:19:50,649
i want to push back on that a little bit because there is something really special

4344
02:19:50,880 --> 02:19:53,689
i know we were joking that science advances one funeral at a time

4345
02:19:53,699 --> 02:19:57,770
but there is something really special about people who have an idea and they see it through

4346
02:19:58,080 --> 02:20:00,330
just like yan lacoon and the deep learning pioneers

4347
02:20:00,640 --> 02:20:04,250
everyone was asking them why the hell are you doing this in the 1990s

4348
02:20:04,800 --> 02:20:07,770
and they and they hung on and it was worth hanging on

4349
02:20:08,160 --> 02:20:09,450
and it's a similar thing with

4350
02:20:09,460 --> 02:20:10,649
with some of the approaches from nomento

4351
02:20:10,880 --> 02:20:12,569
they've been at this for such a long time now

4352
02:20:12,880 --> 02:20:16,729
and there is some really really kind of biologically inspired

4353
02:20:16,960 --> 02:20:20,490
and plausible reasons why this might lead to something interesting in the future

4354
02:20:20,960 --> 02:20:25,050
but clearly they've been at this for a long time and and they've been sticking at it

4355
02:20:25,840 --> 02:20:26,090
well

4356
02:20:26,100 --> 02:20:26,330
nothing

4357
02:20:26,479 --> 02:20:27,530
nothing's wrong with perseverance

4358
02:20:28,160 --> 02:20:31,530
but perseverance without adaptation is wrong

4359
02:20:33,280 --> 02:20:35,130
it's just perseverance when you're right

4360
02:20:35,280 --> 02:20:35,770
is right

4361
02:20:35,920 --> 02:20:37,050
and prince raised when you're wrong

4362
02:20:37,120 --> 02:20:37,530
is bad

4363
02:20:38,240 --> 02:20:38,890
you're never 100

4364
02:20:39,359 --> 02:20:39,609
right

4365
02:20:39,619 --> 02:20:40,010
and you're right

4366
02:20:40,319 --> 02:20:40,890
you're never 100.

4367
02:20:41,040 --> 02:20:42,330
so that's the whole point is

4368
02:20:42,340 --> 02:20:43,210
you've got to be able to learn

4369
02:20:43,439 --> 02:20:44,250
you have to be able to

4370
02:20:44,260 --> 02:20:44,330
well

4371
02:20:44,340 --> 02:20:45,770
the learning rate needs to be about 0. 01

4372
02:20:47,760 --> 02:20:48,010
sure

4373
02:20:48,319 --> 02:20:49,370
and the party centers

4374
02:20:49,520 --> 02:20:51,130
3 e minus 4

4375
02:20:51,140 --> 02:20:55,689
i think is the correct learning rate for all matters in life

4376
02:20:55,760 --> 02:20:56,010
yeah

4377
02:20:56,020 --> 02:20:57,609
all the matters specifically

4378
02:20:58,479 --> 02:20:58,729
yeah

4379
02:20:58,960 --> 02:20:59,530
and i agree

4380
02:20:59,600 --> 02:21:00,090
like also

4381
02:21:00,319 --> 02:21:00,569
i

4382
02:21:00,720 --> 02:21:04,170
i'm really glad i got to talk to jeff about essential risk or something

4383
02:21:04,180 --> 02:21:04,649
because of course

4384
02:21:04,659 --> 02:21:04,970
you know

4385
02:21:05,200 --> 02:21:06,569
i take some songs pretty seriously

4386
02:21:06,880 --> 02:21:07,130
yeah

4387
02:21:07,520 --> 02:21:10,729
it's a very common experience to hear people dismiss and extensively

4388
02:21:11,040 --> 02:21:12,810
because they've heard the bad versions of the arguments

4389
02:21:13,120 --> 02:21:16,490
if there are some heroin encouragements of these arguments

4390
02:21:17,120 --> 02:21:19,689
that it's even from some very smart people like

4391
02:21:19,699 --> 02:21:19,850
yeah

4392
02:21:19,860 --> 02:21:24,649
some smart people i feel do the field a great justice than i just can't feel bad

4393
02:21:24,659 --> 02:21:24,970
they're against

4394
02:21:25,600 --> 02:21:25,850
um

4395
02:21:26,720 --> 02:21:29,370
and it was really nice to see you know him

4396
02:21:29,380 --> 02:21:29,689
say like

4397
02:21:29,699 --> 02:21:29,850
yeah

4398
02:21:29,860 --> 02:21:31,210
you know what something we're thinking about

4399
02:21:31,359 --> 02:21:31,609
and

4400
02:21:32,160 --> 02:21:32,410
um

4401
02:21:33,040 --> 02:21:33,290
i

4402
02:21:33,300 --> 02:21:33,609
i think

4403
02:21:33,619 --> 02:21:33,850
honestly

4404
02:21:34,399 --> 02:21:36,810
a lot of the disagreement comes down to that

4405
02:21:37,120 --> 02:21:37,370
yeah

4406
02:21:37,840 --> 02:21:38,090
like

4407
02:21:38,100 --> 02:21:39,689
i agree with him and almost everything really

4408
02:21:39,920 --> 02:21:40,890
so they're not those relationships

4409
02:21:41,600 --> 02:21:43,530
i was really impressed with with his response

4410
02:21:43,600 --> 02:21:44,170
and as you say

4411
02:21:44,180 --> 02:21:45,689
you can see it from so many different perspectives

4412
02:21:45,920 --> 02:21:47,770
so you can cover it from a grand perspective

4413
02:21:48,479 --> 02:21:50,649
talking about paper clips and um

4414
02:21:51,359 --> 02:21:51,930
oh my god

4415
02:21:52,080 --> 02:21:52,330
but

4416
02:21:52,840 --> 02:21:53,090
um

4417
02:21:54,319 --> 02:21:55,450
i lost my train of thought

4418
02:21:55,460 --> 02:21:55,609
yeah

4419
02:21:55,619 --> 02:21:56,090
so he

4420
02:21:56,800 --> 02:21:57,370
i read the

4421
02:21:57,380 --> 02:21:57,609
uh

4422
02:21:57,619 --> 02:22:00,010
the less wrong article which you linked to me was that steve burns

4423
02:22:00,800 --> 02:22:01,050
yeah

4424
02:22:01,120 --> 02:22:01,370
secrets

4425
02:22:02,000 --> 02:22:02,250
yeah

4426
02:22:02,319 --> 02:22:03,609
so that was fascinating

4427
02:22:03,920 --> 02:22:05,530
so so he was saying that

4428
02:22:05,600 --> 02:22:05,850
um

4429
02:22:06,640 --> 02:22:07,370
at the end of the day

4430
02:22:07,439 --> 02:22:10,810
you've got a robot rover on mars or something like that

4431
02:22:10,820 --> 02:22:14,010
and at some point you need to actually give it something to do

4432
02:22:14,560 --> 02:22:16,250
and you might give it an instruction

4433
02:22:17,120 --> 02:22:20,090
and then it might really want to complete that instruction

4434
02:22:20,319 --> 02:22:22,729
so it might predict that you're about to give it another instruction

4435
02:22:22,960 --> 02:22:23,290
it says

4436
02:22:23,300 --> 02:22:23,770
oh my god

4437
02:22:23,780 --> 02:22:24,810
he's about to give me another instruction

4438
02:22:25,040 --> 02:22:26,170
i've not done the first thing yet

4439
02:22:26,319 --> 02:22:27,370
so i need to kill him

4440
02:22:27,840 --> 02:22:30,170
and that's not completely beyond the realms of possibility

4441
02:22:30,479 --> 02:22:35,290
now i i agree with charlotte that i think intelligence is externalized

4442
02:22:35,760 --> 02:22:38,649
it is embodied and there are many like natural environmental

4443
02:22:39,200 --> 02:22:40,330
limiting steps to intelligence

4444
02:22:40,560 --> 02:22:43,770
and you could talk about all the replication stuff as hawkins did

4445
02:22:43,780 --> 02:22:45,130
but that's just a simple example

4446
02:22:45,359 --> 02:22:50,090
i'm just an intelligent agent on on mars and i i need to have motivation

4447
02:22:50,399 --> 02:22:51,770
because i need to be told what to do

4448
02:22:51,780 --> 02:22:52,330
what am i doing

4449
02:22:52,340 --> 02:22:57,370
am i building a base and then i can plausibly see how that could be

4450
02:22:57,680 --> 02:22:57,930
um

4451
02:22:58,720 --> 02:22:59,050
you know

4452
02:22:59,120 --> 02:22:59,370
misdirected

4453
02:23:00,800 --> 02:23:01,050
yeah

4454
02:23:01,120 --> 02:23:04,250
i feel like it's it the intelligence explosion

4455
02:23:04,479 --> 02:23:04,729
arguments

4456
02:23:04,960 --> 02:23:06,490
i'll get mixed up with these

4457
02:23:06,500 --> 02:23:06,890
like motivational

4458
02:23:07,680 --> 02:23:07,930
uh

4459
02:23:08,560 --> 02:23:08,810
concerns

4460
02:23:09,120 --> 02:23:13,450
which i think is unfortunate because they really aren't blocking all like you could like these

4461
02:23:13,600 --> 02:23:14,729
these concerns about

4462
02:23:14,880 --> 02:23:15,130
you

4463
02:23:15,140 --> 02:23:15,210
know

4464
02:23:15,220 --> 02:23:17,370
alignment and about motivation will crop up

4465
02:23:17,380 --> 02:23:18,090
whether you know

4466
02:23:18,100 --> 02:23:21,290
it takes 10 years or a thousand years for ai to come to be

4467
02:23:21,600 --> 02:23:22,250
at some point

4468
02:23:22,640 --> 02:23:23,370
s hawkins

4469
02:23:23,520 --> 02:23:23,850
i think

4470
02:23:23,860 --> 02:23:24,250
gonna pick

4471
02:23:24,260 --> 02:23:24,890
so when we put

4472
02:23:24,900 --> 02:23:26,810
the work needs to be done at some point

4473
02:23:27,040 --> 02:23:28,410
you just have to build a safety mechanism

4474
02:23:28,640 --> 02:23:29,130
at some point

4475
02:23:29,140 --> 02:23:30,649
you have to write a motivation system

4476
02:23:31,120 --> 02:23:32,330
so that's why i think this

4477
02:23:32,340 --> 02:23:32,729
you know

4478
02:23:32,739 --> 02:23:33,210
kind of research

4479
02:23:33,680 --> 02:23:34,010
you know

4480
02:23:34,020 --> 02:23:34,970
even if intelligence blows

4481
02:23:34,980 --> 02:23:38,250
it doesn't happen is still something needs to get done

4482
02:23:40,560 --> 02:23:40,810
yeah

4483
02:23:40,880 --> 02:23:42,729
and and there is this thing

4484
02:23:42,739 --> 02:23:42,970
um

4485
02:23:42,980 --> 02:23:44,410
i mean you can explain this better than me

4486
02:23:44,420 --> 02:23:45,290
but instrumental convergence

4487
02:23:45,680 --> 02:23:46,970
which is the idea that

4488
02:23:47,280 --> 02:23:47,530
um

4489
02:23:47,760 --> 02:23:48,170
you know

4490
02:23:48,180 --> 02:23:55,530
many seemingly innocuous goals incidentally lead to dangerous motivations like self-preservation and self-replication and goal preservation

4491
02:23:56,000 --> 02:23:56,810
and um

4492
02:23:56,960 --> 02:23:58,569
i i can sign on to that

4493
02:23:58,579 --> 02:23:59,450
that seems very plausible

4494
02:24:00,560 --> 02:24:00,810
yeah

4495
02:24:00,820 --> 02:24:00,810
exactly

4496
02:24:00,880 --> 02:24:02,569
i mean you give a perfect example

4497
02:24:02,640 --> 02:24:03,370
but like you know

4498
02:24:03,380 --> 02:24:05,530
you give the rover and the the the job

4499
02:24:05,540 --> 02:24:05,770
will

4500
02:24:05,780 --> 02:24:06,010
you know

4501
02:24:06,020 --> 02:24:06,490
build a thing

4502
02:24:06,560 --> 02:24:07,370
but actually it's like

4503
02:24:07,380 --> 02:24:07,530
oh

4504
02:24:07,540 --> 02:24:07,689
actually

4505
02:24:07,699 --> 02:24:08,649
i wanted to build something different

4506
02:24:09,520 --> 02:24:09,930
you know

4507
02:24:10,080 --> 02:24:10,810
still was motivated

4508
02:24:11,120 --> 02:24:12,330
but i went to the first thing

4509
02:24:12,340 --> 02:24:13,050
so unless we have

4510
02:24:13,060 --> 02:24:13,770
like some really sophisticated

4511
02:24:14,479 --> 02:24:14,810
you know

4512
02:24:14,820 --> 02:24:16,729
system inside of it allows to like

4513
02:24:16,739 --> 02:24:18,090
you could equally value

4514
02:24:18,560 --> 02:24:20,090
doing this or do with another human

4515
02:24:20,359 --> 02:24:22,970
self-encourageability and which we don't currently really love

4516
02:24:22,980 --> 02:24:23,770
but implement formally

4517
02:24:24,479 --> 02:24:24,729
um

4518
02:24:25,680 --> 02:24:26,890
it's very likely that it will

4519
02:24:26,900 --> 02:24:27,130
you know

4520
02:24:27,140 --> 02:24:29,290
having instrumental goals like keeping itself alive

4521
02:24:29,439 --> 02:24:29,689
so

4522
02:24:29,760 --> 02:24:32,010
and where you're stopping you from giving it your orders

4523
02:24:32,080 --> 02:24:34,170
like it maybe will damage its entirement or something

4524
02:24:34,240 --> 02:24:35,210
so you can't give it orders

4525
02:24:35,520 --> 02:24:36,010
or maybe

4526
02:24:36,240 --> 02:24:36,649
if you

4527
02:24:36,880 --> 02:24:37,770
if you're now

4528
02:24:38,000 --> 02:24:40,250
if you try to destroy the broker because this malfunctioning

4529
02:24:40,880 --> 02:24:41,609
maybe they'll retaliate

4530
02:24:42,160 --> 02:24:44,250
because if it's destroyed it can't build habitat

4531
02:24:44,479 --> 02:24:44,729
no

4532
02:24:44,739 --> 02:24:45,130
can it

4533
02:24:45,520 --> 02:24:46,330
so so yeah

4534
02:24:46,399 --> 02:24:47,770
there's a lot of ghouls like this

4535
02:24:47,920 --> 02:24:50,970
also also like delayed power seeking goals to sleep obviously

4536
02:24:51,600 --> 02:24:51,850
yeah

4537
02:24:51,920 --> 02:24:53,530
if you don't know how to achieve a goal

4538
02:24:53,540 --> 02:24:56,010
and gaining your power is probably like always a good move

4539
02:24:56,080 --> 02:24:56,410
you know

4540
02:24:56,420 --> 02:24:57,050
getting more money

4541
02:24:57,200 --> 02:24:57,930
more social capital

4542
02:24:58,319 --> 02:24:58,649
you know

4543
02:24:58,659 --> 02:24:59,370
controlling for people

4544
02:24:59,380 --> 02:25:00,090
the driver resources

4545
02:25:00,560 --> 02:25:01,770
like almost always a good thing

4546
02:25:01,780 --> 02:25:02,170
like almost

4547
02:25:02,240 --> 02:25:03,290
no matter how innocuous

4548
02:25:03,600 --> 02:25:04,410
the goal might seem

4549
02:25:04,800 --> 02:25:05,050
yeah

4550
02:25:05,060 --> 02:25:05,450
but see

4551
02:25:05,520 --> 02:25:05,770
like

4552
02:25:05,780 --> 02:25:06,330
on the other hand

4553
02:25:06,800 --> 02:25:09,210
i don't care if a rover on mars tries to retaliate

4554
02:25:10,080 --> 02:25:13,530
it has like no capability to retaliate on us at all

4555
02:25:14,000 --> 02:25:17,530
and so the only kinds of like artificial intelligence i worry about are ones

4556
02:25:17,600 --> 02:25:19,850
that are capable of expanding in some capacity

4557
02:25:20,640 --> 02:25:22,729
like they have to be able to build things

4558
02:25:22,880 --> 02:25:26,010
and they actually have to be able to build intelligent things

4559
02:25:26,319 --> 02:25:28,410
so that's why i brought up the point of

4560
02:25:28,420 --> 02:25:28,649
you know

4561
02:25:28,880 --> 02:25:29,609
if we have a robot

4562
02:25:29,760 --> 02:25:31,210
we're gonna need one that can build

4563
02:25:31,600 --> 02:25:32,010
you know

4564
02:25:32,560 --> 02:25:32,810
somehow

4565
02:25:32,880 --> 02:25:33,290
or another

4566
02:25:33,439 --> 02:25:35,370
replicas of other robots to expand

4567
02:25:35,680 --> 02:25:36,569
conduct more work

4568
02:25:36,880 --> 02:25:38,170
replace ones that were broken

4569
02:25:38,319 --> 02:25:38,569
whatever

4570
02:25:39,120 --> 02:25:41,050
and it's not so infeasible to believe that

4571
02:25:41,359 --> 02:25:43,210
at some point in the next few hundred years

4572
02:25:43,280 --> 02:25:50,010
you know we'll have 3d printing technology more than sufficient to build silicon chips and you know whatever else

4573
02:25:50,560 --> 02:25:53,210
and as soon as you have a system that's self-replicating

4574
02:25:55,120 --> 02:25:58,010
and we don't live in a perfect world and we have random variation

4575
02:25:58,720 --> 02:25:58,970
right

4576
02:25:59,520 --> 02:26:01,050
and they have sets of instructions

4577
02:26:01,600 --> 02:26:03,850
and also we're going to need them to adapt to new environments

4578
02:26:04,240 --> 02:26:05,130
like it's going to be diff

4579
02:26:05,140 --> 02:26:05,130
difficult

4580
02:26:05,520 --> 02:26:05,770
different

4581
02:26:05,920 --> 02:26:10,970
building it on the plains of mars versus deep down in the valleys and mining water or whatever

4582
02:26:11,040 --> 02:26:14,090
so they're going to have to have some capability to allow variation

4583
02:26:14,800 --> 02:26:15,130
you know

4584
02:26:15,140 --> 02:26:17,370
then you have the mixture for evolution

4585
02:26:18,240 --> 02:26:19,290
and once you have evolution

4586
02:26:20,479 --> 02:26:21,609
even if you had no goals

4587
02:26:21,840 --> 02:26:22,090
like

4588
02:26:22,100 --> 02:26:22,810
think of it this way

4589
02:26:22,820 --> 02:26:28,810
the ultimate thing which has no motivations whatsoever was inanimate matter

4590
02:26:29,280 --> 02:26:31,609
and that's what the earth was four billion years ago

4591
02:26:31,760 --> 02:26:34,810
and we wound up with animate objects that have goals

4592
02:26:35,439 --> 02:26:36,970
so despite philosophers

4593
02:26:37,439 --> 02:26:38,729
and this is kind of an interesting thing

4594
02:26:38,739 --> 02:26:39,930
philosophers are always like

4595
02:26:39,940 --> 02:26:41,930
you can never get ought from is

4596
02:26:42,080 --> 02:26:43,210
but interestingly enough

4597
02:26:43,840 --> 02:26:45,930
ought arose from just stuff

4598
02:26:46,720 --> 02:26:46,970
right

4599
02:26:49,200 --> 02:26:49,450
yeah

4600
02:26:49,600 --> 02:26:51,210
the very elegantly said

4601
02:26:53,040 --> 02:26:55,850
there's this weird kind of symmetry breaking that happens somewhere

4602
02:26:56,240 --> 02:26:56,970
i mean in evolution

4603
02:26:57,200 --> 02:26:59,050
the symmetry breaking happened is because of physics

4604
02:26:59,200 --> 02:27:00,410
because things that don't want to replicate

4605
02:27:00,800 --> 02:27:01,290
don't replicate

4606
02:27:01,520 --> 02:27:02,330
so we don't see them

4607
02:27:02,399 --> 02:27:03,609
so we found the first one from

4608
02:27:03,619 --> 02:27:09,130
is comes from just the fact that if there is a possibility of replicator and the possibility non-replicator eventually

4609
02:27:09,280 --> 02:27:10,170
you will only see replicators

4610
02:27:10,720 --> 02:27:12,250
and that's your first symmetry breaking

4611
02:27:13,120 --> 02:27:14,170
and corporate occurs

4612
02:27:14,180 --> 02:27:14,890
that hopefully be

4613
02:27:15,200 --> 02:27:15,530
you know

4614
02:27:15,600 --> 02:27:20,410
when we build intelligent systems is that maybe we can break the symmetry in a more

4615
02:27:20,420 --> 02:27:20,810
you know

4616
02:27:20,820 --> 02:27:26,890
in a less dystopian you know than a malthusian way and build systems that aren't more motivated

4617
02:27:27,120 --> 02:27:29,770
but i think to be considered better than just for your replication

4618
02:27:30,319 --> 02:27:32,250
but it's very non-obvious to me

4619
02:27:33,040 --> 02:27:34,569
you know how to do that safely

4620
02:27:34,720 --> 02:27:35,130
it's hard

4621
02:27:35,200 --> 02:27:36,170
it's a hard trouble

4622
02:27:37,680 --> 02:27:42,649
but hawkins did say though that he was more worried about the immediate threat of um humans

4623
02:27:42,960 --> 02:27:45,050
controlling the kind of ai that we have now

4624
02:27:45,060 --> 02:27:50,170
or the kind of ai we have now being used for bad purposes or or even things like

4625
02:27:50,240 --> 02:27:50,490
um

4626
02:27:50,500 --> 02:27:51,530
the spread of false beliefs

4627
02:27:52,720 --> 02:27:54,970
that's incredibly dangerous as well

4628
02:27:55,200 --> 02:27:57,290
and that that replicates like a virus

4629
02:27:58,000 --> 02:27:58,649
that's why that

4630
02:27:58,659 --> 02:27:58,890
um

4631
02:27:58,900 --> 02:28:00,490
you know that social dilemma documentary

4632
02:28:00,800 --> 02:28:01,050
like

4633
02:28:01,060 --> 02:28:03,609
i thought this was a very brilliant quote that was in there

4634
02:28:03,619 --> 02:28:06,410
and a lot of other people think it's like not an interesting quote

4635
02:28:06,420 --> 02:28:08,569
but to me it was beautiful when

4636
02:28:08,579 --> 02:28:08,810
uh

4637
02:28:08,820 --> 02:28:09,210
you know

4638
02:28:09,280 --> 02:28:09,530
undermining

4639
02:28:10,880 --> 02:28:11,130
exactly

4640
02:28:11,280 --> 02:28:14,649
he said we've always been worried about when ai would overcome our strengths

4641
02:28:15,520 --> 02:28:18,250
when we should have been worried about when it would overcome our weaknesses

4642
02:28:20,080 --> 02:28:20,890
right and that

4643
02:28:21,040 --> 02:28:21,689
and it's already

4644
02:28:22,160 --> 02:28:22,490
you know

4645
02:28:22,500 --> 02:28:23,290
even though we don't have

4646
02:28:23,680 --> 02:28:24,330
maybe we forget

4647
02:28:24,399 --> 02:28:26,170
if we have real intelligence or not

4648
02:28:26,180 --> 02:28:26,490
or whatever

4649
02:28:27,280 --> 02:28:27,530
clearly

4650
02:28:27,760 --> 02:28:32,170
these machine systems have overcome lots of our weaknesses

4651
02:28:32,720 --> 02:28:36,010
and they've created things that are more addictive than they've ever been found

4652
02:28:36,020 --> 02:28:38,810
ways to waste more of our time than now

4653
02:28:38,820 --> 02:28:40,410
of course there's offsetting things like

4654
02:28:40,420 --> 02:28:40,649
yeah

4655
02:28:40,659 --> 02:28:42,970
we also get greater productivity here

4656
02:28:43,200 --> 02:28:43,450
etc

4657
02:28:43,600 --> 02:28:43,850
etc

4658
02:28:44,160 --> 02:28:46,170
but we should be worried about

4659
02:28:46,180 --> 02:28:49,770
when it overcomes our weaknesses and what that's doing to us and our children

4660
02:28:50,080 --> 02:28:50,729
and you know

4661
02:28:51,439 --> 02:28:56,090
one of my favorite examples is it's called the food optimizer

4662
02:28:56,640 --> 02:28:57,450
so this comes from

4663
02:28:58,000 --> 02:28:58,490
and yeah

4664
02:28:58,500 --> 02:28:58,970
called roku

4665
02:28:59,520 --> 02:29:01,290
and he makes this example that

4666
02:29:01,439 --> 02:29:01,689
so

4667
02:29:01,699 --> 02:29:03,290
the way he describes it is

4668
02:29:03,300 --> 02:29:09,450
that in like the 19th century we collectively summoned a weak super intelligence to feed all humans

4669
02:29:09,760 --> 02:29:11,290
i think the deal was kind of this

4670
02:29:11,359 --> 02:29:11,850
this capitalist

4671
02:29:12,319 --> 02:29:12,649
you know

4672
02:29:12,659 --> 02:29:16,090
market system will produce really cheap food for us

4673
02:29:16,319 --> 02:29:20,410
but the system was optimizing a reward signal based on profit

4674
02:29:20,800 --> 02:29:21,210
of course

4675
02:29:21,359 --> 02:29:22,490
let's see how these things work

4676
02:29:22,500 --> 02:29:23,689
so it's kind of like a weak

4677
02:29:23,840 --> 02:29:24,649
it's not sent huge

4678
02:29:24,720 --> 02:29:26,010
it was obviously not intelligent

4679
02:29:26,399 --> 02:29:27,689
the weight you know humans are

4680
02:29:27,699 --> 02:29:28,729
but a weed is option

4681
02:29:28,880 --> 02:29:29,530
rising system

4682
02:29:30,000 --> 02:29:31,689
it optimized for more and more food

4683
02:29:31,699 --> 02:29:33,210
and now we're at the point that you know what

4684
02:29:33,220 --> 02:29:35,850
60 of adults in the western world are obese

4685
02:29:36,000 --> 02:29:36,250
something

4686
02:29:36,479 --> 02:29:38,490
we are literally eating ourselves to death

4687
02:29:38,640 --> 02:29:39,609
and this is a very

4688
02:29:40,000 --> 02:29:45,609
very weak intelligence quote and this is already powerful enough in a way to you know

4689
02:29:46,080 --> 02:29:47,930
get people to basically kill themselves voluntarily

4690
02:29:48,640 --> 02:29:50,970
so imagine if you gave such a motivation

4691
02:29:51,439 --> 02:29:51,770
you know

4692
02:29:52,000 --> 02:29:52,250
biggest

4693
02:29:52,399 --> 02:29:52,729
you know

4694
02:29:52,800 --> 02:29:53,850
food corporation in the world

4695
02:29:53,920 --> 02:29:54,330
you know

4696
02:29:54,340 --> 02:29:54,729
build their big

4697
02:29:54,739 --> 02:29:55,370
their big agi

4698
02:29:55,680 --> 02:29:56,010
you know

4699
02:29:56,240 --> 02:29:56,970
using hawkins

4700
02:29:57,359 --> 02:29:57,770
you know

4701
02:29:57,780 --> 02:29:58,649
thousand brandon algorithm

4702
02:29:58,800 --> 02:29:59,130
of course

4703
02:29:59,760 --> 02:30:00,010
uh

4704
02:30:00,020 --> 02:30:01,210
and give you the motivation of

4705
02:30:01,220 --> 02:30:01,370
you know

4706
02:30:01,439 --> 02:30:01,930
smith roth

4707
02:30:03,359 --> 02:30:03,609
yeah

4708
02:30:03,619 --> 02:30:04,010
but this

4709
02:30:04,020 --> 02:30:08,010
this gets to utility and similarly hawkins said

4710
02:30:08,479 --> 02:30:08,729
oh

4711
02:30:08,739 --> 02:30:10,090
you should never clone yourself

4712
02:30:10,479 --> 02:30:11,450
because you're not cloning

4713
02:30:11,600 --> 02:30:12,090
you're forking

4714
02:30:13,040 --> 02:30:14,170
i thought that was a bit of a contradiction

4715
02:30:14,399 --> 02:30:14,649
though

4716
02:30:14,659 --> 02:30:15,689
because he was talking about

4717
02:30:15,840 --> 02:30:16,090
um

4718
02:30:16,880 --> 02:30:17,210
you know

4719
02:30:17,220 --> 02:30:19,609
sending clones of yourself to mars or seeding mars

4720
02:30:19,680 --> 02:30:20,170
that's right

4721
02:30:20,479 --> 02:30:21,450
and that's the same thing

4722
02:30:21,460 --> 02:30:21,850
isn't it

4723
02:30:21,860 --> 02:30:25,850
you're actually forking the human race when you have a population of humans on on mars

4724
02:30:25,920 --> 02:30:26,649
i don't think

4725
02:30:26,659 --> 02:30:26,890
um

4726
02:30:27,600 --> 02:30:29,689
who doesn't like forking what's wrong with that

4727
02:30:29,920 --> 02:30:30,170
well

4728
02:30:30,180 --> 02:30:30,330
no

4729
02:30:30,340 --> 02:30:31,450
but the thing is because you

4730
02:30:31,460 --> 02:30:31,609
we

4731
02:30:32,160 --> 02:30:33,850
we're doing it because we want to leave a legacy

4732
02:30:34,080 --> 02:30:34,490
you know

4733
02:30:34,500 --> 02:30:34,729
i

4734
02:30:34,739 --> 02:30:36,569
i like doing this youtube channel because you know

4735
02:30:36,579 --> 02:30:37,050
after i'm dead

4736
02:30:37,060 --> 02:30:39,850
people can see who is that tim scarf guy

4737
02:30:39,860 --> 02:30:40,569
but um

4738
02:30:40,720 --> 02:30:42,890
but you know you might say i'm going to create a clone of myself

4739
02:30:42,960 --> 02:30:44,010
because i want to have a legacy

4740
02:30:44,160 --> 02:30:44,970
but it's not a legacy

4741
02:30:45,120 --> 02:30:46,490
because after

4742
02:30:46,720 --> 02:30:48,890
after the next day it becomes a

4743
02:30:48,960 --> 02:30:49,290
you know

4744
02:30:49,520 --> 02:30:50,330
it's a different person

4745
02:30:50,479 --> 02:30:51,210
it's not connor anymore

4746
02:30:51,359 --> 02:30:51,770
and similarly

4747
02:30:52,160 --> 02:30:53,210
when you were talking about

4748
02:30:53,600 --> 02:30:53,850
um

4749
02:30:54,080 --> 02:30:54,490
you know

4750
02:30:54,500 --> 02:30:58,330
why don't we restructure society because let's take away social media

4751
02:30:58,800 --> 02:31:02,170
let's let's fork and take away facebook and everything else

4752
02:31:02,319 --> 02:31:03,689
but what's your utility function

4753
02:31:03,760 --> 02:31:04,330
how could you

4754
02:31:04,560 --> 02:31:09,530
how could you justify that ourselves in that parallel universe would be better off than we are now

4755
02:31:10,000 --> 02:31:10,250
well

4756
02:31:10,260 --> 02:31:10,729
first of all

4757
02:31:10,739 --> 02:31:12,250
i mean i think it would be a legacy

4758
02:31:12,560 --> 02:31:13,210
so you

4759
02:31:13,220 --> 02:31:17,290
and i both think that that the pixels on youtube are going to be a legacy

4760
02:31:17,600 --> 02:31:20,010
and those are a pale pale shadow of a

4761
02:31:20,399 --> 02:31:20,810
you know

4762
02:31:20,820 --> 02:31:21,930
projection of us right

4763
02:31:22,160 --> 02:31:23,370
so i would

4764
02:31:23,380 --> 02:31:23,530
totally

4765
02:31:23,760 --> 02:31:24,090
you know

4766
02:31:24,100 --> 02:31:28,970
a clone of you or a robot programmed with your mind state or something that then can go off

4767
02:31:28,980 --> 02:31:29,930
and live its own life

4768
02:31:30,319 --> 02:31:33,050
that's exactly what a legacy is like that

4769
02:31:33,060 --> 02:31:34,410
that is a legacy

4770
02:31:34,560 --> 02:31:35,290
it's not me

4771
02:31:35,520 --> 02:31:36,090
it's a legacy

4772
02:31:37,520 --> 02:31:37,770
yeah

4773
02:31:37,780 --> 02:31:41,130
like i described to even more extreme crazy

4774
02:31:41,680 --> 02:31:41,930
uh

4775
02:31:41,940 --> 02:31:44,170
versions of this kind of thing is like i

4776
02:31:44,180 --> 02:31:45,290
i fundamentally believe

4777
02:31:45,359 --> 02:31:46,649
i'm like you know a non-dual identity

4778
02:31:47,040 --> 02:31:47,930
and like that

4779
02:31:48,399 --> 02:31:50,330
most of the kinds of identity is kind of incoherent

4780
02:31:50,720 --> 02:31:54,010
it's just it's just a convenient abstraction in humans

4781
02:31:54,160 --> 02:31:55,770
like humans tend to be discreet

4782
02:31:56,160 --> 02:32:00,090
that's just a convenient property that humans have to have happen to have

4783
02:32:00,240 --> 02:32:04,890
there's no reason that identity or minds in the sense have to be discrete like this

4784
02:32:04,900 --> 02:32:07,130
it just happens to be the case that humans come in packages

4785
02:32:07,600 --> 02:32:09,050
it looks like they become in

4786
02:32:09,060 --> 02:32:09,370
you know

4787
02:32:09,439 --> 02:32:11,770
one intelligence please one unit of intelligence

4788
02:32:12,319 --> 02:32:13,370
but there's everything you should study

4789
02:32:14,160 --> 02:32:14,729
they have it

4790
02:32:14,739 --> 02:32:15,210
they have a mark

4791
02:32:15,439 --> 02:32:18,970
they have to have a markov blanket exactly exactly

4792
02:32:19,120 --> 02:32:21,609
but there's no reason you can't have systems that don't really

4793
02:32:22,080 --> 02:32:25,370
there's not like a clear separation between them like the

4794
02:32:25,439 --> 02:32:28,569
the only reason i think really me and tim aren't the same person

4795
02:32:29,040 --> 02:32:32,330
it's because the bandwidth between our brains is really low is interbreed

4796
02:32:32,800 --> 02:32:34,250
it's lower that it is intraprey

4797
02:32:34,880 --> 02:32:35,130
like

4798
02:32:35,200 --> 02:32:36,729
why is the right heart for memory

4799
02:32:36,800 --> 02:32:38,090
maybe that's the person in the left hand

4800
02:32:38,100 --> 02:32:38,970
also a different person

4801
02:32:39,280 --> 02:32:40,170
depending on your definition

4802
02:32:40,720 --> 02:32:41,210
like you know

4803
02:32:41,220 --> 02:32:42,010
there's like split brain patients

4804
02:32:42,160 --> 02:32:47,210
where like the brain has to actually disagree and like take different actions and like fight with each other

4805
02:32:47,359 --> 02:32:48,569
so like really different people

4806
02:32:48,720 --> 02:32:49,210
i don't think

4807
02:32:49,220 --> 02:32:49,770
so i think

4808
02:32:49,780 --> 02:32:52,569
what makes identity a synchronization in a sense or like alignment

4809
02:32:52,960 --> 02:32:54,250
so you see is that identity

4810
02:32:54,560 --> 02:32:57,210
the best way to define identity is alignment with agua

4811
02:32:57,600 --> 02:32:57,930
is that

4812
02:32:58,000 --> 02:32:58,250
like

4813
02:32:58,260 --> 02:32:58,649
does the

4814
02:32:58,659 --> 02:33:01,130
the system coherently act without

4815
02:33:01,140 --> 02:33:01,530
you know

4816
02:33:01,540 --> 02:33:02,569
like shooting itself from the foot

4817
02:33:02,640 --> 02:33:03,210
so to speak

4818
02:33:03,220 --> 02:33:03,770
or like you know

4819
02:33:03,780 --> 02:33:04,569
doing at all

4820
02:33:05,040 --> 02:33:05,450
so like

4821
02:33:05,460 --> 02:33:07,609
that's why i think it's a really great argument that

4822
02:33:07,619 --> 02:33:07,850
for example

4823
02:33:08,000 --> 02:33:08,569
ant colonies

4824
02:33:08,880 --> 02:33:11,210
for one animal that happens to have

4825
02:33:11,359 --> 02:33:11,609
like

4826
02:33:11,619 --> 02:33:11,930
you know

4827
02:33:12,560 --> 02:33:13,850
very big individual parts

4828
02:33:14,399 --> 02:33:15,850
because in a way it is aligned

4829
02:33:15,920 --> 02:33:17,050
you know not all the parts were produced

4830
02:33:17,120 --> 02:33:18,170
you have a reproducing part

4831
02:33:18,399 --> 02:33:18,810
you have

4832
02:33:18,880 --> 02:33:19,210
you know

4833
02:33:19,520 --> 02:33:20,170
fighting for death

4834
02:33:20,180 --> 02:33:20,970
eating parts or whatever

4835
02:33:21,200 --> 02:33:21,450
food

4836
02:33:21,520 --> 02:33:22,410
getting parts or whatever

4837
02:33:22,880 --> 02:33:27,609
and they happen to not all be one body that's just an implication detail

4838
02:33:28,240 --> 02:33:30,170
you were also just colonies of cells

4839
02:33:30,479 --> 02:33:30,810
you know

4840
02:33:30,820 --> 02:33:31,050
it's

4841
02:33:31,060 --> 02:33:31,530
it's just

4842
02:33:31,540 --> 02:33:33,530
it's just implementation details and matters

4843
02:33:33,680 --> 02:33:35,850
is the coherency with the synchronization

4844
02:33:36,880 --> 02:33:37,130
of

4845
02:33:37,520 --> 02:33:37,930
you know

4846
02:33:38,240 --> 02:33:39,930
knowledge and goals in life

4847
02:33:40,399 --> 02:33:40,649
yeah

4848
02:33:40,720 --> 02:33:41,370
but again

4849
02:33:41,380 --> 02:33:42,569
a lot of that is is emergent

4850
02:33:44,319 --> 02:33:45,770
but you were saying as well that

4851
02:33:45,780 --> 02:33:46,010
um

4852
02:33:46,399 --> 02:33:47,770
there's a kind of there's a

4853
02:33:47,780 --> 02:33:49,370
there's an i o bottleneck when we communicate

4854
02:33:49,920 --> 02:33:50,970
and this is

4855
02:33:50,980 --> 02:33:51,130
um

4856
02:33:51,760 --> 02:33:54,729
a lot of people talk about neural link in this context as well

4857
02:33:55,120 --> 02:33:56,330
but i don't think there is

4858
02:33:56,340 --> 02:33:57,130
because when we communicate

4859
02:33:58,240 --> 02:33:59,850
there's common knowledge that we both have

4860
02:34:00,240 --> 02:34:03,130
so it's a fairly efficient communication mechanism

4861
02:34:03,520 --> 02:34:07,290
and we're invoking lots and lots of very complex knowledge that we've reconciled

4862
02:34:07,920 --> 02:34:10,010
so when we read a book or when we

4863
02:34:10,560 --> 02:34:11,689
when we take on board information

4864
02:34:12,080 --> 02:34:13,450
the bottleneck is comprehension

4865
02:34:14,640 --> 02:34:15,689
i don't think it's the

4866
02:34:16,240 --> 02:34:16,649
the bandwidth

4867
02:34:17,760 --> 02:34:18,010
sure

4868
02:34:18,080 --> 02:34:19,450
as it depends on how like

4869
02:34:19,460 --> 02:34:20,010
and by bandwidth

4870
02:34:20,160 --> 02:34:21,609
i also meant comprehension back

4871
02:34:21,840 --> 02:34:22,890
it's like if i could

4872
02:34:22,900 --> 02:34:23,050
literally

4873
02:34:23,760 --> 02:34:24,649
i imagine we

4874
02:34:24,659 --> 02:34:24,810
could

4875
02:34:24,820 --> 02:34:25,130
you know

4876
02:34:25,140 --> 02:34:25,370
take

4877
02:34:25,380 --> 02:34:27,050
two human brain would put them in the same skull

4878
02:34:27,120 --> 02:34:30,410
and could like to connect them through some super cleopas closeum or whatever

4879
02:34:30,720 --> 02:34:31,370
and you know

4880
02:34:31,380 --> 02:34:35,130
let the whole thing synchronize and you're not diet seizures immediately or whatever

4881
02:34:35,359 --> 02:34:36,890
i think there's an arsenal argument to be made

4882
02:34:36,900 --> 02:34:38,010
that is one entity

4883
02:34:38,880 --> 02:34:42,810
but you could also make an argument's two connected entities and simply just becomes continuous

4884
02:34:43,359 --> 02:34:46,250
like a classic play open when something seems confusing

4885
02:34:46,800 --> 02:34:47,770
if there's a discrete quantity

4886
02:34:47,920 --> 02:34:48,170
evolved

4887
02:34:48,319 --> 02:34:52,010
try try continualizing the quantity and see if the confusion disappears

4888
02:34:53,840 --> 02:34:54,410
very cool

4889
02:34:55,680 --> 02:34:55,930
well

4890
02:34:56,479 --> 02:34:56,729
gentlemen

4891
02:34:57,680 --> 02:34:58,410
it's been a pleasure

4892
02:35:00,720 --> 02:35:01,770
so see you next week

4893
02:35:01,780 --> 02:35:03,450
folks see you next week

