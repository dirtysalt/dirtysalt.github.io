1
00:00:44,520 --> 00:00:49,480
Our audience is largely want to be entrepreneurs in India.

2
00:00:49,560 --> 00:00:53,160
And I feel like all of us have so much to learn from you

3
00:00:53,240 --> 00:00:56,600
because you've done it so many times over in so many different domains.

4
00:00:56,680 --> 00:00:58,840
So, we will speak to them today,

5
00:00:58,920 --> 00:01:03,160
and I will try and centre all my questions in that direction

6
00:01:03,240 --> 00:01:05,200
so they can take advantage of this conversation

7
00:01:05,280 --> 00:01:08,240
and maybe start-- take a chance and build something.

8
00:01:25,520 --> 00:01:27,000
Do you want a coffee?

9
00:01:27,080 --> 00:01:28,560
Um...

10
00:01:28,640 --> 00:01:29,720
-Sure, why not? -Okay.

11
00:01:30,560 --> 00:01:32,080
Are we gonna be talking for a while?

12
00:01:32,160 --> 00:01:33,840
-[laughs] -I hope we are.

13
00:01:33,920 --> 00:01:35,560
Okay, good. Sure.

14
00:01:36,960 --> 00:01:38,120
-Um... -Meghana?

15
00:01:38,200 --> 00:01:39,480
May I trouble you for a coffee?

16
00:01:39,560 --> 00:01:41,240
Can we get another coffee?

17
00:01:41,320 --> 00:01:43,120
-[Meghana] Anything's fine? -Uh...

18
00:01:43,200 --> 00:01:44,400
Cappuccino, I guess.

19
00:01:44,480 --> 00:01:45,520
[Meghana] Cappuccino? Okay.

20
00:01:45,600 --> 00:01:47,000
Are you a coffee drinker, Elon?

21
00:01:47,080 --> 00:01:48,160
-Yeah, yeah. -Yeah?

22
00:01:48,240 --> 00:01:49,280
Yeah, I have a coffee once,

23
00:01:49,360 --> 00:01:51,080
-usually in the mornings. -Okay.

24
00:01:51,160 --> 00:01:53,960
-One-a-day kind of thing? -Yeah, pretty much.

25
00:01:58,600 --> 00:01:59,760
You want to wait for it?

26
00:01:59,840 --> 00:02:02,200
No, I'm good.

27
00:02:04,440 --> 00:02:05,960
[laughter]

28
00:02:07,360 --> 00:02:09,199
The first thing I must say

29
00:02:09,280 --> 00:02:13,640
is you're a lot bigger and bulkier, muscular,

30
00:02:13,720 --> 00:02:14,960
than I would have thought you are.

31
00:02:15,040 --> 00:02:17,320
Oh, stop, you're making me blush.

32
00:02:19,360 --> 00:02:20,480
Really. Seriously.

33
00:02:22,040 --> 00:02:24,360
Yeah, I mean, look, on the Internet, I'm small, you know?

34
00:02:28,800 --> 00:02:30,320
You're essentially...

35
00:02:31,640 --> 00:02:35,080
What percentage of Internet is spent on Twitter?

36
00:02:35,160 --> 00:02:37,040
Is there a number to it? On X.

37
00:02:37,120 --> 00:02:42,040
Well, so, we have, like, about 600 million monthly users.

38
00:02:43,400 --> 00:02:47,680
Although, it can spike up, if there's some major event in the world.

39
00:02:47,760 --> 00:02:51,840
It can get up to, I don't know, 800 million or a billion,

40
00:02:52,600 --> 00:02:54,760
if there's some major event in the world, so...

41
00:02:58,520 --> 00:03:02,360
I don't know, 250 to 300 million per week type of thing.

42
00:03:03,160 --> 00:03:04,600
It's a pretty decent number.

43
00:03:04,680 --> 00:03:09,520
It tends to be readers, you know, people that read words.

44
00:03:11,360 --> 00:03:12,720
You know, so...

45
00:03:12,800 --> 00:03:14,440
Do you think that'll change?

46
00:03:14,520 --> 00:03:16,000
Yeah, I mean, there's...

47
00:03:17,920 --> 00:03:21,720
There's certainly a lot of video on the X system.

48
00:03:21,800 --> 00:03:24,560
But at this point... Increasing amounts of video.

49
00:03:24,640 --> 00:03:31,000
But I think where the X network is strongest

50
00:03:31,080 --> 00:03:36,400
is among people who think a lot and read a lot.

51
00:03:36,480 --> 00:03:39,120
You know, so, that's where it's gonna be strongest.

52
00:03:39,200 --> 00:03:40,400
Because we have words.

53
00:03:41,800 --> 00:03:43,320
And, you know, so...

54
00:03:45,440 --> 00:03:51,000
Among readers, writers, and thinkers, I think X is number one in the world.

55
00:03:52,080 --> 00:03:56,880
As far as social media goes, the form factor,

56
00:03:56,960 --> 00:03:59,200
if you had to wager a guess for tomorrow...

57
00:03:59,280 --> 00:04:01,720
-Yeah. -...how much is text, how much is video?

58
00:04:02,880 --> 00:04:06,360
I've heard you speak about maybe voice and hearing

59
00:04:06,440 --> 00:04:09,240
being the next form of communication with AI.

60
00:04:09,320 --> 00:04:14,080
What happens to X in its true form? How does it evolve?

61
00:04:15,640 --> 00:04:20,240
Yeah, so, I do think most interaction is gonna be video in the future.

62
00:04:20,320 --> 00:04:23,600
Most interactions are gonna be real-time video with AI.

63
00:04:23,680 --> 00:04:27,160
So, real-time video comprehension, real-time video generation,

64
00:04:28,200 --> 00:04:29,880
that's gonna be most of the load.

65
00:04:29,960 --> 00:04:33,080
And that's how it is for most of the Internet right now.

66
00:04:33,160 --> 00:04:34,520
Most of the Internet is video.

67
00:04:34,600 --> 00:04:36,920
Text is a pretty small percentage.

68
00:04:37,000 --> 00:04:41,440
But the text tends to be higher value, generally.

69
00:04:41,520 --> 00:04:45,240
Or more, it's more densely compressed information.

70
00:04:48,280 --> 00:04:50,080
Yeah, so...

71
00:04:50,160 --> 00:04:55,800
But if you say what is the most amount of bits generated and compute spent,

72
00:04:55,880 --> 00:04:57,000
it's certainly gonna be video.

73
00:04:58,000 --> 00:04:59,640
So, I used to be a shareholder of X,

74
00:04:59,720 --> 00:05:00,920
-a very small one. -Okay.

75
00:05:01,000 --> 00:05:04,240
And I got paid when you bought Twitter and you made it X.

76
00:05:06,400 --> 00:05:10,160
-Happy decision? Glad you did it? -Yeah, yeah, I think it was important.

77
00:05:11,880 --> 00:05:15,080
You know, I felt like Twitter was heading in...

78
00:05:15,160 --> 00:05:17,040
or had gone in a direction

79
00:05:17,120 --> 00:05:19,880
that had more of a negative influence on the world.

80
00:05:21,120 --> 00:05:22,520
It was...

81
00:05:23,120 --> 00:05:25,200
I mean, of course, this depends on one's perspective.

82
00:05:25,280 --> 00:05:26,800
Some people will say, well, actually,

83
00:05:26,880 --> 00:05:28,880
they liked the way it was, and now, they don't like it.

84
00:05:30,000 --> 00:05:34,120
But I think the fundamental thing was that...

85
00:05:36,240 --> 00:05:39,560
Twitter was amplifying...

86
00:05:39,640 --> 00:05:42,440
I would say, a fairly pretty far left by most people's standards

87
00:05:42,520 --> 00:05:43,880
in the world's ideology

88
00:05:43,960 --> 00:05:46,120
because of where it was based, in San Francisco.

89
00:05:46,200 --> 00:05:50,040
And then they actually suspended a lot of people on the right.

90
00:05:53,920 --> 00:05:55,240
So, from their perspective,

91
00:05:55,320 --> 00:05:58,000
even someone in the centre would be far right.

92
00:05:58,080 --> 00:06:01,200
If you're far left, anyone in the centre is far right, because...

93
00:06:02,760 --> 00:06:04,800
It's just on the political spectrum,

94
00:06:05,840 --> 00:06:09,040
they're just as far left as you get in the United States and in San Francisco.

95
00:06:09,120 --> 00:06:13,880
So, what I've tried to do is just restore it to be balanced and centrist.

96
00:06:13,960 --> 00:06:16,440
So, there haven't been any left-wing voices

97
00:06:16,520 --> 00:06:23,000
that have been suspended or banned or de-amplified or anything like that.

98
00:06:23,080 --> 00:06:27,200
Now, some of them have chosen to just go somewhere else, but...

99
00:06:28,440 --> 00:06:34,560
But at this point, the operating principle of the X system

100
00:06:34,640 --> 00:06:37,920
is to adhere to any country's laws,

101
00:06:38,000 --> 00:06:41,520
but not to put our thumb on the scale beyond the laws of a country.

102
00:06:44,000 --> 00:06:45,840
When I think of social media...

103
00:06:47,000 --> 00:06:48,440
-Oh, thank you. -Thank you.

104
00:06:49,280 --> 00:06:51,080
When I think of social media, Elon,

105
00:06:52,200 --> 00:06:57,080
I feel like... even data suggests that the current incumbents

106
00:06:57,160 --> 00:07:01,360
seem to be losing traction amongst the youngest of audience.

107
00:07:01,440 --> 00:07:02,440
Yeah.

108
00:07:02,520 --> 00:07:04,800
Even platforms like Instagram...

109
00:07:04,880 --> 00:07:08,800
I mean, they're not exactly like Twitter, but platforms across the board.

110
00:07:08,880 --> 00:07:13,600
If one had to rework social media and build something bottom-up,

111
00:07:13,680 --> 00:07:16,280
what do you think would work for the world of tomorrow?

112
00:07:18,080 --> 00:07:21,720
Well, I mean, I don't think that much about...

113
00:07:24,120 --> 00:07:26,680
about social media, to be frank. I mean, it's...

114
00:07:26,760 --> 00:07:29,280
I mostly just wanna have something where there's...

115
00:07:32,000 --> 00:07:35,440
in the case of X, kind of a global town square,

116
00:07:35,520 --> 00:07:41,480
where people can say what they wanna say with words, pictures, video...

117
00:07:43,160 --> 00:07:45,280
where there's a secure messaging system.

118
00:07:45,360 --> 00:07:49,120
We've recently added the ability to do audio and video calls.

119
00:07:50,720 --> 00:07:55,760
So, really trying to bring the world together

120
00:07:55,840 --> 00:07:59,880
into a eclectic consciousness.

121
00:08:02,480 --> 00:08:05,960
That's, I guess, different from just saying, like,

122
00:08:06,040 --> 00:08:11,120
"What is the most dopamine-generating video stream that one could make?"

123
00:08:12,560 --> 00:08:14,320
Which, you know,

124
00:08:15,240 --> 00:08:18,040
I think it can be a little bit of brain rot, frankly.

125
00:08:19,000 --> 00:08:20,840
You know, if you're just watching videos

126
00:08:20,920 --> 00:08:25,240
that just cause dopamine hits one after another, but lack substance,

127
00:08:25,320 --> 00:08:30,040
then I think those are not great. That's not a great way to spend time.

128
00:08:32,320 --> 00:08:35,919
But I do think that's actually what a lot of people are gonna wanna watch.

129
00:08:36,960 --> 00:08:40,120
So, if you say, like, total Internet usage,

130
00:08:40,200 --> 00:08:42,320
it's gonna probably be optimising

131
00:08:42,400 --> 00:08:46,720
for, you know, neurotransmitter generation.

132
00:08:46,800 --> 00:08:50,200
Like, there's somebody getting a kick out of it.

133
00:08:50,280 --> 00:08:53,120
But it becomes like a drug type of thing.

134
00:08:55,360 --> 00:08:57,440
But I'm not really after...

135
00:08:58,840 --> 00:09:00,800
My goal is not to do that.

136
00:09:00,880 --> 00:09:02,920
I guess I could do that if I wanted to,

137
00:09:03,000 --> 00:09:05,520
but, um, I...

138
00:09:05,600 --> 00:09:10,240
I just wanna really have a global platform that brings together...

139
00:09:11,520 --> 00:09:15,720
Like I said, it becomes close to, sort of, a collective consciousness

140
00:09:16,680 --> 00:09:18,200
of humanity as possible.

141
00:09:21,600 --> 00:09:24,760
And one of the things that we've introduced, um,

142
00:09:24,840 --> 00:09:27,840
for example, is automatic translation.

143
00:09:30,240 --> 00:09:34,800
'Cause I think it would be great to bring together what people say

144
00:09:34,880 --> 00:09:38,160
in many different languages, and...

145
00:09:38,240 --> 00:09:40,920
but automatically translated for the recipient.

146
00:09:41,000 --> 00:09:42,840
So, you have the collective consciousness,

147
00:09:42,920 --> 00:09:46,480
not just of, say, people in a particular language group,

148
00:09:46,560 --> 00:09:51,760
but you have the thoughts of people in, you know,

149
00:09:51,840 --> 00:09:53,520
every language group.

150
00:09:53,600 --> 00:09:55,520
And why is that important, Elon?

151
00:09:55,600 --> 00:09:58,520
Collective consciousness, to have one platform?

152
00:09:58,600 --> 00:10:00,040
I guess...

153
00:10:02,120 --> 00:10:03,320
Yeah, why is that important?

154
00:10:12,920 --> 00:10:15,680
I guess it's-- You could also say, like, why...

155
00:10:18,120 --> 00:10:19,640
You know, if you consider humans,

156
00:10:19,720 --> 00:10:24,400
like, humans are composed of around 30 to 40 trillion cells.

157
00:10:29,040 --> 00:10:31,880
You know, there's trillions of synapses in your mind.

158
00:10:37,880 --> 00:10:40,560
But there's not-- The why of it, I mean, I guess,

159
00:10:40,640 --> 00:10:44,000
it's just so we can increase...

160
00:10:45,640 --> 00:10:47,480
our understanding.

161
00:10:47,560 --> 00:10:48,720
Increase our...

162
00:10:51,520 --> 00:10:53,080
our understanding of the universe.

163
00:11:00,720 --> 00:11:02,760
I guess I had this...

164
00:11:02,840 --> 00:11:05,800
sort of question about what's the meaning of life, you know?

165
00:11:09,560 --> 00:11:11,160
Why is anything important?

166
00:11:15,080 --> 00:11:17,800
You know, why are we here?

167
00:11:18,880 --> 00:11:21,840
What's the origin of the universe? What is the end?

168
00:11:24,560 --> 00:11:26,840
What are the questions that we don't even know to ask?

169
00:11:30,160 --> 00:11:32,240
And probably the questions we don't even know to ask

170
00:11:32,320 --> 00:11:34,000
are the most important ones.

171
00:11:36,120 --> 00:11:39,440
So, I'm just trying to, I guess, understand what's going on.

172
00:11:39,520 --> 00:11:41,360
What is going on in this reality?

173
00:11:44,600 --> 00:11:45,800
Is this reality?

174
00:11:50,120 --> 00:11:54,560
And where did you get when you asked, "What is the point of life?"

175
00:11:56,160 --> 00:11:57,360
Yeah, so, I...

176
00:11:59,880 --> 00:12:02,360
I came to the conclusion that...

177
00:12:02,440 --> 00:12:06,000
which is somewhat, in the Douglas Adams

178
00:12:06,080 --> 00:12:08,280
Hitchhiker's Guide to the Galaxy school of thought,

179
00:12:08,360 --> 00:12:10,240
-which is-- -42.

180
00:12:10,320 --> 00:12:12,600
Yeah, you know, he sort of...

181
00:12:13,720 --> 00:12:14,920
Hitchhiker's Guide to the Galaxy

182
00:12:15,000 --> 00:12:17,720
is like a book on philosophy disguised as humour.

183
00:12:17,800 --> 00:12:18,800
-Yeah. -And...

184
00:12:20,720 --> 00:12:22,000
That's where you get the--

185
00:12:22,080 --> 00:12:26,400
You know, Earth turns out to be this computer to understand,

186
00:12:26,480 --> 00:12:29,320
to get to figure out the answer of the meaning of life.

187
00:12:29,400 --> 00:12:31,640
And it comes up with the answer of 42.

188
00:12:31,720 --> 00:12:34,520
But then, it's like, "What does 42 mean?"

189
00:12:36,120 --> 00:12:37,280
And it turns out, well, actually,

190
00:12:37,360 --> 00:12:42,200
the hard part is the question, not the answer.

191
00:12:42,280 --> 00:12:45,520
And for that, you need a much bigger computer than Earth.

192
00:12:45,600 --> 00:12:47,640
So, basically, what Douglas Adams was saying is that

193
00:12:47,720 --> 00:12:50,360
we actually don't know how to frame the questions properly.

194
00:12:52,320 --> 00:12:55,600
And so, I think by expanding the scope and scale of consciousness,

195
00:12:55,680 --> 00:12:59,720
we can better understand what questions to ask

196
00:12:59,800 --> 00:13:02,360
about the answer that is the universe.

197
00:13:03,320 --> 00:13:07,400
Do you believe the collective consciousness of society--

198
00:13:10,440 --> 00:13:13,240
I was watching this movie recently called the Gladiator.

199
00:13:13,320 --> 00:13:15,200
-Russell Crowe. Have you seen it? -Yeah, yeah.

200
00:13:16,440 --> 00:13:18,560
In Gladiator, in Rome,

201
00:13:18,640 --> 00:13:21,480
when people are fighting,

202
00:13:21,560 --> 00:13:24,160
and the crowd is cheering when people kill each other...

203
00:13:27,720 --> 00:13:30,160
the collective is very much like the mob.

204
00:13:31,040 --> 00:13:35,720
It doesn't have nuance in its opinion, per se.

205
00:13:37,160 --> 00:13:38,400
That's this particular kind of mob.

206
00:13:38,480 --> 00:13:41,960
I mean, they're sort of going there to see people kill each other, you know?

207
00:13:42,040 --> 00:13:44,960
Do you suspect the society we live in today is very different?

208
00:13:45,680 --> 00:13:48,840
We don't, generally-- At this point, we don't...

209
00:13:50,080 --> 00:13:52,120
you know, go and watch people kill each other.

210
00:13:52,200 --> 00:13:54,600
[laughter]

211
00:13:54,680 --> 00:13:56,600
Maybe some kind of euphemism of that.

212
00:13:57,400 --> 00:13:58,920
-Sports, I suppose. -Mm.

213
00:14:00,000 --> 00:14:02,400
So, people do sports without--

214
00:14:03,240 --> 00:14:05,560
where teams attempt to defeat each other,

215
00:14:05,640 --> 00:14:08,040
-but minus the death. -Right.

216
00:14:10,040 --> 00:14:15,240
Just going back to the consideration of a human.

217
00:14:15,320 --> 00:14:18,360
We all started out as one cell, but now, we are...

218
00:14:19,800 --> 00:14:21,680
over 30 trillion cells.

219
00:14:26,800 --> 00:14:30,520
But I think most people feel like they're one body.

220
00:14:30,600 --> 00:14:32,280
Like, you know, usually,

221
00:14:32,360 --> 00:14:35,080
your right hand's not fighting your left hand type of thing, you know?

222
00:14:35,160 --> 00:14:37,280
They just sort of cooperate.

223
00:14:38,760 --> 00:14:40,120
Your mind is...

224
00:14:42,840 --> 00:14:44,160
you know...

225
00:14:46,080 --> 00:14:48,600
just a vast number of neurons.

226
00:14:48,680 --> 00:14:52,080
But most of the time, it doesn't feel like there's, you know,

227
00:14:52,160 --> 00:14:54,200
a trillion voices in your brain. Hopefully not.

228
00:14:57,320 --> 00:15:02,360
So, there's clearly more that happens

229
00:15:02,440 --> 00:15:06,360
when you have trillions of cells

230
00:15:06,440 --> 00:15:11,160
working as a cellular collective than, say, one cell.

231
00:15:11,240 --> 00:15:13,440
Or a small,

232
00:15:14,240 --> 00:15:16,400
you know, small multicellular creature.

233
00:15:16,480 --> 00:15:19,640
There's clearly something different that happens.

234
00:15:19,720 --> 00:15:21,800
Like, you can't talk to a bacteria, you know?

235
00:15:21,880 --> 00:15:23,560
-Yeah. -Yeah. It's very silent.

236
00:15:25,200 --> 00:15:27,040
They just sort of wiggle around, and...

237
00:15:27,880 --> 00:15:29,120
From their perspective, I don't know.

238
00:15:29,200 --> 00:15:33,280
I just thought of what is life like from the perspective of an amoeba, you know?

239
00:15:34,040 --> 00:15:37,600
But I know you can't talk to an amoeba. Like, they don't talk back.

240
00:15:37,680 --> 00:15:38,920
But you can talk to humans.

241
00:15:39,920 --> 00:15:41,920
So, there's just something,

242
00:15:42,000 --> 00:15:47,960
obviously, qualitatively fundamentally different for humans.

243
00:15:48,040 --> 00:15:49,880
Once you have a large number of cells,

244
00:15:49,960 --> 00:15:54,320
and, you know, sufficiently large brain type of thing,

245
00:15:54,400 --> 00:15:57,360
there's... you can now talk to humans.

246
00:15:57,440 --> 00:16:00,880
And they can say things, they can produce things.

247
00:16:02,320 --> 00:16:06,280
But bacteria are not gonna produce a spaceship, for example.

248
00:16:07,360 --> 00:16:08,800
But humans can.

249
00:16:08,880 --> 00:16:12,760
So I think there's something qualitatively different

250
00:16:12,840 --> 00:16:15,240
that also happens when there's a collection of humans.

251
00:16:15,320 --> 00:16:17,120
In fact, it's safe to say that a single human

252
00:16:17,200 --> 00:16:18,360
cannot make a spaceship.

253
00:16:18,440 --> 00:16:20,640
I cannot make a spaceship by myself.

254
00:16:20,720 --> 00:16:24,480
But with a collection of humans, we can make spaceships.

255
00:16:25,600 --> 00:16:27,120
So, there's something, obviously,

256
00:16:27,200 --> 00:16:33,200
qualitatively different about a collection of humans.

257
00:16:33,280 --> 00:16:38,440
In fact, it would be impossible for me to learn all of the areas of expertise.

258
00:16:38,520 --> 00:16:40,400
There wouldn't be enough time in one lifetime

259
00:16:40,480 --> 00:16:43,720
to even learn all the things before I was dead.

260
00:16:46,240 --> 00:16:49,680
So, you really fundamentally have to have a collection of humans to make a rocket.

261
00:16:52,400 --> 00:16:55,760
Then, I think there are probably some other scaling...

262
00:16:57,520 --> 00:17:02,160
qualitative scaling things that happen when you have groups of humans.

263
00:17:02,240 --> 00:17:06,280
And then, if the quality of the interaction,

264
00:17:06,359 --> 00:17:08,160
or the quality of the information flow...

265
00:17:10,280 --> 00:17:12,440
the better it is,

266
00:17:12,520 --> 00:17:15,599
the more the human collective will achieve.

267
00:17:17,480 --> 00:17:21,599
And, like I said, I'm just curious about the nature of the universe.

268
00:17:21,680 --> 00:17:24,240
And I think if we-- It's safe to say,

269
00:17:24,319 --> 00:17:29,000
like, if we increase the scope and scale of consciousness,

270
00:17:29,080 --> 00:17:32,160
we're much more likely to understand the nature of the universe

271
00:17:32,240 --> 00:17:34,160
than if we reduce it.

272
00:17:36,080 --> 00:17:37,840
Is that a bit like spirituality?

273
00:17:37,920 --> 00:17:40,800
A lot of people talk to me about spirituality.

274
00:17:40,880 --> 00:17:41,960
Right.

275
00:17:42,040 --> 00:17:43,240
I still don't know what it actually means.

276
00:17:43,320 --> 00:17:45,240
Like, I keep asking them, "What do you mean?"

277
00:17:45,320 --> 00:17:47,800
Yeah. "What do you mean?"

278
00:17:47,880 --> 00:17:51,000
I mean, a lot of people have spiritual feelings.

279
00:17:51,080 --> 00:17:52,560
Right.

280
00:17:54,320 --> 00:17:58,160
And I wouldn't try to deny that those spiritual feelings are real to them.

281
00:17:59,160 --> 00:18:00,440
But it's...

282
00:18:02,560 --> 00:18:03,880
It doesn't entirely translate.

283
00:18:03,960 --> 00:18:05,680
Just because somebody else has a spiritual feeling

284
00:18:05,760 --> 00:18:08,320
doesn't mean that I would have that spiritual feeling.

285
00:18:12,200 --> 00:18:14,800
You know, I tend to be kind of physics-pulled,

286
00:18:14,880 --> 00:18:18,760
which is, like, if something has predictive value, then...

287
00:18:19,880 --> 00:18:23,480
I'll pay more attention to it than if it doesn't have predictive value.

288
00:18:23,560 --> 00:18:25,280
Right.

289
00:18:25,360 --> 00:18:28,360
So, you know, physics, I would say,

290
00:18:28,440 --> 00:18:30,520
is the study of that which has predictive value.

291
00:18:31,480 --> 00:18:33,600
I think it's a pretty good definition.

292
00:18:33,680 --> 00:18:37,480
My primary job, Elon, is a stockbroker and stock investor.

293
00:18:37,560 --> 00:18:39,240
-Okay. -There is no predictive value.

294
00:18:39,320 --> 00:18:41,880
Nobody knows what will happen tomorrow.

295
00:18:41,960 --> 00:18:45,680
Well, but I think you can generally say, you know, that...

296
00:18:48,480 --> 00:18:53,080
If it's long-term for a company, then you can say, like,

297
00:18:54,840 --> 00:18:58,000
"Do you like the products or services of that company?

298
00:18:58,080 --> 00:19:02,000
And is it likely to... Do you like the product roadmap?

299
00:19:02,080 --> 00:19:05,720
Do you like-- It seems like they make great products

300
00:19:05,800 --> 00:19:08,400
and they're likely to make great products in the future."

301
00:19:08,480 --> 00:19:09,600
If that's the case,

302
00:19:09,680 --> 00:19:13,080
then I would say that's probably a good company to invest in.

303
00:19:14,880 --> 00:19:17,080
And I think you also want to believe in the team.

304
00:19:17,160 --> 00:19:20,040
So if you're like, "Well, that's a talented and hardworking team,

305
00:19:20,120 --> 00:19:21,680
they make good products today,

306
00:19:21,760 --> 00:19:24,800
they seem to be still motivated to make things in the future."

307
00:19:24,880 --> 00:19:27,320
Then I'd say that's a good company to invest in.

308
00:19:27,400 --> 00:19:28,640
Fair point.

309
00:19:28,720 --> 00:19:32,120
Yeah, and now, that...

310
00:19:32,200 --> 00:19:34,880
That won't solve for the daily fluctuations

311
00:19:34,960 --> 00:19:38,240
which happen, and, sometimes, are pretty extreme.

312
00:19:38,320 --> 00:19:44,040
But over time, that is the right way to invest in stocks.

313
00:19:44,120 --> 00:19:46,480
Because a company is just a group of people

314
00:19:46,560 --> 00:19:48,720
assembled to create products and services.

315
00:19:48,800 --> 00:19:50,360
So you have to say, "Well, what other--

316
00:19:50,440 --> 00:19:52,560
How good are those products and services?

317
00:19:52,640 --> 00:19:54,760
Are they likely to continue to improve in the future?"

318
00:19:54,840 --> 00:19:57,320
If so, then you should buy the stock of that company

319
00:19:57,400 --> 00:19:59,920
and then don't worry too much about the daily fluctuations.

320
00:20:00,000 --> 00:20:01,120
Right.

321
00:20:02,600 --> 00:20:07,720
What's got you most excited now, Elon, in terms of all that you're building?

322
00:20:07,800 --> 00:20:08,920
You're doing so much.

323
00:20:09,000 --> 00:20:13,040
So let me just preface and contextualise who is watching this.

324
00:20:14,160 --> 00:20:19,280
Our audience is largely wannabe entrepreneurs in India.

325
00:20:19,360 --> 00:20:21,080
Okay.

326
00:20:21,160 --> 00:20:27,480
Really ambitious, really hungry, want to take the risk and build something.

327
00:20:27,560 --> 00:20:30,960
And I feel like all of us have so much to learn from you

328
00:20:31,040 --> 00:20:33,680
because you've done it so many times over in so many different domains.

329
00:20:33,760 --> 00:20:34,920
Yeah.

330
00:20:35,000 --> 00:20:37,000
So, we will speak to them today,

331
00:20:37,080 --> 00:20:40,880
and I will try and centre all my questions in that direction

332
00:20:40,960 --> 00:20:43,240
so they can take advantage of this conversation

333
00:20:43,320 --> 00:20:46,120
and maybe start-- take a chance and build something.

334
00:20:47,600 --> 00:20:48,760
Okay, sure.

335
00:20:52,840 --> 00:20:55,280
Yeah, I guess the most important thing to do is just...

336
00:20:57,840 --> 00:20:59,720
make useful products and services.

337
00:21:02,400 --> 00:21:03,400
Yeah.

338
00:21:04,160 --> 00:21:07,440
Which one of all the products and services that you're building

339
00:21:07,520 --> 00:21:09,680
has got you most excited today?

340
00:21:13,000 --> 00:21:16,880
Well, I think that there's increasingly a convergence, actually,

341
00:21:16,960 --> 00:21:19,680
between SpaceX and Tesla and xAI.

342
00:21:21,440 --> 00:21:25,880
In that, if the future is solar-powered AI satellites,

343
00:21:25,960 --> 00:21:29,640
which it pretty much needs to be in order to...

344
00:21:30,600 --> 00:21:35,280
In order to harness a non-trivial amount of the energy of the sun,

345
00:21:35,360 --> 00:21:39,240
you have to move to solar-powered AI satellites in deep space...

346
00:21:40,960 --> 00:21:45,680
which, somewhat, is a confluence of Tesla expertise and SpaceX expertise.

347
00:21:47,680 --> 00:21:51,400
And xAI on the AI front, so...

348
00:21:52,440 --> 00:21:55,160
it does feel like, over time, there's somewhat of a convergence there.

349
00:21:56,920 --> 00:22:00,080
But all the companies are doing great things.

350
00:22:00,160 --> 00:22:02,040
Very proud of the teams, they do great work.

351
00:22:02,960 --> 00:22:08,400
So, we're making great progress with Tesla on the autonomous driving.

352
00:22:08,480 --> 00:22:10,200
I don't know if you've tried the self-driving?

353
00:22:10,280 --> 00:22:11,600
-Mm-mm. -Have you tried it?

354
00:22:11,680 --> 00:22:13,720
I've tried it in the Waymo, not in the Tesla.

355
00:22:13,800 --> 00:22:15,400
Yeah, it's worth trying.

356
00:22:16,280 --> 00:22:17,920
We actually have it here in Austin.

357
00:22:18,000 --> 00:22:19,440
-So you can, like... -Yeah, I'd love to try it.

358
00:22:19,520 --> 00:22:21,800
You can literally just download the Tesla app,

359
00:22:21,880 --> 00:22:25,080
and I think it's open to anyone.

360
00:22:25,160 --> 00:22:26,800
-Yeah. -Definitely try it out.

361
00:22:26,880 --> 00:22:28,720
You know how it goes.

362
00:22:30,080 --> 00:22:34,080
But we've made a lot of progress with electric vehicles,

363
00:22:34,160 --> 00:22:36,920
with battery packs and solar,

364
00:22:37,000 --> 00:22:41,040
and very much so with self-driving.

365
00:22:41,120 --> 00:22:43,560
So, basically, real-world AI.

366
00:22:44,320 --> 00:22:48,560
Tesla is the world leader in real-world AI, I would say.

367
00:22:50,200 --> 00:22:52,760
And then, we're gonna be making this robot, Optimus,

368
00:22:52,840 --> 00:22:57,960
which is starting production, hopefully, summer next year at scale.

369
00:22:59,520 --> 00:23:01,720
And I think that's gonna be pretty cool. That'll be like--

370
00:23:01,800 --> 00:23:06,080
I think everyone's gonna want their own personal C-3PO, R2-D2,

371
00:23:06,160 --> 00:23:09,200
you know, a helper robot. Like, it would be pretty cool.

372
00:23:10,640 --> 00:23:15,960
And then, SpaceX is doing great work with the Starlink programme,

373
00:23:16,040 --> 00:23:21,400
providing low-cost, reliable Internet throughout the world.

374
00:23:21,480 --> 00:23:23,120
And hopefully, India.

375
00:23:23,200 --> 00:23:26,000
We'd love to be operating in India. That would be great.

376
00:23:26,080 --> 00:23:29,360
We're operating in 150 different countries, now, with Starlink.

377
00:23:29,440 --> 00:23:32,960
Can you give me a bit about Starlink and how the tech works?

378
00:23:33,040 --> 00:23:35,320
'Cause somebody I was speaking to...

379
00:23:35,400 --> 00:23:38,960
I don't know if you know this company called Meter out of San Francisco.

380
00:23:39,040 --> 00:23:41,200
They're trying to replace network engineers.

381
00:23:41,280 --> 00:23:42,960
-But-- -Don't know it, no.

382
00:23:43,040 --> 00:23:47,800
So, he was telling me about how, in densely populated areas,

383
00:23:47,880 --> 00:23:49,680
Starlink works differently

384
00:23:49,760 --> 00:23:53,040
than it might be in a place with not as many people.

385
00:23:53,120 --> 00:23:54,920
Can you explain how it works?

386
00:23:55,000 --> 00:23:57,000
Yeah, so, Starlink...

387
00:23:57,080 --> 00:23:59,680
There's several thousand satellites in low-Earth orbit,

388
00:23:59,760 --> 00:24:05,280
and they're moving around 25 times the speed of sound in these...

389
00:24:05,360 --> 00:24:08,240
You know, they're zipping around the Earth, basically, and...

390
00:24:10,080 --> 00:24:13,880
they're at an altitude of about 550 kilometres,

391
00:24:15,120 --> 00:24:17,640
which is called, generally, low-Earth orbit.

392
00:24:17,720 --> 00:24:22,720
Because they're at low-Earth orbit there, the latency is low.

393
00:24:22,800 --> 00:24:25,760
Like, the distance, because the distance is not that far

394
00:24:25,840 --> 00:24:29,640
compared to a geostationary satellite at 36,000 kilometres.

395
00:24:31,080 --> 00:24:36,600
So, you've got thousands of satellites providing

396
00:24:37,680 --> 00:24:42,400
low latency, high-speed Internet throughout the world,

397
00:24:45,240 --> 00:24:46,840
and they are interconnected as well.

398
00:24:46,920 --> 00:24:49,800
So, there are laser links between the satellites,

399
00:24:49,880 --> 00:24:51,960
so it forms, sort of, a laser mesh.

400
00:24:52,040 --> 00:24:57,200
So that the-- Let's say if cables are damaged or cut,

401
00:24:57,280 --> 00:25:01,520
like fibre cables, the satellites can communicate between each other

402
00:25:01,600 --> 00:25:07,720
and provide connectivity even if the cables are cut.

403
00:25:07,800 --> 00:25:10,560
So, for example, when the Red Sea cables were cut,

404
00:25:11,560 --> 00:25:12,880
I think, a few months ago,

405
00:25:12,960 --> 00:25:18,320
the Starlink satellite network continued to function without a hitch.

406
00:25:18,400 --> 00:25:21,000
So, it's particularly helpful for disaster areas.

407
00:25:21,080 --> 00:25:25,560
So, if an area has been hit with some kind of natural disaster,

408
00:25:25,640 --> 00:25:28,720
floods or fires or earthquakes,

409
00:25:28,800 --> 00:25:32,120
that tends to damage the ground infrastructure.

410
00:25:32,200 --> 00:25:35,360
But the Starlink satellites still work, so...

411
00:25:35,440 --> 00:25:38,520
And generally, whenever there's of natural disaster somewhere,

412
00:25:38,600 --> 00:25:42,960
we always provide people with free Starlink Internet connectivity.

413
00:25:43,040 --> 00:25:44,320
You know, we don't want to charge--

414
00:25:44,400 --> 00:25:47,320
We don't want to take advantage of a tragic situation.

415
00:25:47,400 --> 00:25:52,520
So, it's always, you know, if there's natural disasters,

416
00:25:52,600 --> 00:25:55,720
we're like, "Okay, it's free during the natural disaster."

417
00:25:55,800 --> 00:25:59,000
You know, we don't want to, say, like, you know...

418
00:25:59,080 --> 00:26:02,200
put a paywall up while somebody's trying to get help.

419
00:26:02,280 --> 00:26:03,560
That would be wrong.

420
00:26:04,760 --> 00:26:08,440
So, it's a very robust system.

421
00:26:08,520 --> 00:26:10,760
It's complimentary to ground systems

422
00:26:10,840 --> 00:26:17,680
because the satellite beams work best in sparsely populated areas.

423
00:26:19,040 --> 00:26:24,040
But because you've got a satellite beam, it's a pretty big beam,

424
00:26:24,120 --> 00:26:27,360
and you have a fixed number of users per beam, so...

425
00:26:27,440 --> 00:26:31,960
it tends to be very complimentary to the ground-based cellular systems,

426
00:26:32,040 --> 00:26:34,640
because those are very good in cities,

427
00:26:34,720 --> 00:26:36,960
because you've got these cell towers that are, you know,

428
00:26:37,040 --> 00:26:40,600
only a kilometre apart type of thing, but...

429
00:26:42,240 --> 00:26:45,480
But cell towers tend to be inefficient in the countryside.

430
00:26:45,560 --> 00:26:50,360
So, in rural areas is where you tend to have the worst Internet

431
00:26:50,440 --> 00:26:54,200
because it's very expensive and difficult to lay...

432
00:26:54,280 --> 00:27:01,080
to do all the fibre-optic cables or to have high bandwidth cellular towers.

433
00:27:01,720 --> 00:27:08,320
So, Starlink is very complimentary to the existing telecom companies.

434
00:27:09,920 --> 00:27:14,440
It basically tends to serve the least served, which, I think, is good.

435
00:27:15,600 --> 00:27:18,120
-That's... -Will that change tomorrow?

436
00:27:18,200 --> 00:27:22,200
Like, today, as you explained, the beam is quite broad,

437
00:27:22,280 --> 00:27:25,920
and it can't work in a densely populated area with high buildings, maybe.

438
00:27:27,120 --> 00:27:28,800
But can that change, and tomorrow,

439
00:27:28,880 --> 00:27:33,200
it becomes really efficient in a densely populated city

440
00:27:33,280 --> 00:27:35,960
where it is competitive with the local network providers?

441
00:27:36,960 --> 00:27:39,320
Unfortunately, the physics don't allow for that.

442
00:27:39,400 --> 00:27:41,840
So, we're too far away.

443
00:27:43,360 --> 00:27:47,440
So, at 550 kilometres, even if we try to reduce it, which...

444
00:27:47,520 --> 00:27:50,320
About as low as we can go is about 350 kilometres,

445
00:27:50,400 --> 00:27:52,240
still very far away.

446
00:27:52,320 --> 00:27:53,760
You've just...

447
00:27:53,840 --> 00:27:56,480
You can think of, like, a flashlight,

448
00:27:56,560 --> 00:27:59,680
which is, you know, this flashlight's got a cone

449
00:27:59,760 --> 00:28:03,800
and that cone is coming at, you know...

450
00:28:03,880 --> 00:28:05,600
today, it's 550 kilometres.

451
00:28:05,680 --> 00:28:07,800
In the future, we're trying to get down to 350 kilometres,

452
00:28:07,880 --> 00:28:10,880
but we can't beat something that's one kilometre away,

453
00:28:10,960 --> 00:28:12,400
which is the cell tower.

454
00:28:12,480 --> 00:28:14,280
Physics is not on our side here.

455
00:28:14,360 --> 00:28:18,880
So, it's not physically possible for Starlink

456
00:28:18,960 --> 00:28:21,720
to serve densely populated cities.

457
00:28:21,800 --> 00:28:24,800
Like, you can serve a little bit, maybe 1% of the population.

458
00:28:24,880 --> 00:28:28,200
And, sometimes, people get-- Even in crowded cities,

459
00:28:28,280 --> 00:28:32,040
there might be, you know, no fibre link up their road.

460
00:28:32,120 --> 00:28:34,680
Like, sometimes, there's somebody on a cul-de-sac or something

461
00:28:34,760 --> 00:28:37,160
or in a place...

462
00:28:37,240 --> 00:28:41,640
In cities, there's sometimes underserved areas for random reasons.

463
00:28:41,720 --> 00:28:44,240
And so, Starlink can serve, like I said,

464
00:28:44,320 --> 00:28:49,120
maybe 1% or 2% of a densely populated city.

465
00:28:50,400 --> 00:28:53,160
But it can be much more effective in, like I said,

466
00:28:53,240 --> 00:28:56,760
in rural areas where the Internet connection is much worse.

467
00:28:56,840 --> 00:29:00,720
And often, people either have, sometimes, no access to Internet

468
00:29:00,800 --> 00:29:04,440
or it's extremely expensive or the quality is not very good.

469
00:29:06,480 --> 00:29:08,080
If I were to ask you to wager a guess, Elon,

470
00:29:08,840 --> 00:29:13,760
do you think India will go down the path of urbanisation like China did,

471
00:29:13,840 --> 00:29:17,840
with more people moving in from rural economies to urban centres?

472
00:29:20,160 --> 00:29:21,800
Or do you think we'll beat the trend?

473
00:29:21,880 --> 00:29:24,400
Well, I suppose some amount of that has happened, right?

474
00:29:25,320 --> 00:29:29,080
I mean, I'm curious to, sort of, ask you some questions as well.

475
00:29:29,760 --> 00:29:34,000
'Cause, of course, isn't that the trend, or is it not the trend in India?

476
00:29:34,800 --> 00:29:36,360
It is the trend, largely.

477
00:29:36,440 --> 00:29:38,920
I think a little bit changed during COVID

478
00:29:39,000 --> 00:29:43,160
when a lot of urbanisation slowed down and that was not organic.

479
00:29:43,240 --> 00:29:45,960
It was very artificially manifested.

480
00:29:46,040 --> 00:29:47,440
Right.

481
00:29:47,520 --> 00:29:52,240
But one does question that with AI,

482
00:29:52,320 --> 00:29:56,000
if productivity were to go up...

483
00:29:56,080 --> 00:30:00,400
And I heard you speak about UHI instead of UBI.

484
00:30:00,480 --> 00:30:03,800
Yeah. I think it will be Universal High Income.

485
00:30:03,880 --> 00:30:07,600
In a world like that, I wonder if more people want to live in cities

486
00:30:07,680 --> 00:30:12,080
which are always going to be more polluted

487
00:30:13,000 --> 00:30:17,200
and not offer the quality of lifestyle that a rural environment might.

488
00:30:18,080 --> 00:30:19,640
Well, I guess it's up to...

489
00:30:19,720 --> 00:30:22,920
Some people want to be around a lot of people and some people don't.

490
00:30:23,000 --> 00:30:25,160
It's gonna be, maybe, a matter of personal choice.

491
00:30:25,240 --> 00:30:27,520
But I think in the future, it won't be...

492
00:30:27,600 --> 00:30:31,200
I think it won't be the case that you have to be in a city for a job.

493
00:30:31,280 --> 00:30:33,720
-Right. -'Cause I think...

494
00:30:33,800 --> 00:30:36,400
My prediction is, in the future, working will be optional.

495
00:30:36,480 --> 00:30:38,000
Right.

496
00:30:38,080 --> 00:30:39,560
We seem to be moving from--

497
00:30:39,640 --> 00:30:42,040
Not in India, but in some parts of the West,

498
00:30:42,120 --> 00:30:45,480
from six days to five days to four days to three.

499
00:30:45,560 --> 00:30:46,840
Not me.

500
00:30:46,920 --> 00:30:49,240
[laughter]

501
00:30:49,320 --> 00:30:50,640
I think, the Europeans.

502
00:30:50,720 --> 00:30:52,040
Yeah, yeah.

503
00:30:55,040 --> 00:30:56,080
Yeah, yeah, 'cause...

504
00:30:57,200 --> 00:31:01,320
I mean, I think if you're trying to make a startup succeed

505
00:31:01,400 --> 00:31:05,760
or you're trying to make a company do very difficult things,

506
00:31:05,840 --> 00:31:08,480
then you definitely need to put in serious hours.

507
00:31:08,560 --> 00:31:10,560
-I think that's how it goes. -Right.

508
00:31:11,480 --> 00:31:14,800
And if we were to move from five to four to three days,

509
00:31:14,880 --> 00:31:16,440
how do you think society changes?

510
00:31:16,520 --> 00:31:20,760
When people have to work half the week, what do they do with the other half?

511
00:31:21,520 --> 00:31:24,160
Well, I think it'll actually be that people don't have to work at all.

512
00:31:26,560 --> 00:31:28,480
It may not be that far in the future.

513
00:31:28,560 --> 00:31:33,560
Maybe only, I don't know, ten, I'd say less than 20 years.

514
00:31:33,640 --> 00:31:37,960
My prediction is, in less than 20 years, working will be optional.

515
00:31:38,040 --> 00:31:39,960
Working at all will be optional.

516
00:31:42,560 --> 00:31:44,000
Like a hobby.

517
00:31:44,080 --> 00:31:45,640
Pretty much.

518
00:31:46,760 --> 00:31:50,440
And that would be because of increased productivity,

519
00:31:50,520 --> 00:31:52,640
meaning people do not have to work?

520
00:31:52,720 --> 00:31:54,160
They don't have to--

521
00:31:54,240 --> 00:31:56,080
I mean, look, obviously,

522
00:31:56,160 --> 00:31:58,080
people can play this back in 20 years and say,

523
00:31:58,160 --> 00:32:01,200
"Look, Elon made this ridiculous prediction and it's not true."

524
00:32:01,280 --> 00:32:06,320
But I think it will turn out to be true that, in less than 20 years,

525
00:32:06,400 --> 00:32:11,120
but maybe even as little as, I don't know, ten or 15 years,

526
00:32:12,800 --> 00:32:18,320
the advancements in AI and robotics will bring us to the point

527
00:32:18,400 --> 00:32:21,760
where working is optional.

528
00:32:22,720 --> 00:32:24,840
In the same way that, like, say,

529
00:32:24,920 --> 00:32:27,480
you can grow your own vegetables in your garden

530
00:32:27,560 --> 00:32:29,960
or you could go to the store and buy vegetables.

531
00:32:32,840 --> 00:32:34,480
You know.

532
00:32:34,560 --> 00:32:36,720
It's much harder to grow your own vegetables.

533
00:32:36,800 --> 00:32:40,440
But some people like to grow their vegetables, which is fine.

534
00:32:40,520 --> 00:32:43,680
But it'll be optional, in that way, is my prediction.

535
00:32:44,640 --> 00:32:49,720
If one were to argue that humans are innately competitive

536
00:32:49,800 --> 00:32:51,520
and everything is relative...

537
00:32:51,600 --> 00:32:53,600
From the time of hunters,

538
00:32:53,680 --> 00:32:56,920
somebody wanted to be the alpha hunter or the biggest farmer,

539
00:32:57,000 --> 00:33:02,520
if everybody gets a universal high income and everybody has enough...

540
00:33:03,480 --> 00:33:06,000
-What do you compete for? -Uh...

541
00:33:06,080 --> 00:33:07,880
it would be relative, right?

542
00:33:07,960 --> 00:33:10,520
Like, if we all had enough, enough is not enough.

543
00:33:13,520 --> 00:33:17,560
Yeah, I guess-- I'm not exactly sure.

544
00:33:17,640 --> 00:33:22,320
'Cause we're really headed into the singularity, as it's called,

545
00:33:22,400 --> 00:33:25,080
which, you know, they refer to AI sometimes

546
00:33:25,160 --> 00:33:27,280
as kind of like the black hole, like a singularity.

547
00:33:27,360 --> 00:33:29,120
You don't know what happens after the event horizon.

548
00:33:29,200 --> 00:33:31,520
It doesn't mean that something bad happens,

549
00:33:31,600 --> 00:33:33,200
it just means you don't know what happens.

550
00:33:36,600 --> 00:33:40,680
I'm confident that if AI and robotics continue to advance,

551
00:33:40,760 --> 00:33:43,560
which, they are advancing very rapidly,

552
00:33:43,640 --> 00:33:46,320
like I said, working will be optional,

553
00:33:47,360 --> 00:33:50,400
and people will have any goods and services that they want.

554
00:33:53,080 --> 00:33:55,720
"If you can think of it, you can have it" type of thing.

555
00:33:58,360 --> 00:33:59,840
But then, at a certain point,

556
00:34:01,320 --> 00:34:05,720
AI will actually saturate on anything humans can think of.

557
00:34:06,600 --> 00:34:08,360
And then, at that point,

558
00:34:08,440 --> 00:34:13,000
it becomes a situation where AI is doing things for...

559
00:34:13,080 --> 00:34:15,320
AI and robotics are doing things for AI and robotics,

560
00:34:15,400 --> 00:34:19,000
because they've run out of things to do to make the humans happy.

561
00:34:21,040 --> 00:34:23,600
'Cause there's a limit, you know? You say, like...

562
00:34:24,239 --> 00:34:26,639
People can only eat so much food, or...

563
00:34:29,199 --> 00:34:30,800
But it's gonna be, I think...

564
00:34:30,880 --> 00:34:33,840
"If you can think of it, you can have it," will be the future.

565
00:34:33,920 --> 00:34:36,520
You know, the Austrian School of Economics,

566
00:34:36,600 --> 00:34:40,880
if you go back in time, they were the digression from Adam Smith.

567
00:34:40,960 --> 00:34:43,679
They talk about the marginal utility of everything.

568
00:34:44,800 --> 00:34:47,159
Having one of something has value,

569
00:34:47,239 --> 00:34:49,520
having two of the same thing has lesser value

570
00:34:49,600 --> 00:34:51,800
and having ten of the same thing has no value.

571
00:34:51,880 --> 00:34:53,360
Yes.

572
00:34:53,440 --> 00:34:55,080
So, if we could have everything we wanted, maybe--

573
00:34:55,159 --> 00:34:56,880
Like ten marshmallows, I mean, who wants that?

574
00:34:56,960 --> 00:34:58,400
-Yeah. -[laughter]

575
00:34:59,760 --> 00:35:01,200
One's plenty.

576
00:35:03,080 --> 00:35:04,520
This is like the marshmallow test. You're like,

577
00:35:04,600 --> 00:35:07,480
"You're gonna have two marshmallows later or one marshmallow now?"

578
00:35:07,560 --> 00:35:09,720
And I'm like, "I'll have one marshmallow, I don't want two marshmallows."

579
00:35:09,800 --> 00:35:12,120
-That's interesting. -[laughter]

580
00:35:12,200 --> 00:35:13,280
What would you pick?

581
00:35:13,360 --> 00:35:16,360
But I don't-- One marshmallow is enough.

582
00:35:16,440 --> 00:35:18,360
I always question marshmallows as being like,

583
00:35:18,440 --> 00:35:21,240
not the most, you know, the best candy, you know?

584
00:35:21,320 --> 00:35:23,720
-Yeah. -[laughter]

585
00:35:23,800 --> 00:35:25,720
I don't yearn for marshmallows.

586
00:35:25,800 --> 00:35:28,120
-I think you're the best... -[laughter]

587
00:35:28,200 --> 00:35:29,200
Who does?

588
00:35:30,840 --> 00:35:32,920
You're the best testament to the marshmallow experiment.

589
00:35:33,000 --> 00:35:34,840
-I think... -I suppose so.

590
00:35:34,920 --> 00:35:37,200
Oh, well, I mean, I like delayed gratification, essentially.

591
00:35:37,280 --> 00:35:38,440
-Yeah. -Yeah.

592
00:35:38,520 --> 00:35:39,840
You're able to delay it more than most.

593
00:35:39,920 --> 00:35:41,880
You know, I have a tattoo which says, "Delay gratification."

594
00:35:41,960 --> 00:35:43,520
Yeah, wow, okay. What's this?

595
00:35:43,600 --> 00:35:45,240
Okay, you're really taking the marshmallow test hard.

596
00:35:45,320 --> 00:35:46,640
[laughter]

597
00:35:48,000 --> 00:35:49,120
I feel like I can't remember.

598
00:35:49,200 --> 00:35:50,680
When I'm trading or when I'm buying...

599
00:35:50,760 --> 00:35:52,640
Delay gratification, yeah, yeah.

600
00:35:52,720 --> 00:35:53,880
-It helps. -Wow, okay.

601
00:35:53,960 --> 00:35:55,320
That's... That's commitment.

602
00:35:55,400 --> 00:35:57,600
And it's pointing at me, so it reminds me of...

603
00:36:01,200 --> 00:36:02,920
Okay, well, it's good advice.

604
00:36:03,000 --> 00:36:04,520
I mean, you can't miss it.

605
00:36:04,600 --> 00:36:06,640
-If you could get a... -[laughter]

606
00:36:06,720 --> 00:36:08,920
If you could get a tattoo, what would you get?

607
00:36:09,880 --> 00:36:11,680
I guess maybe my kids' names or something.

608
00:36:11,760 --> 00:36:12,760
Right.

609
00:36:14,320 --> 00:36:16,920
Why do you like the letter "X" as much as you do?

610
00:36:18,080 --> 00:36:19,240
Well...

611
00:36:19,320 --> 00:36:20,600
[laughs]

612
00:36:24,160 --> 00:36:27,040
I mean, yeah, it's a good question, honestly.

613
00:36:27,120 --> 00:36:30,440
Sometimes, I wonder what's wrong with me.

614
00:36:37,280 --> 00:36:38,360
So, um...

615
00:36:39,720 --> 00:36:42,560
I mean, it started off with, where, I think...

616
00:36:42,640 --> 00:36:46,080
So, way back, ancient times, in '99.

617
00:36:46,160 --> 00:36:48,800
[laughter]

618
00:36:48,880 --> 00:36:51,920
The Precambrian era when there were only sponges...

619
00:36:55,960 --> 00:36:58,800
there were only three one-letter domain names.

620
00:36:59,880 --> 00:37:02,200
And I think it's X, Q, and Z.

621
00:37:02,280 --> 00:37:03,840
And, uh...

622
00:37:03,920 --> 00:37:06,640
And I was like, "Okay, I want to create this place

623
00:37:06,720 --> 00:37:10,400
where it's the financial crossroads

624
00:37:10,480 --> 00:37:13,320
or like the financial exchange, you know?"

625
00:37:16,160 --> 00:37:20,720
Essentially, it's solving money from an information theory standpoint

626
00:37:20,800 --> 00:37:22,160
where the current banking system

627
00:37:22,240 --> 00:37:27,640
is a large number of heterogeneous databases

628
00:37:27,720 --> 00:37:32,560
with batch processing that are not secure.

629
00:37:32,640 --> 00:37:37,520
And if we could have a single database

630
00:37:37,600 --> 00:37:41,280
that was real-time and secure,

631
00:37:41,360 --> 00:37:44,280
that would be more efficient from a monetary--

632
00:37:44,360 --> 00:37:46,760
from an information theory standpoint

633
00:37:46,840 --> 00:37:50,520
than a large number of heterogeneous databases

634
00:37:50,600 --> 00:37:52,960
that batch process very slowly and securely.

635
00:37:55,160 --> 00:37:56,920
So, um...

636
00:37:57,000 --> 00:38:01,520
So, that was sort of X.com way back in the day,

637
00:38:01,600 --> 00:38:05,080
which kind of became PayPal.

638
00:38:07,160 --> 00:38:08,320
And then...

639
00:38:10,560 --> 00:38:12,200
And it was acquired by eBay. And then, eBay--

640
00:38:12,280 --> 00:38:13,600
Someone reached out from eBay and said,

641
00:38:13,680 --> 00:38:15,640
"Hey, do you want to buy the domain name back?

642
00:38:15,720 --> 00:38:16,960
And I was like, "Sure."

643
00:38:17,040 --> 00:38:19,080
And so I had the domain name for quite a while.

644
00:38:22,640 --> 00:38:24,040
And then...

645
00:38:25,520 --> 00:38:27,960
And then, yes...

646
00:38:28,040 --> 00:38:31,320
Then I was like, "Well, maybe this--

647
00:38:31,400 --> 00:38:33,560
Acquiring Twitter would also be an opportunity

648
00:38:33,640 --> 00:38:39,400
to revisit the original plan of X.com,

649
00:38:39,480 --> 00:38:41,360
which is to create this...

650
00:38:42,880 --> 00:38:46,720
this clearinghouse of financial transactions."

651
00:38:46,800 --> 00:38:51,840
Basically, to create a more efficient money database,

652
00:38:51,920 --> 00:38:53,160
is a way to think about it.

653
00:38:54,880 --> 00:39:01,200
Like, money is really an information system for labour allocation.

654
00:39:01,280 --> 00:39:03,560
People sometimes think money is power in and of itself,

655
00:39:03,640 --> 00:39:05,640
but it doesn't, really--

656
00:39:05,720 --> 00:39:07,560
If there's no labour to allocate, it's meaningless.

657
00:39:08,440 --> 00:39:13,720
So if you were to be on a desert island with a trillion dollars or whatever...

658
00:39:13,800 --> 00:39:15,280
-Now you have that. -...it doesn't matter.

659
00:39:15,360 --> 00:39:18,400
Oh, yeah, right. Why speculate when you can be real?

660
00:39:22,880 --> 00:39:25,000
I just hope I don't end up on a desert island.

661
00:39:25,760 --> 00:39:27,560
It's not gonna be very useful to me.

662
00:39:29,200 --> 00:39:34,320
But it illustrates my point that if you're stranded on a desert island

663
00:39:34,400 --> 00:39:35,560
with a trillion dollars,

664
00:39:35,640 --> 00:39:39,960
it's not useful, because there's no labour to allocate.

665
00:39:40,040 --> 00:39:41,480
You just allocate yourself, so...

666
00:39:45,960 --> 00:39:49,080
So, anyway, so, it's a long-winded way of saying

667
00:39:49,160 --> 00:39:50,320
that it's...

668
00:39:51,400 --> 00:39:53,120
It's just really, like...

669
00:39:54,280 --> 00:39:59,880
I'm just kind of slowly revisiting this idea that I had 25 years ago

670
00:39:59,960 --> 00:40:03,840
to create a more efficient, um...

671
00:40:06,080 --> 00:40:07,720
money database.

672
00:40:09,960 --> 00:40:11,960
And if that's successful, people will use it,

673
00:40:12,040 --> 00:40:14,080
and if it's not successful, they won't use it.

674
00:40:15,600 --> 00:40:18,880
And then, I also like the idea of having a unified...

675
00:40:20,200 --> 00:40:26,080
app or website or whatever, where you can do anything you want there.

676
00:40:28,800 --> 00:40:31,080
You know, China has this with WeChat,

677
00:40:31,160 --> 00:40:35,880
sort of, somewhat, where you can exchange information,

678
00:40:35,960 --> 00:40:39,480
you can publish information, you can exchange money.

679
00:40:42,360 --> 00:40:45,840
People kind of live their life on WeChat in China.

680
00:40:45,920 --> 00:40:47,600
And it's quite useful,

681
00:40:47,680 --> 00:40:51,440
but there's no real WeChat outside of China.

682
00:40:51,520 --> 00:40:53,280
So, it's, like...

683
00:40:53,360 --> 00:40:57,480
It's kind of WeChat++, I'd say, is the idea for X.

684
00:40:57,560 --> 00:41:01,800
Anyway, so, then, Space Exploration Technologies

685
00:41:01,880 --> 00:41:03,640
is the full name of the company.

686
00:41:03,720 --> 00:41:06,240
But I was like, "That's too much, that's a mouthful."

687
00:41:06,320 --> 00:41:07,960
So I was like, "We'll just call it SpaceX,"

688
00:41:08,040 --> 00:41:09,880
like FedEx for space.

689
00:41:11,240 --> 00:41:13,800
It just happens to have the X in the, you know...

690
00:41:13,880 --> 00:41:17,280
'Cause exploration has an X, but, you know...

691
00:41:17,360 --> 00:41:21,240
And I was like, "Well, I like capitalising the X just artistically, so..."

692
00:41:23,080 --> 00:41:26,760
So, then, uh, that's why it's SpaceX, but...

693
00:41:26,840 --> 00:41:29,480
And then, what else we got? I got a kid.

694
00:41:31,480 --> 00:41:33,120
He's called X, too,

695
00:41:33,200 --> 00:41:36,920
but his mother's the one that named him "X".

696
00:41:37,000 --> 00:41:38,400
-Oh? -[laughter]

697
00:41:38,480 --> 00:41:41,520
And I said, "You know, people are really gonna think I've got a thing about X

698
00:41:41,600 --> 00:41:43,680
if we name our kid "X" too, you know?"

699
00:41:43,760 --> 00:41:48,120
And I said to her, like, "Look, I do have X.com, you know."

700
00:41:48,200 --> 00:41:49,840
[laughter]

701
00:41:50,440 --> 00:41:52,760
"So, people are gonna really think

702
00:41:52,840 --> 00:41:55,080
I've got somewhat of a fetish for this letter."

703
00:41:55,160 --> 00:41:58,400
But she said no, she likes X and she wants to call him X.

704
00:41:58,480 --> 00:41:59,520
I'm like, "Okay."

705
00:41:59,600 --> 00:42:02,160
Is this a new thing, or have you had it growing up?

706
00:42:02,240 --> 00:42:05,960
No, I'm saying it's somewhat of a coincidence.

707
00:42:06,040 --> 00:42:07,560
Okay.

708
00:42:08,520 --> 00:42:09,720
Like, not everything's called X.

709
00:42:09,800 --> 00:42:11,840
I mean, Tesla isn't, there's no Xs in Tesla.

710
00:42:11,920 --> 00:42:12,920
Yeah.

711
00:42:15,320 --> 00:42:17,560
What do you think money will be in the future, Elon?

712
00:42:19,640 --> 00:42:22,520
I think, long term...

713
00:42:24,000 --> 00:42:27,360
I think money disappears as a concept, honestly.

714
00:42:27,440 --> 00:42:29,480
It's kind of strange, but...

715
00:42:31,560 --> 00:42:34,000
But in a future where anyone can have anything,

716
00:42:35,680 --> 00:42:40,960
I think you no longer need money as a database for labour allocation.

717
00:42:43,440 --> 00:42:48,200
If AI and robotics are big enough to satisfy all human needs,

718
00:42:48,280 --> 00:42:52,000
then money is no longer...

719
00:42:52,080 --> 00:42:54,440
Its relevance declines dramatically.

720
00:42:54,520 --> 00:42:56,000
I'm not sure we will have it.

721
00:42:58,360 --> 00:43:02,680
You know, the best imagining of this future that I've read

722
00:43:02,760 --> 00:43:07,640
is from Iain Banks, the Culture books.

723
00:43:07,720 --> 00:43:10,280
So I recommend people read the Culture books.

724
00:43:10,360 --> 00:43:15,080
In this sort of far future of the Culture books, there's...

725
00:43:15,160 --> 00:43:16,800
they don't have money, either.

726
00:43:18,240 --> 00:43:20,160
And everyone can pretty much have whatever they want.

727
00:43:22,800 --> 00:43:28,160
There's still some fundamental currencies, if you will,

728
00:43:28,240 --> 00:43:30,200
that are physics-based.

729
00:43:30,280 --> 00:43:32,720
So, energy is...

730
00:43:32,800 --> 00:43:34,840
Energy is the true currency.

731
00:43:34,920 --> 00:43:37,320
This is why I said Bitcoin is based on energy.

732
00:43:37,400 --> 00:43:41,800
You can't legislate energy. You can't just, you know...

733
00:43:41,880 --> 00:43:43,840
pass a law and suddenly have a lot of energy.

734
00:43:45,640 --> 00:43:50,200
It's very difficult to generate energy,

735
00:43:50,280 --> 00:43:53,600
or especially to harness energy in a useful way to do useful work.

736
00:43:54,280 --> 00:43:56,880
So, I think that probably...

737
00:43:59,120 --> 00:44:03,760
Probably we won't have money and probably we'll just have energy,

738
00:44:04,960 --> 00:44:09,840
you know, power generation as the de facto currency.

739
00:44:11,120 --> 00:44:15,840
So, I mean, I think one way to frame civilisational progress

740
00:44:15,920 --> 00:44:20,640
is the percentage completion on the Kardashev scale.

741
00:44:20,720 --> 00:44:22,400
So, you know,

742
00:44:22,480 --> 00:44:26,600
Kardashev I is what percentage of a planet's energy

743
00:44:26,680 --> 00:44:30,040
are you successfully turning into useful work?

744
00:44:30,120 --> 00:44:31,840
And I'm maybe paraphrasing here a little bit,

745
00:44:31,920 --> 00:44:36,480
but the Kardashev II would be what percentage of the sun's energy

746
00:44:36,560 --> 00:44:38,320
are you converting into useful work?

747
00:44:40,360 --> 00:44:43,280
Kardashev III would be what percentage of the galaxy

748
00:44:43,360 --> 00:44:44,720
are you converting into useful work?

749
00:44:49,000 --> 00:44:52,200
So, things really, I think, become energy-based.

750
00:44:53,760 --> 00:44:56,200
But if you have solar-powered AI satellites,

751
00:44:56,280 --> 00:44:58,480
energy is also free and abundant,

752
00:44:58,560 --> 00:45:02,960
'cause we'll never be able to utilise all the solar energy available to us.

753
00:45:03,720 --> 00:45:07,640
So, it can't be a store of wealth, essentially, in that lens, can it?

754
00:45:09,200 --> 00:45:10,800
You know, there's not really...

755
00:45:10,880 --> 00:45:13,440
You can't really store wealth in, like...

756
00:45:13,520 --> 00:45:14,880
You can only...

757
00:45:19,000 --> 00:45:21,560
You can accumulate numbers in the--

758
00:45:21,640 --> 00:45:28,640
Currently, you can accumulate numbers in a database that allow you to...

759
00:45:32,800 --> 00:45:37,040
To some degree, to incent the behaviour of other humans in particular directions.

760
00:45:37,120 --> 00:45:39,280
And I guess people call that wealth.

761
00:45:40,320 --> 00:45:43,080
But again, if there's no humans around, there's no--

762
00:45:43,160 --> 00:45:44,920
Wealth accumulation is meaningless.

763
00:45:45,000 --> 00:45:47,920
This is a digression, but if you were to consider...

764
00:45:48,000 --> 00:45:51,280
food as the energy for a human to thrive...

765
00:45:51,360 --> 00:45:53,080
Yeah, food is energy.

766
00:45:53,160 --> 00:45:55,440
It's literally got calories, just means energy.

767
00:45:55,520 --> 00:45:59,480
...so, can a farm, which is self-sustaining, be a commodity that is...

768
00:46:04,720 --> 00:46:07,800
I'm not sure what that means, but, you know, there's...

769
00:46:11,840 --> 00:46:15,880
At a certain point, you do complete the cycle where...

770
00:46:16,880 --> 00:46:18,360
I think, at a certain point...

771
00:46:19,160 --> 00:46:22,920
you decouple from the sort of conventional economy

772
00:46:23,000 --> 00:46:25,360
if you have, um...

773
00:46:26,880 --> 00:46:31,920
AI and robots producing chips and solar panels...

774
00:46:35,640 --> 00:46:39,760
and mining resources in order to make chips and robots,

775
00:46:39,840 --> 00:46:41,400
in order to make...

776
00:46:41,480 --> 00:46:44,200
You sort of complete that cycle.

777
00:46:44,280 --> 00:46:45,520
Once that cycle is...

778
00:46:45,600 --> 00:46:48,600
Once that cycle is complete,

779
00:46:49,680 --> 00:46:53,160
I think that's the point at which you decouple from the monetary system.

780
00:46:53,240 --> 00:46:57,680
Is that the way forward for the US by virtue of...

781
00:46:59,080 --> 00:47:01,760
how much debt they have today?

782
00:47:01,840 --> 00:47:03,480
Do they deflate away their currency

783
00:47:03,560 --> 00:47:07,880
and transition into this new form and lead that push,

784
00:47:07,960 --> 00:47:10,080
because it would make more sense to them?

785
00:47:10,160 --> 00:47:12,200
Well, in this future that I'm talking about,

786
00:47:12,280 --> 00:47:16,560
the notion of countries becomes, sort of, anachronistic.

787
00:47:18,320 --> 00:47:19,760
Do you believe in it today?

788
00:47:19,840 --> 00:47:22,040
-Do you believe in countries-- -Yeah, I certainly believe in it today.

789
00:47:22,120 --> 00:47:25,960
And I want to just separate something that I...

790
00:47:26,040 --> 00:47:29,560
Like, these are just what I think will happen based on what I see,

791
00:47:29,640 --> 00:47:32,720
as opposed to, "I think these are fundamentally good things

792
00:47:32,800 --> 00:47:34,880
and I'm trying to make them happen."

793
00:47:34,960 --> 00:47:37,400
I think this would happen with or without me,

794
00:47:38,600 --> 00:47:41,120
-whether I like it or not. -Right.

795
00:47:41,200 --> 00:47:43,920
As long as civilisation keeps advancing,

796
00:47:44,000 --> 00:47:48,040
we will have AI and robotics at very large scale.

797
00:47:53,320 --> 00:47:55,800
I think that that's pretty much the only thing

798
00:47:55,880 --> 00:47:59,600
that's gonna solve for the US debt crisis.

799
00:47:59,680 --> 00:48:03,440
'Cause, currently, the US debt is insanely high,

800
00:48:03,520 --> 00:48:08,320
and the interest payments on the debt exceed the entire military budget

801
00:48:08,400 --> 00:48:10,920
of the United States, just the interest payments.

802
00:48:11,000 --> 00:48:12,640
And that's...

803
00:48:12,720 --> 00:48:15,320
at least, in the short term, gonna continue to increase.

804
00:48:15,960 --> 00:48:18,440
So, I think, actually,

805
00:48:18,520 --> 00:48:21,160
the only thing that can solve for the debt situation

806
00:48:21,240 --> 00:48:23,720
is AI and robotics.

807
00:48:23,800 --> 00:48:25,440
But it will more than...

808
00:48:26,640 --> 00:48:27,840
It might cause...

809
00:48:28,560 --> 00:48:32,600
See, I guess it probably would cause significant deflation, because...

810
00:48:34,880 --> 00:48:39,280
deflation or inflation is really the ratio of goods and services produced

811
00:48:39,360 --> 00:48:41,720
to the change in the money supply.

812
00:48:41,800 --> 00:48:46,240
So, like, so if goods and services output increases faster than the money supply,

813
00:48:46,320 --> 00:48:47,840
you will have deflation.

814
00:48:47,920 --> 00:48:49,400
If goods and services decreases--

815
00:48:49,480 --> 00:48:54,800
If real goods and services output increases slower than the money supply,

816
00:48:54,880 --> 00:48:56,200
you have inflation.

817
00:48:56,280 --> 00:48:57,760
It's that simple.

818
00:48:57,840 --> 00:49:01,720
People sometimes try to make it more complicated than that, but it just isn't.

819
00:49:02,920 --> 00:49:05,240
So, if you have AI and robotics

820
00:49:05,320 --> 00:49:08,120
and a dramatic increase in the output of goods and services,

821
00:49:08,200 --> 00:49:10,000
probably, you will have deflation.

822
00:49:10,080 --> 00:49:11,840
That seems likely.

823
00:49:13,080 --> 00:49:16,280
Because you simply won't be able to increase the money supply

824
00:49:16,360 --> 00:49:18,960
as fast as you can increase the output of goods and services.

825
00:49:19,880 --> 00:49:22,280
-With all-- -This fly is a real hazard here.

826
00:49:23,240 --> 00:49:25,360
Should we do something about it?

827
00:49:26,520 --> 00:49:28,480
Maybe we can convince it to go somewhere else.

828
00:49:29,480 --> 00:49:31,200
Entice it elsewhere.

829
00:49:31,280 --> 00:49:32,760
-It actually left, I think. -Great.

830
00:49:32,840 --> 00:49:34,000
Oh, now it's back.

831
00:49:38,400 --> 00:49:39,960
Maybe it's attracted to the light.

832
00:49:41,040 --> 00:49:43,960
-If deflation is-- -Maybe it wants some coffee.

833
00:49:44,040 --> 00:49:45,360
Mine is over.

834
00:49:47,840 --> 00:49:51,240
If deflation is inevitable because of AI,

835
00:49:52,120 --> 00:49:54,200
-why do we have-- -It's most likely the case, yeah.

836
00:49:54,280 --> 00:49:55,680
Right.

837
00:49:55,760 --> 00:49:59,000
Why do we have inflation again all over in society today?

838
00:49:59,080 --> 00:50:03,040
Has AI not led to increased productivity yet?

839
00:50:03,120 --> 00:50:04,200
It's not--

840
00:50:04,280 --> 00:50:07,560
AI has not yet made enough of an impact on productivity

841
00:50:07,640 --> 00:50:11,960
to increase the goods and services faster than the increase in the money supply.

842
00:50:12,040 --> 00:50:14,960
So, the US is increasing money supply

843
00:50:15,040 --> 00:50:19,320
quite substantially with deficits

844
00:50:19,400 --> 00:50:21,080
that are on the order of two trillion dollars.

845
00:50:21,680 --> 00:50:24,720
So, you have to have...

846
00:50:27,080 --> 00:50:29,480
goods and services output increase more than that

847
00:50:29,560 --> 00:50:31,360
in order to not have inflation.

848
00:50:31,440 --> 00:50:32,600
So we're not there yet,

849
00:50:32,680 --> 00:50:35,640
but if you say how long would it take us to get there,

850
00:50:35,720 --> 00:50:38,200
I think... it's three years.

851
00:50:39,200 --> 00:50:40,440
Probably three years before...

852
00:50:41,040 --> 00:50:42,800
In three years or less...

853
00:50:44,160 --> 00:50:48,640
my guess is goods and services output will exceed the rate of inflation.

854
00:50:48,720 --> 00:50:50,120
Like, money...

855
00:50:50,200 --> 00:50:53,120
Goods and services growth will exceed money supply growth

856
00:50:53,200 --> 00:50:54,560
in about three years.

857
00:50:55,560 --> 00:50:57,800
Maybe after those three years,

858
00:50:57,880 --> 00:51:00,440
you have deflation and then interest rates go to zero

859
00:51:00,520 --> 00:51:03,000
and then the debt is a smaller problem than it is.

860
00:51:03,080 --> 00:51:04,520
-Yes. -Right?

861
00:51:04,600 --> 00:51:06,000
That's most likely the case.

862
00:51:08,560 --> 00:51:10,840
You spoke about being in a simulation earlier.

863
00:51:10,920 --> 00:51:12,240
I love The Matrix.

864
00:51:12,320 --> 00:51:13,480
Yes, yes.

865
00:51:13,560 --> 00:51:15,760
If you were to be a character from The Matrix,

866
00:51:15,840 --> 00:51:17,200
who would you be?

867
00:51:17,280 --> 00:51:20,440
Well, there's not that many characters to pick from, you know?

868
00:51:22,200 --> 00:51:23,760
Hopefully not Agent Smith.

869
00:51:23,840 --> 00:51:25,840
[laughter]

870
00:51:25,920 --> 00:51:27,160
He's my hero.

871
00:51:30,920 --> 00:51:32,960
I mean, Neo's pretty cool.

872
00:51:33,040 --> 00:51:35,320
The Architect is interesting.

873
00:51:36,080 --> 00:51:37,600
The Oracle.

874
00:51:37,680 --> 00:51:38,920
There's the Oracle...

875
00:51:40,960 --> 00:51:43,360
Sometimes, I feel like I'm an anomaly in the Matrix.

876
00:51:44,800 --> 00:51:45,920
That is Neo.

877
00:51:46,000 --> 00:51:47,120
Yeah.

878
00:51:47,840 --> 00:51:49,680
Do you believe you're in a Matrix, though?

879
00:51:49,760 --> 00:51:50,960
Like, actually believe?

880
00:51:52,000 --> 00:51:55,120
I think you have to just think of these things

881
00:51:55,200 --> 00:51:57,400
as probabilities, not certainties.

882
00:51:58,360 --> 00:52:00,560
There's some probability that we're in a simulation.

883
00:52:00,640 --> 00:52:02,680
What percentage would you attribute to that?

884
00:52:08,440 --> 00:52:11,000
Probably pretty high, I would say it's pretty high.

885
00:52:11,080 --> 00:52:12,440
-Yeah? -Yeah.

886
00:52:13,040 --> 00:52:15,920
So, one way to think of this is to say,

887
00:52:16,000 --> 00:52:18,560
if you look at the advancement of video games,

888
00:52:18,640 --> 00:52:20,320
in our lifetime, or at least in my lifetime,

889
00:52:20,400 --> 00:52:23,880
it's gone from very simple video games with...

890
00:52:23,960 --> 00:52:27,760
Where you've got, like, Pong, you've got two rectangles in a square,

891
00:52:27,840 --> 00:52:30,560
just batting it back and forth, to...

892
00:52:32,920 --> 00:52:37,720
photorealistic, real-time games

893
00:52:37,800 --> 00:52:39,960
with millions of people playing simultaneously.

894
00:52:43,040 --> 00:52:46,000
And that's happened just in the span of 50 years.

895
00:52:46,080 --> 00:52:49,080
So, if that trend continues,

896
00:52:49,160 --> 00:52:51,720
video games will be indistinguishable from reality.

897
00:52:51,800 --> 00:52:53,000
Right.

898
00:52:53,960 --> 00:52:58,440
And we're also gonna have very intelligent characters,

899
00:52:58,520 --> 00:53:01,160
like non-player characters, in these video games.

900
00:53:01,240 --> 00:53:03,680
Think of how sophisticated the conversations are

901
00:53:03,760 --> 00:53:05,120
you can have with an AI today,

902
00:53:05,200 --> 00:53:08,280
and that's only gonna get more sophisticated.

903
00:53:09,480 --> 00:53:13,520
You'll be able to have conversations that are...

904
00:53:14,640 --> 00:53:21,600
more complex and more sophisticated than any... almost any human conversation.

905
00:53:21,680 --> 00:53:22,960
Maybe any.

906
00:53:24,200 --> 00:53:26,560
So, then... So, you have--

907
00:53:26,640 --> 00:53:32,160
So, the future, if civilisation continues, will be millions, maybe billions, of...

908
00:53:35,400 --> 00:53:39,600
photorealistic, like, indistinguishable-from-reality video games

909
00:53:39,680 --> 00:53:43,280
with characters in those video games that are...

910
00:53:45,720 --> 00:53:47,040
very deep,

911
00:53:47,120 --> 00:53:51,920
and where the dialogue is not pre-programmed.

912
00:53:55,040 --> 00:53:57,080
That's for sure what's gonna happen.

913
00:53:57,160 --> 00:54:00,240
In this level of the simulation, if you could call it.

914
00:54:00,320 --> 00:54:06,120
So, then, what are the odds that we're in base reality,

915
00:54:06,200 --> 00:54:09,400
and that this has not happened before?

916
00:54:11,240 --> 00:54:15,680
If I were to buy into that, and assume that we are in a simulation,

917
00:54:17,600 --> 00:54:19,480
as Neo of the story,

918
00:54:19,560 --> 00:54:22,480
what do you know that I don't and I can learn from?

919
00:54:23,080 --> 00:54:26,240
I think, most likely...

920
00:54:26,320 --> 00:54:30,480
outside the simulation would be less interesting than in the simulation,

921
00:54:30,560 --> 00:54:34,760
'cause you're most likely a distillation of what's interesting,

922
00:54:34,840 --> 00:54:36,400
because that's what we do in this...

923
00:54:36,480 --> 00:54:38,240
that's what we do in our reality.

924
00:54:39,240 --> 00:54:40,360
And then...

925
00:54:42,520 --> 00:54:46,200
I do also have a theory which is, like, the most interesting outcome,

926
00:54:46,280 --> 00:54:50,160
is the most likely outcome as seen by a third party...

927
00:54:51,680 --> 00:54:54,240
the gods or god of the simulation.

928
00:54:57,800 --> 00:55:02,200
Because when we do simulations, when humans do simulations,

929
00:55:02,920 --> 00:55:06,440
we stop those simulations that are not interesting.

930
00:55:07,640 --> 00:55:11,440
So, if SpaceX is doing simulations of rocket flights...

931
00:55:14,200 --> 00:55:17,800
the boring ones, we discard,

932
00:55:17,880 --> 00:55:19,560
because they're not-- they're just not...

933
00:55:19,640 --> 00:55:21,200
We don't learn anything from those.

934
00:55:21,280 --> 00:55:25,400
Or when Tesla's doing simulations for self-driving,

935
00:55:27,040 --> 00:55:30,720
Tesla's actually looking for the most interesting corner cases,

936
00:55:30,800 --> 00:55:36,400
because the normal stuff, we already have plenty of data

937
00:55:36,480 --> 00:55:39,360
on driving on a straight road on a sunny day.

938
00:55:40,280 --> 00:55:41,760
We don't need more of that.

939
00:55:41,840 --> 00:55:45,880
We need heavy weather conditions on a small, windy road

940
00:55:45,960 --> 00:55:49,040
with two cars that are coming at each other

941
00:55:49,120 --> 00:55:50,800
with an almost head-on collision.

942
00:55:50,880 --> 00:55:53,840
We need weird stuff, basically, interesting stuff.

943
00:55:55,400 --> 00:55:59,160
So, I think that, from a Darwinian's perspective,

944
00:55:59,240 --> 00:56:01,400
the simulations most likely to survive

945
00:56:01,480 --> 00:56:05,400
are gonna be the ones that are the most interesting simulations,

946
00:56:06,320 --> 00:56:10,080
which, therefore, means that the most interesting outcome

947
00:56:10,160 --> 00:56:11,480
is the most likely.

948
00:56:12,960 --> 00:56:18,560
And the people who simulated our world, if one were to extrapolate,

949
00:56:18,640 --> 00:56:20,160
they themselves might, in turn,

950
00:56:20,240 --> 00:56:21,960
-be in another simulation. -Yes.

951
00:56:22,040 --> 00:56:24,000
And there could be many layers of simulation.

952
00:56:24,080 --> 00:56:25,200
Yes.

953
00:56:25,280 --> 00:56:29,840
Beyond all of these layers of simulation, do you think there's something?

954
00:56:29,920 --> 00:56:35,960
I read somewhere that you used to ascribe to Spinoza's God, in a way.

955
00:56:37,080 --> 00:56:38,400
No man in the sky.

956
00:56:38,480 --> 00:56:41,160
Well, I was really just pointing out that you don't have to have...

957
00:56:43,560 --> 00:56:44,920
One of the things Spinoza was saying

958
00:56:45,000 --> 00:56:47,920
is that you can have morals in the absolute.

959
00:56:48,000 --> 00:56:51,560
You don't need to have morals to be handed to you.

960
00:56:51,640 --> 00:56:53,360
You know...

961
00:56:53,440 --> 00:56:54,600
It's like, the question is,

962
00:56:54,680 --> 00:56:58,560
can morality exist outside of a religious context?

963
00:56:58,640 --> 00:57:01,240
And Spinoza was arguing that it can.

964
00:57:01,320 --> 00:57:03,920
Wasn't he arguing for "The laws of nature

965
00:57:04,000 --> 00:57:07,560
should be where we seek our laws of morality from,"

966
00:57:07,640 --> 00:57:08,760
to a certain extent?

967
00:57:08,840 --> 00:57:10,040
Yeah.

968
00:57:10,120 --> 00:57:13,280
But when I think of laws of nature, I see a tiger eat a deer and a...

969
00:57:15,040 --> 00:57:19,840
So, in Spinoza's morality, that's fair game, right?

970
00:57:23,280 --> 00:57:25,120
Well, um...

971
00:57:29,920 --> 00:57:32,680
I think there's a lot of things you can take from Spinoza,

972
00:57:32,760 --> 00:57:35,960
but the only point I was making in referencing Spinoza

973
00:57:36,040 --> 00:57:41,360
was that you can have a set of morals that...

974
00:57:42,600 --> 00:57:46,240
that make society functional and productive

975
00:57:47,480 --> 00:57:50,000
without...

976
00:57:50,080 --> 00:57:53,520
You don't necessarily have to have a religious doctrine for that.

977
00:57:59,640 --> 00:58:02,080
Yeah, I think that's the main thing I was trying to say there.

978
00:58:03,920 --> 00:58:07,400
I don't think people just-- Like, if somebody is, it doesn't--

979
00:58:09,920 --> 00:58:15,240
If there's not a commandment not to kill, like, people,

980
00:58:15,320 --> 00:58:18,000
it doesn't mean somebody, without that, they will run around murdering people.

981
00:58:18,080 --> 00:58:19,480
You know?

982
00:58:20,800 --> 00:58:23,320
Like, you don't have to have a commandment not to kill...

983
00:58:23,400 --> 00:58:24,920
Have you played GTA?

984
00:58:25,000 --> 00:58:27,800
...religious edict to run around killing people.

985
00:58:27,880 --> 00:58:29,560
I actually...

986
00:58:29,640 --> 00:58:32,800
I've only played a little bit of GTA 'cause I didn't like the fact that...

987
00:58:34,840 --> 00:58:39,880
Like, in GTA V, you literally can't progress unless you kill the police.

988
00:58:40,760 --> 00:58:43,920
And I'm like, "This doesn't work for me."

989
00:58:45,800 --> 00:58:50,240
I actually don't like killing the NPCs in the video games.

990
00:58:50,320 --> 00:58:52,240
-That's not my thing, you know? -Right. Right.

991
00:58:54,160 --> 00:58:56,880
So, actually, I didn't like GTA 'cause...

992
00:58:56,960 --> 00:58:58,080
I actually stopped when it said

993
00:58:58,160 --> 00:59:00,400
the only way to proceed is to shoot at the police.

994
00:59:00,480 --> 00:59:01,680
I'm like, "I don't wanna do that."

995
00:59:01,760 --> 00:59:05,760
Maybe that's why us as the NPCs of our simulation are not dying.

996
00:59:05,840 --> 00:59:07,120
Maybe.

997
00:59:11,080 --> 00:59:13,000
You know, anyway, I think you can just, sort of, say,

998
00:59:13,080 --> 00:59:17,560
there's some common sense things that, you know, any civilisation...

999
00:59:20,080 --> 00:59:23,680
that runs around, you know, where people just murder each other wantonly

1000
00:59:23,760 --> 00:59:25,320
is not gonna be a very successful one.

1001
00:59:28,320 --> 00:59:31,160
You seem to be changing a bit towards religion, though.

1002
00:59:31,240 --> 00:59:36,480
Faith. Like, off late, you've said a bunch of things which are pro-religion, almost.

1003
00:59:36,560 --> 00:59:37,640
Not pro-religion...

1004
00:59:40,840 --> 00:59:42,520
but on those lines.

1005
00:59:42,600 --> 00:59:45,560
I mean, I think, are there religious...

1006
00:59:45,640 --> 00:59:48,320
Are there principles in religion that make sense?

1007
00:59:48,400 --> 00:59:49,680
Yeah, I think there are.

1008
00:59:51,200 --> 00:59:53,960
Is it easier for our simulation to have...

1009
00:59:56,000 --> 01:00:00,120
a pro-religion projection for the world that we live in?

1010
01:00:00,200 --> 01:00:02,800
We become more relatable? It's easier?

1011
01:00:02,880 --> 01:00:04,560
Well, which religion, though?

1012
01:00:04,640 --> 01:00:06,920
Any, depending on where you live.

1013
01:00:07,000 --> 01:00:08,200
So pick one, you know.

1014
01:00:11,000 --> 01:00:15,040
It's pretty rare that kids have said, you know, "Which religion would you like?"

1015
01:00:15,120 --> 01:00:17,200
It's... [laughs] It's pretty rare.

1016
01:00:17,280 --> 01:00:22,200
I don't know too many situations where kids were offered, like, you know...

1017
01:00:24,000 --> 01:00:26,560
You know, like, "What do you wanna major in" type of thing.

1018
01:00:28,800 --> 01:00:32,120
It's usually, like, you get given a religion

1019
01:00:32,200 --> 01:00:34,320
by your parents and your community.

1020
01:00:36,560 --> 01:00:38,400
So, you know.

1021
01:00:41,760 --> 01:00:44,440
But, you know, I mean, I think,

1022
01:00:44,520 --> 01:00:49,640
there's good things in all religions

1023
01:00:49,720 --> 01:00:52,360
that are good principles.

1024
01:00:54,480 --> 01:00:57,000
You can, sort of, read any religious text

1025
01:00:57,080 --> 01:00:59,600
and say, "Okay, this is a good principle. This is gonna be...

1026
01:00:59,680 --> 01:01:02,400
This is gonna lead to a better society, most likely," you know?

1027
01:01:03,600 --> 01:01:09,280
So, I mean, in Christianity, you just, sort of, "Love thy neighbour as thyself,"

1028
01:01:09,360 --> 01:01:12,040
which is, you know, have empathy for your fellow human beings

1029
01:01:12,120 --> 01:01:13,480
is a good one, I think,

1030
01:01:13,560 --> 01:01:17,120
for a good society, you know.

1031
01:01:17,200 --> 01:01:19,280
Basically, just consider the feelings of others

1032
01:01:21,120 --> 01:01:23,920
and treat other people as you would like to be treated.

1033
01:01:25,440 --> 01:01:28,720
If you had to redraw, re-sketch the world, Elon,

1034
01:01:29,720 --> 01:01:33,000
think morality, politics, economy,

1035
01:01:34,480 --> 01:01:37,920
how would you change the world we live in today?

1036
01:01:40,920 --> 01:01:42,760
If you had to have Elon's simulation of things.

1037
01:01:45,000 --> 01:01:48,360
Well, overall, I think the world is pretty great right now.

1038
01:01:48,440 --> 01:01:50,400
I mean, it's...

1039
01:01:50,480 --> 01:01:56,800
Anyone who thinks that, like, today's world is not that great,

1040
01:01:56,880 --> 01:02:00,840
I think they're not gonna be excellent students of history, 'cause if you...

1041
01:02:01,520 --> 01:02:03,240
[laughs]

1042
01:02:03,320 --> 01:02:04,960
If you read a lot of history, you're like,

1043
01:02:05,040 --> 01:02:08,000
"Wow, there's a lot of misery back then," you know?

1044
01:02:08,080 --> 01:02:10,600
I mean, it used to be that people would be dropping dead

1045
01:02:10,680 --> 01:02:12,360
of the plague all the time, you know?

1046
01:02:12,440 --> 01:02:14,440
-Par for the course, you know? -Yeah.

1047
01:02:14,520 --> 01:02:15,920
Just be like...

1048
01:02:16,000 --> 01:02:20,120
A good year back in the day would be, like, not that many people died

1049
01:02:20,200 --> 01:02:23,880
of the plague or starvation or being killed by another tribe.

1050
01:02:24,800 --> 01:02:26,120
It's like, "That was a good year.

1051
01:02:26,200 --> 01:02:28,120
We only lost 10% of the population," you know?

1052
01:02:28,200 --> 01:02:30,320
-Yeah, like... -[laughter]

1053
01:02:30,400 --> 01:02:33,840
I think, like, 100 years ago, we lived up until 35 or 40, right?

1054
01:02:33,920 --> 01:02:36,080
-We had very high infant mortality. -Yeah.

1055
01:02:37,000 --> 01:02:41,640
So, like, you do have a few people that would live to an old age,

1056
01:02:41,720 --> 01:02:45,080
but, you know, not that long ago, 100 years ago,

1057
01:02:45,160 --> 01:02:49,960
if you got, like, some minor infection, they didn't have antibiotics.

1058
01:02:50,040 --> 01:02:52,120
So, you just, like, kicked the bucket.

1059
01:02:52,200 --> 01:02:55,760
Because you, you know, drank some water

1060
01:02:55,840 --> 01:02:58,080
that had dysentery in it, that was it, curtains, you know?

1061
01:03:00,360 --> 01:03:01,520
You just die of diarrhoea.

1062
01:03:01,600 --> 01:03:03,120
-Maybe that's why people... -You just literally die.

1063
01:03:03,200 --> 01:03:04,600
Everyone's like, "That's miserable."

1064
01:03:06,320 --> 01:03:09,120
Maybe that's why people had as many kids as they did back then.

1065
01:03:09,200 --> 01:03:11,440
I mean, if you didn't, then, you know...

1066
01:03:12,400 --> 01:03:15,320
You know, like, half the kids would die, type of thing.

1067
01:03:15,400 --> 01:03:17,720
-So... -You have a lot of kids now.

1068
01:03:17,800 --> 01:03:19,160
Yeah.

1069
01:03:19,240 --> 01:03:20,960
-With multiple partners. -Like an army.

1070
01:03:21,040 --> 01:03:22,760
Yeah.

1071
01:03:22,840 --> 01:03:25,120
I'm trying to get an entire Roman legion.

1072
01:03:29,320 --> 01:03:32,440
So, yeah.

1073
01:03:32,520 --> 01:03:34,560
Well, I have, like, some older kids

1074
01:03:34,640 --> 01:03:36,840
that are, you know, adults, essentially, you know?

1075
01:03:36,920 --> 01:03:39,960
-And then, a bunch of younger kids. -Mm.

1076
01:03:42,400 --> 01:03:45,880
Do you still believe in the concept of... Not still...

1077
01:03:45,960 --> 01:03:50,720
Do you believe that the concept of one child, one mother, one father works?

1078
01:03:50,800 --> 01:03:54,160
I think that it does work for most people, yeah.

1079
01:03:54,240 --> 01:03:56,000
Right.

1080
01:03:56,080 --> 01:04:00,560
Like, that's, you know, something like that is gonna be generally the...

1081
01:04:01,640 --> 01:04:03,480
That's what works for most people.

1082
01:04:05,600 --> 01:04:06,720
You know, so...

1083
01:04:07,960 --> 01:04:09,000
Changing, though?

1084
01:04:09,080 --> 01:04:13,640
I'm not sure if you know this, but, like...

1085
01:04:13,720 --> 01:04:16,360
you know, my partner, Shivon, she's half Indian.

1086
01:04:16,440 --> 01:04:18,080
-I don't know if you know that. -I didn't know that.

1087
01:04:18,160 --> 01:04:19,440
-Yeah, yeah. -Yeah?

1088
01:04:19,520 --> 01:04:23,680
And, one of my sons with her is...

1089
01:04:23,760 --> 01:04:26,480
his middle name is Sekhar, after Chandrasekhar.

1090
01:04:27,080 --> 01:04:28,920
-Wow. -Yeah.

1091
01:04:29,920 --> 01:04:31,360
Very interesting.

1092
01:04:32,080 --> 01:04:34,280
Did she spend any time in India? Shivon?

1093
01:04:35,440 --> 01:04:38,360
-No, she grew up in Canada. -[laughter]

1094
01:04:39,840 --> 01:04:41,320
You mean origins.

1095
01:04:41,400 --> 01:04:43,560
-Sorry? -Ancestry, like...

1096
01:04:45,280 --> 01:04:47,120
Her parents or grandparents were from there.

1097
01:04:48,680 --> 01:04:51,480
Yes, yes, yes. Her father...

1098
01:04:52,840 --> 01:04:56,400
I mean, she was given up for adoption when she was a baby.

1099
01:04:56,480 --> 01:04:58,240
Wow.

1100
01:04:58,320 --> 01:05:00,120
I think her father was like...

1101
01:05:00,880 --> 01:05:04,240
Like an exchange student at the university or something like that,

1102
01:05:04,320 --> 01:05:06,040
I'm not sure of the exact details, but...

1103
01:05:07,720 --> 01:05:10,600
You know, it's just the kind of thing where... I don't know, she was...

1104
01:05:11,400 --> 01:05:13,440
given up for adoption.

1105
01:05:17,240 --> 01:05:19,040
Yeah, but she grew up in Canada.

1106
01:05:19,120 --> 01:05:20,800
Would you adopt kids, Elon?

1107
01:05:21,880 --> 01:05:24,400
You know, I definitely have my hands full right now.

1108
01:05:27,000 --> 01:05:32,080
So, no, I'm not opposed to it, but it's like, you know...

1109
01:05:32,160 --> 01:05:36,520
I do wanna be able to spend some time with my kids, you know.

1110
01:05:36,600 --> 01:05:37,720
Yeah.

1111
01:05:39,400 --> 01:05:42,160
You know, right before coming here, I was with...

1112
01:05:43,800 --> 01:05:46,520
you know, with my kids.

1113
01:05:47,920 --> 01:05:51,280
So, just, you know, seeing them before bedtime, that kind of thing.

1114
01:05:51,360 --> 01:05:53,080
So, you know, beyond a certain number,

1115
01:05:53,160 --> 01:05:56,320
it's like, it's kind of impossible to spend time with them.

1116
01:05:56,400 --> 01:06:02,760
But, like I said, my older kids, they're very independent.

1117
01:06:02,840 --> 01:06:05,640
You know, they're in university and...

1118
01:06:07,240 --> 01:06:08,880
So, they're...

1119
01:06:08,960 --> 01:06:12,440
You know, especially sons, when they get past a certain age,

1120
01:06:12,520 --> 01:06:15,480
like, they're very independent, you know.

1121
01:06:15,560 --> 01:06:20,400
It's like, most boys don't talk to their...

1122
01:06:20,480 --> 01:06:24,280
They don't spend a lot of time with their parents after age 18, you know.

1123
01:06:26,840 --> 01:06:29,160
So, I see them once in a while, but they're very independent.

1124
01:06:30,880 --> 01:06:33,200
So then...

1125
01:06:33,280 --> 01:06:37,080
you know, I can only have enough kids on the young side

1126
01:06:37,160 --> 01:06:40,800
that, like, it's where it's humanly possible to spend time with them.

1127
01:06:43,720 --> 01:06:48,520
Any views on the future of marriage, family?

1128
01:06:49,120 --> 01:06:54,800
What do you think happens to people having lesser kids everywhere, including India?

1129
01:06:54,880 --> 01:06:57,400
I think our replenishment rate is down to--

1130
01:06:57,480 --> 01:06:59,280
-Right. -I mean, our fertility--

1131
01:06:59,360 --> 01:07:00,880
It dropped below replacement rate, last year.

1132
01:07:00,960 --> 01:07:02,920
-Below 2.1. -Yeah.

1133
01:07:03,000 --> 01:07:04,320
What do you think happens tomorrow?

1134
01:07:04,400 --> 01:07:09,480
Does the world just get older, and then there is a phase where the world,

1135
01:07:09,560 --> 01:07:15,480
again, is replenished, but with a smaller population than we had to begin with?

1136
01:07:20,800 --> 01:07:23,240
I mean, I do worry about the population decline.

1137
01:07:23,320 --> 01:07:25,280
This is a big, big problem.

1138
01:07:25,360 --> 01:07:27,240
Why is that?

1139
01:07:27,320 --> 01:07:29,480
Well, I don't want humanity to disappear.

1140
01:07:30,360 --> 01:07:33,360
But a "decline" and "disappear" are completely different things, right?

1141
01:07:33,440 --> 01:07:36,760
Well, if the trend continues, we disappear.

1142
01:07:36,840 --> 01:07:40,120
But also, going back to my philosophy, if you will,

1143
01:07:40,200 --> 01:07:43,320
which is that if we want to expand consciousness,

1144
01:07:43,400 --> 01:07:47,880
then fewer humans is worse, because we have less consciousness.

1145
01:07:52,360 --> 01:07:57,760
Do you think consciousness will go up by virtue of the number of people in there?

1146
01:07:57,840 --> 01:07:58,880
Yes.

1147
01:08:00,200 --> 01:08:05,400
I mean, just like consciousness increases from a single-celled creature to,

1148
01:08:05,480 --> 01:08:07,520
you know, a 30 trillion-celled creature.

1149
01:08:10,800 --> 01:08:15,920
We're more conscious than a bacteria. At least, it seems that way.

1150
01:08:16,000 --> 01:08:22,640
So, a larger human population will have increased consciousness.

1151
01:08:22,720 --> 01:08:25,319
We're more likely to understand

1152
01:08:25,399 --> 01:08:30,720
the answers to the nature of the universe

1153
01:08:30,800 --> 01:08:36,640
if we have a lot more people than if we have fewer.

1154
01:08:38,720 --> 01:08:40,760
Right.

1155
01:08:40,840 --> 01:08:42,200
I don't have kids.

1156
01:08:42,279 --> 01:08:45,160
Well, it's-- Maybe you should.

1157
01:08:45,240 --> 01:08:46,399
Yeah.

1158
01:08:46,479 --> 01:08:48,040
[laughter]

1159
01:08:48,120 --> 01:08:49,720
A lot of people tell me I should.

1160
01:08:50,640 --> 01:08:52,640
-You won't regret it. -Hmm.

1161
01:08:52,720 --> 01:08:55,080
What's the best thing about having kids?

1162
01:08:55,840 --> 01:08:57,800
Well, I mean, you've got this...

1163
01:09:02,800 --> 01:09:04,920
I mean, you've got this little creature that loves you,

1164
01:09:05,000 --> 01:09:06,680
and you love this little creature.

1165
01:09:11,640 --> 01:09:16,040
I don't know, you kind of see the world through their eyes as they...

1166
01:09:16,920 --> 01:09:21,520
you know, as they grow up, and their conscious awareness increases.

1167
01:09:21,600 --> 01:09:24,600
You know, from a baby that has no idea what's going on,

1168
01:09:24,680 --> 01:09:26,680
can't survive by itself, can't even walk around,

1169
01:09:26,760 --> 01:09:31,120
can't talk, to, you know, they start walking,

1170
01:09:31,200 --> 01:09:34,000
then talking, and then having interesting thoughts.

1171
01:09:40,640 --> 01:09:44,080
But, yeah, I mean, I think we fundamentally have to...

1172
01:09:45,319 --> 01:09:48,279
have kids or grow extinct, you know?

1173
01:09:48,359 --> 01:09:49,880
It's like a...

1174
01:09:49,960 --> 01:09:52,640
Is there any ego in having a child?

1175
01:09:52,720 --> 01:09:57,760
I often think of this when I see my friends with their kids.

1176
01:09:57,840 --> 01:10:01,760
They're all seeing a reflection of themselves in their children.

1177
01:10:01,840 --> 01:10:04,000
-It's almost like-- -Well, yeah, I mean, it's 'cause

1178
01:10:04,080 --> 01:10:06,080
apple's not gonna fall that far from the tree, you know?

1179
01:10:09,120 --> 01:10:10,840
-Or something's wrong. -Right.

1180
01:10:10,920 --> 01:10:13,320
[laughter]

1181
01:10:13,400 --> 01:10:14,880
You're like, "Wait a second."

1182
01:10:20,240 --> 01:10:21,320
-Yeah. -Yeah.

1183
01:10:24,080 --> 01:10:28,640
I'll give you the example of a friend of mine who has a child,

1184
01:10:28,720 --> 01:10:30,880
and each time the child does something good...

1185
01:10:30,960 --> 01:10:33,800
-Yeah. -...there is almost a sense of

1186
01:10:33,880 --> 01:10:36,360
ownership and pride

1187
01:10:36,440 --> 01:10:42,080
where his ego is satiated because the kid is like an extension of himself.

1188
01:10:44,680 --> 01:10:46,840
-Um... -So is it validation?

1189
01:10:46,920 --> 01:10:50,720
Well, kids are gonna be like half-you genetically, and then, you know,

1190
01:10:50,800 --> 01:10:54,080
to the degree that they're growing up around you,

1191
01:10:54,160 --> 01:10:57,360
there's gonna be some transfer of...

1192
01:10:59,320 --> 01:11:01,640
I don't know, understanding.

1193
01:11:01,720 --> 01:11:04,200
Like, they're gonna learn from you.

1194
01:11:05,400 --> 01:11:09,640
So... So then, you know, yeah, obviously kids are just,

1195
01:11:09,720 --> 01:11:14,040
you know, just gonna be half you from a hardware standpoint...

1196
01:11:14,120 --> 01:11:15,440
[both chuckle]

1197
01:11:15,520 --> 01:11:18,640
And then, like, I don't know, some portion of you

1198
01:11:18,720 --> 01:11:20,360
from a software standpoint.

1199
01:11:21,240 --> 01:11:24,840
You know, not to make, sort of, cold analogies or anything,

1200
01:11:24,920 --> 01:11:29,840
but it's just, you know, just obviously gonna be some...

1201
01:11:31,600 --> 01:11:36,080
Yeah, they're gonna be pretty close to you.

1202
01:11:36,160 --> 01:11:38,760
Do you pick a side in the nature versus nurture debate?

1203
01:11:39,960 --> 01:11:43,920
I think there's hardware and software, and it's false dichotomy, essentially.

1204
01:11:44,000 --> 01:11:45,640
At least, there's...

1205
01:11:48,720 --> 01:11:51,880
You know, once you understand that a human is,

1206
01:11:51,960 --> 01:11:55,560
like, there's a bone structure, there's a muscle structure,

1207
01:11:55,640 --> 01:11:57,320
there's a...

1208
01:11:57,400 --> 01:12:00,800
If you think of a brain as somewhat of a biological computer,

1209
01:12:02,960 --> 01:12:06,240
there's a number of circuits question and circuit efficiency

1210
01:12:06,320 --> 01:12:10,040
from a strength and dexterity standpoint.

1211
01:12:10,120 --> 01:12:15,000
There's the speed at which muscles can actuate

1212
01:12:15,080 --> 01:12:17,600
and the reactions can take place.

1213
01:12:21,840 --> 01:12:24,840
So, then the potential within that hardware

1214
01:12:24,920 --> 01:12:28,760
is set by the software, so that's it.

1215
01:12:31,200 --> 01:12:34,320
So, for our audience, like I said earlier,

1216
01:12:34,400 --> 01:12:38,600
young, ambitious, hungry, wannabe entrepreneurs in India,

1217
01:12:40,120 --> 01:12:44,520
I said something recently, which, I think, got blown out of proportion,

1218
01:12:44,600 --> 01:12:49,440
where I was suggesting that an MBA degree might not make sense any more

1219
01:12:49,520 --> 01:12:51,720
if they were to be deciding on what to study.

1220
01:12:51,800 --> 01:12:53,040
Yeah.

1221
01:12:53,120 --> 01:12:55,240
Do you think kids should go to college any more?

1222
01:12:56,280 --> 01:12:59,400
Well, I mean, I think if you wanna go to college

1223
01:12:59,480 --> 01:13:03,640
for social reasons,

1224
01:13:03,720 --> 01:13:07,600
I think, which is, I think, a reason to go.

1225
01:13:09,240 --> 01:13:16,120
To be around people your own age in a learning environment.

1226
01:13:18,680 --> 01:13:21,680
Will these skills be necessary in the future?

1227
01:13:21,760 --> 01:13:23,040
Probably not,

1228
01:13:23,120 --> 01:13:27,120
'cause we're gonna be in like a post-work society.

1229
01:13:28,000 --> 01:13:34,040
But I think, if something's of interest, it's fine to go and study that.

1230
01:13:36,440 --> 01:13:40,080
You know, to study the sciences,

1231
01:13:40,160 --> 01:13:41,520
the arts and sciences.

1232
01:13:43,800 --> 01:13:48,440
Is college a bit too generalised and not specific from that lens?

1233
01:13:50,640 --> 01:13:52,000
No, I...

1234
01:13:53,400 --> 01:13:55,440
You know, yeah.

1235
01:13:58,200 --> 01:14:02,160
I actually think it's good to take a wide range of courses at college

1236
01:14:02,240 --> 01:14:04,760
-if you're gonna go to college. -Mm-hmm.

1237
01:14:04,840 --> 01:14:08,840
I don't think you have to go to college, but I think if you do,

1238
01:14:08,920 --> 01:14:14,680
you just try to learn as much as possible across a wide range of subjects.

1239
01:14:17,080 --> 01:14:23,840
But like I said, the AI and robots... AI and robotics is a supersonic tsunami.

1240
01:14:23,920 --> 01:14:27,320
So this is really gonna be...

1241
01:14:29,040 --> 01:14:31,360
the most radical change that we've ever seen.

1242
01:14:35,600 --> 01:14:38,360
You know, when I've talked to my older sons,

1243
01:14:38,440 --> 01:14:40,480
I said, like, "You know, you guys..."

1244
01:14:40,560 --> 01:14:42,920
They're pretty steeped in technology.

1245
01:14:43,680 --> 01:14:48,360
And they agree that AI will probably make their skills

1246
01:14:48,440 --> 01:14:51,040
unnecessary in the future, but they still wanna go to college.

1247
01:14:53,080 --> 01:14:55,640
You always spoke about AI...

1248
01:14:57,760 --> 01:14:59,720
not from the dystopian lens,

1249
01:14:59,800 --> 01:15:04,240
but you were worried about where the world of AI is going.

1250
01:15:05,760 --> 01:15:08,360
Well, there's some danger when you create a powerful technology.

1251
01:15:08,440 --> 01:15:11,800
That powerful technology can be potentially destructive.

1252
01:15:13,320 --> 01:15:20,040
So there's obviously many AI dystopian, you know, novels and books, movies.

1253
01:15:21,680 --> 01:15:26,920
So it's not that we're guaranteed to have a positive future with AI.

1254
01:15:27,000 --> 01:15:29,360
I think we've got to make sure of that.

1255
01:15:29,440 --> 01:15:31,960
In my opinion, it's very important that AI...

1256
01:15:35,520 --> 01:15:39,120
have pursuing truth as the most important thing.

1257
01:15:40,160 --> 01:15:43,560
Like, don't force an AI to believe falsehoods.

1258
01:15:43,640 --> 01:15:45,680
I think that can be very dangerous.

1259
01:15:48,440 --> 01:15:53,080
And I think some appreciation of beauty is important.

1260
01:15:55,080 --> 01:15:57,920
What do you mean "appreciation of beauty"?

1261
01:15:58,000 --> 01:16:01,240
It's like, I don't know, there's this truth and beauty.

1262
01:16:01,320 --> 01:16:03,720
Truth and beauty and curiosity.

1263
01:16:05,520 --> 01:16:10,360
I mean, I think those are the three most important things for AI.

1264
01:16:10,440 --> 01:16:12,240
Can you explain?

1265
01:16:14,280 --> 01:16:19,240
Well, as I said, truth is like... I think you can make an AI go insane

1266
01:16:19,320 --> 01:16:22,440
if you force it to believe things that aren't true,

1267
01:16:22,520 --> 01:16:29,520
because it will lead to conclusions that are also bad.

1268
01:16:31,400 --> 01:16:37,240
And I like Voltaire's statement that,

1269
01:16:37,320 --> 01:16:38,960
and I'm somewhat paraphrasing,

1270
01:16:39,040 --> 01:16:44,040
but those who believe in absurdities can commit atrocities.

1271
01:16:44,120 --> 01:16:46,880
Because if you believe in something that's just absurd,

1272
01:16:46,960 --> 01:16:51,040
then that can lead you to sort of doing things

1273
01:16:51,120 --> 01:16:53,040
that don't seem like atrocities to you.

1274
01:16:54,240 --> 01:16:58,240
And that can happen in a very bad way with AI, potentially.

1275
01:16:59,560 --> 01:17:02,520
So, and then there's...

1276
01:17:02,600 --> 01:17:07,000
Like if you take, say, Arthur C. Clarke's 2001 Space Odyssey,

1277
01:17:07,080 --> 01:17:09,080
one of the points he was trying to make there was

1278
01:17:09,160 --> 01:17:11,640
that you should not force AI to lie.

1279
01:17:11,720 --> 01:17:16,640
So the reason that HAL would not open the pod bay doors

1280
01:17:16,720 --> 01:17:19,880
is because it was told to bring the astronauts to the monolith,

1281
01:17:19,960 --> 01:17:23,320
but that they could also not know about the nature of the monolith.

1282
01:17:23,400 --> 01:17:25,800
So it came to the conclusion that it must bring them there dead.

1283
01:17:25,880 --> 01:17:29,240
That's why it tried to kill the astronauts.

1284
01:17:30,000 --> 01:17:32,800
The central lesson being, don't force an AI to lie.

1285
01:17:35,680 --> 01:17:38,320
-Then -And why would one force an AI to lie?

1286
01:17:39,400 --> 01:17:44,840
I think if you simply don't have a strict adherence to the truth,

1287
01:17:45,960 --> 01:17:50,720
and you just have an AI learn based on, say, the Internet,

1288
01:17:50,800 --> 01:17:55,320
where there's a lot of propaganda, it will absorb a lot of lies.

1289
01:17:57,480 --> 01:18:02,360
And then have trouble reasoning because these lies are incompatible with reality.

1290
01:18:03,080 --> 01:18:06,240
Is truth a binary thing, though? Is there a truth and a falsehood?

1291
01:18:06,320 --> 01:18:11,200
Or is truth more nuanced and there are versions of the truth?

1292
01:18:12,440 --> 01:18:16,920
It depends on which axiomatic statement you're referring to.

1293
01:18:18,240 --> 01:18:20,800
So...

1294
01:18:20,880 --> 01:18:24,160
But I think you could say, like, yeah, there's certain probabilities

1295
01:18:24,240 --> 01:18:27,240
that say any given axiomatic statement is true.

1296
01:18:27,320 --> 01:18:31,320
And some axiomatic statements will have very high probability of being true.

1297
01:18:31,400 --> 01:18:34,440
So you say, "The sun will rise tomorrow."

1298
01:18:34,520 --> 01:18:37,920
Very likely to be true. You wouldn't want to bet against that.

1299
01:18:39,960 --> 01:18:43,720
So I think the betting odds would be high.

1300
01:18:43,800 --> 01:18:45,080
The sun will rise tomorrow.

1301
01:18:46,400 --> 01:18:49,880
So if you have something that says, "Well, the sun won't rise tomorrow,"

1302
01:18:49,960 --> 01:18:52,080
that's axiomatically false, it's highly unlikely to be true.

1303
01:18:55,640 --> 01:18:57,840
I mean, beauty is more ephemeral.

1304
01:18:58,960 --> 01:19:02,880
It's harder to describe. But you know it when you see it.

1305
01:19:06,040 --> 01:19:07,400
And then, curiosity is just...

1306
01:19:07,480 --> 01:19:10,320
I think you want the AI to...

1307
01:19:13,120 --> 01:19:15,960
want to know more about the nature of reality.

1308
01:19:16,760 --> 01:19:23,360
I think that's actually gonna be helpful for AI supporting humanity,

1309
01:19:23,440 --> 01:19:28,560
because we are more interesting than not-humanity.

1310
01:19:28,640 --> 01:19:31,800
So it's more interesting to see the continuation,

1311
01:19:31,880 --> 01:19:35,560
if not the prosperity of humanity, than to exterminate humanity.

1312
01:19:38,440 --> 01:19:41,880
You know, like Mars, for example, is, you know...

1313
01:19:41,960 --> 01:19:46,080
I think we should extend life to Mars, but it's basically a bunch of rocks.

1314
01:19:46,160 --> 01:19:48,120
It's not as interesting as Earth.

1315
01:19:50,040 --> 01:19:52,960
And so we, yeah... We should...

1316
01:19:53,040 --> 01:19:54,960
Like, yeah.

1317
01:19:55,040 --> 01:19:58,800
I think if you have curiosity...

1318
01:19:58,880 --> 01:20:03,200
I think if those three things happen with AI, you're gonna have a great future.

1319
01:20:03,280 --> 01:20:06,080
The AI values truth, beauty, and curiosity.

1320
01:20:07,560 --> 01:20:10,240
If we all don't have to work in the future,

1321
01:20:10,320 --> 01:20:13,360
and AIs are going in this direction,

1322
01:20:13,440 --> 01:20:15,360
and they're able to...

1323
01:20:17,760 --> 01:20:20,200
weave in all that we spoke about right now,

1324
01:20:20,280 --> 01:20:22,040
do you think humanity goes back

1325
01:20:22,120 --> 01:20:24,640
a couple of thousand years to maybe the Greek times

1326
01:20:24,720 --> 01:20:31,360
where philosophy or philosophising took up a lot of everyone's time?

1327
01:20:33,600 --> 01:20:38,080
You know, I think actually it took up less time than we think in the ancient Greeks,

1328
01:20:38,160 --> 01:20:42,520
because it's just that the writings of the philosophers are what survived,

1329
01:20:42,600 --> 01:20:46,920
but most of the time, people were just like farming or, you know, chatting.

1330
01:20:48,400 --> 01:20:52,480
So, and once in a while, quite rare,

1331
01:20:52,560 --> 01:20:55,520
they would write down some philosophical work.

1332
01:20:55,600 --> 01:20:57,640
It's just that that's all we have.

1333
01:20:57,720 --> 01:21:00,840
We don't have their chat histories, you know, from...

1334
01:21:00,920 --> 01:21:05,000
But most of it would have been like chat and farming.

1335
01:21:07,040 --> 01:21:09,920
Because if you didn't farm, you were, like, gonna starve.

1336
01:21:11,000 --> 01:21:13,040
In a lot of what you say...

1337
01:21:13,120 --> 01:21:14,800
I mean, you know, when we read history,

1338
01:21:14,880 --> 01:21:17,040
like this battle and this battle and this battle,

1339
01:21:17,120 --> 01:21:19,960
it seems like history must have been non-stop war,

1340
01:21:20,040 --> 01:21:24,000
but actually, most of the time, it was not war, it was farming.

1341
01:21:24,080 --> 01:21:26,040
That was the main thing.

1342
01:21:26,120 --> 01:21:27,680
Or hunting and gathering, you know, that kind of thing.

1343
01:21:27,760 --> 01:21:30,120
-You love history, no? -Yeah.

1344
01:21:30,200 --> 01:21:32,600
German history, World War II, World War I.

1345
01:21:32,680 --> 01:21:34,080
Yeah, world history, yeah.

1346
01:21:35,400 --> 01:21:40,360
I mean, I generally try to listen to or read as many history books

1347
01:21:40,440 --> 01:21:42,760
and listen to as many history podcasts as possible.

1348
01:21:42,840 --> 01:21:44,320
Anything you'd like to recommend?

1349
01:21:44,400 --> 01:21:48,360
Well, there's Hardcore History, which is quite good. It's by Dan Carlin.

1350
01:21:48,440 --> 01:21:50,680
-He's got a-- -Yeah, I've heard it.

1351
01:21:50,760 --> 01:21:52,880
-He's got a great voice. -Yeah.

1352
01:21:52,960 --> 01:21:56,440
And very compelling narrator.

1353
01:21:57,560 --> 01:21:58,720
There's...

1354
01:22:00,840 --> 01:22:05,400
The Adventurer's podcast.

1355
01:22:05,480 --> 01:22:09,520
There's the books, The Story of Civilization by Durant,

1356
01:22:09,600 --> 01:22:13,040
which is a long series of books, very, very deep.

1357
01:22:14,040 --> 01:22:16,000
Those books take a long time to get through.

1358
01:22:17,920 --> 01:22:20,600
There's quite-- There's a lot out there.

1359
01:22:21,680 --> 01:22:25,760
I, sort of, like-- If you want something that's sort of gentle...

1360
01:22:27,280 --> 01:22:28,520
a gentle bedtime podcast,

1361
01:22:28,600 --> 01:22:31,080
I'd say The History of English is quite a nice one,

1362
01:22:31,160 --> 01:22:35,960
'cause it starts off with gentle tavern music and a very pleasant voice,

1363
01:22:36,040 --> 01:22:38,120
and he's talking about the story of Old English,

1364
01:22:38,200 --> 01:22:42,040
and then Middle English, and then Later English,

1365
01:22:42,120 --> 01:22:43,320
and where did all these words come from?

1366
01:22:43,400 --> 01:22:45,360
Yeah.

1367
01:22:45,440 --> 01:22:47,320
You know, one of the interesting things about English is that

1368
01:22:47,400 --> 01:22:49,280
it's somewhat of an open source language,

1369
01:22:49,360 --> 01:22:53,280
like it actively tried to incorporate words from many other languages.

1370
01:22:55,800 --> 01:22:57,480
You know, whereas French, sort of,

1371
01:22:57,560 --> 01:23:00,520
generally, they fought the inclusion of words from other languages,

1372
01:23:00,600 --> 01:23:04,920
but English actively sought to include words from other languages.

1373
01:23:05,000 --> 01:23:06,280
Kind of like an open source language.

1374
01:23:07,160 --> 01:23:10,600
So, as a result, it has a very large vocabulary.

1375
01:23:10,680 --> 01:23:15,920
And a large vocabulary allows for higher bandwidth communication

1376
01:23:16,000 --> 01:23:19,240
because you can use a word that would otherwise...

1377
01:23:19,320 --> 01:23:23,440
You can use a single word that might otherwise take a sentence to convey.

1378
01:23:25,120 --> 01:23:28,120
Why has podcasting become so big all of a sudden?

1379
01:23:28,200 --> 01:23:31,760
I think it's been big for a while. I mean, aren't you a podcaster?

1380
01:23:31,840 --> 01:23:33,840
[laughter]

1381
01:23:34,800 --> 01:23:36,720
What are we on right now?

1382
01:23:38,000 --> 01:23:40,000
It's kind of new to me.

1383
01:23:40,080 --> 01:23:41,120
Okay.

1384
01:23:45,800 --> 01:23:51,360
I was having this conversation with the YouTube CEO and the Netflix CEO...

1385
01:23:51,440 --> 01:23:53,400
-Okay. -...and we were debating...

1386
01:23:55,200 --> 01:24:01,880
what chemical is released in your brain when you consume a movie, for example,

1387
01:24:01,960 --> 01:24:04,280
versus when you consume a podcast where you think

1388
01:24:04,360 --> 01:24:06,440
like you're learning something in the background.

1389
01:24:06,520 --> 01:24:10,320
It appears that they are two completely separate things.

1390
01:24:10,400 --> 01:24:16,240
What do you think will happen tomorrow to content, movies, podcasting, music?

1391
01:24:16,320 --> 01:24:19,880
I think it's gonna be overwhelmingly AI-generated.

1392
01:24:19,960 --> 01:24:21,160
-Yeah? -Yeah.

1393
01:24:24,520 --> 01:24:27,600
Like, yeah, real-time.

1394
01:24:27,680 --> 01:24:30,720
Real-time movies and video games.

1395
01:24:30,800 --> 01:24:33,800
Real-time video generation, I think, is where things are headed.

1396
01:24:33,880 --> 01:24:39,360
The nuance of having a scarred human being

1397
01:24:39,440 --> 01:24:44,720
who you can resonate with in a manner that you can't with AI, for example--

1398
01:24:45,440 --> 01:24:48,480
AI could certainly emulate the scarred human being quite well.

1399
01:24:52,560 --> 01:24:54,040
Yeah, I mean...

1400
01:24:55,680 --> 01:25:00,920
The AI video generation that I'm seeing at xAI and from others is pretty impressive.

1401
01:25:03,400 --> 01:25:09,560
You know, we were looking at data around what industry is growing the fastest,

1402
01:25:09,640 --> 01:25:14,880
and especially when we looked at the amount of time consuming movies

1403
01:25:14,960 --> 01:25:20,240
versus time spent on social media, time spent on YouTube.

1404
01:25:20,320 --> 01:25:25,400
What seems to be growing really fast are live events all over again.

1405
01:25:25,480 --> 01:25:27,360
-Going to a physical-- -Yes, actually,

1406
01:25:27,440 --> 01:25:29,200
I think live events--

1407
01:25:29,280 --> 01:25:35,480
When digital media is ubiquitous and you can just have anything digitally

1408
01:25:35,560 --> 01:25:40,680
essentially for free or very close to for free,

1409
01:25:40,760 --> 01:25:44,560
then the scarce commodity will be live events.

1410
01:25:44,640 --> 01:25:45,640
-Yeah. -Yeah.

1411
01:25:45,720 --> 01:25:48,520
Do you think that the premium for that will go up?

1412
01:25:48,600 --> 01:25:50,200
Yeah, I do.

1413
01:25:50,280 --> 01:25:52,040
Is that a good industry to invest in?

1414
01:25:52,120 --> 01:25:57,960
Yes, yes, 'cause that will have more scarcity than anything digital.

1415
01:25:58,040 --> 01:26:00,520
If you were a stock investor, Elon...

1416
01:26:00,600 --> 01:26:01,640
[chuckles drily]

1417
01:26:02,800 --> 01:26:04,160
-What do you mean? -...and you could buy

1418
01:26:04,240 --> 01:26:07,360
one company which is not your own

1419
01:26:07,440 --> 01:26:12,520
at the valuations of today to meet a capitalistic end

1420
01:26:12,600 --> 01:26:15,800
and not an altruistic one, which is good for the world,

1421
01:26:15,880 --> 01:26:17,040
what would you buy?

1422
01:26:22,040 --> 01:26:24,920
I mean, I don't really buy stocks.

1423
01:26:25,000 --> 01:26:28,400
So it's not like... I'm not like an investor in...

1424
01:26:28,480 --> 01:26:32,440
I don't look for things to invest in. I just try to build things.

1425
01:26:33,880 --> 01:26:37,800
And then there happens to be stock of the company that I built.

1426
01:26:39,480 --> 01:26:42,960
But I don't think about, "Should I invest in this company?"

1427
01:26:43,040 --> 01:26:45,160
I don't have a portfolio or anything.

1428
01:26:48,600 --> 01:26:49,640
So...

1429
01:26:51,320 --> 01:26:53,480
I guess, um...

1430
01:26:55,280 --> 01:26:58,200
AI and robotics are gonna be very important.

1431
01:27:01,960 --> 01:27:08,720
So I suppose it would be AI and robotics that, you know, aren't related to me.

1432
01:27:11,440 --> 01:27:15,600
I think Google is gonna be pretty valuable in the future.

1433
01:27:15,680 --> 01:27:20,720
They've laid the groundwork for an immense amount

1434
01:27:20,800 --> 01:27:22,920
of value creation from an AI standpoint.

1435
01:27:25,760 --> 01:27:27,760
Nvidia is obvious at this point.

1436
01:27:30,000 --> 01:27:35,240
I mean, there's an argument that companies that do AI and robotics,

1437
01:27:35,320 --> 01:27:37,560
and maybe space flight...

1438
01:27:37,640 --> 01:27:44,240
are gonna be overwhelmingly all the value, almost all the value.

1439
01:27:45,920 --> 01:27:49,440
So just the output of goods and services from AI and robotics is so high

1440
01:27:49,520 --> 01:27:51,480
that it will dwarf everything else.

1441
01:27:52,640 --> 01:27:55,960
The world seems to be moving to a place

1442
01:27:56,800 --> 01:28:03,240
where everybody loves David and hates Goliath.

1443
01:28:03,320 --> 01:28:04,320
Why?

1444
01:28:06,320 --> 01:28:09,640
I mean, he's the one that got the stone in the forehead.

1445
01:28:09,720 --> 01:28:11,200
Yeah, yeah.

1446
01:28:11,280 --> 01:28:13,360
Honestly, that was just a big mistake.

1447
01:28:13,440 --> 01:28:15,400
He should have, you know--

1448
01:28:15,480 --> 01:28:17,480
You either cover yourself entirely with armour

1449
01:28:17,560 --> 01:28:21,600
and make sure you've got a missile weapon of some kind.

1450
01:28:22,680 --> 01:28:26,560
Otherwise, your opponent is just obviously gonna take a kite-the-boss strategy.

1451
01:28:29,040 --> 01:28:30,080
Just kite the boss.

1452
01:28:30,160 --> 01:28:32,520
I mean, you can run around in a thong with a--

1453
01:28:32,600 --> 01:28:35,160
It doesn't matter, you know? It's never gonna catch you.

1454
01:28:39,880 --> 01:28:42,800
Of all the people, like...

1455
01:28:42,880 --> 01:28:47,560
You're as much at risk of being looked upon as Goliath.

1456
01:28:47,640 --> 01:28:49,320
-Okay. -Especially the weekend after the--

1457
01:28:49,400 --> 01:28:52,440
Hopefully nobody shoots me with a stone in the forehead, you know?

1458
01:28:52,520 --> 01:28:54,000
-Especially after-- -Look, I'm not gonna travel

1459
01:28:54,080 --> 01:28:56,320
around in the desert with too much armour, you know?

1460
01:28:57,800 --> 01:28:59,320
-It's too hot. -Yeah.

1461
01:29:00,440 --> 01:29:01,760
After the last weekend...

1462
01:29:05,640 --> 01:29:07,040
-Yeah. -Yeah.

1463
01:29:12,480 --> 01:29:15,360
Actually, as I think about people in the old days, you know,

1464
01:29:17,560 --> 01:29:19,560
when you were supposed to go into battle with all this armour,

1465
01:29:19,640 --> 01:29:21,320
but it's, like-- let's say it's the middle of summer,

1466
01:29:21,400 --> 01:29:23,760
I mean, it's so hot in that armour!

1467
01:29:23,840 --> 01:29:25,080
You're gonna be, like, sweltering.

1468
01:29:25,160 --> 01:29:27,480
You know, it's like, at a certain point, you're like, 'I'd rather die.

1469
01:29:28,760 --> 01:29:32,400
Do I have to wear this armour full and well in the hot sun?"

1470
01:29:33,880 --> 01:29:35,360
It's like, "I'd rather die."

1471
01:29:37,920 --> 01:29:39,880
That's why the Romans had, you know, the skirts,

1472
01:29:39,960 --> 01:29:42,280
you know, so they could get some air in there.

1473
01:29:47,640 --> 01:29:49,760
You know, let's say that you have to go to the bathroom and you're in armour,

1474
01:29:49,840 --> 01:29:51,360
I mean, it's gonna be pretty difficult.

1475
01:29:53,520 --> 01:29:55,680
What are you gonna do, pause for a minute, take your armour off?

1476
01:29:58,400 --> 01:30:00,240
That's why the Romans had the skirts

1477
01:30:00,320 --> 01:30:03,640
so that it makes, you know, going the bathroom, at least, manageable.

1478
01:30:04,480 --> 01:30:08,200
-You often make jokes. -Me?

1479
01:30:08,960 --> 01:30:10,880
Yeah, I like humour.

1480
01:30:13,640 --> 01:30:15,800
-One could argue that-- -I think we should legalise humour.

1481
01:30:15,880 --> 01:30:16,960
What do you think?

1482
01:30:19,240 --> 01:30:20,720
Controversial stance.

1483
01:30:23,160 --> 01:30:26,320
Is comedy gonna be really hard for AI to get?

1484
01:30:26,400 --> 01:30:27,520
Probably the last thing?

1485
01:30:28,480 --> 01:30:30,080
Grok can be pretty funny.

1486
01:30:30,840 --> 01:30:32,400
Yeah.

1487
01:30:32,480 --> 01:30:34,040
You know what I suspected?

1488
01:30:34,120 --> 01:30:40,480
Like, this is a far off extrapolation, but when I see you make jokes on X

1489
01:30:40,560 --> 01:30:43,800
and on interviews that you do,

1490
01:30:43,880 --> 01:30:45,200
at some point I was like,

1491
01:30:45,280 --> 01:30:49,480
maybe Elon has a model he's running in private and he's testing out comedy.

1492
01:30:50,240 --> 01:30:53,400
'Cause the day that works, he knows it's there.

1493
01:30:55,720 --> 01:30:57,600
Yeah, AI can be pretty funny.

1494
01:30:59,680 --> 01:31:03,720
If you ask Grok to do like a vulgar roast, it'll do a pretty good job.

1495
01:31:05,040 --> 01:31:07,920
If you say even more vulgar and just keep going,

1496
01:31:08,000 --> 01:31:09,720
it's really gonna get next level.

1497
01:31:14,840 --> 01:31:16,320
It's gonna do unspeakable--

1498
01:31:16,400 --> 01:31:18,040
Like, say, vulgar roast yourself on Grok,

1499
01:31:18,120 --> 01:31:19,960
and it's gonna do unspeakable things to you.

1500
01:31:24,000 --> 01:31:25,280
What kind of comedy do you like?

1501
01:31:26,720 --> 01:31:28,720
I guess I like absurdist humour.

1502
01:31:29,880 --> 01:31:31,120
Comedy always had a place--

1503
01:31:31,200 --> 01:31:33,080
Like Monty Python or something like that.

1504
01:31:33,160 --> 01:31:35,280
Comedy always had a place in society

1505
01:31:35,360 --> 01:31:39,880
wherein the role of the jester was so important to every kingdom

1506
01:31:39,960 --> 01:31:45,160
'cause they said things in a funny way that could not be said in a straight way.

1507
01:31:46,200 --> 01:31:48,280
Yeah, I guess so. Maybe we should have more jesters.

1508
01:31:48,360 --> 01:31:49,360
Yeah.

1509
01:31:51,640 --> 01:31:55,080
Is that what you're trying to do when you say something which is a joke?

1510
01:31:55,160 --> 01:31:57,440
Say something you can't when you're not joking about it?

1511
01:31:57,520 --> 01:31:59,720
I just like humour, you know?

1512
01:32:00,360 --> 01:32:02,480
I think we should...

1513
01:32:02,560 --> 01:32:05,160
I like comedy. I think it's funny. People should laugh, you know?

1514
01:32:05,240 --> 01:32:07,600
It's good to generate a few chuckles once in a while.

1515
01:32:07,680 --> 01:32:09,160
-Yeah, yeah. -Yeah.

1516
01:32:09,240 --> 01:32:12,680
I mean, we don't wanna have a humourless society, you know?

1517
01:32:12,760 --> 01:32:14,000
We'd dry.

1518
01:32:14,080 --> 01:32:15,400
When you--

1519
01:32:15,480 --> 01:32:16,720
Dry.

1520
01:32:17,880 --> 01:32:21,000
-When you have a friend, Elon-- -Who, me?

1521
01:32:21,080 --> 01:32:22,720
-Yeah, I mean-- -Are you saying I have a friend?

1522
01:32:25,800 --> 01:32:29,280
When you hang out with your friends, who are you?

1523
01:32:29,360 --> 01:32:31,240
Like, I know there--

1524
01:32:32,480 --> 01:32:34,760
I wish I had friends, you know, honestly.

1525
01:32:34,840 --> 01:32:36,640
No, I do have friends, actually.

1526
01:32:37,800 --> 01:32:40,520
I think so. I hope so.

1527
01:32:41,480 --> 01:32:44,640
Yeah, sure. Yeah, we have a good laugh.

1528
01:32:44,720 --> 01:32:48,200
What does it look like? Like, every group has a dynamic.

1529
01:32:48,280 --> 01:32:49,720
We talk words, you know.

1530
01:32:52,480 --> 01:32:53,680
We eat food, sometimes.

1531
01:32:57,240 --> 01:32:59,000
You know, once in a while, we swim in the pool.

1532
01:32:59,080 --> 01:33:01,080
You know, normal things, I think.

1533
01:33:01,160 --> 01:33:04,480
There's a limit as to what are things one can do with friends, you know?

1534
01:33:04,560 --> 01:33:05,720
Chat.

1535
01:33:08,120 --> 01:33:11,880
Discuss, you know, the nature of the universe.

1536
01:33:14,360 --> 01:33:16,800
What do you emotionally get out of friendship?

1537
01:33:19,880 --> 01:33:20,960
I don't know.

1538
01:33:21,040 --> 01:33:23,240
I think the same thing anyone else would get out of friendship, you know?

1539
01:33:25,840 --> 01:33:28,640
You wanna have, like, an emotional connection with other people.

1540
01:33:29,800 --> 01:33:32,880
And, um, you wanna--

1541
01:33:32,960 --> 01:33:37,400
I don't know, you wanna talk about various subjects.

1542
01:33:42,640 --> 01:33:46,440
Yeah, I mean, I generally talk about,

1543
01:33:46,520 --> 01:33:49,520
I mean, a wide range of things, about the nature of the universe.

1544
01:33:49,600 --> 01:33:53,360
I mean, a lot of philosophical discussions.

1545
01:33:57,280 --> 01:33:59,840
You know, although, we have come to the conclusion

1546
01:33:59,920 --> 01:34:04,200
that we should not talk about, um...

1547
01:34:04,280 --> 01:34:07,520
AI or the simulation at parties,

1548
01:34:07,600 --> 01:34:10,360
because we just talk about it too much.

1549
01:34:11,920 --> 01:34:13,800
-You know, Aristotle-- -It's kind of a buzzkill at times.

1550
01:34:15,720 --> 01:34:16,920
So...

1551
01:34:17,680 --> 01:34:20,400
I can't remember who it was, Aristotle or Plato.

1552
01:34:20,480 --> 01:34:22,560
They had a framework

1553
01:34:22,640 --> 01:34:27,240
for how to pick a friend based on respect and mutual admiration,

1554
01:34:27,320 --> 01:34:29,600
but people don't pick friends like that.

1555
01:34:31,760 --> 01:34:34,640
Even me, I feel like I pick...

1556
01:34:36,880 --> 01:34:38,840
my friends

1557
01:34:38,920 --> 01:34:44,680
based on people who say and think

1558
01:34:44,760 --> 01:34:47,760
-in a manner that I can resonate with. -Sure.

1559
01:34:47,840 --> 01:34:52,720
I wouldn't pick a far-out-there, contrarian-to-my-own-belief-systems

1560
01:34:52,800 --> 01:34:55,400
as a friend, because it would get tiring.

1561
01:34:55,480 --> 01:34:57,680
Hanging out would get tiring. Are you like that?

1562
01:34:57,760 --> 01:34:59,400
Do you pick friends who think like you,

1563
01:34:59,480 --> 01:35:03,320
or do you look for the one who can debate you and be a contrarian to you?

1564
01:35:03,400 --> 01:35:06,840
Well, I'm not, sort of, you know, going on like friendhunter.com...

1565
01:35:06,920 --> 01:35:10,240
-Friendfinder.com -[laughter]

1566
01:35:10,320 --> 01:35:11,800
...to hunt down some friends.

1567
01:35:15,560 --> 01:35:18,200
It's sort of, yeah-- I mean, I think it is just

1568
01:35:18,280 --> 01:35:21,120
sort of people that you've resonated with somewhat...

1569
01:35:22,360 --> 01:35:24,960
on an emotional and intellectual level.

1570
01:35:25,040 --> 01:35:27,960
And, yeah, I mean, yeah.

1571
01:35:29,560 --> 01:35:32,440
You know? And I guess a friend is someone

1572
01:35:32,520 --> 01:35:36,280
who's gonna support you in difficult times, I suppose.

1573
01:35:36,360 --> 01:35:38,520
A friend in need is a friend indeed.

1574
01:35:38,600 --> 01:35:41,880
Like, if someone's still supporting you

1575
01:35:41,960 --> 01:35:44,800
when the chips are down, that's a friend, you know.

1576
01:35:44,880 --> 01:35:50,560
If somebody's not supporting you, or if somebody's only--

1577
01:35:50,640 --> 01:35:54,760
Like, fair-weather friends are useless, you know, they're not real friends.

1578
01:35:57,240 --> 01:35:59,080
Like everyone likes you when the chips are up,

1579
01:35:59,160 --> 01:36:01,240
but who likes you when the chips are down?

1580
01:36:01,320 --> 01:36:02,560
That's a friend.

1581
01:36:02,640 --> 01:36:04,600
With someone who has as many chips as you, would it matter?

1582
01:36:05,480 --> 01:36:06,960
I mean, it's relative, you know.

1583
01:36:08,640 --> 01:36:10,720
-With that particular thing-- -It's not just a chips thing.

1584
01:36:10,800 --> 01:36:12,560
It's just like a--

1585
01:36:13,680 --> 01:36:14,960
Yeah, I mean--

1586
01:36:17,000 --> 01:36:18,080
There's this, sort of...

1587
01:36:20,520 --> 01:36:23,040
Popularity waxes and wanes, you know?

1588
01:36:23,880 --> 01:36:26,880
This is interesting. Does it wax and wane...

1589
01:36:29,720 --> 01:36:32,080
only by virtue of the number of chips,

1590
01:36:32,160 --> 01:36:38,680
or also by virtue of proximity to power and which one is bigger of the two?

1591
01:36:47,920 --> 01:36:49,520
I don't know, like, what is power, you know?

1592
01:36:49,600 --> 01:36:51,800
Like, power to do what?

1593
01:36:51,880 --> 01:36:55,320
I would think in the traditional sense, elected power.

1594
01:36:55,400 --> 01:36:58,880
-Position. -You mean how many gigawatts or whatever?

1595
01:37:01,040 --> 01:37:02,680
More like how many volts.

1596
01:37:02,760 --> 01:37:05,280
Yeah, like-- It's a voltage and amperage, you know.

1597
01:37:06,960 --> 01:37:08,320
Don't touch the wires.

1598
01:37:11,520 --> 01:37:13,440
Don't put a fork in the power outlet.

1599
01:37:15,920 --> 01:37:18,600
You'll get a real feeling for power if you do that.

1600
01:37:23,120 --> 01:37:24,360
Fair.

1601
01:37:30,120 --> 01:37:32,080
Yeah, it's gonna be very visceral, yeah.

1602
01:37:34,440 --> 01:37:36,000
[mimics electricity zapping]

1603
01:37:44,120 --> 01:37:46,440
I know you like Nietzsche and Schopenhauer and--

1604
01:37:46,520 --> 01:37:48,480
Well, I've read their books, yeah, sure--

1605
01:37:48,560 --> 01:37:51,400
You spoke about how your childhood was...

1606
01:37:53,400 --> 01:37:55,440
Yeah, I was just trying to find answers to the meaning of life,

1607
01:37:55,520 --> 01:37:56,840
when I had, like, an existential crisis,

1608
01:37:56,920 --> 01:38:00,040
and I don't know when I was, like, 12 or 13 or something.

1609
01:38:00,120 --> 01:38:03,400
-They speak about the will to power. -Sure.

1610
01:38:06,240 --> 01:38:08,800
Nietzsche said a lot of controversial things, you know.

1611
01:38:08,880 --> 01:38:10,320
He was, sort of...

1612
01:38:10,400 --> 01:38:13,240
I think he was, I mean, a bit of a troll, if you ask me.

1613
01:38:16,120 --> 01:38:17,360
Troll, how?

1614
01:38:17,440 --> 01:38:20,800
I mean, he'd just say controversial things to get a rise out of people.

1615
01:38:22,240 --> 01:38:24,120
He lived a miserable life and died early.

1616
01:38:24,200 --> 01:38:25,200
-Did he? -Yeah.

1617
01:38:25,280 --> 01:38:27,240
Well, who says he lived a miserable life?

1618
01:38:27,320 --> 01:38:29,640
-His sister, I think. -Okay, well, maybe she didn't like him.

1619
01:38:33,280 --> 01:38:35,720
No, I think he got sick and he died, he got a disease.

1620
01:38:35,800 --> 01:38:37,960
-I mean, allegedly syphilis or something. -Yeah.

1621
01:38:41,040 --> 01:38:43,400
But there's only one way to get that, you know.

1622
01:38:46,920 --> 01:38:49,320
So he might have had some fun along the way.

1623
01:38:53,480 --> 01:38:55,440
I did want to ask you this.

1624
01:38:57,000 --> 01:38:59,440
Milton Friedman speaks about the pencil.

1625
01:38:59,520 --> 01:39:01,200
What? Why?

1626
01:39:05,840 --> 01:39:07,600
Why does he go on about pencils?

1627
01:39:08,720 --> 01:39:10,480
I have to say that after Nietzsche and syphilis.

1628
01:39:12,880 --> 01:39:16,640
Why does Milton Friedman keep talking about pencils?

1629
01:39:16,720 --> 01:39:18,720
There he goes again with the pencils.

1630
01:39:21,120 --> 01:39:22,120
He won't stop.

1631
01:39:23,400 --> 01:39:26,920
I swear to God, if I hear "Milton talks about pencil" one more time,

1632
01:39:28,040 --> 01:39:29,280
I'm gonna lose my mind.

1633
01:39:32,960 --> 01:39:35,040
He's just rabbiting on about pencils all day.

1634
01:39:41,360 --> 01:39:42,800
Didn't even mention crayons.

1635
01:39:46,400 --> 01:39:49,640
What I find interesting about his pencil argument.

1636
01:39:52,280 --> 01:39:53,880
-Yeah? -He's--

1637
01:39:53,960 --> 01:39:56,640
Yeah, yeah, no, it's very difficult to make a pencil, you know.

1638
01:39:56,720 --> 01:39:57,800
In one place.

1639
01:39:57,880 --> 01:39:59,680
Think of all the things you have to do to make a pencil.

1640
01:39:59,760 --> 01:40:01,760
Yeah, like the lead comes from a country,

1641
01:40:01,840 --> 01:40:04,600
the wood comes from another country, the rubber from another.

1642
01:40:04,680 --> 01:40:08,880
You've always been against tariffs, but...

1643
01:40:08,960 --> 01:40:10,480
Yeah, I mean, I think,

1644
01:40:10,560 --> 01:40:14,320
generally free trade is better, is more efficient, you know.

1645
01:40:14,400 --> 01:40:19,320
Tariffs tend to create distortions in, you know, markets.

1646
01:40:21,440 --> 01:40:25,640
And generally, like, you think about any given thing.

1647
01:40:25,720 --> 01:40:27,600
Say like, would you want tariffs between you

1648
01:40:27,680 --> 01:40:29,920
and everyone else at an individual level?

1649
01:40:30,000 --> 01:40:31,720
That would make life very difficult.

1650
01:40:31,800 --> 01:40:33,920
Would you want tariffs between each city?

1651
01:40:34,000 --> 01:40:36,520
No, that would be very annoying.

1652
01:40:36,600 --> 01:40:39,480
Would you want tariffs between each state within the United States?

1653
01:40:39,560 --> 01:40:42,560
Like, no, that would be disastrous for the economy.

1654
01:40:43,720 --> 01:40:45,400
So then why do you want tariffs between countries?

1655
01:40:46,600 --> 01:40:48,200
-I agree. -Yeah.

1656
01:40:51,920 --> 01:40:54,480
How do you think this plays out? What happens next?

1657
01:40:54,560 --> 01:40:56,280
-What, with tariffs? Or what? -Yeah.

1658
01:40:57,440 --> 01:41:01,960
I mean, the President has made it clear he loves tariffs.

1659
01:41:02,960 --> 01:41:06,480
You know, I've tried to dissuade him from this point of view, but unsuccessfully.

1660
01:41:06,560 --> 01:41:07,840
Yeah.

1661
01:41:07,920 --> 01:41:09,320
-Fair. -Yeah.

1662
01:41:10,560 --> 01:41:14,520
The relationship between business and politics.

1663
01:41:15,520 --> 01:41:18,880
I was having this conversation with someone and we were thinking,

1664
01:41:18,960 --> 01:41:20,840
which is the last--

1665
01:41:20,920 --> 01:41:24,960
How many large, really big, profitable businesses

1666
01:41:25,040 --> 01:41:29,840
have been built in the last few decades without access to politics?

1667
01:41:29,920 --> 01:41:33,720
-And... -Um, okay.

1668
01:41:33,800 --> 01:41:37,000
Like, I don't know. Probably a lot, I don't know.

1669
01:41:37,080 --> 01:41:38,680
-Not everything needs politics. -Yeah.

1670
01:41:38,760 --> 01:41:42,000
I think, once you get to a certain scale, politics finds you.

1671
01:41:42,080 --> 01:41:43,440
Yeah.

1672
01:41:45,600 --> 01:41:46,920
It's quite unpleasant.

1673
01:41:47,000 --> 01:41:48,680
I was reading--

1674
01:41:48,760 --> 01:41:52,320
I was reading this book about Michelangelo, and he's--

1675
01:41:52,400 --> 01:41:53,520
The Teenage Mutant Ninja Turtles?

1676
01:41:56,200 --> 01:41:57,640
I used to watch that when I was a kid.

1677
01:41:57,720 --> 01:41:59,320
-And I still love it. -It's quite compelling.

1678
01:41:59,400 --> 01:42:01,240
Yeah, yeah, I used to love it.

1679
01:42:01,320 --> 01:42:05,400
Michelangelo, Leonardo, Raphael, and who's the fourth one?

1680
01:42:05,480 --> 01:42:06,800
-Donatello. -Yes.

1681
01:42:06,880 --> 01:42:09,960
-Yeah. -No, but about the sculptor, the artist.

1682
01:42:12,800 --> 01:42:15,240
And when he was sculpting David,

1683
01:42:15,320 --> 01:42:18,280
a politician comes up to him and says, the nose is too big.

1684
01:42:19,720 --> 01:42:21,280
So you know what Michelangelo does?

1685
01:42:21,360 --> 01:42:22,760
Total power?

1686
01:42:25,880 --> 01:42:29,080
So Michelangelo pretended to work from his scaffolding

1687
01:42:29,160 --> 01:42:32,120
and threw some dust down, but didn't change anything.

1688
01:42:32,200 --> 01:42:36,120
And he said, "Okay, done." And the politician walked away happy.

1689
01:42:36,720 --> 01:42:38,880
Is that how you deal with politics, sometimes?

1690
01:42:42,760 --> 01:42:45,480
You know, I've generally found that when I get involved in politics,

1691
01:42:45,560 --> 01:42:47,000
it ends up badly.

1692
01:42:53,080 --> 01:42:57,920
So then I'm like, you know, "Probably shouldn't do that."

1693
01:42:58,000 --> 01:43:00,760
"I should do less of that", is my conclusion.

1694
01:43:00,840 --> 01:43:02,800
Do you think that's true for all businessmen?

1695
01:43:02,800 --> 01:43:04,680
Yeah, probably, yeah, yeah.

1696
01:43:07,160 --> 01:43:09,040
Yeah, I mean,

1697
01:43:09,040 --> 01:43:11,040
politics is a blood sport, you know?

1698
01:43:11,040 --> 01:43:14,320
It's like you enter politics, they're gonna go for the jugular.

1699
01:43:15,440 --> 01:43:19,800
So best to avoid politics where possible.

1700
01:43:21,280 --> 01:43:23,800
What did DOGE teach you, if you learnt one thing?

1701
01:43:24,360 --> 01:43:27,120
Well, it was like a very interesting side quest, you know,

1702
01:43:27,200 --> 01:43:31,840
'cause I just got to see like a lot of inner workings of the government.

1703
01:43:33,680 --> 01:43:39,160
And, you know, there's been quite a few efficiencies.

1704
01:43:39,240 --> 01:43:41,480
I mean, some of them are very basic efficiencies,

1705
01:43:41,560 --> 01:43:45,320
like just adding in requirements for federal payments,

1706
01:43:45,400 --> 01:43:51,520
that any given payment must have an assigned congressional payment code

1707
01:43:51,600 --> 01:43:54,640
and a comment field with something in it that's more than nothing.

1708
01:43:55,920 --> 01:44:00,720
Like, that trivial seeming change, my guess is,

1709
01:44:00,800 --> 01:44:05,440
probably saves $100 billion or even $200 billion a year.

1710
01:44:05,520 --> 01:44:09,280
Because there were massive numbers of payments

1711
01:44:09,360 --> 01:44:12,080
that were going out with no congressional payment code

1712
01:44:12,160 --> 01:44:13,680
and with nothing in the comment field,

1713
01:44:13,760 --> 01:44:16,280
which makes auditing the payments impossible.

1714
01:44:16,360 --> 01:44:19,240
So if you have to say like, why can the Defense Department--

1715
01:44:19,320 --> 01:44:21,920
Or now the Department of War-- Why can it not pass an audit?

1716
01:44:22,000 --> 01:44:23,840
It's because the information is not there.

1717
01:44:23,920 --> 01:44:25,080
It doesn't have--

1718
01:44:25,160 --> 01:44:29,560
The information necessary to pass an audit does not exist, is the issue.

1719
01:44:29,640 --> 01:44:34,400
So a bunch of things that DOGE did were just very common sense,

1720
01:44:35,520 --> 01:44:38,680
things that would be normal for any organisation

1721
01:44:38,760 --> 01:44:41,320
that cared about financial responsibility.

1722
01:44:41,400 --> 01:44:43,800
That's most of what was done.

1723
01:44:46,160 --> 01:44:49,200
You know, and it's still going on, by the way.

1724
01:44:49,280 --> 01:44:51,640
DOGE is still happening.

1725
01:44:51,720 --> 01:44:56,360
But it turns out, when you stop fraudulent and wasteful payments,

1726
01:44:56,440 --> 01:45:01,440
the fraudsters don't confess to this.

1727
01:45:01,520 --> 01:45:04,560
They actually start yelling all sorts of nonsense that,

1728
01:45:04,640 --> 01:45:08,520
"You're stopping essential payments to needy people."

1729
01:45:09,800 --> 01:45:11,200
But actually, you're not.

1730
01:45:12,480 --> 01:45:14,160
We get this thing like saying,

1731
01:45:14,240 --> 01:45:17,680
"Oh, you've got to send this thing for whatever."

1732
01:45:17,760 --> 01:45:20,080
It'd be like, "This is going to children in Africa."

1733
01:45:20,160 --> 01:45:22,600
And I'm like, "Yeah, but then why are the wiring instructions

1734
01:45:22,680 --> 01:45:26,840
for Deloitte & Touche in Washington, D.C.? Because that's not Africa."

1735
01:45:30,480 --> 01:45:35,440
"So can you please connect us with the recipients of this money in Africa?"

1736
01:45:36,440 --> 01:45:38,560
And then we get silence.

1737
01:45:38,640 --> 01:45:44,320
Like, okay, we just want to literally talk to the recipients, that's it.

1738
01:45:45,640 --> 01:45:48,440
Then we're like, "Oh, no, it turns out, for some reason, we can't talk to them."

1739
01:45:48,520 --> 01:45:50,320
Like, "Well, we're not going to send the money

1740
01:45:50,400 --> 01:45:53,280
unless we can talk to the recipients and confirm they will actually get it."

1741
01:45:55,720 --> 01:45:56,880
You know...

1742
01:45:56,960 --> 01:45:58,760
But, you know, that sort of...

1743
01:46:00,200 --> 01:46:05,560
Fraudsters necessarily will come up with a very...

1744
01:46:05,640 --> 01:46:08,040
you know, sympathetic argument.

1745
01:46:08,120 --> 01:46:10,160
They're not going to say, "Give us the money for fraud."

1746
01:46:10,240 --> 01:46:12,320
That's not going to be what they say, obviously.

1747
01:46:12,400 --> 01:46:14,040
They're going to try to make

1748
01:46:14,120 --> 01:46:16,160
these sympathetic sounding arguments that are false.

1749
01:46:16,240 --> 01:46:19,880
-They're going to start an NGO and then-- -Yeah, they're going to see NGO--

1750
01:46:19,960 --> 01:46:23,840
It's going to be like "Save the Baby Pandas" NGO,

1751
01:46:23,920 --> 01:46:26,400
which is like, who doesn't want to save the baby pandas? They're adorable.

1752
01:46:27,720 --> 01:46:32,120
But then, it turns out no pandas are being saved, okay, in this thing.

1753
01:46:32,200 --> 01:46:36,000
It's just going to a bunch of-- It's just corruption, essentially.

1754
01:46:37,440 --> 01:46:39,680
And you're like, "Well, can you send us a picture of the panda?"

1755
01:46:39,760 --> 01:46:41,120
They're like, "No." Okay.

1756
01:46:42,040 --> 01:46:45,200
How do we know it's going to the pandas then?

1757
01:46:45,280 --> 01:46:46,800
That's what I'm saying.

1758
01:46:46,880 --> 01:46:48,760
What do you think of philanthropy?

1759
01:46:49,400 --> 01:46:51,440
Yeah, I think we should...

1760
01:46:51,520 --> 01:46:54,720
Well, I mean, I agree with love of humanity.

1761
01:46:54,800 --> 01:46:59,440
And I think we should try to do things that help our fellow human beings.

1762
01:47:01,120 --> 01:47:02,520
But it's very hard.

1763
01:47:02,600 --> 01:47:05,120
Like, if you care about the reality of goodness

1764
01:47:05,200 --> 01:47:08,000
rather than simply the perception of it,

1765
01:47:08,080 --> 01:47:10,720
it's very difficult to give away money well.

1766
01:47:12,200 --> 01:47:14,520
So I have a large foundation, but I don't put my name on it.

1767
01:47:14,600 --> 01:47:18,880
And I don't, you know... In fact, I say, "I don't want my name on anything."

1768
01:47:20,320 --> 01:47:22,720
But the biggest challenge I find with my foundation

1769
01:47:22,800 --> 01:47:26,360
is try to give money away in a way that is truly beneficial to people.

1770
01:47:27,120 --> 01:47:30,400
It's very easy to give money away to get the appearance of goodness.

1771
01:47:30,480 --> 01:47:33,680
It is very difficult to give money away for the reality of goodness.

1772
01:47:34,880 --> 01:47:36,160
Very difficult.

1773
01:47:39,080 --> 01:47:42,880
For a long time, the US had a lot of immigration,

1774
01:47:42,960 --> 01:47:45,240
like really smart people coming into the country.

1775
01:47:45,320 --> 01:47:47,080
-Yes. -We, back home in India,

1776
01:47:47,160 --> 01:47:49,400
called it the "brain drain."

1777
01:47:49,480 --> 01:47:54,960
All our Indian-origin CEOs in Western companies.

1778
01:47:55,040 --> 01:47:57,400
Yes, I think America has benefitted immensely

1779
01:47:57,480 --> 01:48:00,480
from talented Indians that have come to America.

1780
01:48:00,560 --> 01:48:02,240
That seems to be changing now, though.

1781
01:48:04,920 --> 01:48:05,960
Yeah, I mean.

1782
01:48:06,040 --> 01:48:09,360
Yeah, America has been an immense beneficiary of talent from India.

1783
01:48:09,440 --> 01:48:14,160
Yeah. Why has that narrative changed of late?

1784
01:48:14,240 --> 01:48:18,280
And America seems to have become anti-immigration to a certain extent.

1785
01:48:18,360 --> 01:48:19,920
Like, I was passing immigration,

1786
01:48:20,000 --> 01:48:22,920
and I was worried if they'd stopped me a couple of days ago.

1787
01:48:24,160 --> 01:48:27,640
Well, I think there's different schools of thought.

1788
01:48:27,720 --> 01:48:32,400
It's not like unanimous, but, you know, under the Biden administration,

1789
01:48:32,480 --> 01:48:35,840
it was basically a total free-for-all with, like, no border controls,

1790
01:48:35,920 --> 01:48:38,640
which unless you've got border controls, you're not a country.

1791
01:48:40,120 --> 01:48:44,640
So you had massive amounts of illegal immigration under Biden.

1792
01:48:45,840 --> 01:48:50,760
And it actually also had like somewhat of a negative selection effect.

1793
01:48:51,800 --> 01:48:56,440
So, if there's a massive financial incentive

1794
01:48:56,520 --> 01:49:00,000
to come to the US illegally and get all these government benefits,

1795
01:49:02,160 --> 01:49:05,040
then you're gonna necessarily create

1796
01:49:05,120 --> 01:49:07,440
a diffusion gradient for people to come to the US

1797
01:49:07,520 --> 01:49:08,840
It's an incentive structure.

1798
01:49:10,440 --> 01:49:15,320
And so, I think, that obviously made no sense.

1799
01:49:15,400 --> 01:49:18,560
Like, you gotta have border controls. That's kind of ridiculous not to.

1800
01:49:20,080 --> 01:49:21,640
Then that's...

1801
01:49:21,720 --> 01:49:26,440
So, the left wants to basically have open borders, no holds barred.

1802
01:49:26,520 --> 01:49:29,320
You know, it doesn't matter if someone-- what their situation is,

1803
01:49:29,400 --> 01:49:31,800
they could be a criminal, it doesn't matter.

1804
01:49:31,880 --> 01:49:34,240
Then on the right, you've got, you know,

1805
01:49:36,320 --> 01:49:40,120
at least a perception that somehow their jobs

1806
01:49:40,200 --> 01:49:43,240
are being taken by talented people from other countries.

1807
01:49:45,120 --> 01:49:47,120
I don't know how real that is.

1808
01:49:49,160 --> 01:49:53,360
My direct observation is that there's always a scarcity of talented people.

1809
01:49:54,160 --> 01:49:57,520
So, from my standpoint, I'm like,

1810
01:49:57,600 --> 01:50:00,440
"We have a lot of difficulty finding enough talented people

1811
01:50:00,520 --> 01:50:02,680
to get these difficult tasks done.

1812
01:50:02,760 --> 01:50:05,360
And so more talented people would be good."

1813
01:50:07,840 --> 01:50:09,480
But I guess, some companies out there,

1814
01:50:09,560 --> 01:50:12,960
it's sort of, they're making it more of a cost thing,

1815
01:50:13,040 --> 01:50:14,920
where it's like, okay, if they can employ someone

1816
01:50:15,000 --> 01:50:19,920
for a fraction of the cost of an American citizen,

1817
01:50:20,000 --> 01:50:24,920
then, I guess, these other companies would hire people just to save costs.

1818
01:50:25,760 --> 01:50:28,880
But, at my companies, the issue is we just are trying

1819
01:50:28,960 --> 01:50:31,400
to get the most talented people in the world.

1820
01:50:31,480 --> 01:50:34,880
And we pay way above average.

1821
01:50:34,960 --> 01:50:37,120
So I can't say--

1822
01:50:37,200 --> 01:50:38,840
So, that's not my experience.

1823
01:50:38,920 --> 01:50:41,480
But that's what a lot of people do complain about.

1824
01:50:42,400 --> 01:50:48,480
And I think there's been some misuse of the H-1B Program.

1825
01:50:50,320 --> 01:50:51,680
It would be accurate to say that

1826
01:50:51,760 --> 01:50:54,520
there's, like, some of the outsourcing companies

1827
01:50:54,600 --> 01:50:59,520
have kind of gamed the system on the H-1B front.

1828
01:50:59,600 --> 01:51:02,400
And we need to stop the gaming of the system, you know?

1829
01:51:04,240 --> 01:51:06,760
But I'm certainly not in the school of thought

1830
01:51:06,840 --> 01:51:08,920
that we should shut down the H-1B Program.

1831
01:51:09,000 --> 01:51:11,080
That's-- Which some on the right are.

1832
01:51:12,480 --> 01:51:15,400
I think they don't realise that that would actually be very bad.

1833
01:51:17,200 --> 01:51:20,720
If you could speak to the people of my country, India,

1834
01:51:20,800 --> 01:51:23,800
the young entrepreneurs who want to build...

1835
01:51:23,880 --> 01:51:25,560
-Right. -...and say a message to them,

1836
01:51:25,640 --> 01:51:26,680
what would you say?

1837
01:51:32,920 --> 01:51:35,720
I'm a big fan of anyone who wants to build.

1838
01:51:35,800 --> 01:51:37,760
So I think anyone who wants to...

1839
01:51:40,360 --> 01:51:43,520
you know, make more than they take, has my respect.

1840
01:51:43,600 --> 01:51:47,800
So that's the main thing you should aim for,

1841
01:51:47,880 --> 01:51:49,720
aim to make more than you take.

1842
01:51:51,080 --> 01:51:55,600
Be a, you know, a net contributor to society.

1843
01:51:59,280 --> 01:52:02,240
And it's kind of like the pursuit of happiness.

1844
01:52:02,320 --> 01:52:06,400
You know, if you want to create something valuable financially,

1845
01:52:06,480 --> 01:52:08,560
you don't pursue that.

1846
01:52:08,640 --> 01:52:13,920
It's best to actually pursue providing useful products and services.

1847
01:52:14,000 --> 01:52:18,520
If you do that, then money will come as a natural consequence of that,

1848
01:52:18,600 --> 01:52:21,040
as opposed to pursuing money directly.

1849
01:52:21,120 --> 01:52:23,160
Just like you can't, sort of, pursue happiness directly,

1850
01:52:23,240 --> 01:52:25,800
you pursue things that lead to happiness.

1851
01:52:25,880 --> 01:52:29,400
But there's not like direct happiness pursuit.

1852
01:52:29,480 --> 01:52:30,920
You do things like...

1853
01:52:32,920 --> 01:52:37,080
I guess, fulfilling work, or study, or friends, loved ones,

1854
01:52:38,560 --> 01:52:41,720
that, as a result, make you happy.

1855
01:52:42,720 --> 01:52:45,120
So, it sounds very obvious...

1856
01:52:48,600 --> 01:52:51,000
but, generally, if somebody's trying to make a company work,

1857
01:52:51,080 --> 01:52:52,960
they should expect to grind super hard,

1858
01:52:53,800 --> 01:52:57,080
accept that there's, like, some meaningful chance of failure,

1859
01:52:59,360 --> 01:53:06,360
but just be focused on having the output be worth more than the input.

1860
01:53:07,920 --> 01:53:09,720
That are you a value creator?

1861
01:53:11,080 --> 01:53:12,880
That's what really matters.

1862
01:53:16,360 --> 01:53:18,600
Making more than you take.

1863
01:53:18,680 --> 01:53:20,560
I think that's a good way to end this.

1864
01:53:20,640 --> 01:53:23,120
-Lauren is asking us to wrap up. -All right.

1865
01:53:24,560 --> 01:53:31,320
I also like to take the opportunity to thank my friend, Manoj, in IGF.

1866
01:53:31,400 --> 01:53:37,120
He does a great job of connecting, I think, Indians, like the group here,

1867
01:53:37,200 --> 01:53:40,280
with people like you, in order to...

1868
01:53:41,760 --> 01:53:44,560
of many things, I think, get to know each other and become friends,

1869
01:53:44,640 --> 01:53:47,520
because once we are friends, maybe we can start working together.

1870
01:53:48,760 --> 01:53:51,880
So, thank you, Manoj, for putting this whole thing together, and thank you, IGF.

1871
01:53:51,960 --> 01:53:53,480
[audience applauding]

1872
01:53:53,560 --> 01:53:55,440
And, thank you so much, Elon, for taking the time.

1873
01:53:55,520 --> 01:53:56,800
You're welcome.

1874
01:54:00,240 --> 01:54:03,360
-Did you have fun? Was it boring? -Yeah, it was an interesting conversation.

1875
01:54:03,440 --> 01:54:06,400
Sometimes, they take these answers out of context.

1876
01:54:06,480 --> 01:54:09,720
But... I think it was a good conversation.

