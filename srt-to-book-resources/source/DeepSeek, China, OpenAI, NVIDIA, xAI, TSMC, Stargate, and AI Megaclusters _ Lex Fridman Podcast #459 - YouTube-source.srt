1
00:00:00,000 --> 00:00:01,378
- The following is a conversation

2
00:00:01,378 --> 00:00:04,548
with Dylan Patel and Nathan Lambert.

3
00:00:04,548 --> 00:00:09,172
Dylan runs SemiAnalysis, a well-respected research

4
00:00:09,172 --> 00:00:12,931
and analysis company that specializes in semiconductors,

5
00:00:12,931 --> 00:00:16,857
GPUs, CPUs, and AI hardware in general.

6
00:00:16,857 --> 00:00:21,311
Nathan is a research scientist at the Allen Institute for AI

7
00:00:21,311 --> 00:00:22,352
and is the author

8
00:00:22,352 --> 00:00:26,269
of the amazing blog on AI called Interconnects.

9
00:00:27,548 --> 00:00:29,909
They are both highly respected, read,

10
00:00:29,909 --> 00:00:32,509
and listened to by the experts, researchers,

11
00:00:32,509 --> 00:00:34,986
and engineers in the field of AI.

12
00:00:34,986 --> 00:00:38,489
And personally, I'm just a fan of the two of them.

13
00:00:38,489 --> 00:00:41,103
So, I used the DeepSeek moment

14
00:00:41,103 --> 00:00:43,562
that shook the AI world a bit

15
00:00:43,562 --> 00:00:48,151
as an opportunity to sit down with them and lay it all out.

16
00:00:48,151 --> 00:00:51,324
From DeepSeek, OpenAI, Google xAI, Meta, Anthropic,

17
00:00:51,324 --> 00:00:55,907
to Nvidia and DSMC, and to US, China, Taiwan relations,

18
00:00:57,523 --> 00:00:58,774
and everything else

19
00:00:58,774 --> 00:01:01,729
that is happening at the cutting edge of AI.

20
00:01:01,729 --> 00:01:03,976
This conversation is a deep dive

21
00:01:03,976 --> 00:01:08,205
into many critical aspects of the AI industry.

22
00:01:08,205 --> 00:01:12,414
While it does get super technical, we try to make sure

23
00:01:12,414 --> 00:01:15,589
that it's still accessible to folks outside of the AI field

24
00:01:15,589 --> 00:01:19,401
by defining terms, stating important concepts explicitly,

25
00:01:19,401 --> 00:01:22,852
spelling out acronyms, and in general, always moving across

26
00:01:22,852 --> 00:01:26,562
the several layers of abstraction and levels of detail.

27
00:01:26,562 --> 00:01:28,762
There is a lot of hype

28
00:01:28,762 --> 00:01:32,651
in the media about what AI is and isn't.

29
00:01:32,651 --> 00:01:33,839
The purpose of this podcast

30
00:01:33,839 --> 00:01:36,793
in part is to cut through the hype,

31
00:01:36,793 --> 00:01:38,325
through the bullshit,

32
00:01:38,325 --> 00:01:41,338
and the low resolution analysis,

33
00:01:41,338 --> 00:01:44,003
and to discuss in detail how stuff works

34
00:01:44,003 --> 00:01:46,731
and what the implications are.

35
00:01:46,731 --> 00:01:48,336
Let me also, if I may comment

36
00:01:48,336 --> 00:01:52,094
on the new OpenAI o3-mini reasoning model.

37
00:01:52,094 --> 00:01:54,058
The release of which we were anticipating

38
00:01:54,058 --> 00:01:55,269
during the conversation,

39
00:01:55,269 --> 00:01:58,534
and it did indeed come out right after.

40
00:01:58,534 --> 00:01:59,990
Its capabilities and costs

41
00:01:59,990 --> 00:02:03,823
are on par with our expectations as we stated.

42
00:02:05,030 --> 00:02:07,918
OpenAI o3-mini is indeed a great model,

43
00:02:07,918 --> 00:02:11,316
but it should be stated that DeeSeek-R1

44
00:02:11,316 --> 00:02:14,315
has similar performance on benchmarks is still cheaper

45
00:02:14,315 --> 00:02:17,637
and it reveals its chain of thought reasoning,

46
00:02:17,637 --> 00:02:19,587
which o3-mini does not.

47
00:02:19,587 --> 00:02:22,784
It only shows a summary of the reasoning.

48
00:02:22,784 --> 00:02:26,367
Plus, R1 is open-weight and o3-mini is not.

49
00:02:29,266 --> 00:02:32,276
By the way, I got a chance to play with o3-mini.

50
00:02:32,276 --> 00:02:36,526
And anecdotal vibe check wise, I felt that o3-mini,

51
00:02:37,854 --> 00:02:42,292
specifically o3-mini-high is better than R1.

52
00:02:42,292 --> 00:02:43,363
Still for me personally,

53
00:02:43,363 --> 00:02:45,850
I find that Claude Sonnet 3.5

54
00:02:45,850 --> 00:02:47,599
is the best model for programming,

55
00:02:47,599 --> 00:02:48,680
except for tricky cases,

56
00:02:48,680 --> 00:02:52,282
where I will use o1 Pro to brainstorm.

57
00:02:52,282 --> 00:02:55,400
Either way, many more better AI models will come,

58
00:02:55,400 --> 00:02:56,963
including reasoning models

59
00:02:56,963 --> 00:03:00,695
both from American and Chinese companies.

60
00:03:00,695 --> 00:03:03,782
They'll continue to shift the cost curve.

61
00:03:03,782 --> 00:03:07,912
But the quote, DeepSeek moment is indeed real.

62
00:03:07,912 --> 00:03:10,684
I think it will still be remembered five years from now

63
00:03:10,684 --> 00:03:13,093
as a pivotal event in tech history,

64
00:03:13,093 --> 00:03:15,893
due in part to the geopolitical implications,

65
00:03:15,893 --> 00:03:19,678
but for other reasons to, as we discuss in detail

66
00:03:19,678 --> 00:03:22,683
from many perspectives in this conversation.

67
00:03:22,683 --> 00:03:24,340
This is the "Lex Fridman Podcast".

68
00:03:24,340 --> 00:03:25,173
To support it,

69
00:03:25,173 --> 00:03:27,769
please check out our sponsors in the description.

70
00:03:27,769 --> 00:03:29,341
And now, dear friends,

71
00:03:29,341 --> 00:03:32,508
here's Dylan Patel and Nathan Lambert.

72
00:03:33,845 --> 00:03:34,980
A lot of people are curious

73
00:03:34,980 --> 00:03:37,563
to understand China's DeepSeek AI models,

74
00:03:37,563 --> 00:03:39,071
so let's lay it out.

75
00:03:39,071 --> 00:03:41,540
Nathan, can you describe what DeepSeek-V3

76
00:03:41,540 --> 00:03:45,722
and DeepSeek-R1 are, how they work, how they're trained?

77
00:03:45,722 --> 00:03:47,409
Let's look at the big picture,

78
00:03:47,409 --> 00:03:49,394
and then we'll zoom in on the details.

79
00:03:49,394 --> 00:03:53,783
- Yeah, so DeepSeek-V3 is a new mixture of experts,

80
00:03:53,783 --> 00:03:56,949
transformer language model from DeepSeek

81
00:03:56,949 --> 00:03:58,720
who is based in China.

82
00:03:58,720 --> 00:04:01,775
They have some new specifics

83
00:04:01,775 --> 00:04:03,929
in the model that we'll get into.

84
00:04:03,929 --> 00:04:06,159
Largely this is a open-weight model

85
00:04:06,159 --> 00:04:08,405
and it's a instruction model

86
00:04:08,405 --> 00:04:10,832
like what you would use in ChatGPT.

87
00:04:10,832 --> 00:04:13,266
They also release what is called the base model,

88
00:04:13,266 --> 00:04:16,392
which is before these techniques of post-training.

89
00:04:16,392 --> 00:04:18,741
Most people use instruction models today

90
00:04:18,741 --> 00:04:21,753
and those are what's served in all sorts of applications.

91
00:04:21,753 --> 00:04:26,552
This was released on, I believe December 26th or that week.

92
00:04:26,552 --> 00:04:28,747
And then, weeks later,

93
00:04:28,747 --> 00:04:32,923
on January 20th, DeepSeek released DeepSeek-R1,

94
00:04:32,923 --> 00:04:34,545
which is a reasoning model,

95
00:04:34,545 --> 00:04:38,359
which really accelerated a lot of this discussion.

96
00:04:38,359 --> 00:04:40,339
This reasoning model, it has a lot

97
00:04:40,339 --> 00:04:43,427
of overlapping training steps to DeepSeek-V3,

98
00:04:43,427 --> 00:04:46,106
and it's confusing that you have a base model

99
00:04:46,106 --> 00:04:50,293
called V3 that you do something to, to get a chat model,

100
00:04:50,293 --> 00:04:51,764
and then you do some different things

101
00:04:51,764 --> 00:04:53,977
to get a reasoning model.

102
00:04:53,977 --> 00:04:55,136
I think a lot of the AI industry

103
00:04:55,136 --> 00:04:57,576
is going through this challenge of communications right now

104
00:04:57,576 --> 00:05:00,254
where OpenAI makes fun of their own naming schemes.

105
00:05:00,254 --> 00:05:03,421
They have GPT-4o, they have OpenAI o1.

106
00:05:04,897 --> 00:05:05,965
And there's a lot of types of models.

107
00:05:05,965 --> 00:05:08,545
So, we're gonna break down what each of them are.

108
00:05:08,545 --> 00:05:11,015
There's a lot of technical specifics on training,

109
00:05:11,015 --> 00:05:13,186
and go through 'em high level to specific,

110
00:05:13,186 --> 00:05:15,049
and go through each of them.

111
00:05:15,049 --> 00:05:16,806
- There's so many places we can go here,

112
00:05:16,806 --> 00:05:19,397
but maybe let's go to open-weights first.

113
00:05:19,397 --> 00:05:21,081
What does it mean for model to be open-weights

114
00:05:21,081 --> 00:05:22,451
and what are the different flavors

115
00:05:22,451 --> 00:05:23,703
of open source in general?

116
00:05:23,703 --> 00:05:24,890
- Yeah, so this discussion

117
00:05:24,890 --> 00:05:27,281
has been going on for a long time in AI.

118
00:05:27,281 --> 00:05:29,402
It became more important since ChatGPT

119
00:05:29,402 --> 00:05:32,891
or more focal since ChatGPT at the end of 2022.

120
00:05:32,891 --> 00:05:36,731
Open-weights is the accepted term for when model weights

121
00:05:36,731 --> 00:05:38,727
of a language model are available

122
00:05:38,727 --> 00:05:40,211
on the internet for people to download,

123
00:05:40,211 --> 00:05:42,800
those weights can have different licenses,

124
00:05:42,800 --> 00:05:44,951
which is the effectively the terms

125
00:05:44,951 --> 00:05:47,000
by which you can use the model.

126
00:05:47,000 --> 00:05:48,762
There are licenses that come from history

127
00:05:48,762 --> 00:05:49,682
and open source software.

128
00:05:49,682 --> 00:05:51,200
There are licenses that are designed

129
00:05:51,200 --> 00:05:53,816
by companies specifically,

130
00:05:53,816 --> 00:05:56,983
all of Llama, DeepSeek, Qwen, Mistral,

131
00:05:58,010 --> 00:06:01,561
these popular names in open-weight models,

132
00:06:01,561 --> 00:06:03,414
have some of their own licenses.

133
00:06:03,414 --> 00:06:04,247
It's complicated,

134
00:06:04,247 --> 00:06:06,947
'cause not all the same models have the same terms.

135
00:06:06,947 --> 00:06:11,280
The big debate is on what makes a model open-weight.

136
00:06:12,124 --> 00:06:13,576
It's like why are we saying this term?

137
00:06:13,576 --> 00:06:14,411
It's a mouthful.

138
00:06:14,411 --> 00:06:17,734
It sounds close to open source but it's not the same.

139
00:06:17,734 --> 00:06:20,304
There's still a lot of debate on the definition

140
00:06:20,304 --> 00:06:22,543
and soul of open source AI.

141
00:06:22,543 --> 00:06:23,376
Open source software

142
00:06:23,376 --> 00:06:26,114
has a rich history on freedom to modify,

143
00:06:26,114 --> 00:06:27,595
freedom to take on your own,

144
00:06:27,595 --> 00:06:28,814
freedom for many restrictions

145
00:06:28,814 --> 00:06:30,773
on how you would use the software,

146
00:06:30,773 --> 00:06:34,117
and what that means for AI is still being defined.

147
00:06:34,117 --> 00:06:39,013
So, for what I do, I work at the Allen Institute for AI.

148
00:06:39,013 --> 00:06:40,502
We're a nonprofit.

149
00:06:40,502 --> 00:06:42,653
We want to make AI open for everybody

150
00:06:42,653 --> 00:06:45,749
and we try to lead on what we think is truly open source.

151
00:06:45,749 --> 00:06:47,343
There's not full agreement in the community,

152
00:06:47,343 --> 00:06:49,732
but for us, that means releasing the training data,

153
00:06:49,732 --> 00:06:51,293
releasing the training code,

154
00:06:51,293 --> 00:06:53,984
and then also having open-weights like this.

155
00:06:53,984 --> 00:06:56,962
And we'll get into the details of the models.

156
00:06:56,962 --> 00:06:59,763
And again and again, as we try to get deeper

157
00:06:59,763 --> 00:07:02,212
into how the models were trained,

158
00:07:02,212 --> 00:07:05,097
we will say things like the data processing,

159
00:07:05,097 --> 00:07:06,744
data filtering, data quality

160
00:07:06,744 --> 00:07:09,837
is the number one determinant of the model quality.

161
00:07:09,837 --> 00:07:12,025
And then, a lot of the training code is the determinant

162
00:07:12,025 --> 00:07:13,648
on how long it takes to train

163
00:07:13,648 --> 00:07:16,257
and how fast your experimentation is.

164
00:07:16,257 --> 00:07:18,675
So, without fully open source models

165
00:07:18,675 --> 00:07:20,245
where you have access to this data,

166
00:07:20,245 --> 00:07:24,006
it is hard to know or it's harder to replicate.

167
00:07:24,006 --> 00:07:26,462
So, we'll get into cost numbers for DeepSeek-V3

168
00:07:26,462 --> 00:07:28,045
on mostly GPU hours

169
00:07:29,146 --> 00:07:32,366
and how much you could pay to rent those yourselves.

170
00:07:32,366 --> 00:07:33,199
But without the data,

171
00:07:33,199 --> 00:07:36,195
the replication cost is going to be far, far higher.

172
00:07:36,195 --> 00:07:37,654
And same goes for the code.

173
00:07:37,654 --> 00:07:40,690
- We should also say that this is probably one

174
00:07:40,690 --> 00:07:43,710
of the more open models out of the frontier models.

175
00:07:43,710 --> 00:07:46,159
- Yes. - So, in this full spectrum

176
00:07:46,159 --> 00:07:48,901
where probably the fullest open source, like you said,

177
00:07:48,901 --> 00:07:51,818
open code, open data, open-weights.

178
00:07:53,522 --> 00:07:58,022
This is not open code, this is probably not open data,

179
00:08:00,037 --> 00:08:01,754
and this is open-weights,

180
00:08:01,754 --> 00:08:05,421
and the licensing is MIT license, or it's...

181
00:08:06,694 --> 00:08:08,456
I mean, there's some nuance in the different models,

182
00:08:08,456 --> 00:08:10,264
but it's towards the free,

183
00:08:10,264 --> 00:08:11,803
in terms of the open source movement,

184
00:08:11,803 --> 00:08:13,744
these are the good guys. - Yeah.

185
00:08:13,744 --> 00:08:15,500
DeepSeek is doing fantastic work

186
00:08:15,500 --> 00:08:18,544
for disseminating understanding of AI.

187
00:08:18,544 --> 00:08:21,764
Their papers are extremely detailed in what they do.

188
00:08:21,764 --> 00:08:25,191
And for other teams around the world,

189
00:08:25,191 --> 00:08:26,832
they're very actionable in terms

190
00:08:26,832 --> 00:08:30,491
of improving your own training techniques.

191
00:08:30,491 --> 00:08:33,522
And we'll talk about licenses more.

192
00:08:33,522 --> 00:08:37,164
The DeepSeek-R1 model has a very permissive license.

193
00:08:37,164 --> 00:08:38,273
It's called the MIT license.

194
00:08:38,273 --> 00:08:41,191
That effectively means there's no downstream restrictions

195
00:08:41,191 --> 00:08:44,153
on commercial use, there's no use case restrictions.

196
00:08:44,153 --> 00:08:45,111
You can use the outputs

197
00:08:45,111 --> 00:08:47,210
from the models to create synthetic data.

198
00:08:47,210 --> 00:08:49,922
And this is all fantastic.

199
00:08:49,922 --> 00:08:52,330
I think the closest peer is something like Llama

200
00:08:52,330 --> 00:08:55,142
where you have the weights and you have a technical report.

201
00:08:55,142 --> 00:08:58,722
And the technical report is very good for Llama.

202
00:08:58,722 --> 00:09:00,799
One of the most read PDFs of the year last year

203
00:09:00,799 --> 00:09:01,937
is the Llama 3 paper,

204
00:09:01,937 --> 00:09:04,572
but in some ways, it's slightly less actionable.

205
00:09:04,572 --> 00:09:06,891
It has less details on the training specifics,

206
00:09:06,891 --> 00:09:09,705
I think less plots, and so on.

207
00:09:09,705 --> 00:09:13,132
And the Llama 3 license is more restrictive than MIT.

208
00:09:13,132 --> 00:09:15,105
And then, between the DeepSeek custom license

209
00:09:15,105 --> 00:09:16,305
and the Llama license,

210
00:09:16,305 --> 00:09:17,633
we could get into this whole rabbit hole.

211
00:09:17,633 --> 00:09:19,001
I think we'll make sure we want

212
00:09:19,001 --> 00:09:21,813
to go down the license rabbit hole before we do specifics.

213
00:09:21,813 --> 00:09:24,027
- Yeah, and so it should be stated that one

214
00:09:24,027 --> 00:09:27,636
of the implications that DeepSeek puts pressure on Llama

215
00:09:27,636 --> 00:09:31,917
and everybody else on OpenAI to push towards open source,

216
00:09:31,917 --> 00:09:33,287
and that's the other side of open source

217
00:09:33,287 --> 00:09:34,497
that you mentioned

218
00:09:34,497 --> 00:09:37,658
is how much is published in detail about it.

219
00:09:37,658 --> 00:09:42,241
So, how open are you with the insights behind the code?

220
00:09:43,213 --> 00:09:45,963
So, how good is the technical reports?

221
00:09:45,963 --> 00:09:48,983
Are they hand wavy or is there actual details in there?

222
00:09:48,983 --> 00:09:49,816
And that's one of the things

223
00:09:49,816 --> 00:09:52,652
that DeepSeek did well is they publish a lot of the details.

224
00:09:52,652 --> 00:09:54,535
- Yeah, especially in the DeepSeek-V3,

225
00:09:54,535 --> 00:09:57,045
which is their pre-training paper, they were very clear

226
00:09:57,045 --> 00:09:59,647
that they are doing interventions

227
00:09:59,647 --> 00:10:03,334
on the technical stack that go at many different levels.

228
00:10:03,334 --> 00:10:06,315
For example, to get highly efficient training,

229
00:10:06,315 --> 00:10:08,425
they're making modifications at

230
00:10:08,425 --> 00:10:12,012
or below the CUDA layer for Nvidia chips.

231
00:10:12,012 --> 00:10:13,544
I have never worked there myself.

232
00:10:13,544 --> 00:10:15,184
And there are a few people in the world

233
00:10:15,184 --> 00:10:17,823
that do that very well and some of them are at DeepSeek.

234
00:10:17,823 --> 00:10:20,922
And these types of people are at DeepSeek

235
00:10:20,922 --> 00:10:23,265
and leading American Frontier Labs,

236
00:10:23,265 --> 00:10:25,278
but there are not many places.

237
00:10:25,278 --> 00:10:26,164
- To help people understand

238
00:10:26,164 --> 00:10:28,231
the other implication of open-weights,

239
00:10:28,231 --> 00:10:32,483
just there's a topic we return to often here.

240
00:10:32,483 --> 00:10:35,983
So, there's a fear that China, the nation,

241
00:10:39,261 --> 00:10:43,411
might have interest in stealing American data,

242
00:10:43,411 --> 00:10:45,817
violating privacy of American citizens.

243
00:10:45,817 --> 00:10:48,166
What can we say about open-weights

244
00:10:48,166 --> 00:10:52,494
to help us understand what the weights are able to do

245
00:10:52,494 --> 00:10:53,335
- Yeah. - in terms

246
00:10:53,335 --> 00:10:55,031
of stealing people's data?

247
00:10:55,031 --> 00:10:55,864
- Yeah, so these weights

248
00:10:55,864 --> 00:10:57,468
that you can download from Hugging Face

249
00:10:57,468 --> 00:11:01,167
or other platforms are very big matrices of numbers.

250
00:11:01,167 --> 00:11:03,096
You can download them to a computer

251
00:11:03,096 --> 00:11:04,796
in your own house that has no internet

252
00:11:04,796 --> 00:11:05,775
and you can run this model,

253
00:11:05,775 --> 00:11:09,447
and you're totally in control of your data.

254
00:11:09,447 --> 00:11:11,601
That is something that is different

255
00:11:11,601 --> 00:11:13,272
than how a lot of language model usage

256
00:11:13,272 --> 00:11:15,466
is actually done today, which is mostly through APIs

257
00:11:15,466 --> 00:11:19,724
where you send your prompt to GPUs run by certain companies,

258
00:11:19,724 --> 00:11:21,364
and these companies will have different distributions

259
00:11:21,364 --> 00:11:23,155
and policies on how your data is stored,

260
00:11:23,155 --> 00:11:25,128
if it is used to train future models,

261
00:11:25,128 --> 00:11:28,443
where it is stored, if it is encrypted, and so on.

262
00:11:28,443 --> 00:11:30,454
So, the open-weights are you have your fate

263
00:11:30,454 --> 00:11:31,848
of data in your own hands

264
00:11:31,848 --> 00:11:35,085
and that is something that is deeply connected

265
00:11:35,085 --> 00:11:37,006
to the soul of open source.

266
00:11:37,006 --> 00:11:39,464
- So, it's not the model that steals your data,

267
00:11:39,464 --> 00:11:40,895
it's whoever's hosting the model,

268
00:11:40,895 --> 00:11:44,326
which could be China if you're using the DeepSeek app,

269
00:11:44,326 --> 00:11:47,302
or it could be Perplexity.

270
00:11:47,302 --> 00:11:49,119
You're trusting them with your data.

271
00:11:49,119 --> 00:11:51,425
Or OpenAI, you're trusting them with your data.

272
00:11:51,425 --> 00:11:52,850
And some of these are American companies,

273
00:11:52,850 --> 00:11:53,991
some of of these are Chinese companies,

274
00:11:53,991 --> 00:11:57,177
but the model itself is not doing the stealing,

275
00:11:57,177 --> 00:11:59,090
it's the host.

276
00:11:59,090 --> 00:12:02,219
All right. So, back to the basics.

277
00:12:02,219 --> 00:12:07,116
What's the difference between DeepSeek-V3 and DeepSeek-R1?

278
00:12:07,116 --> 00:12:09,629
Can we try to lay out

279
00:12:09,629 --> 00:12:12,024
the confusion potential? - Yes.

280
00:12:12,024 --> 00:12:13,995
So, for one, I have very understanding

281
00:12:13,995 --> 00:12:16,712
of many people being confused by these two model names.

282
00:12:16,712 --> 00:12:18,975
So, I would say the best way to think about this

283
00:12:18,975 --> 00:12:21,084
is that when training a language model,

284
00:12:21,084 --> 00:12:22,444
you have what is called pre-training,

285
00:12:22,444 --> 00:12:25,303
which is when you're predicting the large amounts

286
00:12:25,303 --> 00:12:26,995
of mostly internet text.

287
00:12:26,995 --> 00:12:28,815
You're trying to predict the next-token.

288
00:12:28,815 --> 00:12:31,825
And what to know about these new DeepSeek models

289
00:12:31,825 --> 00:12:36,033
is that they do this internet large-scale pre-training once

290
00:12:36,033 --> 00:12:38,791
to get what is called DeepSeek-V3 base.

291
00:12:38,791 --> 00:12:39,641
This is a base model.

292
00:12:39,641 --> 00:12:42,637
It's just going to finish your sentences for you.

293
00:12:42,637 --> 00:12:45,576
It's going to be harder to work with than ChatGPT.

294
00:12:45,576 --> 00:12:47,317
And then, what DeepSeek did

295
00:12:47,317 --> 00:12:50,273
is they've done two different post-training regimes

296
00:12:50,273 --> 00:12:54,988
to make the models have specific desirable behaviors.

297
00:12:54,988 --> 00:12:57,337
So, what is the more normal model

298
00:12:57,337 --> 00:12:59,596
in terms of the last few years of AI

299
00:12:59,596 --> 00:13:01,818
and instruct model, a chat model,

300
00:13:01,818 --> 00:13:03,968
a, quote, unquote, "aligned model", a helpful model,

301
00:13:03,968 --> 00:13:05,407
there are many ways to describe this,

302
00:13:05,407 --> 00:13:08,093
is more standard post-training.

303
00:13:08,093 --> 00:13:10,137
So, this is things like instruction tuning,

304
00:13:10,137 --> 00:13:12,100
reinforcement learning from human feedback.

305
00:13:12,100 --> 00:13:13,924
We'll get into some of these words.

306
00:13:13,924 --> 00:13:17,457
And this is what they did to create the DeepSeek-V3 model.

307
00:13:17,457 --> 00:13:20,228
This was the first model to be released

308
00:13:20,228 --> 00:13:23,102
and it is very highly performant,

309
00:13:23,102 --> 00:13:26,602
competitive with GPT-4, Llama 405b, so on.

310
00:13:27,597 --> 00:13:31,075
And then, when this release was happening,

311
00:13:31,075 --> 00:13:32,446
we don't know their exact timeline,

312
00:13:32,446 --> 00:13:34,616
or soon after they were finishing the training

313
00:13:34,616 --> 00:13:36,537
of a different training process

314
00:13:36,537 --> 00:13:39,892
from the same next-token prediction-based model

315
00:13:39,892 --> 00:13:41,088
that I talked about,

316
00:13:41,088 --> 00:13:43,116
which is when this new reasoning training

317
00:13:43,116 --> 00:13:45,294
that people have heard about comes in,

318
00:13:45,294 --> 00:13:48,788
in order to create the model that is called DeepSeek-R1.

319
00:13:48,788 --> 00:13:50,158
The R through this conversation

320
00:13:50,158 --> 00:13:51,875
is good for grounding for reasoning.

321
00:13:51,875 --> 00:13:54,199
And the name is also similar to OpenAI's o1,

322
00:13:54,199 --> 00:13:55,409
which is the other reasoning model

323
00:13:55,409 --> 00:13:57,496
that people have heard about.

324
00:13:57,496 --> 00:13:59,409
And we'll have to break down the training

325
00:13:59,409 --> 00:14:01,056
for R1 in more detail,

326
00:14:01,056 --> 00:14:03,766
because for one, we have a paper detailing it,

327
00:14:03,766 --> 00:14:06,515
but also, it is a far newer set of techniques

328
00:14:06,515 --> 00:14:07,894
for the AI community.

329
00:14:07,894 --> 00:14:12,052
So, it is a much more rapidly evolving area of research.

330
00:14:12,052 --> 00:14:15,387
- Maybe we should also say the big two categories

331
00:14:15,387 --> 00:14:18,777
of training of pre-training and post-training.

332
00:14:18,777 --> 00:14:20,663
These umbrella terms that people use.

333
00:14:20,663 --> 00:14:24,768
So, what is pre-training and what is post-training,

334
00:14:24,768 --> 00:14:26,137
and what are the different flavors of things

335
00:14:26,137 --> 00:14:27,943
underneath post-training umbrella?

336
00:14:27,943 --> 00:14:30,198
- Yeah, so pre-training, I'm using some of the same words

337
00:14:30,198 --> 00:14:31,856
to really get the message across,

338
00:14:31,856 --> 00:14:34,369
is you're doing what is called autoregressive prediction

339
00:14:34,369 --> 00:14:37,188
to predict the next-token in a series of documents.

340
00:14:37,188 --> 00:14:41,895
This is done over standard practice is trillions of tokens.

341
00:14:41,895 --> 00:14:43,848
So, this is a ton of data

342
00:14:43,848 --> 00:14:46,609
that is mostly scraped from the web.

343
00:14:46,609 --> 00:14:49,239
And some of DeepSeek's earlier papers,

344
00:14:49,239 --> 00:14:50,765
they talk about their training data

345
00:14:50,765 --> 00:14:54,382
being distilled for math, I shouldn't use this word yet,

346
00:14:54,382 --> 00:14:56,564
but taken from Common Crawl,

347
00:14:56,564 --> 00:14:59,759
and that's a public access that anyone listening to this

348
00:14:59,759 --> 00:15:02,116
could go download data from the Common Crawl website.

349
00:15:02,116 --> 00:15:04,293
This is a crawler that is maintained publicly.

350
00:15:04,293 --> 00:15:05,483
Yes, other tech companies

351
00:15:05,483 --> 00:15:07,594
eventually shift to their own crawler

352
00:15:07,594 --> 00:15:09,930
and DeepSeek likely has done this as well,

353
00:15:09,930 --> 00:15:11,442
as most Frontier Labs do.

354
00:15:11,442 --> 00:15:13,253
But this sort of data is something

355
00:15:13,253 --> 00:15:14,461
that people can get started with

356
00:15:14,461 --> 00:15:18,004
and you're just predicting text in a series of documents.

357
00:15:18,004 --> 00:15:21,254
This can be scaled to be very efficient

358
00:15:23,342 --> 00:15:24,355
and there's a lot of numbers

359
00:15:24,355 --> 00:15:26,027
that are thrown around in AI training,

360
00:15:26,027 --> 00:15:29,788
like how many floating point operations or FLOPS are used.

361
00:15:29,788 --> 00:15:31,019
And then, you can also look

362
00:15:31,019 --> 00:15:35,187
at how many hours of these GPUs that are used.

363
00:15:35,187 --> 00:15:37,477
And it's largely one loss function

364
00:15:37,477 --> 00:15:42,477
taken to a very large amount (chuckles) of compute usage.

365
00:15:42,813 --> 00:15:45,215
You set up really efficient systems.

366
00:15:45,215 --> 00:15:47,438
And then, at the end of that, you have this base model.

367
00:15:47,438 --> 00:15:50,425
And pre-training is where there is a lot more

368
00:15:50,425 --> 00:15:53,037
of complexity in terms

369
00:15:53,037 --> 00:15:57,226
of how the process is emerging or evolving,

370
00:15:57,226 --> 00:16:00,253
and the different types of training losses that you'll use.

371
00:16:00,253 --> 00:16:02,827
I think this is a lot of techniques

372
00:16:02,827 --> 00:16:06,133
grounded in the natural language processing literature.

373
00:16:06,133 --> 00:16:08,078
The oldest technique which is still used today

374
00:16:08,078 --> 00:16:09,625
is something called instruction tuning,

375
00:16:09,625 --> 00:16:12,266
or also known as supervised fine-tuning.

376
00:16:12,266 --> 00:16:14,833
These acronyms will be IFT or SFT

377
00:16:14,833 --> 00:16:18,099
that people really go back and forth throughout them,

378
00:16:18,099 --> 00:16:19,242
and I'll probably do the same,

379
00:16:19,242 --> 00:16:23,104
which is where you add this formatting to the model,

380
00:16:23,104 --> 00:16:25,505
where it knows to take a question

381
00:16:25,505 --> 00:16:30,505
that is like explain the history of the Roman Empire to me.

382
00:16:30,813 --> 00:16:32,224
Or something, a sort of question

383
00:16:32,224 --> 00:16:33,838
you'll see on Reddit or Stack Overflow,

384
00:16:33,838 --> 00:16:35,440
and then the model will respond

385
00:16:35,440 --> 00:16:38,955
in a information-dense but presentable manner.

386
00:16:38,955 --> 00:16:39,913
The core of that formatting

387
00:16:39,913 --> 00:16:41,953
is in this instruction tuning phase.

388
00:16:41,953 --> 00:16:44,063
And then, there's two other categories

389
00:16:44,063 --> 00:16:47,384
of loss functions that are being used today.

390
00:16:47,384 --> 00:16:49,503
One I'll classify as preference fine-tuning.

391
00:16:49,503 --> 00:16:52,248
Preference fine-tuning is a generalized term

392
00:16:52,248 --> 00:16:54,329
for what came out of reinforcement learning

393
00:16:54,329 --> 00:16:57,598
from human feedback, which is RLHF.

394
00:16:57,598 --> 00:17:00,129
This reinforcement learning from human feedback

395
00:17:00,129 --> 00:17:02,276
is credited as the technique

396
00:17:02,276 --> 00:17:05,026
that helped ChatGPT breakthrough.

397
00:17:06,205 --> 00:17:08,315
It is a technique to make the responses

398
00:17:08,315 --> 00:17:10,676
that are nicely formatted, like these Reddit answers,

399
00:17:10,676 --> 00:17:14,406
more in tune with what a human would like to read.

400
00:17:14,406 --> 00:17:16,425
This is done by collecting pairwise preferences

401
00:17:16,425 --> 00:17:19,815
from actual humans out in the world to start.

402
00:17:19,815 --> 00:17:21,472
And now, AIs are also labeling this data

403
00:17:21,472 --> 00:17:23,520
and we'll get into those trade-offs.

404
00:17:23,520 --> 00:17:26,924
And you have this kind of contrastive loss function

405
00:17:26,924 --> 00:17:28,736
between a good answer and a bad answer.

406
00:17:28,736 --> 00:17:31,290
And the model learns to pick up these trends.

407
00:17:31,290 --> 00:17:33,176
There's different implementation ways.

408
00:17:33,176 --> 00:17:35,075
You have things called reward models.

409
00:17:35,075 --> 00:17:37,002
You could have direct alignment algorithms.

410
00:17:37,002 --> 00:17:39,521
There's a lot of really specific things you can do.

411
00:17:39,521 --> 00:17:42,333
But all of this is about fine-tuning to human preferences.

412
00:17:42,333 --> 00:17:44,874
And the final stage is much newer

413
00:17:44,874 --> 00:17:48,065
and will link to what is done in R1.

414
00:17:48,065 --> 00:17:49,288
And these reasoning models

415
00:17:49,288 --> 00:17:52,240
is I think OpenAI's name for this.

416
00:17:52,240 --> 00:17:53,642
They had this new API in the fall,

417
00:17:53,642 --> 00:17:57,566
which they called the reinforcement fine-tuning API.

418
00:17:57,566 --> 00:17:59,392
This is the idea that you use the techniques

419
00:17:59,392 --> 00:18:02,966
of reinforcement learning, which is a whole framework of AI.

420
00:18:02,966 --> 00:18:04,036
There's a deep literature here.

421
00:18:04,036 --> 00:18:07,678
To summarize, it's often known as trial and error learning

422
00:18:07,678 --> 00:18:10,313
or the subfield of AI where you're trying

423
00:18:10,313 --> 00:18:12,142
to make sequential decisions

424
00:18:12,142 --> 00:18:16,148
in a certain potentially noisy environment.

425
00:18:16,148 --> 00:18:18,174
There's a lot of ways we could go down that.

426
00:18:18,174 --> 00:18:20,034
But fine-tuning language models

427
00:18:20,034 --> 00:18:22,098
where they can generate an answer.

428
00:18:22,098 --> 00:18:23,512
And then, you check to see

429
00:18:23,512 --> 00:18:25,560
if the answer matches the true solution.

430
00:18:25,560 --> 00:18:26,443
For math or code,

431
00:18:26,443 --> 00:18:29,772
you have an exactly correct answer for math.

432
00:18:29,772 --> 00:18:31,233
You can have unit tests for code.

433
00:18:31,233 --> 00:18:32,280
And what we're doing

434
00:18:32,280 --> 00:18:34,455
is we are checking the language model's work

435
00:18:34,455 --> 00:18:36,175
and we're giving it multiple opportunities

436
00:18:36,175 --> 00:18:38,335
on the same questions to see if it is right.

437
00:18:38,335 --> 00:18:40,047
And if you keep doing this, the models can learn

438
00:18:40,047 --> 00:18:44,412
to improve in verifiable domains to a great extent.

439
00:18:44,412 --> 00:18:45,286
It works really well.

440
00:18:45,286 --> 00:18:47,776
It's a newer technique in the academic literature.

441
00:18:47,776 --> 00:18:50,287
It's been used at Frontier Labs in the US

442
00:18:50,287 --> 00:18:53,175
that don't share every detail for multiple years.

443
00:18:53,175 --> 00:18:56,724
So, this is the idea of using reinforcement learning

444
00:18:56,724 --> 00:18:58,736
with language models, and it has been taking off,

445
00:18:58,736 --> 00:19:01,029
especially in this DeepSeek moment.

446
00:19:01,029 --> 00:19:01,972
- And we should say that there's a lot

447
00:19:01,972 --> 00:19:05,836
of exciting stuff going on the, again, across the stack.

448
00:19:05,836 --> 00:19:07,914
But the post-training probably this year,

449
00:19:07,914 --> 00:19:09,655
there's going to be a lot of interesting developments

450
00:19:09,655 --> 00:19:10,488
in the post-training.

451
00:19:10,488 --> 00:19:12,117
We'll talk about it.

452
00:19:12,117 --> 00:19:14,366
I almost forgot to talk about the difference

453
00:19:14,366 --> 00:19:18,142
between DeepSeek-V3 and R1 on the user experience side.

454
00:19:18,142 --> 00:19:21,329
So, forget the technical stuff, forget all of that.

455
00:19:21,329 --> 00:19:23,657
Just people that don't know anything about AI,

456
00:19:23,657 --> 00:19:25,856
they show up like what's the actual experience,

457
00:19:25,856 --> 00:19:27,691
what's the use case for each one

458
00:19:27,691 --> 00:19:29,978
when they actually type and talk to it?

459
00:19:29,978 --> 00:19:31,168
- Yeah. - What is each good at

460
00:19:31,168 --> 00:19:32,001
and that kind of thing.

461
00:19:32,001 --> 00:19:33,490
- So, let's start with DeepSeek-V3.

462
00:19:33,490 --> 00:19:36,117
Again, it's more people would have tried something like it.

463
00:19:36,117 --> 00:19:37,200
You ask it a question,

464
00:19:37,200 --> 00:19:40,125
it'll start generating tokens very fast,

465
00:19:40,125 --> 00:19:44,276
and those tokens will look like a very human legible answer.

466
00:19:44,276 --> 00:19:46,796
It'll be some sort of markdown list.

467
00:19:46,796 --> 00:19:47,971
It might have formatting

468
00:19:47,971 --> 00:19:52,157
to help you draw to the core details in the answer.

469
00:19:52,157 --> 00:19:55,442
And it'll generate tens to hundreds of tokens.

470
00:19:55,442 --> 00:19:58,774
Say token is normally a word for common words

471
00:19:58,774 --> 00:20:02,044
or a sub-word part in a longer word.

472
00:20:02,044 --> 00:20:05,908
And it'll look like a very high quality Reddit

473
00:20:05,908 --> 00:20:07,006
or Stack Overflow answer.

474
00:20:07,006 --> 00:20:09,401
These models are really getting good

475
00:20:09,401 --> 00:20:12,370
at doing these across a wide variety of domains, I think.

476
00:20:12,370 --> 00:20:14,828
Even things that if you're an expert,

477
00:20:14,828 --> 00:20:17,173
things that are close to the fringe of knowledge,

478
00:20:17,173 --> 00:20:19,576
they will still be fairly good at, I think.

479
00:20:19,576 --> 00:20:22,163
Cutting edge AI topics that I do research on,

480
00:20:22,163 --> 00:20:25,936
these models are capable for study aide,

481
00:20:25,936 --> 00:20:28,266
and they're regularly updated.

482
00:20:28,266 --> 00:20:31,278
Where this changes is with the DeepSeek-R1,

483
00:20:31,278 --> 00:20:33,079
what is called these reasoning models,

484
00:20:33,079 --> 00:20:36,492
is when you see tokens coming from these models to start,

485
00:20:36,492 --> 00:20:40,311
it will be a large chain of thought process.

486
00:20:40,311 --> 00:20:42,393
We'll get back to chain of thought in a second,

487
00:20:42,393 --> 00:20:44,733
which looks like a lot of tokens,

488
00:20:44,733 --> 00:20:47,131
where the model is explaining the problem.

489
00:20:47,131 --> 00:20:48,585
The model will often break down the problem

490
00:20:48,585 --> 00:20:50,696
and be like, okay, they asked me for this,

491
00:20:50,696 --> 00:20:52,121
let's break down the problem.

492
00:20:52,121 --> 00:20:53,062
I'm going to need to do this.

493
00:20:53,062 --> 00:20:55,787
And you'll see all of this generating from the model.

494
00:20:55,787 --> 00:20:58,147
It'll come very fast in most user experiences.

495
00:20:58,147 --> 00:20:59,557
These APIs are very fast.

496
00:20:59,557 --> 00:21:00,657
So, you'll see a lot of tokens,

497
00:21:00,657 --> 00:21:01,973
a lot of words show up really fast.

498
00:21:01,973 --> 00:21:03,836
It'll keep flowing on the screen

499
00:21:03,836 --> 00:21:05,284
and this is all the reasoning process.

500
00:21:05,284 --> 00:21:09,237
And then, eventually, the model will change its tone in R1

501
00:21:09,237 --> 00:21:10,105
and it'll write the answer,

502
00:21:10,105 --> 00:21:12,987
where it summarizes its reasoning process

503
00:21:12,987 --> 00:21:16,202
and writes a similar answer to the first types of model.

504
00:21:16,202 --> 00:21:18,844
But in DeepSeek's case, which is part

505
00:21:18,844 --> 00:21:22,786
of why this was so popular even outside the AI community,

506
00:21:22,786 --> 00:21:24,826
is that you can see how the language model

507
00:21:24,826 --> 00:21:26,695
is breaking down problems.

508
00:21:26,695 --> 00:21:28,144
And then, you get this answer.

509
00:21:28,144 --> 00:21:29,255
On a technical side,

510
00:21:29,255 --> 00:21:31,676
they train the model to do this specifically,

511
00:21:31,676 --> 00:21:33,467
where they have a section which is reasoning,

512
00:21:33,467 --> 00:21:35,106
and then it generates a special token,

513
00:21:35,106 --> 00:21:37,147
which is probably hidden from the user most of the time,

514
00:21:37,147 --> 00:21:38,697
which says, okay, I'm starting the answer.

515
00:21:38,697 --> 00:21:40,108
So, the model is trained

516
00:21:40,108 --> 00:21:42,627
to do this two-stage process on its own.

517
00:21:42,627 --> 00:21:45,045
If you use a similar model in say, OpenAI,

518
00:21:45,045 --> 00:21:46,455
OpenAI's user interface

519
00:21:46,455 --> 00:21:50,818
is trying to summarize this process for you nicely

520
00:21:50,818 --> 00:21:54,817
by showing the sections that the model is doing,

521
00:21:54,817 --> 00:21:55,923
and it'll click through,

522
00:21:55,923 --> 00:22:00,062
it'll say breaking down the problem, making X calculation,

523
00:22:00,062 --> 00:22:01,029
cleaning the result,

524
00:22:01,029 --> 00:22:03,760
and then the answer will come for something like OpenAI.

525
00:22:03,760 --> 00:22:05,275
- Maybe it's useful here to go through

526
00:22:05,275 --> 00:22:08,674
like an example of it, DeepSeek-R1 reasoning.

527
00:22:08,674 --> 00:22:09,507
- Yeah.

528
00:22:09,507 --> 00:22:11,553
So, if you're looking at the screen here,

529
00:22:11,553 --> 00:22:15,475
what you'll see is a screenshot of the DeepSeek chat app.

530
00:22:15,475 --> 00:22:19,277
And at the top is Thought for 157 seconds

531
00:22:19,277 --> 00:22:20,745
with the dropdown arrow.

532
00:22:20,745 --> 00:22:23,155
Underneath that, if we were in an app that we were running,

533
00:22:23,155 --> 00:22:24,745
the dropdown arrow would have the reasoning.

534
00:22:24,745 --> 00:22:27,186
- So, in this case, the question,

535
00:22:27,186 --> 00:22:29,325
the specific question which,

536
00:22:29,325 --> 00:22:31,545
I'm philosophically/pothead-inclined,

537
00:22:31,545 --> 00:22:33,962
so this is asking DeepSeek-R1

538
00:22:35,196 --> 00:22:38,613
for one truly novel insight about humans.

539
00:22:39,874 --> 00:22:41,334
And it reveals the reasoning.

540
00:22:41,334 --> 00:22:44,778
And basically, the truly novel aspect

541
00:22:44,778 --> 00:22:47,416
is what's pushing the reasoning to constantly,

542
00:22:47,416 --> 00:22:49,638
the model asking itself, is this truly novel?

543
00:22:49,638 --> 00:22:52,324
So, it's actually challenging itself to be more novel,

544
00:22:52,324 --> 00:22:56,157
more counterintuitive, less cringe, I suppose.

545
00:22:57,245 --> 00:23:01,157
So, some of the reasoning says, this is just snapshots.

546
00:23:01,157 --> 00:23:04,205
Alternatively, humans have a unique meta-emotion

547
00:23:04,205 --> 00:23:06,271
where they feel emotions about their own emotions,

548
00:23:06,271 --> 00:23:08,094
e.g, feeling guilty about being angry.

549
00:23:08,094 --> 00:23:10,335
This recursive emotional layering creates complex

550
00:23:10,335 --> 00:23:13,185
motivational drives that don't exist in other animals.

551
00:23:13,185 --> 00:23:15,284
The inside is that human emotions are nested.

552
00:23:15,284 --> 00:23:16,117
So, it's like,

553
00:23:16,117 --> 00:23:19,655
it's reasoning through how humans feel emotions,

554
00:23:19,655 --> 00:23:21,173
it's reasoning about meta-emotions.

555
00:23:21,173 --> 00:23:22,006
- It's gonna have pages

556
00:23:22,006 --> 00:23:22,839
and pages of this. - Yeah.

557
00:23:22,839 --> 00:23:24,472
- It's almost too much to actually read,

558
00:23:24,472 --> 00:23:26,621
but it's nice to skim as it's coming.

559
00:23:26,621 --> 00:23:27,454
- It's a stream of con...

560
00:23:27,454 --> 00:23:29,485
It's a James Joyce-like stream of consciousness.

561
00:23:29,485 --> 00:23:31,015
And then, it goes, wait,

562
00:23:31,015 --> 00:23:33,990
the user wants something that's not seen anywhere else.

563
00:23:33,990 --> 00:23:35,721
Let me dig deeper.

564
00:23:35,721 --> 00:23:38,062
And consider the human ability

565
00:23:38,062 --> 00:23:40,192
to hold contradictory beliefs simultaneously.

566
00:23:40,192 --> 00:23:41,452
Cognitive dissonance is known,

567
00:23:41,452 --> 00:23:43,304
but perhaps the function

568
00:23:43,304 --> 00:23:47,662
is to allow flexible adaptation, so on and so forth.

569
00:23:47,662 --> 00:23:50,413
That really captures the public imagination,

570
00:23:50,413 --> 00:23:52,830
that holy shit, this isn't...

571
00:23:54,619 --> 00:23:59,619
I mean, intelligence/almost like an inkling of sentience,

572
00:24:00,513 --> 00:24:02,394
because you're thinking through,

573
00:24:02,394 --> 00:24:04,282
you're self-reflecting, you're deliberating.

574
00:24:04,282 --> 00:24:07,627
And the final result of that after 157 seconds

575
00:24:07,627 --> 00:24:12,323
is humans instinctively convert selfish desires

576
00:24:12,323 --> 00:24:13,826
into cooperative systems

577
00:24:13,826 --> 00:24:15,753
by collectively pretending abstract rules,

578
00:24:15,753 --> 00:24:18,354
money, laws, rights, are real.

579
00:24:18,354 --> 00:24:21,414
These shared hallucinations act as, quote, games,

580
00:24:21,414 --> 00:24:23,814
where competition is secretly redirected

581
00:24:23,814 --> 00:24:28,762
to benefit the group, turning conflict into society's fuel.

582
00:24:28,762 --> 00:24:30,115
Pretty profound.

583
00:24:30,115 --> 00:24:31,439
I mean, you know?

584
00:24:31,439 --> 00:24:33,037
Comment- - This is a potential

585
00:24:33,037 --> 00:24:36,009
digression, but a lot of people have found

586
00:24:36,009 --> 00:24:37,419
that these reasoning models

587
00:24:37,419 --> 00:24:40,929
can sometimes produce much more eloquent text.

588
00:24:40,929 --> 00:24:43,492
That is at least interesting example, I think,

589
00:24:43,492 --> 00:24:45,071
depending on how open-minded you are,

590
00:24:45,071 --> 00:24:46,833
you find language models interesting or not,

591
00:24:46,833 --> 00:24:48,243
and there's a spectrum there.

592
00:24:48,243 --> 00:24:49,772
- Well, it's some of the...

593
00:24:49,772 --> 00:24:51,401
We'll talk about different benchmarks, and so on.

594
00:24:51,401 --> 00:24:53,122
But some is just a vibe.

595
00:24:53,122 --> 00:24:57,488
Like that in itself is a, let's say, quote, fire tweet.

596
00:24:57,488 --> 00:24:59,694
- Yeah. (laughs) - If I'm trying

597
00:24:59,694 --> 00:25:03,005
to produce something, where people are like, oh shit,

598
00:25:03,005 --> 00:25:05,054
okay, so that's chain of thought.

599
00:25:05,054 --> 00:25:08,079
We'll probably return to it more.

600
00:25:08,079 --> 00:25:11,745
How were they able to achieve such low cost

601
00:25:11,745 --> 00:25:13,509
on the training and the inference?

602
00:25:13,509 --> 00:25:15,344
Maybe you could talk the training first.

603
00:25:15,344 --> 00:25:17,486
- Yeah, so there's two main techniques

604
00:25:17,486 --> 00:25:18,914
that they implemented

605
00:25:18,914 --> 00:25:23,094
that are probably the majority of their efficiency.

606
00:25:23,094 --> 00:25:24,876
And then, there's a lot of implementation details

607
00:25:24,876 --> 00:25:26,275
that maybe we'll gloss over

608
00:25:26,275 --> 00:25:28,867
or get into later that contribute to it.

609
00:25:28,867 --> 00:25:31,136
But those two main things are,

610
00:25:31,136 --> 00:25:33,865
one, is they went to a mixture of experts model,

611
00:25:33,865 --> 00:25:35,940
which we'll define in a second.

612
00:25:35,940 --> 00:25:36,773
And then, the other thing

613
00:25:36,773 --> 00:25:38,308
is that they invented this new technique

614
00:25:38,308 --> 00:25:40,500
called MLA latent attention.

615
00:25:40,500 --> 00:25:42,047
Both of these are big deals.

616
00:25:42,047 --> 00:25:43,469
Mixture of experts is something

617
00:25:43,469 --> 00:25:46,109
that's been in the literature for a handful of years.

618
00:25:46,109 --> 00:25:48,438
And OpenAI with GPT-4 was the first one

619
00:25:48,438 --> 00:25:51,725
to productize a mixture of experts model.

620
00:25:51,725 --> 00:25:52,766
And what this means

621
00:25:52,766 --> 00:25:55,384
is when you look at the common models around

622
00:25:55,384 --> 00:25:57,209
that most people have been able

623
00:25:57,209 --> 00:25:59,258
to interact with that are open.

624
00:25:59,258 --> 00:26:02,759
Think Llama, Llama is a dense model.

625
00:26:02,759 --> 00:26:04,849
i.e, every single parameter

626
00:26:04,849 --> 00:26:07,980
or neuron is activated as you're going through the model

627
00:26:07,980 --> 00:26:10,761
for every single token you generate.

628
00:26:10,761 --> 00:26:13,670
Now, with a mixture of experts model, you don't do that.

629
00:26:13,670 --> 00:26:15,968
How does the human actually work is like,

630
00:26:15,968 --> 00:26:17,158
oh, well, my visual cortex

631
00:26:17,158 --> 00:26:20,238
is active when I'm thinking about vision tasks

632
00:26:20,238 --> 00:26:21,136
and other things.

633
00:26:21,136 --> 00:26:23,585
My amygdala is when I'm scared.

634
00:26:23,585 --> 00:26:25,520
These different aspects of your brain

635
00:26:25,520 --> 00:26:27,021
are focused on different things.

636
00:26:27,021 --> 00:26:28,429
A mixture of experts models

637
00:26:28,429 --> 00:26:30,408
attempts to approximate this to some extent.

638
00:26:30,408 --> 00:26:32,309
So, nowhere close to what a brain architecture is,

639
00:26:32,309 --> 00:26:35,326
but different portions of the model activate.

640
00:26:35,326 --> 00:26:38,758
You'll have a set number of experts in the model

641
00:26:38,758 --> 00:26:40,790
and a set number that are activated each time.

642
00:26:40,790 --> 00:26:43,160
And this dramatically reduces both your training

643
00:26:43,160 --> 00:26:44,001
and inference cost.

644
00:26:44,001 --> 00:26:48,039
Because now, if you think about the parameter count

645
00:26:48,039 --> 00:26:51,059
as the total embedding space for all of this knowledge

646
00:26:51,059 --> 00:26:54,113
that you're compressing down during training,

647
00:26:54,113 --> 00:26:56,359
one, you're embedding this data in,

648
00:26:56,359 --> 00:26:59,102
instead of having to activate every single parameter,

649
00:26:59,102 --> 00:27:01,358
every single time you're training or running inference.

650
00:27:01,358 --> 00:27:03,233
Now, you can just activate on a subset.

651
00:27:03,233 --> 00:27:04,692
And the model will learn

652
00:27:04,692 --> 00:27:07,448
which expert to route to for different tasks.

653
00:27:07,448 --> 00:27:11,168
And so, this is a humongous innovation in terms of, hey,

654
00:27:11,168 --> 00:27:12,117
I can continue to grow

655
00:27:12,117 --> 00:27:14,207
the total embedding space of parameters.

656
00:27:14,207 --> 00:27:15,397
And so, DeepSeek's model

657
00:27:15,397 --> 00:27:17,967
is 600 something billion parameters.

658
00:27:17,967 --> 00:27:21,388
Relative to Llama 405b, it's 405 billion parameters.

659
00:27:21,388 --> 00:27:24,194
Llama relative to Llama 70b, it's 70 billion parameters.

660
00:27:24,194 --> 00:27:25,269
So, this model technically

661
00:27:25,269 --> 00:27:27,916
has more embedding space for information,

662
00:27:27,916 --> 00:27:29,912
to compress all of the world's knowledge

663
00:27:29,912 --> 00:27:31,114
that's on the internet down.

664
00:27:31,114 --> 00:27:32,214
But at the same time,

665
00:27:32,214 --> 00:27:36,693
it is only activating around 37 billion of the parameters.

666
00:27:36,693 --> 00:27:38,682
So, only 37 billion of these parameters

667
00:27:38,682 --> 00:27:39,822
actually need to be computed

668
00:27:39,822 --> 00:27:42,193
every single time you're training data

669
00:27:42,193 --> 00:27:43,792
or inferencing data out of it.

670
00:27:43,792 --> 00:27:46,253
And so, versus again a Llama model,

671
00:27:46,253 --> 00:27:48,253
70 billion parameters must be activated

672
00:27:48,253 --> 00:27:50,485
or 405 billion parameters must be activated.

673
00:27:50,485 --> 00:27:52,674
So, you've dramatically reduced your compute cost

674
00:27:52,674 --> 00:27:54,374
when you're doing training and inference

675
00:27:54,374 --> 00:27:57,315
with this mixture of experts architecture.

676
00:27:57,315 --> 00:27:59,315
- So, we break down where it actually applies

677
00:27:59,315 --> 00:28:00,987
and go into the transformer.

678
00:28:00,987 --> 00:28:01,974
Is that useful?

679
00:28:01,974 --> 00:28:03,154
- Let's go, let's go

680
00:28:03,154 --> 00:28:03,987
into the transformer. - Okay.

681
00:28:03,987 --> 00:28:04,820
So, the transformer (Lex laughing)

682
00:28:04,820 --> 00:28:06,408
is a thing that is talked about a lot

683
00:28:06,408 --> 00:28:09,136
and we will not cover every detail.

684
00:28:09,136 --> 00:28:11,589
Essentially, the transformer is built

685
00:28:11,589 --> 00:28:14,578
on repeated blocks of this attention mechanism,

686
00:28:14,578 --> 00:28:16,415
and then a traditional dense,

687
00:28:16,415 --> 00:28:19,077
fully connected multi-layer perception,

688
00:28:19,077 --> 00:28:20,488
whatever word you want to use

689
00:28:20,488 --> 00:28:21,919
for your normal neural network.

690
00:28:21,919 --> 00:28:23,086
And you alternate these blocks.

691
00:28:23,086 --> 00:28:24,293
There's other details.

692
00:28:24,293 --> 00:28:25,962
And where mixture of experts

693
00:28:25,962 --> 00:28:27,781
is applied is that this dense model.

694
00:28:27,781 --> 00:28:31,000
The dense model holds most of the weights

695
00:28:31,000 --> 00:28:34,071
if you count them in a transformer model.

696
00:28:34,071 --> 00:28:35,482
So, you can get really big gains

697
00:28:35,482 --> 00:28:38,548
from those mixture of experts on parameter efficiency

698
00:28:38,548 --> 00:28:39,381
at training and inference,

699
00:28:39,381 --> 00:28:41,772
because you get this efficiency

700
00:28:41,772 --> 00:28:44,160
by not activating all of these parameters.

701
00:28:44,160 --> 00:28:46,302
- [Lex] We should also say that a transformer

702
00:28:46,302 --> 00:28:48,212
is a giant neural network.

703
00:28:48,212 --> 00:28:49,469
- Yeah. - And then,

704
00:28:49,469 --> 00:28:51,980
there's for 15 years now,

705
00:28:51,980 --> 00:28:54,188
there's what's called the deep learning revolution.

706
00:28:54,188 --> 00:28:56,446
Networks gotten larger and larger.

707
00:28:56,446 --> 00:28:58,799
And a certain point, the scaling laws appeared

708
00:28:58,799 --> 00:29:00,649
where people realized...

709
00:29:00,649 --> 00:29:02,057
- This is a scaling law shirt, by the way.

710
00:29:02,057 --> 00:29:03,041
(group laughing)

711
00:29:03,041 --> 00:29:03,874
- Representing.

712
00:29:03,874 --> 00:29:06,571
Scaling laws where it became more

713
00:29:06,571 --> 00:29:09,130
and more formalized that bigger is better

714
00:29:09,130 --> 00:29:12,692
across multiple dimensions of what bigger means.

715
00:29:12,692 --> 00:29:16,060
But these are all neural networks we're talking about.

716
00:29:16,060 --> 00:29:18,052
And we're talking about different architectures

717
00:29:18,052 --> 00:29:20,532
of how to construct these neural networks

718
00:29:20,532 --> 00:29:22,468
such that the training

719
00:29:22,468 --> 00:29:24,414
and the inference on them is super efficient.

720
00:29:24,414 --> 00:29:25,660
- Yeah, every different type of model

721
00:29:25,660 --> 00:29:27,529
has a different scaling law for it,

722
00:29:27,529 --> 00:29:30,661
which is effectively for how much compute you put in,

723
00:29:30,661 --> 00:29:33,529
the architecture will get

724
00:29:33,529 --> 00:29:36,571
to different levels of performance at test tasks.

725
00:29:36,571 --> 00:29:39,311
A mixture of experts is one of the ones at training time.

726
00:29:39,311 --> 00:29:41,221
Even if you don't consider the inference benefits,

727
00:29:41,221 --> 00:29:42,390
which are also big.

728
00:29:42,390 --> 00:29:44,782
At training time, your efficiency with your GPUs

729
00:29:44,782 --> 00:29:46,179
is dramatically improved

730
00:29:46,179 --> 00:29:48,631
by using this architecture if it is well-implemented.

731
00:29:48,631 --> 00:29:52,723
So, you can get effectively the same performance model

732
00:29:52,723 --> 00:29:57,721
and evaluation scores with numbers like 30% less compute.

733
00:29:57,721 --> 00:29:59,152
I think there's gonna be a wide variation,

734
00:29:59,152 --> 00:30:01,329
depending on your implementation details and stuff.

735
00:30:01,329 --> 00:30:04,461
But it is just important to realize that this type

736
00:30:04,461 --> 00:30:08,232
of technical innovation is something that gives huge gains.

737
00:30:08,232 --> 00:30:11,184
And I expect most companies that are serving their models

738
00:30:11,184 --> 00:30:14,369
to move to this mixture of experts implementation.

739
00:30:14,369 --> 00:30:16,824
Historically, the reason why not everyone might do it

740
00:30:16,824 --> 00:30:19,129
is because it's a implementation complexity,

741
00:30:19,129 --> 00:30:21,121
especially when doing these big models.

742
00:30:21,121 --> 00:30:24,172
So, this is one of the things that DeepSeek gets credit for

743
00:30:24,172 --> 00:30:26,020
is they do this extremely well.

744
00:30:26,020 --> 00:30:27,940
They do a mixture of experts extremely well.

745
00:30:27,940 --> 00:30:31,649
This architecture for what is called DeepSeekMoE.

746
00:30:31,649 --> 00:30:33,095
MoE is the shortened version

747
00:30:33,095 --> 00:30:35,951
of mixture of experts, is multiple papers old.

748
00:30:35,951 --> 00:30:38,029
This part of their training infrastructure

749
00:30:38,029 --> 00:30:40,940
is not new to these models alone,

750
00:30:40,940 --> 00:30:43,057
and same goes for what Dylan mentioned,

751
00:30:43,057 --> 00:30:45,090
with multi-head latent attention.

752
00:30:45,090 --> 00:30:48,346
This is all about reducing memory usage during inference.

753
00:30:48,346 --> 00:30:50,082
And same things during training,

754
00:30:50,082 --> 00:30:53,809
by using some fancy low rank approximation math.

755
00:30:53,809 --> 00:30:56,569
If you get into the details with this latent attention,

756
00:30:56,569 --> 00:30:58,099
it's one of those things I look at,

757
00:30:58,099 --> 00:30:58,932
and it's like, okay,

758
00:30:58,932 --> 00:31:01,639
they're doing really complex implementations,

759
00:31:01,639 --> 00:31:03,077
'cause there's other parts of language models

760
00:31:03,077 --> 00:31:05,210
such as embeddings

761
00:31:05,210 --> 00:31:06,959
that are used to extend the context length.

762
00:31:06,959 --> 00:31:09,143
The common one that DeepSeek used

763
00:31:09,143 --> 00:31:12,783
is rotary positional impendings, which is called RoPE.

764
00:31:12,783 --> 00:31:15,428
And if you want to use RoPE with a normal MoE,

765
00:31:15,428 --> 00:31:17,850
it's a sequential thing.

766
00:31:17,850 --> 00:31:19,348
You take two of the attention matrices

767
00:31:19,348 --> 00:31:22,622
and you rotate them by a complex value rotation,

768
00:31:22,622 --> 00:31:24,547
which is a matrix multiplication.

769
00:31:24,547 --> 00:31:27,879
With DeepSeek's MLA, with this new attention architecture,

770
00:31:27,879 --> 00:31:29,310
they need to do some clever things,

771
00:31:29,310 --> 00:31:31,187
because they're not set up the same

772
00:31:31,187 --> 00:31:34,126
and it just makes the implementation complexity much higher.

773
00:31:34,126 --> 00:31:35,739
So, they're managing all of these things,

774
00:31:35,739 --> 00:31:39,068
and these are probably the sort of things that OpenAI,

775
00:31:39,068 --> 00:31:40,478
these closed labs are doing.

776
00:31:40,478 --> 00:31:42,290
We don't know if they're doing the exact same techniques,

777
00:31:42,290 --> 00:31:44,106
but they actually shared them with the world,

778
00:31:44,106 --> 00:31:45,506
which is really nice to be like,

779
00:31:45,506 --> 00:31:46,475
this is the cutting edge

780
00:31:46,475 --> 00:31:48,930
of efficient language model training.

781
00:31:48,930 --> 00:31:51,949
- And some of this requires low level engineering,

782
00:31:51,949 --> 00:31:55,486
just is a giant mess in trickery.

783
00:31:55,486 --> 00:31:57,835
So, as I understand, that went below CUDA,

784
00:31:57,835 --> 00:32:01,187
so they go super low programming of GPUs.

785
00:32:01,187 --> 00:32:04,842
- Effectively, Nvidia builds this library called NCCL.

786
00:32:04,842 --> 00:32:07,223
In which, when you're training a model,

787
00:32:07,223 --> 00:32:08,842
you have all these communications

788
00:32:08,842 --> 00:32:10,905
between every single layer of the model

789
00:32:10,905 --> 00:32:12,524
and you may have over 100 layers.

790
00:32:12,524 --> 00:32:14,852
- What does the NCCL stand for? It's N-C-C-L?

791
00:32:14,852 --> 00:32:16,573
- Nvidia Communications Collectives Library.

792
00:32:16,573 --> 00:32:18,170
- [Lex] Nice. (Nathan laughing)

793
00:32:18,170 --> 00:32:19,263
Damn. - And so,

794
00:32:19,263 --> 00:32:20,973
(group laughing)

795
00:32:20,973 --> 00:32:22,745
when you're training a model,

796
00:32:22,745 --> 00:32:25,876
you're gonna have all reduces and all gathers.

797
00:32:25,876 --> 00:32:29,616
Between each layer, between the multi-layer perception

798
00:32:29,616 --> 00:32:30,607
or feedforward network

799
00:32:30,607 --> 00:32:32,169
and the attention mechanism you'll have,

800
00:32:32,169 --> 00:32:34,936
you'll have basically the model synchronized.

801
00:32:34,936 --> 00:32:38,142
Or you'll have all reduce and all gather.

802
00:32:38,142 --> 00:32:40,009
And this is a communication

803
00:32:40,009 --> 00:32:41,346
between all the GPUs in the network,

804
00:32:41,346 --> 00:32:43,081
whether it's in training or inference.

805
00:32:43,081 --> 00:32:44,832
So, Nvidia has a standard library.

806
00:32:44,832 --> 00:32:47,179
This is one of the reasons why it's really difficult

807
00:32:47,179 --> 00:32:49,642
to use anyone else's hardware for training

808
00:32:49,642 --> 00:32:51,421
is because no one's really built

809
00:32:51,421 --> 00:32:53,501
a standard communications library.

810
00:32:53,501 --> 00:32:55,850
And Nvidia's done this at a higher level.

811
00:32:55,850 --> 00:32:58,712
At DeepSeek, because they have certain limitations

812
00:32:58,712 --> 00:33:00,593
around the GPUs that they have access to,

813
00:33:00,593 --> 00:33:03,841
the interconnects are limited to some extent

814
00:33:03,841 --> 00:33:05,890
by the restrictions of the GPUs

815
00:33:05,890 --> 00:33:07,205
that were shipped into China legally,

816
00:33:07,205 --> 00:33:09,443
not the ones that are smuggled, but legally shipped in,

817
00:33:09,443 --> 00:33:11,258
that they used to train this model.

818
00:33:11,258 --> 00:33:14,629
They had to figure out how to get efficiencies.

819
00:33:14,629 --> 00:33:17,149
And one of those things is that instead

820
00:33:17,149 --> 00:33:20,269
of just calling the Nvidia library NCCL,

821
00:33:20,269 --> 00:33:22,176
they instead created their,

822
00:33:22,176 --> 00:33:24,384
they scheduled their own communications,

823
00:33:24,384 --> 00:33:26,551
which some of the labs do.

824
00:33:27,434 --> 00:33:29,156
Meta talked about in Llama 3

825
00:33:29,156 --> 00:33:31,743
how they made their own custom version of NCCL.

826
00:33:31,743 --> 00:33:33,383
They didn't talk about the implementation details.

827
00:33:33,383 --> 00:33:34,945
This is some of what they did.

828
00:33:34,945 --> 00:33:37,705
Probably not as well as, maybe not as well as DeepSeek

829
00:33:37,705 --> 00:33:40,422
because DeepSeek necessity is the mother of innovation

830
00:33:40,422 --> 00:33:42,276
and they had to do this.

831
00:33:42,276 --> 00:33:44,436
Whereas in the ca...

832
00:33:44,436 --> 00:33:45,852
OpenAI has people

833
00:33:45,852 --> 00:33:47,698
that do this sort of stuff, Anthropic, et cetera.

834
00:33:47,698 --> 00:33:50,435
But DeepSeek certainly did it publicly

835
00:33:50,435 --> 00:33:51,605
and they may have done it even better

836
00:33:51,605 --> 00:33:53,895
because they were gimp on a certain aspect

837
00:33:53,895 --> 00:33:55,915
of the chips that they have access to.

838
00:33:55,915 --> 00:33:58,998
And so, they scheduled communications

839
00:34:00,585 --> 00:34:02,703
by scheduling specific SMs.

840
00:34:02,703 --> 00:34:05,880
SMs you could think of as like the core on a GPU.

841
00:34:05,880 --> 00:34:07,911
So, there's hundreds of cores

842
00:34:07,911 --> 00:34:11,670
or there's a bit over 100 cores, SMs, on a GPU,

843
00:34:11,670 --> 00:34:12,842
and they were specifically scheduling,

844
00:34:12,842 --> 00:34:14,500
hey, which ones are running the model?

845
00:34:14,500 --> 00:34:15,928
Which ones are doing all reduce?

846
00:34:15,928 --> 00:34:17,058
Which one are doing all gather?

847
00:34:17,058 --> 00:34:19,080
And they would flip back and forth between them.

848
00:34:19,080 --> 00:34:21,880
And this requires extremely low level programming.

849
00:34:21,880 --> 00:34:23,518
- This is what NCCL does automatically

850
00:34:23,518 --> 00:34:25,069
or other Nvidia libraries

851
00:34:25,069 --> 00:34:26,489
handle this automatically usually.

852
00:34:26,489 --> 00:34:27,322
- Yeah, exactly.

853
00:34:27,322 --> 00:34:30,128
And so, technically, they're using PTX,

854
00:34:30,128 --> 00:34:32,188
which is like, you could think of it

855
00:34:32,188 --> 00:34:33,887
as like an assembly type language.

856
00:34:33,887 --> 00:34:35,981
It's not exactly that or instruction set.

857
00:34:35,981 --> 00:34:38,217
Coding directly to assembly or instruction set.

858
00:34:38,217 --> 00:34:39,164
It's not exactly that,

859
00:34:39,164 --> 00:34:42,087
but that's still part of technically CUDA.

860
00:34:42,087 --> 00:34:44,315
But it's like, do I wanna write in Python,

861
00:34:44,315 --> 00:34:47,181
PyTorch equivalent, and call Nvidia libraries?

862
00:34:47,181 --> 00:34:48,812
Do I want to go down to the C level.

863
00:34:48,812 --> 00:34:50,887
Or in code, even lower level?

864
00:34:50,887 --> 00:34:51,719
Or do I wanna go all the way down

865
00:34:51,719 --> 00:34:53,360
to the assembly or ISA level?

866
00:34:53,360 --> 00:34:54,413
And there are cases

867
00:34:54,413 --> 00:34:57,054
where you go all the way down there at the very big labs,

868
00:34:57,054 --> 00:34:59,306
but most companies just do not do that

869
00:34:59,306 --> 00:35:00,827
because it's a waste of time

870
00:35:00,827 --> 00:35:03,767
and the efficiency gains you get are not worth it.

871
00:35:03,767 --> 00:35:06,516
But DeepSeek's implementation is so complex.

872
00:35:06,516 --> 00:35:09,458
Especially with their mixture of experts.

873
00:35:09,458 --> 00:35:10,550
People have done mixture of experts,

874
00:35:10,550 --> 00:35:13,069
but they're generally 8, 16 experts.

875
00:35:13,069 --> 00:35:14,358
And they activate too.

876
00:35:14,358 --> 00:35:15,556
So, one of the words

877
00:35:15,556 --> 00:35:19,327
that we like to use is sparsity factor or usage.

878
00:35:19,327 --> 00:35:23,467
So, you might have four, one fourth of your model activate.

879
00:35:23,467 --> 00:35:27,503
And that's what misdraws mixed role model.

880
00:35:27,503 --> 00:35:29,982
They're model that really catapulted them to like,

881
00:35:29,982 --> 00:35:32,165
oh my god, they're really, really good.

882
00:35:32,165 --> 00:35:34,223
OpenAI has also had models that are MoE,

883
00:35:34,223 --> 00:35:37,633
and so have all the other labs that are major closed.

884
00:35:37,633 --> 00:35:40,713
But what DeepSeek did that maybe only the leading labs

885
00:35:40,713 --> 00:35:42,283
have only just started recently doing

886
00:35:42,283 --> 00:35:44,221
is have such a high sparsity factor.

887
00:35:44,221 --> 00:35:45,412
It's not one fourth of the model.

888
00:35:45,412 --> 00:35:47,612
Two out of eight experts activating

889
00:35:47,612 --> 00:35:49,131
every time you go through the model.

890
00:35:49,131 --> 00:35:51,150
It's 8 out of 256.

891
00:35:51,150 --> 00:35:53,481
- And there's different implementations from mixture

892
00:35:53,481 --> 00:35:56,630
of experts where you can have some of these experts

893
00:35:56,630 --> 00:35:58,563
that are always activated,

894
00:35:58,563 --> 00:36:02,091
which this just looks like a small neural network.

895
00:36:02,091 --> 00:36:03,701
And then, all the tokens go through that.

896
00:36:03,701 --> 00:36:05,590
And then, they also go through some

897
00:36:05,590 --> 00:36:08,772
that are selected by this routing mechanism.

898
00:36:08,772 --> 00:36:12,942
And one of the innovations in DeepSeek's architecture

899
00:36:12,942 --> 00:36:14,911
is that they change the routing mechanism

900
00:36:14,911 --> 00:36:16,553
in mixture of expert models.

901
00:36:16,553 --> 00:36:18,672
There's something called an auxiliary loss,

902
00:36:18,672 --> 00:36:21,063
which effectively means during training,

903
00:36:21,063 --> 00:36:23,200
you want to make sure that all of these experts

904
00:36:23,200 --> 00:36:26,611
are used across the tasks that the model sees.

905
00:36:26,611 --> 00:36:28,679
Why there can be failures in mixture of experts

906
00:36:28,679 --> 00:36:32,093
is that when you're doing this training,

907
00:36:32,093 --> 00:36:35,733
the one objective is token prediction accuracy.

908
00:36:35,733 --> 00:36:37,474
And if you just let turning go

909
00:36:37,474 --> 00:36:39,320
with a mixture of expert model on your own,

910
00:36:39,320 --> 00:36:41,932
it can be that the model learns

911
00:36:41,932 --> 00:36:44,754
to only use a subset of the experts.

912
00:36:44,754 --> 00:36:46,782
And in the MoE literature,

913
00:36:46,782 --> 00:36:48,611
there's something called the auxiliary loss,

914
00:36:48,611 --> 00:36:50,289
which helps balance them.

915
00:36:50,289 --> 00:36:53,861
But if you think about the loss functions of deep learning,

916
00:36:53,861 --> 00:36:55,806
this even connects to the bitter lesson,

917
00:36:55,806 --> 00:36:59,556
is that you want to have the minimum inductive bias

918
00:36:59,556 --> 00:37:02,168
in your model to let the model learn maximally.

919
00:37:02,168 --> 00:37:03,437
And this auxiliary loss,

920
00:37:03,437 --> 00:37:07,196
this balancing across experts could be seen as intention

921
00:37:07,196 --> 00:37:09,584
with the prediction accuracy of the tokens.

922
00:37:09,584 --> 00:37:11,236
So, we don't know the exact extent

923
00:37:11,236 --> 00:37:13,262
that the DeepSeek MoE change,

924
00:37:13,262 --> 00:37:15,465
which is instead of doing an auxiliary loss,

925
00:37:15,465 --> 00:37:17,876
they have an extra parameter in their routing,

926
00:37:17,876 --> 00:37:19,035
which after the batches,

927
00:37:19,035 --> 00:37:21,532
they update this parameter to make sure

928
00:37:21,532 --> 00:37:24,453
that the next batches all have a similar use of experts.

929
00:37:24,453 --> 00:37:26,384
And this type of change can be big,

930
00:37:26,384 --> 00:37:28,436
it can be small, but they add up over time.

931
00:37:28,436 --> 00:37:29,527
And this is the sort of thing

932
00:37:29,527 --> 00:37:31,248
that just points to them innovating.

933
00:37:31,248 --> 00:37:33,784
And I'm sure all the labs that are training big MoEs

934
00:37:33,784 --> 00:37:35,208
are looking at this sort of things,

935
00:37:35,208 --> 00:37:37,252
which is getting away from the auxiliary loss.

936
00:37:37,252 --> 00:37:38,492
Some of them might already use it,

937
00:37:38,492 --> 00:37:40,345
but you keep accumulating gains.

938
00:37:40,345 --> 00:37:43,207
And we'll talk about the philosophy of training

939
00:37:43,207 --> 00:37:45,774
and how you organize these organizations.

940
00:37:45,774 --> 00:37:48,315
And a lot of it is just compounding small improvements

941
00:37:48,315 --> 00:37:50,644
over time in your data, in your architecture,

942
00:37:50,644 --> 00:37:51,477
in your post-training,

943
00:37:51,477 --> 00:37:53,885
and how they integrate with each other.

944
00:37:53,885 --> 00:37:55,125
And DeepSeek does the same thing

945
00:37:55,125 --> 00:37:57,225
and some of 'em are shared or a lot,

946
00:37:57,225 --> 00:37:58,805
we have to take them on face value

947
00:37:58,805 --> 00:38:00,565
that they share their most important details.

948
00:38:00,565 --> 00:38:02,421
The architecture and the weights are out there,

949
00:38:02,421 --> 00:38:05,135
so we're seeing what they're doing, and it adds up.

950
00:38:05,135 --> 00:38:08,342
- Going back to the efficiency and complexity point.

951
00:38:08,342 --> 00:38:10,925
It's 32 versus 4 for mixed draw

952
00:38:12,069 --> 00:38:14,701
and other MoE models that have been publicly released.

953
00:38:14,701 --> 00:38:16,257
So, this ratio is extremely high.

954
00:38:16,257 --> 00:38:18,434
And what Nathan was getting at there was,

955
00:38:18,434 --> 00:38:21,633
when you have such a different level of sparsity,

956
00:38:21,633 --> 00:38:25,481
you can't just have every GPU have the entire model.

957
00:38:25,481 --> 00:38:27,591
The model's too big, there's too much complexity there.

958
00:38:27,591 --> 00:38:29,043
So, you have to split up the model

959
00:38:29,043 --> 00:38:30,988
with different types of parallelism.

960
00:38:30,988 --> 00:38:33,043
And so, you might have different experts

961
00:38:33,043 --> 00:38:34,159
on different GPU nodes,

962
00:38:34,159 --> 00:38:38,268
but now what happens when this set of data that you get,

963
00:38:38,268 --> 00:38:40,383
hey, all of it looks like this one way

964
00:38:40,383 --> 00:38:44,127
and all of it should route to one part of my model.

965
00:38:44,127 --> 00:38:47,157
So, when all of it routes to one part of the model,

966
00:38:47,157 --> 00:38:49,990
then you can have this overloading

967
00:38:51,287 --> 00:38:53,495
of a certain set of the GPU resources

968
00:38:53,495 --> 00:38:55,107
or a certain set of the GPUs,

969
00:38:55,107 --> 00:38:59,028
and then the rest of the training network sits idle,

970
00:38:59,028 --> 00:39:00,760
because all of the tokens are just routing to that.

971
00:39:00,760 --> 00:39:01,858
So, this is the biggest complexity,

972
00:39:01,858 --> 00:39:02,711
one of the big complexities

973
00:39:02,711 --> 00:39:06,952
with running a very sparse mixture of experts model,

974
00:39:06,952 --> 00:39:10,786
i.e, this 32 ratio versus this 4 ratio,

975
00:39:10,786 --> 00:39:12,708
is that you end up with so many

976
00:39:12,708 --> 00:39:14,443
of the experts just sitting there idle.

977
00:39:14,443 --> 00:39:16,078
So, how do I load balance between them?

978
00:39:16,078 --> 00:39:18,507
How do I schedule the communications between them?

979
00:39:18,507 --> 00:39:21,834
This is a lot of the extremely low level detailed work

980
00:39:21,834 --> 00:39:25,163
that they figured out in the public first

981
00:39:25,163 --> 00:39:27,793
and potentially second or third in the world,

982
00:39:27,793 --> 00:39:29,961
and maybe even first in some cases.

983
00:39:29,961 --> 00:39:33,162
- What lesson do you, in the direction

984
00:39:33,162 --> 00:39:36,098
of the bitter lesson, do you take from all of this?

985
00:39:36,098 --> 00:39:37,532
Is this going to be the direction

986
00:39:37,532 --> 00:39:39,270
where a lot of the gain is going to be,

987
00:39:39,270 --> 00:39:41,801
which is this kind of low level optimization?

988
00:39:41,801 --> 00:39:44,074
Or is this a short-term thing

989
00:39:44,074 --> 00:39:46,369
where the biggest gains will be more

990
00:39:46,369 --> 00:39:50,680
on the algorithmic high level side of post-training?

991
00:39:50,680 --> 00:39:52,630
Is this like a short-term leap

992
00:39:52,630 --> 00:39:54,642
because they've figured out like a hack,

993
00:39:54,642 --> 00:39:58,611
because constraints, necessity is the mother of invention,

994
00:39:58,611 --> 00:40:00,998
or is there still a lot of gains?

995
00:40:00,998 --> 00:40:01,988
- I think we should summarize

996
00:40:01,988 --> 00:40:04,010
what the bitter lesson actually is about,

997
00:40:04,010 --> 00:40:05,880
is that the bitter lesson, - Okay.

998
00:40:05,880 --> 00:40:07,590
- essentially, if you paraphrase it,

999
00:40:07,590 --> 00:40:10,280
is that the types of training

1000
00:40:10,280 --> 00:40:11,978
that will win out in deep learning

1001
00:40:11,978 --> 00:40:16,120
as we go are those methods that are which are scalable

1002
00:40:16,120 --> 00:40:18,811
in learning and search is what it calls out.

1003
00:40:18,811 --> 00:40:23,061
And the scale word gets a lot of attention in this.

1004
00:40:24,981 --> 00:40:27,739
The interpretation that I use is effectively

1005
00:40:27,739 --> 00:40:32,572
to avoid adding the human priors to your learning process.

1006
00:40:33,764 --> 00:40:35,091
And if you read the original essay,

1007
00:40:35,091 --> 00:40:36,107
this is what it talks about

1008
00:40:36,107 --> 00:40:39,169
is how researchers will try to come up

1009
00:40:39,169 --> 00:40:42,689
with what clever solutions to their specific problem

1010
00:40:42,689 --> 00:40:45,868
that might get them small gains in the short-term

1011
00:40:45,868 --> 00:40:48,796
while simply enabling these deep learning systems

1012
00:40:48,796 --> 00:40:50,500
to work efficiently.

1013
00:40:50,500 --> 00:40:52,947
And for these bigger problems in the long-term

1014
00:40:52,947 --> 00:40:57,947
might be more likely to scale and continue to drive success.

1015
00:40:58,209 --> 00:40:59,884
And therefore, we were talking

1016
00:40:59,884 --> 00:41:02,447
about relatively small implementation changes

1017
00:41:02,447 --> 00:41:05,198
to the mixture of experts model.

1018
00:41:05,198 --> 00:41:06,714
And therefore, it's like, okay,

1019
00:41:06,714 --> 00:41:09,658
we will need a few more years to know if one

1020
00:41:09,658 --> 00:41:12,247
of these were actually really crucial to the bitter lesson.

1021
00:41:12,247 --> 00:41:13,805
But the bitter lesson is really

1022
00:41:13,805 --> 00:41:17,390
this long-term arc of how simplicity can often win.

1023
00:41:17,390 --> 00:41:19,007
And there's a lot of sayings in the industry,

1024
00:41:19,007 --> 00:41:20,557
like the models just wanna learn.

1025
00:41:20,557 --> 00:41:23,577
You have to give them the simple loss landscape

1026
00:41:23,577 --> 00:41:25,497
where you put compute through the model,

1027
00:41:25,497 --> 00:41:29,468
and they will learn and getting barriers out of the way.

1028
00:41:29,468 --> 00:41:31,915
- That's where the power, something like NCCL comes in,

1029
00:41:31,915 --> 00:41:34,515
where standardized code

1030
00:41:34,515 --> 00:41:36,196
that could be used by a lot of people

1031
00:41:36,196 --> 00:41:39,371
to create sort of simple innovations that can scale.

1032
00:41:39,371 --> 00:41:42,776
Which is why the hacks, I imagine the code base

1033
00:41:42,776 --> 00:41:45,289
for DeepSeek is probably a giant mess.

1034
00:41:45,289 --> 00:41:46,122
- I'm sure they have,

1035
00:41:46,122 --> 00:41:49,086
DeepSeek definitely has code bases that are extremely messy,

1036
00:41:49,086 --> 00:41:50,595
where they're testing these new ideas,

1037
00:41:50,595 --> 00:41:51,937
multi-head latent attention,

1038
00:41:51,937 --> 00:41:55,300
probably could start in something like a Jupyter Notebook

1039
00:41:55,300 --> 00:41:57,573
or somebody try something on a few GPUs.

1040
00:41:57,573 --> 00:41:59,715
And that is really messy.

1041
00:41:59,715 --> 00:42:02,081
But the stuff that trains the DeepSeek-V3

1042
00:42:02,081 --> 00:42:04,495
and DeepSeek-R1, those libraries,

1043
00:42:04,495 --> 00:42:06,801
if you were to present them to us,

1044
00:42:06,801 --> 00:42:10,210
I would guess are extremely high quality code.

1045
00:42:10,210 --> 00:42:12,604
- It's a high quality readable code. Yeah.

1046
00:42:12,604 --> 00:42:14,776
- I think there is one aspect to note though.

1047
00:42:14,776 --> 00:42:17,776
Is that there is the general ability

1048
00:42:18,735 --> 00:42:21,254
for that to transfer across different types of runs.

1049
00:42:21,254 --> 00:42:23,834
You may make really, really high quality code

1050
00:42:23,834 --> 00:42:26,432
for one specific model architecture

1051
00:42:26,432 --> 00:42:27,900
at one size. - Yeah.

1052
00:42:27,900 --> 00:42:30,193
- And then, that is not transferable to,

1053
00:42:30,193 --> 00:42:32,030
hey, when I make this architecture tweak,

1054
00:42:32,030 --> 00:42:33,263
everything's broken again.

1055
00:42:33,263 --> 00:42:35,763
That's something that could be

1056
00:42:36,739 --> 00:42:40,444
with their specific low level coding of scheduling SMs,

1057
00:42:40,444 --> 00:42:43,491
is specific to this model architecture and size.

1058
00:42:43,491 --> 00:42:46,202
And whereas like Nvidia's collectives library

1059
00:42:46,202 --> 00:42:48,701
is more like, hey, it'll work for anything.

1060
00:42:48,701 --> 00:42:50,429
You wanna do an all reduce, great,

1061
00:42:50,429 --> 00:42:52,652
I don't care what your model architecture is, it'll work.

1062
00:42:52,652 --> 00:42:54,332
And you're giving up a lot of performance

1063
00:42:54,332 --> 00:42:56,413
when you do that in many cases.

1064
00:42:56,413 --> 00:42:57,792
But it's worthwhile for them

1065
00:42:57,792 --> 00:43:01,980
to do the specific optimization for the specific run,

1066
00:43:01,980 --> 00:43:04,403
given the constraints that they have regarding compute.

1067
00:43:04,403 --> 00:43:07,721
- I wonder how stressful it is to like,

1068
00:43:07,721 --> 00:43:10,601
these frontier models, like initiate training,

1069
00:43:10,601 --> 00:43:11,965
like to have the code-

1070
00:43:11,965 --> 00:43:13,152
- [Nathan] Push the button.

1071
00:43:13,152 --> 00:43:14,092
- to push the button (Nathan laughing)

1072
00:43:14,092 --> 00:43:18,688
that you're now spending a large amount of money

1073
00:43:18,688 --> 00:43:21,239
and time to train this.

1074
00:43:21,239 --> 00:43:25,347
There must be a lot of innovation on the debugging stage

1075
00:43:25,347 --> 00:43:29,417
of making sure there's no issues that you're monitoring

1076
00:43:29,417 --> 00:43:30,479
and visualizing every aspect

1077
00:43:30,479 --> 00:43:32,969
of the training, all that kind of stuff.

1078
00:43:32,969 --> 00:43:34,119
- When people are training,

1079
00:43:34,119 --> 00:43:35,347
they have all these various dashboards,

1080
00:43:35,347 --> 00:43:36,867
but the most simple one - Yeah.

1081
00:43:36,867 --> 00:43:38,774
- is your loss. - Right.

1082
00:43:38,774 --> 00:43:40,104
- And it continues to go down.

1083
00:43:40,104 --> 00:43:41,273
But in reality,

1084
00:43:41,273 --> 00:43:43,816
especially with more complicated stuff like MoE,

1085
00:43:43,816 --> 00:43:45,183
the biggest problem with it,

1086
00:43:45,183 --> 00:43:47,240
or FP8 training, which is another innovation,

1087
00:43:47,240 --> 00:43:49,180
going to a lower precision number format,

1088
00:43:49,180 --> 00:43:52,009
i.e, less accurate, is that you end up with loss spikes.

1089
00:43:52,009 --> 00:43:54,474
And no one knows why the loss spike happened.

1090
00:43:54,474 --> 00:43:56,171
And for a lot- - Some of them you do.

1091
00:43:56,171 --> 00:43:57,784
- Some of them you do. - Some of them are bad data.

1092
00:43:57,784 --> 00:43:59,384
I give AI2's example

1093
00:43:59,384 --> 00:44:01,087
of what blew up our earlier models

1094
00:44:01,087 --> 00:44:03,489
is a subreddit called Microwave Gang.

1095
00:44:03,489 --> 00:44:04,816
We love to shoutout this out. (Lex laughing)

1096
00:44:04,816 --> 00:44:05,649
It's a real thing.

1097
00:44:05,649 --> 00:44:07,169
You can pull up Microwave Gang.

1098
00:44:07,169 --> 00:44:08,261
Essentially, it's a subreddit

1099
00:44:08,261 --> 00:44:11,050
where everybody makes posts that are just the letter M.

1100
00:44:11,050 --> 00:44:12,449
So, it's like, mm.

1101
00:44:12,449 --> 00:44:14,457
So, there's extremely long sequences

1102
00:44:14,457 --> 00:44:16,939
of the letter M, and then the comments are like, beep beep.

1103
00:44:16,939 --> 00:44:17,826
'Cause it's in the microwave end.

1104
00:44:17,826 --> 00:44:18,876
- Yeah. - If you pass this

1105
00:44:18,876 --> 00:44:21,685
into a model that's trained to be a normal producing text,

1106
00:44:21,685 --> 00:44:23,097
it's extremely high loss.

1107
00:44:23,097 --> 00:44:23,996
'Cause normally, - Yeah.

1108
00:44:23,996 --> 00:44:25,007
- you see an M. (Lex laughing)

1109
00:44:25,007 --> 00:44:27,267
You don't predict Ms for a long time.

1110
00:44:27,267 --> 00:44:29,878
So, this is something that cause a loss spikes for us.

1111
00:44:29,878 --> 00:44:31,049
But when you have much like,

1112
00:44:31,049 --> 00:44:33,150
this is old, this is not recent.

1113
00:44:33,150 --> 00:44:35,220
And when you have more mature data systems,

1114
00:44:35,220 --> 00:44:37,219
that's not the thing that causes the loss spike.

1115
00:44:37,219 --> 00:44:38,592
And what Dylan is saying is true,

1116
00:44:38,592 --> 00:44:41,617
but it's levels to this sort of idea.

1117
00:44:41,617 --> 00:44:43,009
- With regards to the stress,

1118
00:44:43,009 --> 00:44:44,194
(Nathan and Lex laughing)

1119
00:44:44,194 --> 00:44:46,199
these people are like, you'll go out to dinner

1120
00:44:46,199 --> 00:44:48,121
with a friend that works at one of these labs.

1121
00:44:48,121 --> 00:44:50,004
And they'll just be like (Nathan laughing)

1122
00:44:50,004 --> 00:44:51,718
looking at their phone every 10 minutes,

1123
00:44:51,718 --> 00:44:53,090
and they're not like...

1124
00:44:53,090 --> 00:44:54,129
It's one thing if they're texting,

1125
00:44:54,129 --> 00:44:55,569
but they're just like, - Yeah.

1126
00:44:55,569 --> 00:44:57,430
- is the loss- - It's literal.

1127
00:44:57,430 --> 00:45:00,788
The tokens per second loss, not blown up.

1128
00:45:00,788 --> 00:45:03,223
They're just watching this. (chuckles)

1129
00:45:03,223 --> 00:45:05,330
- And the heart rate goes up if there's a spike.

1130
00:45:05,330 --> 00:45:07,031
- And some level of spikes is normal.

1131
00:45:07,031 --> 00:45:08,920
It'll recover and be back.

1132
00:45:08,920 --> 00:45:10,589
Sometimes a lot of the old strategy was like,

1133
00:45:10,589 --> 00:45:13,029
you just stop the run, restart from an old version,

1134
00:45:13,029 --> 00:45:15,930
and then change the data mix, and then it keeps going.

1135
00:45:15,930 --> 00:45:17,822
- There are even different types of spikes.

1136
00:45:17,822 --> 00:45:21,131
So, Dirk Groeneveld has a theory of AI too

1137
00:45:21,131 --> 00:45:22,600
that's like fast spikes and slow spikes,

1138
00:45:22,600 --> 00:45:25,020
where there are sometimes where you're looking at the loss

1139
00:45:25,020 --> 00:45:25,920
and there are other parameters,

1140
00:45:25,920 --> 00:45:28,749
you can see it start to creep up, and then blow up.

1141
00:45:28,749 --> 00:45:30,132
And that's really hard to recover from.

1142
00:45:30,132 --> 00:45:31,499
So, you have to go back much further.

1143
00:45:31,499 --> 00:45:33,336
So, you have the stressful period, where it's like flat

1144
00:45:33,336 --> 00:45:35,198
or it might start going up and you're like, what do I do?

1145
00:45:35,198 --> 00:45:37,690
Whereas there are also lost spikes that are, it looks good.

1146
00:45:37,690 --> 00:45:39,298
And then, there's one spiky data point.

1147
00:45:39,298 --> 00:45:41,202
And what you could do is you just skip those.

1148
00:45:41,202 --> 00:45:42,694
You see that there's a spike.

1149
00:45:42,694 --> 00:45:44,171
You're like, okay, I can ignore this data,

1150
00:45:44,171 --> 00:45:45,172
don't update the model,

1151
00:45:45,172 --> 00:45:47,230
and do the next one, and it'll recover quickly.

1152
00:45:47,230 --> 00:45:50,440
But these like untrickier implementation,

1153
00:45:50,440 --> 00:45:52,983
so as you get more complex in your architecture

1154
00:45:52,983 --> 00:45:54,910
and you scale up to more GPUs,

1155
00:45:54,910 --> 00:45:58,162
you have more potential for your loss blowing up.

1156
00:45:58,162 --> 00:45:59,109
So, it's like there's

1157
00:45:59,109 --> 00:46:00,595
a distribution. - And then,

1158
00:46:00,595 --> 00:46:02,145
the whole idea of grokking also comes in.

1159
00:46:02,145 --> 00:46:04,606
It's like, just because it's slowed down from improving

1160
00:46:04,606 --> 00:46:06,054
and loss doesn't mean it's not learning,

1161
00:46:06,054 --> 00:46:07,947
because all of a sudden, it could be like this

1162
00:46:07,947 --> 00:46:09,595
and it could just spike down in loss again

1163
00:46:09,595 --> 00:46:12,466
because it learned, truly learned something.

1164
00:46:12,466 --> 00:46:14,204
And it took some time for it to learn that.

1165
00:46:14,204 --> 00:46:15,882
It's not like a gradual process.

1166
00:46:15,882 --> 00:46:17,435
And that's what humans are like,

1167
00:46:17,435 --> 00:46:18,353
that's what models are like.

1168
00:46:18,353 --> 00:46:21,023
It's really a stressful task as you mentioned.

1169
00:46:21,023 --> 00:46:24,419
- And the whole time, the dollar count is going up.

1170
00:46:24,419 --> 00:46:26,401
- Every company has failed runs.

1171
00:46:26,401 --> 00:46:27,234
You need failed run

1172
00:46:27,234 --> 00:46:29,135
to push the envelope on your infrastructure.

1173
00:46:29,135 --> 00:46:30,311
So, a lot of news cycles

1174
00:46:30,311 --> 00:46:33,564
are made of X company had Y failed run.

1175
00:46:33,564 --> 00:46:34,861
Every company that's trying

1176
00:46:34,861 --> 00:46:37,992
to push the frontier of AI has these.

1177
00:46:37,992 --> 00:46:40,710
So, yes, it's noteworthy because it's a lot of money

1178
00:46:40,710 --> 00:46:43,000
and it can be week to month setback,

1179
00:46:43,000 --> 00:46:44,884
but it is part of the process.

1180
00:46:44,884 --> 00:46:47,537
- But how do you get, if you're DeepSeek,

1181
00:46:47,537 --> 00:46:50,063
how do you get to a place where holy shit,

1182
00:46:50,063 --> 00:46:52,793
there's a successful combination of hyperparameters?

1183
00:46:52,793 --> 00:46:54,485
- A lot of small failed runs.

1184
00:46:54,485 --> 00:46:57,469
- And so, rapid iteration (Nathan chuckles)

1185
00:46:57,469 --> 00:46:58,377
through failed runs,

1186
00:46:58,377 --> 00:47:00,639
until- - Yeah, and successful ones.

1187
00:47:00,639 --> 00:47:01,750
You just- - And then, you build up

1188
00:47:01,750 --> 00:47:05,703
some intuition like this, this mixture of expert works,

1189
00:47:05,703 --> 00:47:09,076
and then this implementation of MLA works.

1190
00:47:09,076 --> 00:47:11,267
- Key hyperparameters like learning rate

1191
00:47:11,267 --> 00:47:14,515
and regularization and things like this.

1192
00:47:14,515 --> 00:47:17,630
And you find the regime that works for your code base.

1193
00:47:17,630 --> 00:47:20,119
Talking to people at Frontier Labs, there's a story

1194
00:47:20,119 --> 00:47:22,030
that you can tell where training language models

1195
00:47:22,030 --> 00:47:25,005
is kind of a path that you need to follow.

1196
00:47:25,005 --> 00:47:27,028
So, you need to unlock the ability

1197
00:47:27,028 --> 00:47:29,733
to train a certain type of model or a certain scale.

1198
00:47:29,733 --> 00:47:31,608
And then, your code base and your internal knowhow

1199
00:47:31,608 --> 00:47:34,289
of which hyperparameters work for it is known.

1200
00:47:34,289 --> 00:47:36,039
And you look at the DeepSeek papers and models,

1201
00:47:36,039 --> 00:47:38,289
they've scaled up, they've added complexity,

1202
00:47:38,289 --> 00:47:39,970
and it's just continuing

1203
00:47:39,970 --> 00:47:41,929
to build the capabilities that they have.

1204
00:47:41,929 --> 00:47:44,170
- There's the concept of a YOLO run.

1205
00:47:44,170 --> 00:47:45,212
(Nathan laughing)

1206
00:47:45,212 --> 00:47:47,470
So, YOLO, you only live once. - Yep.

1207
00:47:47,470 --> 00:47:48,596
- And what it is, is like,

1208
00:47:48,596 --> 00:47:52,686
there's all this experimentation you do at the small-scale.

1209
00:47:52,686 --> 00:47:55,484
Research ablations, you have your Jupyter Notebook with,

1210
00:47:55,484 --> 00:47:59,101
you're experimenting with MLA on three GPUs or whatever,

1211
00:47:59,101 --> 00:48:01,765
and you're doing all these different things like, hey,

1212
00:48:01,765 --> 00:48:04,734
do I do 4 expert, 4 active experts, 128 experts,

1213
00:48:04,734 --> 00:48:06,742
do I arrange the experts this way?

1214
00:48:06,742 --> 00:48:08,995
All these different model architecture things

1215
00:48:08,995 --> 00:48:10,534
you're testing at a very small-scale.

1216
00:48:10,534 --> 00:48:13,857
Couple researchers, few GPUs, tens of GPUs,

1217
00:48:13,857 --> 00:48:15,273
hundreds of GPUs, whatever it is.

1218
00:48:15,273 --> 00:48:16,191
And then, all of a sudden,

1219
00:48:16,191 --> 00:48:18,695
you're like, okay guys, no more fucking around.

1220
00:48:18,695 --> 00:48:19,780
No more screwing around.

1221
00:48:19,780 --> 00:48:22,921
Everyone, take all the resources we have,

1222
00:48:22,921 --> 00:48:25,690
let's pick what we think will work, and just go for it.

1223
00:48:25,690 --> 00:48:26,523
YOLO. - Yeah.

1224
00:48:26,523 --> 00:48:28,657
- And this is where that sort of stress comes in is like,

1225
00:48:28,657 --> 00:48:30,309
well, I know it works here,

1226
00:48:30,309 --> 00:48:32,342
but some things that work here don't work here,

1227
00:48:32,342 --> 00:48:34,911
and some things that work here don't work down here

1228
00:48:34,911 --> 00:48:36,151
- Yeah. - in this terms of scale.

1229
00:48:36,151 --> 00:48:39,332
So, it's really truly a YOLO run,

1230
00:48:39,332 --> 00:48:43,770
and there's this discussion of certain researchers

1231
00:48:43,770 --> 00:48:45,370
just have this methodical nature.

1232
00:48:45,370 --> 00:48:47,151
They can find the whole search space

1233
00:48:47,151 --> 00:48:49,592
and figure out all the ablations of different research,

1234
00:48:49,592 --> 00:48:51,212
and really see what is best.

1235
00:48:51,212 --> 00:48:53,712
And there's certain researchers who just like,

1236
00:48:53,712 --> 00:48:56,169
have that innate gut instinct of like,

1237
00:48:56,169 --> 00:48:57,484
this is the YOLO run.

1238
00:48:57,484 --> 00:48:58,621
I'm looking at the data, (Dylan drowns out Nathan)

1239
00:48:58,621 --> 00:48:59,853
I think this is it.

1240
00:48:59,853 --> 00:49:01,584
- This is why you wanna work in post-training,

1241
00:49:01,584 --> 00:49:03,611
because the GPU cost for training is lower.

1242
00:49:03,611 --> 00:49:05,004
So, you can make a higher percentage

1243
00:49:05,004 --> 00:49:06,464
of your training runs YOLO runs

1244
00:49:06,464 --> 00:49:07,874
- Yeah. - For now.

1245
00:49:07,874 --> 00:49:10,530
- Yeah, for now, for now. - For now, for now. (laughs)

1246
00:49:10,530 --> 00:49:14,523
- So, some of this is fundamentally luck still.

1247
00:49:14,523 --> 00:49:16,552
- Luck is skill in many cases.

1248
00:49:16,552 --> 00:49:18,864
- Yeah, it looks lucky right when you're-

1249
00:49:18,864 --> 00:49:22,031
- But the hill to climb, if you're on one of these labs

1250
00:49:22,031 --> 00:49:23,601
and you have an evaluation and you're not crushing,

1251
00:49:23,601 --> 00:49:27,000
there's a repeated playbook of how you improve things.

1252
00:49:27,000 --> 00:49:27,999
There are localized improvements,

1253
00:49:27,999 --> 00:49:29,151
which might be data improvements.

1254
00:49:29,151 --> 00:49:29,984
And these add up

1255
00:49:29,984 --> 00:49:31,972
into the whole model just being much better.

1256
00:49:31,972 --> 00:49:33,419
And when you zoom in really close,

1257
00:49:33,419 --> 00:49:34,651
it can be really obvious

1258
00:49:34,651 --> 00:49:37,011
that this model's just really bad at this thing

1259
00:49:37,011 --> 00:49:38,082
and we can fix it.

1260
00:49:38,082 --> 00:49:39,281
And you just add these up.

1261
00:49:39,281 --> 00:49:41,211
So, some of it feels like luck,

1262
00:49:41,211 --> 00:49:42,108
but on the ground,

1263
00:49:42,108 --> 00:49:44,810
especially with these new reasoning models we're talking to,

1264
00:49:44,810 --> 00:49:47,478
is just so many ways that we can poke around.

1265
00:49:47,478 --> 00:49:51,361
And normally, it's that some of them give big improvements.

1266
00:49:51,361 --> 00:49:52,470
- The search space is near infinite.

1267
00:49:52,470 --> 00:49:54,348
And yet, the amount of compute

1268
00:49:54,348 --> 00:49:56,946
and time you have is very low.

1269
00:49:56,946 --> 00:49:59,716
And you have to hit release schedules,

1270
00:49:59,716 --> 00:50:02,257
you have to not get blown past by everyone.

1271
00:50:02,257 --> 00:50:05,279
Otherwise, what happened with DeepSeek

1272
00:50:05,279 --> 00:50:07,421
crushing Meta and Misral and Cohere,

1273
00:50:07,421 --> 00:50:08,846
and all these guys, they moved too slow.

1274
00:50:08,846 --> 00:50:10,547
They maybe were too methodical.

1275
00:50:10,547 --> 00:50:11,957
I don't know, they didn't hit the YOLO run,

1276
00:50:11,957 --> 00:50:15,118
whatever the reason was, maybe they weren't as skill.

1277
00:50:15,118 --> 00:50:16,566
You can call it luck if you want,

1278
00:50:16,566 --> 00:50:18,006
but at the end of the day, it's skill.

1279
00:50:18,006 --> 00:50:21,374
- So, 2025 is the year of the YOLO run.

1280
00:50:21,374 --> 00:50:25,139
It seems like all the labs are going in.

1281
00:50:25,139 --> 00:50:29,477
- I think it's even more impressive what OpenAI did in 2022.

1282
00:50:29,477 --> 00:50:31,523
At the time, no one believed in mixture

1283
00:50:31,523 --> 00:50:35,453
of experts models at Google who had all the researchers.

1284
00:50:35,453 --> 00:50:37,374
OpenAI had such little compute

1285
00:50:37,374 --> 00:50:41,148
and they devoted all of their compute for many months,

1286
00:50:41,148 --> 00:50:44,718
all of it, 100% for many months, to GPT-4,

1287
00:50:44,718 --> 00:50:48,205
with a brand new architecture with no belief that, hey,

1288
00:50:48,205 --> 00:50:50,204
let me spend a couple $100 million,

1289
00:50:50,204 --> 00:50:52,821
which is all of the money I have, on this model.

1290
00:50:52,821 --> 00:50:54,527
That is truly YOLO,

1291
00:50:54,527 --> 00:50:55,695
right? - Yeah, yeah.

1292
00:50:55,695 --> 00:50:56,837
- Now, people are like,

1293
00:50:56,837 --> 00:50:59,944
all these training run failures that are in the media,

1294
00:50:59,944 --> 00:51:00,946
it's like, okay, great,

1295
00:51:00,946 --> 00:51:04,013
but actually, a huge chunk of my GPs are doing inference.

1296
00:51:04,013 --> 00:51:06,312
I still have a bunch doing research constantly.

1297
00:51:06,312 --> 00:51:07,766
And yes, my biggest cluster is training,

1298
00:51:07,766 --> 00:51:09,813
but on this YOLO run,

1299
00:51:09,813 --> 00:51:12,503
but that YOLO run is much less risky

1300
00:51:12,503 --> 00:51:14,605
than what OpenAI did in 2022,

1301
00:51:14,605 --> 00:51:16,562
or maybe what DeepSeek did now,

1302
00:51:16,562 --> 00:51:19,732
or like, hey, we're just gonna throw everything at it.

1303
00:51:19,732 --> 00:51:21,484
- The big winners throughput human history

1304
00:51:21,484 --> 00:51:26,003
are the ones who are willing to do YOLO at some point.

1305
00:51:26,003 --> 00:51:27,045
Okay.

1306
00:51:27,045 --> 00:51:27,878
What do we understand

1307
00:51:27,878 --> 00:51:29,994
about the hardware it's been trained on?

1308
00:51:29,994 --> 00:51:30,922
DeepSeek.

1309
00:51:30,922 --> 00:51:32,948
- DeepSeek is very interesting.

1310
00:51:32,948 --> 00:51:34,153
A second to take us

1311
00:51:34,153 --> 00:51:35,781
to zoom out out of who they are, first of all.

1312
00:51:35,781 --> 00:51:38,034
High-Flyer is a hedge fund

1313
00:51:38,034 --> 00:51:40,654
that has historically done quantitative trading

1314
00:51:40,654 --> 00:51:42,541
in China as well as elsewhere.

1315
00:51:42,541 --> 00:51:45,374
And they have always had a significant number of GPUs.

1316
00:51:45,374 --> 00:51:47,414
In the past, a lot of these high frequency trading,

1317
00:51:47,414 --> 00:51:50,180
algorithmic, quant traders, used FPGAs,

1318
00:51:50,180 --> 00:51:52,363
but it shifted to GPUs, definitely.

1319
00:51:52,363 --> 00:51:53,196
And there's both.

1320
00:51:53,196 --> 00:51:56,044
But GPUs especially, and High-Flyer,

1321
00:51:56,044 --> 00:51:57,564
which is the hedge fund that owns DeepSeek.

1322
00:51:57,564 --> 00:51:59,111
And everyone who works for DeepSeek

1323
00:51:59,111 --> 00:52:02,168
is part of High-Flyer to some extent.

1324
00:52:02,168 --> 00:52:04,916
Same parent company, same owner, same CEO.

1325
00:52:04,916 --> 00:52:09,357
They had all these resources and infrastructure for trading,

1326
00:52:09,357 --> 00:52:12,055
and then they devoted a humongous portion of them

1327
00:52:12,055 --> 00:52:15,991
to training models, both language models and otherwise.

1328
00:52:15,991 --> 00:52:20,326
Because these techniques were heavily AI-influenced.

1329
00:52:20,326 --> 00:52:23,135
More recently, people have realized,

1330
00:52:23,135 --> 00:52:27,454
hey, trading with, even when you go back to Renaissance

1331
00:52:27,454 --> 00:52:29,785
and all these quantitative firms,

1332
00:52:29,785 --> 00:52:30,813
natural language processing

1333
00:52:30,813 --> 00:52:33,195
is the key to trading really fast,

1334
00:52:33,195 --> 00:52:35,996
understanding a press release and making the right trade.

1335
00:52:35,996 --> 00:52:38,754
And so, DeepSeek has always been really good at this.

1336
00:52:38,754 --> 00:52:40,834
And even as far back as 2021,

1337
00:52:40,834 --> 00:52:44,068
they have press releases and papers saying like, hey,

1338
00:52:44,068 --> 00:52:45,885
we're the first company in China

1339
00:52:45,885 --> 00:52:47,914
with an A100 cluster this large.

1340
00:52:47,914 --> 00:52:51,475
It was 10,000 A100 GPUs. This is in 2021.

1341
00:52:51,475 --> 00:52:54,686
Now, this wasn't all for training large language models.

1342
00:52:54,686 --> 00:52:56,325
This is mostly for training models

1343
00:52:56,325 --> 00:52:58,083
for their quantitative aspects,

1344
00:52:58,083 --> 00:53:00,725
or quantitative trading, as well as a lot of that

1345
00:53:00,725 --> 00:53:03,224
was natural language processing, to be clear.

1346
00:53:03,224 --> 00:53:04,986
And so, this is the history.

1347
00:53:04,986 --> 00:53:07,076
So, verifiable fact is that in 2021,

1348
00:53:07,076 --> 00:53:09,686
they built the largest cluster, at least they claim

1349
00:53:09,686 --> 00:53:12,336
it was the largest cluster in China, 10,000 GPUs.

1350
00:53:12,336 --> 00:53:14,034
- Before expert controls started.

1351
00:53:14,034 --> 00:53:15,206
- Yeah. - It's like,

1352
00:53:15,206 --> 00:53:16,306
they've had a huge cluster

1353
00:53:16,306 --> 00:53:18,445
before any conversation of export controls.

1354
00:53:18,445 --> 00:53:20,035
So, then you step it forward to like,

1355
00:53:20,035 --> 00:53:23,086
what have they done over the last four years since then?

1356
00:53:23,086 --> 00:53:24,132
Obviously, they've continued

1357
00:53:24,132 --> 00:53:26,688
to operate the hedge fund, probably make tons of money.

1358
00:53:26,688 --> 00:53:27,521
And the other thing

1359
00:53:27,521 --> 00:53:30,214
is that they've leaned more and more and more into AI.

1360
00:53:30,214 --> 00:53:33,494
the CEO, Liang Wenfeng, Liang-

1361
00:53:33,494 --> 00:53:34,917
- You're not putting me on spot on this.

1362
00:53:34,917 --> 00:53:37,926
We discussed this. (laughs) - Liang Feng, the CEO,

1363
00:53:37,926 --> 00:53:39,331
he owns maybe- - We're all fam. (chuckles)

1364
00:53:39,331 --> 00:53:40,553
- Liang Feng, he owns maybe

1365
00:53:40,553 --> 00:53:44,040
a little bit more than half the company, allegedly,

1366
00:53:44,040 --> 00:53:47,236
an extremely like Elon-Jensen kind of figure,

1367
00:53:47,236 --> 00:53:50,301
where he is just involved in everything.

1368
00:53:50,301 --> 00:53:51,745
And so, over that time period,

1369
00:53:51,745 --> 00:53:53,595
he's gotten really in depth into AI.

1370
00:53:53,595 --> 00:53:55,845
He actually has a bit of a like a,

1371
00:53:55,845 --> 00:53:57,345
if you see some of his statements,

1372
00:53:57,345 --> 00:53:59,358
a bit of an EAC vibe almost.

1373
00:53:59,358 --> 00:54:01,356
- Total AGI vibes.

1374
00:54:01,356 --> 00:54:02,758
And like, we need to do this,

1375
00:54:02,758 --> 00:54:06,564
we need to make a new ecosystem of OpenAI.

1376
00:54:06,564 --> 00:54:09,216
We need China to lead on this sort of ecosystem,

1377
00:54:09,216 --> 00:54:10,096
because historically,

1378
00:54:10,096 --> 00:54:13,817
the Western countries have led on software ecosystems,

1379
00:54:13,817 --> 00:54:16,255
and straight up acknowledges,

1380
00:54:16,255 --> 00:54:19,416
in order to do this, we need to do something different.

1381
00:54:19,416 --> 00:54:21,137
DeepSeek is his way of doing this.

1382
00:54:21,137 --> 00:54:22,846
Some of the translated interviews with him

1383
00:54:22,846 --> 00:54:24,257
are fantastic. - So, he has done interviews?

1384
00:54:24,257 --> 00:54:25,136
- Yeah. - Do you think

1385
00:54:25,136 --> 00:54:26,977
he would do a Western interview or no?

1386
00:54:26,977 --> 00:54:27,827
Or is there controls

1387
00:54:27,827 --> 00:54:29,233
on the channel? - There hasn't been one yet,

1388
00:54:29,233 --> 00:54:31,522
but I would try it. - Okay. All right.

1389
00:54:31,522 --> 00:54:32,587
(Nathan laughing) Well, I just got

1390
00:54:32,587 --> 00:54:34,590
a Chinese translator, so it was great.

1391
00:54:34,590 --> 00:54:36,308
This is all push.

1392
00:54:36,308 --> 00:54:40,647
So, fascinating figure, engineer pushing full on into AI,

1393
00:54:40,647 --> 00:54:44,240
leveraging the success from the high frequency trading.

1394
00:54:44,240 --> 00:54:46,768
- Very direct quotes, like we will not switch

1395
00:54:46,768 --> 00:54:49,807
to closed source when asked about this stuff.

1396
00:54:49,807 --> 00:54:52,295
Very long-term motivated

1397
00:54:52,295 --> 00:54:56,036
in how the ecosystem of AI should work.

1398
00:54:56,036 --> 00:54:58,625
And I think from a Chinese perspective,

1399
00:54:58,625 --> 00:55:02,945
he wants a Chinese company to build this vision.

1400
00:55:02,945 --> 00:55:05,016
- And so, this is like the,

1401
00:55:05,016 --> 00:55:06,726
quote, unquote, "visionary behind the company".

1402
00:55:06,726 --> 00:55:09,237
This hedge fund still exists, this quantitative firm.

1403
00:55:09,237 --> 00:55:11,904
And so, DeepSeek is the sort of,

1404
00:55:13,384 --> 00:55:15,979
solely, he got turned to this full view

1405
00:55:15,979 --> 00:55:17,807
of like AI, everything about this.

1406
00:55:17,807 --> 00:55:18,640
But at some point,

1407
00:55:18,640 --> 00:55:21,276
it slowly maneuvered and he made DeepSeek.

1408
00:55:21,276 --> 00:55:23,076
And DeepSeek has done multiple models since then.

1409
00:55:23,076 --> 00:55:24,997
They've acquired more and more GPUs.

1410
00:55:24,997 --> 00:55:28,193
They share infrastructure with the fund.

1411
00:55:28,193 --> 00:55:31,363
And so, there is no exact number

1412
00:55:31,363 --> 00:55:33,716
of public GPU resources that they have,

1413
00:55:33,716 --> 00:55:37,896
but besides this 10,000 GPUs that they bought in 2021,

1414
00:55:37,896 --> 00:55:39,769
and they were fantastically profitable.

1415
00:55:39,769 --> 00:55:44,093
And then, this paper claims they did only 2,000 H800 GPUs,

1416
00:55:44,093 --> 00:55:45,767
which are a restricted GPU

1417
00:55:45,767 --> 00:55:47,477
that was previously allowed in China,

1418
00:55:47,477 --> 00:55:49,065
but no longer allowed, and there's a new version.

1419
00:55:49,065 --> 00:55:52,396
But it's basically Nvidia's H100 for China.

1420
00:55:52,396 --> 00:55:54,159
And there's some restrictions on it,

1421
00:55:54,159 --> 00:55:57,502
specifically around the communications sort of speed,

1422
00:55:57,502 --> 00:55:58,335
the interconnect speed,

1423
00:55:58,335 --> 00:56:02,628
which is why they had to do this crazy SM scheduling stuff.

1424
00:56:02,628 --> 00:56:03,679
So, going back to that.

1425
00:56:03,679 --> 00:56:05,981
It's like this is obviously

1426
00:56:05,981 --> 00:56:08,288
not true in terms of their total GPU count.

1427
00:56:08,288 --> 00:56:09,795
- Obvious available GPUs.

1428
00:56:09,795 --> 00:56:11,765
But for this training run,

1429
00:56:11,765 --> 00:56:14,165
you think 2,000 is the correct number or no?

1430
00:56:14,165 --> 00:56:17,248
- So, this is where it takes a significant amount

1431
00:56:17,248 --> 00:56:18,331
of zoning in.

1432
00:56:19,250 --> 00:56:21,570
What do you call your training run?

1433
00:56:21,570 --> 00:56:25,096
You count all of the research and ablations that you ran,

1434
00:56:25,096 --> 00:56:26,414
picking all this stuff,

1435
00:56:26,414 --> 00:56:27,943
because, yes, you can do a YOLO run,

1436
00:56:27,943 --> 00:56:29,005
but at some level,

1437
00:56:29,005 --> 00:56:30,495
you have to do the test at the small-scale,

1438
00:56:30,495 --> 00:56:32,369
and then you have to do some test at medium-scale

1439
00:56:32,369 --> 00:56:33,505
before you go to a large-scale.

1440
00:56:33,505 --> 00:56:36,726
- Accepted practices that for any given model

1441
00:56:36,726 --> 00:56:38,335
that is a notable advancement,

1442
00:56:38,335 --> 00:56:40,214
you're gonna do 2 to 4x compute

1443
00:56:40,214 --> 00:56:43,327
of the full training run in experiments alone.

1444
00:56:43,327 --> 00:56:45,231
- So, a lot of this compute that's being scaled up

1445
00:56:45,231 --> 00:56:47,464
is probably used in large part

1446
00:56:47,464 --> 00:56:49,344
at this time for research. - Yeah.

1447
00:56:49,344 --> 00:56:52,030
And research begets the new ideas

1448
00:56:52,030 --> 00:56:53,436
that lets you get huge efficiency-

1449
00:56:53,436 --> 00:56:54,612
- Research gets you o1.

1450
00:56:54,612 --> 00:56:57,902
Research gets you breakthrough, so you need to bet on it.

1451
00:56:57,902 --> 00:56:58,887
- So, some of the pricing strategy

1452
00:56:58,887 --> 00:57:02,556
that we'll discuss has the research baked into the price.

1453
00:57:02,556 --> 00:57:05,374
- So, the numbers that DeepSeek specifically said publicly

1454
00:57:05,374 --> 00:57:07,863
are just the 10,000 GPUs in 2021,

1455
00:57:07,863 --> 00:57:12,015
and then 2,000 GPUs for only the pre-training for V3.

1456
00:57:12,015 --> 00:57:14,221
They did not discuss cost on R1.

1457
00:57:14,221 --> 00:57:17,203
They did not discuss cost on all the other RL,

1458
00:57:17,203 --> 00:57:19,513
for the instruct model that they made.

1459
00:57:19,513 --> 00:57:22,386
They only discussed the pre-training for the base model,

1460
00:57:22,386 --> 00:57:25,225
and they did not discuss anything on research and ablations.

1461
00:57:25,225 --> 00:57:26,844
And they do not talk about any of the resources

1462
00:57:26,844 --> 00:57:28,667
that are shared in terms of, hey,

1463
00:57:28,667 --> 00:57:30,696
the fund is using all these GPUs.

1464
00:57:30,696 --> 00:57:32,814
And we know that they're very profitable

1465
00:57:32,814 --> 00:57:35,147
in that 10,000 GPUs in 2021.

1466
00:57:36,437 --> 00:57:40,306
So, some of the research that we've found

1467
00:57:40,306 --> 00:57:43,597
is that we actually believe they have closer to 50,000 GPUs.

1468
00:57:43,597 --> 00:57:44,717
- We, as SemiAnalysis,

1469
00:57:44,717 --> 00:57:45,558
so we should say - Yeah.

1470
00:57:45,558 --> 00:57:49,536
- that you're one of the world experts in figuring out

1471
00:57:49,536 --> 00:57:51,895
what everybody's doing in terms of the semiconductor,

1472
00:57:51,895 --> 00:57:53,004
in terms of cluster buildouts,

1473
00:57:53,004 --> 00:57:57,767
in terms of who's doing what in terms of training runs.

1474
00:57:57,767 --> 00:57:59,369
So, yeah. So, that's the we.

1475
00:57:59,369 --> 00:58:00,565
Okay, go ahead. - Yeah, sorry, sorry.

1476
00:58:00,565 --> 00:58:02,455
We believe they actually have something closer

1477
00:58:02,455 --> 00:58:04,049
to 50,000 GPUs. - Yeah.

1478
00:58:04,049 --> 00:58:05,735
- [Dylan] Now, this is split across many tasks.

1479
00:58:05,735 --> 00:58:09,578
Again, the fund, research and ablations.

1480
00:58:09,578 --> 00:58:11,785
- For ballpark, how much would OpenAI or Anthropic had?

1481
00:58:11,785 --> 00:58:14,194
I think the clearest example we have,

1482
00:58:14,194 --> 00:58:15,343
because Meta is also open,

1483
00:58:15,343 --> 00:58:17,491
they talk about order of 60k

1484
00:58:17,491 --> 00:58:21,372
to a 100k H100 equivalent GPUs in their training clusters.

1485
00:58:21,372 --> 00:58:25,542
- Right. So, like Llama 3, they trained on 16,000 H100s.

1486
00:58:25,542 --> 00:58:27,415
But the company of Meta last year

1487
00:58:27,415 --> 00:58:28,834
publicly disclosed they bought

1488
00:58:28,834 --> 00:58:30,152
like 400 something thousand GPUs.

1489
00:58:30,152 --> 00:58:31,551
- Yeah. (chuckles) - So, of course,

1490
00:58:31,551 --> 00:58:32,990
tiny percentage on the training.

1491
00:58:32,990 --> 00:58:34,401
Again, most of it is like

1492
00:58:34,401 --> 00:58:37,729
serving me the best Instagram reels, or whatever.

1493
00:58:37,729 --> 00:58:39,672
- We could get into a cost of like, what is the cost

1494
00:58:39,672 --> 00:58:44,197
of ownership for a 2,000-GPU cluster, 10,000?

1495
00:58:44,197 --> 00:58:45,770
There's just different sizes of companies

1496
00:58:45,770 --> 00:58:46,742
that can afford these things,

1497
00:58:46,742 --> 00:58:49,291
and DeepSeek is reasonably big.

1498
00:58:49,291 --> 00:58:51,845
Their compute allocation compared

1499
00:58:51,845 --> 00:58:55,300
is one of the top few in the world.

1500
00:58:55,300 --> 00:58:56,913
It's not OpenAI, Anthropic, et cetera,

1501
00:58:56,913 --> 00:58:58,126
but they have a lot of compute.

1502
00:58:58,126 --> 00:58:59,478
- Can you in general, actually just zoom out

1503
00:58:59,478 --> 00:59:01,725
and also talk about the Hopper architecture,

1504
00:59:01,725 --> 00:59:03,955
the Nvidia Hopper GPU architecture,

1505
00:59:03,955 --> 00:59:07,372
and the difference between H100 and H800,

1506
00:59:08,227 --> 00:59:09,922
like you mentioned the interconnects.

1507
00:59:09,922 --> 00:59:13,437
- Yeah, so Ampere was the A100, and then H100, Hopper.

1508
00:59:13,437 --> 00:59:15,677
People use them synonymously in the US

1509
00:59:15,677 --> 00:59:18,467
because really, there's just H100 and now there's H200.

1510
00:59:18,467 --> 00:59:20,300
But same thing mostly.

1511
00:59:21,175 --> 00:59:22,037
In China, they've had two,

1512
00:59:22,037 --> 00:59:24,523
there've been different salvos of export restrictions.

1513
00:59:24,523 --> 00:59:26,086
So, initially, the US government

1514
00:59:26,086 --> 00:59:27,917
limited on a two-factor scale,

1515
00:59:27,917 --> 00:59:30,854
which is chip interconnect versus FLOPS.

1516
00:59:30,854 --> 00:59:33,644
So, any chip that had interconnects above a certain level

1517
00:59:33,644 --> 00:59:36,305
and FLOPS above a certain floating point operations,

1518
00:59:36,305 --> 00:59:38,595
above a certain level, was restricted.

1519
00:59:38,595 --> 00:59:40,276
Later, the government realized

1520
00:59:40,276 --> 00:59:42,116
that this was a flaw in the restriction,

1521
00:59:42,116 --> 00:59:46,037
and they cut it down to just floating point operations.

1522
00:59:46,037 --> 00:59:48,454
And so- - H800 had high FLOPS,

1523
00:59:50,150 --> 00:59:52,196
low communication. - Exactly.

1524
00:59:52,196 --> 00:59:56,016
So, the H800 was the same performance as H100 on FLOPS,

1525
00:59:56,016 --> 00:59:58,825
but it just had the interconnect bandwidth cut.

1526
00:59:58,825 --> 01:00:01,834
DeepSeek knew how to utilize this res...

1527
01:00:01,834 --> 01:00:04,251
Hey, even though we're cut back on the interconnect,

1528
01:00:04,251 --> 01:00:05,484
we can do all this fancy stuff

1529
01:00:05,484 --> 01:00:08,854
to figure out how to use the GPU fully anyways.

1530
01:00:08,854 --> 01:00:12,433
And so, that was back in October, 2022.

1531
01:00:12,433 --> 01:00:16,516
But later in 2023, into 2023 implemented in 2024,

1532
01:00:17,852 --> 01:00:19,946
the US government banned the H800.

1533
01:00:19,946 --> 01:00:21,920
And so, by the way, this H800 cluster,

1534
01:00:21,920 --> 01:00:24,868
these 2,000 GPUs was not even purchased in 2024.

1535
01:00:24,868 --> 01:00:27,092
It was purchased in late 2023.

1536
01:00:27,092 --> 01:00:27,925
- [Lex] Mm-hmm.

1537
01:00:27,925 --> 01:00:28,965
- And they're just getting the model out now,

1538
01:00:28,965 --> 01:00:31,051
because it takes a lot of research, et cetera.

1539
01:00:31,051 --> 01:00:34,686
H800 was banned and now there's a new chip called the H20.

1540
01:00:34,686 --> 01:00:37,436
The H20 is cutback on only FLOPS,

1541
01:00:38,305 --> 01:00:40,006
but the interconnect bandwidth is the same.

1542
01:00:40,006 --> 01:00:42,755
And in fact, in some ways, it's better than the H100

1543
01:00:42,755 --> 01:00:45,486
because it has better memory bandwidth and memory capacity.

1544
01:00:45,486 --> 01:00:48,046
So, there are, Nvidia's working within the constraints

1545
01:00:48,046 --> 01:00:49,571
of what the government sets,

1546
01:00:49,571 --> 01:00:52,363
and then builds the best possible GPU for China.

1547
01:00:52,363 --> 01:00:53,971
- Can we take this actual tangent

1548
01:00:53,971 --> 01:00:55,873
and we'll return back to the hardware?

1549
01:00:55,873 --> 01:00:57,580
Is the philosophy,

1550
01:00:57,580 --> 01:01:01,773
the motivation, the case for export controls, what is it?

1551
01:01:01,773 --> 01:01:03,852
Dario Amodei just published a blog post

1552
01:01:03,852 --> 01:01:05,512
about export controls.

1553
01:01:05,512 --> 01:01:09,201
The case he makes is that if AI becomes super powerful,

1554
01:01:09,201 --> 01:01:14,019
and he says by 2026, will have AGI or super powerful AI,

1555
01:01:14,019 --> 01:01:16,431
and that's going to give a significant, whoever builds that

1556
01:01:16,431 --> 01:01:19,479
will have a significant military advantage.

1557
01:01:19,479 --> 01:01:23,107
And so, because the United States is a democracy,

1558
01:01:23,107 --> 01:01:26,758
and as he says, China is authoritarian

1559
01:01:26,758 --> 01:01:29,737
or has authoritarian elements,

1560
01:01:29,737 --> 01:01:34,244
you want a unipolar world where the super powerful military,

1561
01:01:34,244 --> 01:01:37,695
because of the AI, is one that's a democracy.

1562
01:01:37,695 --> 01:01:40,637
It's a much more complicated world geopolitically

1563
01:01:40,637 --> 01:01:42,701
when you have two superpowers

1564
01:01:42,701 --> 01:01:46,919
with super powerful AI and one is authoritarian.

1565
01:01:46,919 --> 01:01:47,801
So, that's the case he makes.

1566
01:01:47,801 --> 01:01:50,889
And so, we wanna, the United States

1567
01:01:50,889 --> 01:01:53,400
wants to use export controls to slow down,

1568
01:01:53,400 --> 01:01:55,751
to make sure that China

1569
01:01:55,751 --> 01:01:58,834
can't do these gigantic training runs

1570
01:01:59,780 --> 01:02:02,842
that will be presumably required to build AGI.

1571
01:02:02,842 --> 01:02:03,791
- This is very abstract.

1572
01:02:03,791 --> 01:02:05,769
I think this can be the goal

1573
01:02:05,769 --> 01:02:08,580
of how some people describe export controls,

1574
01:02:08,580 --> 01:02:11,009
is this super powerful AI.

1575
01:02:11,009 --> 01:02:13,767
And you touched on the training run idea.

1576
01:02:13,767 --> 01:02:18,704
There's not many worlds where China cannot train AI models.

1577
01:02:18,704 --> 01:02:23,076
I think export controls are decapping the amount of compute

1578
01:02:23,076 --> 01:02:25,746
or the density of compute that China can have.

1579
01:02:25,746 --> 01:02:29,399
And if you think about the AI ecosystem right now,

1580
01:02:29,399 --> 01:02:31,328
as all of these AI companies,

1581
01:02:31,328 --> 01:02:32,849
revenue numbers are up and to the right,

1582
01:02:32,849 --> 01:02:35,493
their AI usage is just continuing to grow,

1583
01:02:35,493 --> 01:02:36,921
more Deep user going to inference.

1584
01:02:36,921 --> 01:02:40,121
A large part of export controls, if they work,

1585
01:02:40,121 --> 01:02:42,413
is just that the amount of AI

1586
01:02:42,413 --> 01:02:45,581
that can be run in China is going to be much lower.

1587
01:02:45,581 --> 01:02:48,064
So, on the training side, DeepSeek-V3 is a great example,

1588
01:02:48,064 --> 01:02:49,623
which you have a very focused team

1589
01:02:49,623 --> 01:02:52,250
that can still get to the frontier of AI.

1590
01:02:52,250 --> 01:02:54,702
This 2,000 GPUs is not that hard to get,

1591
01:02:54,702 --> 01:02:56,754
all considering in the world.

1592
01:02:56,754 --> 01:02:58,741
They're still gonna have those GPUs,

1593
01:02:58,741 --> 01:03:00,151
they're still gonna be able to train models.

1594
01:03:00,151 --> 01:03:02,172
But if there's gonna be a huge market for AI,

1595
01:03:02,172 --> 01:03:03,703
if you have strong export controls

1596
01:03:03,703 --> 01:03:05,412
and you wanna have 100,000 GPUs

1597
01:03:05,412 --> 01:03:07,813
just serving the equivalent of ChatGPT clusters,

1598
01:03:07,813 --> 01:03:09,223
with good export controls,

1599
01:03:09,223 --> 01:03:13,664
it also just makes it so that AI can be used much less.

1600
01:03:13,664 --> 01:03:16,918
And I think that is a much easier goal

1601
01:03:16,918 --> 01:03:20,101
to achieve than trying to debate on what AGI is.

1602
01:03:20,101 --> 01:03:23,518
And if you have these extremely intelligent autonomous AIs

1603
01:03:23,518 --> 01:03:25,393
and data centers, those are the things

1604
01:03:25,393 --> 01:03:27,881
that could be running in these GPU clusters

1605
01:03:27,881 --> 01:03:30,403
in the United States, but not in China.

1606
01:03:30,403 --> 01:03:33,372
- To some extent, training a model does effectively nothing.

1607
01:03:33,372 --> 01:03:34,725
Like have a model. - Yeah. (chuckles)

1608
01:03:34,725 --> 01:03:39,072
- The thing that Dario is speaking to is the implementation

1609
01:03:39,072 --> 01:03:41,383
of that model once trained

1610
01:03:41,383 --> 01:03:44,141
to then create huge economic growth,

1611
01:03:44,141 --> 01:03:46,204
huge increases in military capabilities,

1612
01:03:46,204 --> 01:03:49,022
huge increases in productivity of people,

1613
01:03:49,022 --> 01:03:51,152
betterment of lives, whatever you want

1614
01:03:51,152 --> 01:03:54,084
to direct super powerful AI towards, you can.

1615
01:03:54,084 --> 01:03:56,613
But that requires a significant amounts of compute.

1616
01:03:56,613 --> 01:03:59,511
And so, the US government has effectively said,

1617
01:03:59,511 --> 01:04:03,444
and forever, like training will always be a portion

1618
01:04:03,444 --> 01:04:04,614
of the total compute.

1619
01:04:04,614 --> 01:04:08,904
We mentioned Meta's 400,000 GPUs, only 16,000 made Llama.

1620
01:04:08,904 --> 01:04:12,141
So, the percentage that Meta is dedicating to inference,

1621
01:04:12,141 --> 01:04:14,284
now this might be for recommendation systems

1622
01:04:14,284 --> 01:04:16,406
that are trying to hack our mind into spending more time

1623
01:04:16,406 --> 01:04:20,549
and watching more ads, or if it's for a super powerful AI

1624
01:04:20,549 --> 01:04:21,556
that's doing productive things,

1625
01:04:21,556 --> 01:04:22,975
doesn't matter about the exact use

1626
01:04:22,975 --> 01:04:25,225
that our economic system decides,

1627
01:04:25,225 --> 01:04:28,582
it's that that can be delivered in whatever way we want.

1628
01:04:28,582 --> 01:04:32,487
Whereas with China, export restrictions, great.

1629
01:04:32,487 --> 01:04:34,641
You're never gonna be able to cut everything off.

1630
01:04:34,641 --> 01:04:37,373
And that's like, I think that's quite a well-understood

1631
01:04:37,373 --> 01:04:40,646
by the US government is that you can't cut everything off.

1632
01:04:40,646 --> 01:04:41,810
- [Nathan] And they'll make their own chips.

1633
01:04:41,810 --> 01:04:42,889
They're- - And they're trying

1634
01:04:42,889 --> 01:04:43,722
to make their own chips.

1635
01:04:43,722 --> 01:04:44,555
They'll be worse than ours.

1636
01:04:44,555 --> 01:04:46,428
But the whole point is to just keep a gap.

1637
01:04:46,428 --> 01:04:47,858
- Yeah. - And therefore,

1638
01:04:47,858 --> 01:04:49,440
at some point, as the AI,

1639
01:04:49,440 --> 01:04:51,960
in a world where 2, 3% economic growth,

1640
01:04:51,960 --> 01:04:53,457
this is really dumb, by the way,

1641
01:04:53,457 --> 01:04:56,627
to cut off high tech and not make money off of it.

1642
01:04:56,627 --> 01:04:59,599
But in a world where super powerful AI comes about,

1643
01:04:59,599 --> 01:05:02,438
and then starts creating significant changes in society,

1644
01:05:02,438 --> 01:05:03,947
which is what all the AI leaders

1645
01:05:03,947 --> 01:05:05,654
and big tech companies believe, I think.

1646
01:05:05,654 --> 01:05:08,417
Super powerful AI is gonna change society massively.

1647
01:05:08,417 --> 01:05:10,158
And therefore, this compounding effect

1648
01:05:10,158 --> 01:05:12,643
of the difference in compute is really important.

1649
01:05:12,643 --> 01:05:17,069
There's some sci-fi out there, where AI is measured

1650
01:05:17,069 --> 01:05:19,319
in how much power is delivered to compute.

1651
01:05:19,319 --> 01:05:21,400
Or how much is being...

1652
01:05:21,400 --> 01:05:23,777
That's a way of thinking about what's the economic output

1653
01:05:23,777 --> 01:05:26,697
is just how much power are you directing towards that AI?

1654
01:05:26,697 --> 01:05:28,363
Should we talk about reasoning models with this?

1655
01:05:28,363 --> 01:05:30,577
As a way that this might be actionable

1656
01:05:30,577 --> 01:05:32,333
as something that people can actually see?

1657
01:05:32,333 --> 01:05:34,618
So, the reasoning models that are coming out with R1 and o1,

1658
01:05:34,618 --> 01:05:37,471
they're designed to use more compute.

1659
01:05:37,471 --> 01:05:41,421
There's a lot of buzzy words in the AI community about this,

1660
01:05:41,421 --> 01:05:44,280
test-time compute, inference time compute, whatever.

1661
01:05:44,280 --> 01:05:46,091
But Dylan has good research on this.

1662
01:05:46,091 --> 01:05:48,010
You can get to the specific numbers on the ratio

1663
01:05:48,010 --> 01:05:49,661
of when you train a model, you can look at things

1664
01:05:49,661 --> 01:05:51,673
about the amount of compute used at training

1665
01:05:51,673 --> 01:05:53,260
and amount of compute use inference.

1666
01:05:53,260 --> 01:05:54,822
These reasoning models are making inference

1667
01:05:54,822 --> 01:05:58,049
way more important to doing complex tasks.

1668
01:05:58,049 --> 01:06:00,348
In the fall in December,

1669
01:06:00,348 --> 01:06:02,632
their OpenAI announced this o3 model.

1670
01:06:02,632 --> 01:06:04,282
There's another thing in AI, when things move fast,

1671
01:06:04,282 --> 01:06:06,284
we get both announcements and releases.

1672
01:06:06,284 --> 01:06:07,686
Announcements are essentially blog posts

1673
01:06:07,686 --> 01:06:08,873
where you pat yourself on the back

1674
01:06:08,873 --> 01:06:09,863
and you say you did things.

1675
01:06:09,863 --> 01:06:11,546
And releases are on the models out there,

1676
01:06:11,546 --> 01:06:12,765
the papers out there, et cetera.

1677
01:06:12,765 --> 01:06:14,605
So, OpenAI has announced o3,

1678
01:06:14,605 --> 01:06:16,805
and we can check if o3-mini is out

1679
01:06:16,805 --> 01:06:18,451
as of recording potentially,

1680
01:06:18,451 --> 01:06:20,033
but that doesn't really change the point,

1681
01:06:20,033 --> 01:06:22,494
which is that the breakthrough result

1682
01:06:22,494 --> 01:06:24,751
was something called ARC-AGI task,

1683
01:06:24,751 --> 01:06:26,993
which is the abstract reasoning corpus,

1684
01:06:26,993 --> 01:06:30,012
a task for artificial general intelligence.

1685
01:06:30,012 --> 01:06:32,959
Francois Chollet is the guy who's been...

1686
01:06:32,959 --> 01:06:34,670
It's a multi-year old paper.

1687
01:06:34,670 --> 01:06:36,156
It's a brilliant benchmark.

1688
01:06:36,156 --> 01:06:39,439
And the number for OpenAI o3 to solve this

1689
01:06:39,439 --> 01:06:43,090
was that it used as some number of samples in the API,

1690
01:06:43,090 --> 01:06:46,109
the API has thinking effort and number of samples.

1691
01:06:46,109 --> 01:06:48,548
They used a thousand samples to solve this task.

1692
01:06:48,548 --> 01:06:53,418
And it comes out to be like 5 to $20 per question,

1693
01:06:53,418 --> 01:06:55,866
which you're putting in effectively a math puzzle.

1694
01:06:55,866 --> 01:06:58,992
And then, it takes orders of dollars to answer one question.

1695
01:06:58,992 --> 01:07:01,006
And this is a lot of compute.

1696
01:07:01,006 --> 01:07:02,052
If those are gonna take off in the US,

1697
01:07:02,052 --> 01:07:05,125
OpenAI needs a ton of GPUs on inference to capture this.

1698
01:07:05,125 --> 01:07:08,410
They have this OpenAI ChatGPT Pro subscription,

1699
01:07:08,410 --> 01:07:09,821
which is $200 a month.

1700
01:07:09,821 --> 01:07:11,246
- [Dylan] Which Sam said they're losing money on.

1701
01:07:11,246 --> 01:07:12,079
- Which means that people

1702
01:07:12,079 --> 01:07:14,174
are burning a lot of GPUs on inference.

1703
01:07:14,174 --> 01:07:16,191
And I've signed up with it, I've played with it.

1704
01:07:16,191 --> 01:07:19,425
I don't think I'm a power user, but I use it.

1705
01:07:19,425 --> 01:07:21,640
That is the thing that a Chinese company

1706
01:07:21,640 --> 01:07:24,670
with mediumly strong export controls,

1707
01:07:24,670 --> 01:07:25,750
there will always be loopholes,

1708
01:07:25,750 --> 01:07:27,360
might not be able to do it all.

1709
01:07:27,360 --> 01:07:29,389
And if that, the main result

1710
01:07:29,389 --> 01:07:32,321
for o3 is also a spectacular coding performance.

1711
01:07:32,321 --> 01:07:33,187
And if that feeds back

1712
01:07:33,187 --> 01:07:37,659
into AI companies being able to experiment better.

1713
01:07:37,659 --> 01:07:41,530
- So, presumably the idea is for an AGI,

1714
01:07:41,530 --> 01:07:43,240
a much larger fraction of the compute

1715
01:07:43,240 --> 01:07:46,677
would be used for this test-time compute for the reasoning,

1716
01:07:46,677 --> 01:07:47,937
for the AGI goes into a room

1717
01:07:47,937 --> 01:07:50,578
and thinks about how to take over the world

1718
01:07:50,578 --> 01:07:52,828
and come back in 2.7 hours.

1719
01:07:54,737 --> 01:07:56,148
- This is what- - And that it's gonna take

1720
01:07:56,148 --> 01:07:57,277
a lot of compute. - This is what people

1721
01:07:57,277 --> 01:08:00,276
like CEO or leaders of OpenAI

1722
01:08:00,276 --> 01:08:03,336
and Anthropic talk about is like autonomous AI models,

1723
01:08:03,336 --> 01:08:04,169
which is you give them a task

1724
01:08:04,169 --> 01:08:05,891
and they work on it in the background.

1725
01:08:05,891 --> 01:08:10,173
I think my personal definition of AGI is much simpler.

1726
01:08:10,173 --> 01:08:12,345
I think language models are a form of AGI

1727
01:08:12,345 --> 01:08:15,116
and all of this super powerful stuff is a next step

1728
01:08:15,116 --> 01:08:16,671
that's great if we get these tools.

1729
01:08:16,671 --> 01:08:19,612
But a language model has so much value in so many domains.

1730
01:08:19,612 --> 01:08:21,229
It is a general intelligence to me.

1731
01:08:21,229 --> 01:08:23,698
But this next step of agentic things

1732
01:08:23,698 --> 01:08:24,618
where they're independent

1733
01:08:24,618 --> 01:08:27,127
and they can do tasks that aren't in the training data

1734
01:08:27,127 --> 01:08:29,460
is what the few year outlook

1735
01:08:30,395 --> 01:08:32,756
that these AI companies are driving for.

1736
01:08:32,756 --> 01:08:34,156
- I think the terminology here

1737
01:08:34,156 --> 01:08:37,066
that Dario uses as super powerful AI.

1738
01:08:37,066 --> 01:08:39,426
So, I agree with you on the AGI.

1739
01:08:39,426 --> 01:08:40,465
I think we already have something

1740
01:08:40,465 --> 01:08:42,437
like that's exceptionally impressive

1741
01:08:42,437 --> 01:08:44,858
that Alan Turing would for sure say is AGI.

1742
01:08:44,858 --> 01:08:49,401
But he's referring more to something once in possession of,

1743
01:08:49,401 --> 01:08:51,888
than you would have a significant military

1744
01:08:51,888 --> 01:08:54,578
and geopolitical advantage over other nations.

1745
01:08:54,578 --> 01:08:58,439
So, it's not just like you can ask it how to cook an omelet.

1746
01:08:58,439 --> 01:09:00,067
- And he has a much more positive view.

1747
01:09:00,067 --> 01:09:01,483
And as I say, machines of love and grace.

1748
01:09:01,483 --> 01:09:03,219
- Yes. - I've read into this.

1749
01:09:03,219 --> 01:09:05,220
I don't have enough background in physical sciences

1750
01:09:05,220 --> 01:09:07,627
to gauge exactly how confident I am

1751
01:09:07,627 --> 01:09:10,191
in if AI can revolutionize biology.

1752
01:09:10,191 --> 01:09:14,629
I am safe saying that AI is going to accelerate the progress

1753
01:09:14,629 --> 01:09:16,063
of any computational science.

1754
01:09:16,063 --> 01:09:18,898
- So, we're doing a depth-first search here on topics,

1755
01:09:18,898 --> 01:09:20,010
taking tangent of a tangent.

1756
01:09:20,010 --> 01:09:23,843
So, let's continue on that depth-first search.

1757
01:09:24,941 --> 01:09:28,157
You said that you're both feeling the AGI,

1758
01:09:28,157 --> 01:09:30,725
so what's your timeline? (Nathan chuckles)

1759
01:09:30,725 --> 01:09:33,892
Dario's 2026 for the super powerful AI

1760
01:09:35,111 --> 01:09:37,273
that's basically agentic to a degree,

1761
01:09:37,273 --> 01:09:41,689
where it's a real security threat, that level of AGI.

1762
01:09:43,464 --> 01:09:44,837
What's your timeline?

1763
01:09:44,837 --> 01:09:46,937
- I don't like to attribute specific abilities

1764
01:09:46,937 --> 01:09:49,689
because predicting specific abilities and when is very hard.

1765
01:09:49,689 --> 01:09:51,402
I think mostly if you're gonna say

1766
01:09:51,402 --> 01:09:52,822
that I'm feeling the AGI

1767
01:09:52,822 --> 01:09:56,131
is that I expect continued rapid surprising progress

1768
01:09:56,131 --> 01:09:57,361
over the next few years.

1769
01:09:57,361 --> 01:10:00,568
So, something like R1 is less surprising

1770
01:10:00,568 --> 01:10:01,518
to me from DeepSeek,

1771
01:10:01,518 --> 01:10:03,581
because I expect there to be new paradigms

1772
01:10:03,581 --> 01:10:05,430
where substantial progress can be made.

1773
01:10:05,430 --> 01:10:07,578
I think DeepSeek-R1 is so unsettling,

1774
01:10:07,578 --> 01:10:10,617
because we're on this path with ChatGPT,

1775
01:10:10,617 --> 01:10:11,450
it's like it's getting better,

1776
01:10:11,450 --> 01:10:12,426
it's getting better, it's getting better.

1777
01:10:12,426 --> 01:10:15,136
And then, we have a new direction for changing the models.

1778
01:10:15,136 --> 01:10:17,705
And we took one step like this and we took a step up.

1779
01:10:17,705 --> 01:10:19,269
So, it looks like a really fast slope.

1780
01:10:19,269 --> 01:10:20,915
And then, we're gonna just take more steps.

1781
01:10:20,915 --> 01:10:22,297
So, this is really unsettling

1782
01:10:22,297 --> 01:10:23,375
when you have these big steps.

1783
01:10:23,375 --> 01:10:26,836
And I expect that to keep happening.

1784
01:10:26,836 --> 01:10:29,277
I've tried OpenAI Operator,

1785
01:10:29,277 --> 01:10:32,136
I've tried Claude computer use, they're not there yet.

1786
01:10:32,136 --> 01:10:33,419
I understand the idea,

1787
01:10:33,419 --> 01:10:36,641
but it's just so hard to predict what is the breakthrough

1788
01:10:36,641 --> 01:10:37,986
that'll make something like that work.

1789
01:10:37,986 --> 01:10:39,620
And I think it's more likely

1790
01:10:39,620 --> 01:10:41,250
that we have breakthroughs that work

1791
01:10:41,250 --> 01:10:42,975
and things that we don't know what they're gonna do.

1792
01:10:42,975 --> 01:10:44,442
So, everyone wants agents.

1793
01:10:44,442 --> 01:10:48,488
Dario has a very eloquent way of describing this.

1794
01:10:48,488 --> 01:10:50,153
And I just think that it's like,

1795
01:10:50,153 --> 01:10:51,564
there's gonna be more than that.

1796
01:10:51,564 --> 01:10:53,965
So, just expect these things to come.

1797
01:10:53,965 --> 01:10:55,371
- I'm gonna have to try to pin you down

1798
01:10:55,371 --> 01:10:58,943
to a date on the AGI timeline, (chuckles)

1799
01:10:58,943 --> 01:11:01,813
like the nuclear weapon moment.

1800
01:11:01,813 --> 01:11:05,396
So, moment where on the geopolitical stage.

1801
01:11:07,026 --> 01:11:09,360
There's a real, like,

1802
01:11:09,360 --> 01:11:11,794
'cause we're talking about export controls,

1803
01:11:11,794 --> 01:11:14,773
when do you think just even to throw out a date,

1804
01:11:14,773 --> 01:11:16,041
when do you think that would be?

1805
01:11:16,041 --> 01:11:19,002
For me, it's probably after 2030,

1806
01:11:19,002 --> 01:11:20,502
so I'm not as- - That's what I would say.

1807
01:11:20,502 --> 01:11:21,730
- So, define that.

1808
01:11:21,730 --> 01:11:24,519
Because to me, it almost has already happened.

1809
01:11:24,519 --> 01:11:26,672
You look at elections in India and Pakistan,

1810
01:11:26,672 --> 01:11:28,271
people get AI voice calls,

1811
01:11:28,271 --> 01:11:31,140
and think they're talking to the politician.

1812
01:11:31,140 --> 01:11:32,444
The AI diffusion rules,

1813
01:11:32,444 --> 01:11:33,816
which was enacted in the last couple weeks

1814
01:11:33,816 --> 01:11:36,369
of the Biden admin and looks like the Trump admin will keep,

1815
01:11:36,369 --> 01:11:38,181
and potentially even strengthen,

1816
01:11:38,181 --> 01:11:41,209
limit cloud computing and GPU sales

1817
01:11:41,209 --> 01:11:44,160
to countries that are not even related to China, it's like.

1818
01:11:44,160 --> 01:11:45,301
This is- - Portugal

1819
01:11:45,301 --> 01:11:47,098
and all these like normal countries

1820
01:11:47,098 --> 01:11:48,167
are on the- - Yeah, and it's like-

1821
01:11:48,167 --> 01:11:49,521
- You need approval from the US list.

1822
01:11:49,521 --> 01:11:50,563
- Like, yeah, Portugal

1823
01:11:50,563 --> 01:11:53,230
and like all these countries that are allies.

1824
01:11:53,230 --> 01:11:54,420
- Yup. - Singapore.

1825
01:11:54,420 --> 01:11:57,758
They freaking have F-35s and we don't let them buy GPUs.

1826
01:11:57,758 --> 01:11:59,241
- Mm-hmm. - This to me

1827
01:11:59,241 --> 01:12:01,949
is already to the scale of like-

1828
01:12:01,949 --> 01:12:05,119
- Well, that just means that the US military

1829
01:12:05,119 --> 01:12:06,866
is really nervous about this new technology.

1830
01:12:06,866 --> 01:12:09,339
That doesn't mean the technology is already there.

1831
01:12:09,339 --> 01:12:12,360
So, they might be just very cautious

1832
01:12:12,360 --> 01:12:14,226
about this thing that they don't quite understand.

1833
01:12:14,226 --> 01:12:18,430
But that's a really good point, the robocalls,

1834
01:12:18,430 --> 01:12:23,198
swarms of semi-intelligent bots could be a weapon,

1835
01:12:23,198 --> 01:12:25,662
could be doing a lot of social engineering.

1836
01:12:25,662 --> 01:12:27,120
- There's tons of talk about,

1837
01:12:27,120 --> 01:12:29,627
from the 2016 elections like Cambridge Analytica,

1838
01:12:29,627 --> 01:12:32,048
and all this stuff, Russian influence.

1839
01:12:32,048 --> 01:12:33,038
Every country in the world

1840
01:12:33,038 --> 01:12:34,617
is pushing stuff onto the internet

1841
01:12:34,617 --> 01:12:36,529
and has narratives they want.

1842
01:12:36,529 --> 01:12:38,497
Every technically competent,

1843
01:12:38,497 --> 01:12:41,519
whether it's Russia, China, US, Israel, et cetera.

1844
01:12:41,519 --> 01:12:44,819
People are pushing viewpoints onto the internet, and mass.

1845
01:12:44,819 --> 01:12:45,707
And language models

1846
01:12:45,707 --> 01:12:48,348
crash the cost of very intelligent sounding

1847
01:12:48,348 --> 01:12:49,771
language. - There's some research

1848
01:12:49,771 --> 01:12:51,509
that shows that the distribution

1849
01:12:51,509 --> 01:12:52,949
is actually a limiting factor.

1850
01:12:52,949 --> 01:12:56,339
So, language models haven't yet made misinformation,

1851
01:12:56,339 --> 01:12:59,672
particularly changed the equation there.

1852
01:13:00,801 --> 01:13:02,011
The internet is still ongoing.

1853
01:13:02,011 --> 01:13:03,739
I think there's a blog, AI Snake Oil,

1854
01:13:03,739 --> 01:13:05,136
and some of my friends at Princeton

1855
01:13:05,136 --> 01:13:05,969
that write on this stuff.

1856
01:13:05,969 --> 01:13:06,802
So, there is research.

1857
01:13:06,802 --> 01:13:08,723
It's a default that everyone assumes.

1858
01:13:08,723 --> 01:13:09,804
And I would've thought the same thing

1859
01:13:09,804 --> 01:13:11,479
is that misinformation

1860
01:13:11,479 --> 01:13:13,002
doesn't gonna get far worse with language models.

1861
01:13:13,002 --> 01:13:15,301
I think in terms of internet posts

1862
01:13:15,301 --> 01:13:17,588
and things that people have been measuring,

1863
01:13:17,588 --> 01:13:19,887
it hasn't been a exponential increase

1864
01:13:19,887 --> 01:13:21,408
or something extremely measurable,

1865
01:13:21,408 --> 01:13:22,438
and things you're talking about

1866
01:13:22,438 --> 01:13:24,209
with voice calls and stuff like that.

1867
01:13:24,209 --> 01:13:27,152
It could be in modalities that are harder to measure.

1868
01:13:27,152 --> 01:13:31,408
So, it's something that it's too soon to tell in terms of,

1869
01:13:31,408 --> 01:13:36,408
I think that's political instability via the web is very,

1870
01:13:36,742 --> 01:13:37,955
it's monitored by a lot

1871
01:13:37,955 --> 01:13:40,834
of researchers to see what's happening.

1872
01:13:40,834 --> 01:13:43,667
You're asking about the AGI thing.

1873
01:13:44,877 --> 01:13:46,298
If you make me give a year,

1874
01:13:46,298 --> 01:13:48,540
I'm be like, okay, I have AI CEOs saying this.

1875
01:13:48,540 --> 01:13:49,568
They've been saying two years

1876
01:13:49,568 --> 01:13:50,479
for a while. - Mm-hmm.

1877
01:13:50,479 --> 01:13:54,668
- I think that they're people like Dario at Anthropic,

1878
01:13:54,668 --> 01:13:57,098
the CEO has thought about this so deeply.

1879
01:13:57,098 --> 01:13:59,889
I need to take their word seriously,

1880
01:13:59,889 --> 01:14:03,579
but also understand that they have different incentives.

1881
01:14:03,579 --> 01:14:05,293
So, I would be like, add a few years to that,

1882
01:14:05,293 --> 01:14:07,238
which is how you get something similar to 2030

1883
01:14:07,238 --> 01:14:08,344
or a little after 2030.

1884
01:14:08,344 --> 01:14:10,367
- I think to some extent, we have capabilities

1885
01:14:10,367 --> 01:14:13,658
that hit a certain point where any one person could say,

1886
01:14:13,658 --> 01:14:15,826
oh, okay, if I can leverage those capabilities

1887
01:14:15,826 --> 01:14:18,529
for X amount of time, this is AGI.

1888
01:14:18,529 --> 01:14:19,957
Call it '27, '28.

1889
01:14:19,957 --> 01:14:23,134
But then the cost of actually operating that capability

1890
01:14:23,134 --> 01:14:24,304
- Yeah, this is gonna be - is

1891
01:14:24,304 --> 01:14:25,863
- my point. (chuckles) - so, so extreme

1892
01:14:25,863 --> 01:14:28,402
that no one can actually deploy it at scale

1893
01:14:28,402 --> 01:14:31,740
and mass to actually completely revolutionize the economy

1894
01:14:31,740 --> 01:14:33,380
on a snap of a finger.

1895
01:14:33,380 --> 01:14:34,213
So, I don't think it will be like

1896
01:14:34,213 --> 01:14:36,013
a snap of the finger moment. - Yeah, it's a physical

1897
01:14:36,013 --> 01:14:37,861
constraint type- - Rather it'll be a,

1898
01:14:37,861 --> 01:14:39,751
oh, the capabilities are here,

1899
01:14:39,751 --> 01:14:41,202
but I can't deploy it everywhere.

1900
01:14:41,202 --> 01:14:45,083
And so, one simple example going back to 2023

1901
01:14:45,083 --> 01:14:47,916
was when being with GPT-4 came out

1902
01:14:49,013 --> 01:14:50,812
and everyone was freaking out about Search.

1903
01:14:50,812 --> 01:14:51,904
- Oh gosh. - Perplexity came out.

1904
01:14:51,904 --> 01:14:53,393
If you did the cost on like, hey,

1905
01:14:53,393 --> 01:14:55,683
implementing GPT-3 into every Google search,

1906
01:14:55,683 --> 01:14:56,527
it was like, oh, okay,

1907
01:14:56,527 --> 01:14:58,497
this is just like physically impossible to implement.

1908
01:14:58,497 --> 01:15:00,321
And as we step forward

1909
01:15:00,321 --> 01:15:05,052
to going back to the test-time compute thing, a query for...

1910
01:15:05,052 --> 01:15:07,901
You asked ChatGPT a question, it cost cents

1911
01:15:07,901 --> 01:15:12,041
for their most capable model of chat to get a query back.

1912
01:15:12,041 --> 01:15:17,041
To solve an ARC-AGI problem though, cost 5 to 20 bucks.

1913
01:15:17,103 --> 01:15:18,513
And this is an a-

1914
01:15:18,513 --> 01:15:19,834
- [Nathan] It's only going up from there.

1915
01:15:19,834 --> 01:15:22,761
- This is 1,000, 10,000x factor difference in cost

1916
01:15:22,761 --> 01:15:26,022
to respond to a query versus do a task.

1917
01:15:26,022 --> 01:15:28,639
And the task of ARC-AGI, it's not like,

1918
01:15:28,639 --> 01:15:31,708
it's simple to some extent,

1919
01:15:31,708 --> 01:15:34,695
but it's also like what are the tasks that we want A...

1920
01:15:34,695 --> 01:15:36,033
Okay, AGI, quote, unquote,

1921
01:15:36,033 --> 01:15:37,406
"what we have today" can do ARC-AGI.

1922
01:15:37,406 --> 01:15:38,494
Three years from now,

1923
01:15:38,494 --> 01:15:40,076
it can do much more complicated problems,

1924
01:15:40,076 --> 01:15:42,947
but the cost is gonna be measured in thousands and thousands

1925
01:15:42,947 --> 01:15:45,756
and hundreds of thousands of dollars of GPU time,

1926
01:15:45,756 --> 01:15:47,277
and there just won't be enough power,

1927
01:15:47,277 --> 01:15:48,685
GPUs, infrastructure - Yeah.

1928
01:15:48,685 --> 01:15:50,116
- to operate this, and therefore,

1929
01:15:50,116 --> 01:15:52,243
shift everything in the world on the snap the finger.

1930
01:15:52,243 --> 01:15:53,654
But at that moment,

1931
01:15:53,654 --> 01:15:57,654
who gets to control and point the AGI at a task.

1932
01:15:58,745 --> 01:16:00,838
And so, this was in Dario's post that he's like,

1933
01:16:00,838 --> 01:16:02,523
hey, China can effectively

1934
01:16:02,523 --> 01:16:07,005
and more quickly than us, point their AGI at military tasks.

1935
01:16:07,005 --> 01:16:08,625
And they have been in many ways,

1936
01:16:08,625 --> 01:16:10,723
faster at adopting certain new technologies

1937
01:16:10,723 --> 01:16:15,077
into their military, especially with regards to drones.

1938
01:16:15,077 --> 01:16:17,210
The US maybe has a long-standing

1939
01:16:17,210 --> 01:16:21,611
large air sort of fighter jet type of thing, bombers,

1940
01:16:21,611 --> 01:16:24,635
but when it comes to asymmetric arms such as drones,

1941
01:16:24,635 --> 01:16:28,156
they've completely leapfrogged the US and the west.

1942
01:16:28,156 --> 01:16:31,142
And the fear that Dario is pointing out there,

1943
01:16:31,142 --> 01:16:33,195
I think, is that, yeah, great,

1944
01:16:33,195 --> 01:16:35,157
we'll have AGI in the commercial sector.

1945
01:16:35,157 --> 01:16:38,036
The US military won't be able to implement it super fast.

1946
01:16:38,036 --> 01:16:39,447
Chinese military could

1947
01:16:39,447 --> 01:16:40,927
and they could direct all their resources

1948
01:16:40,927 --> 01:16:42,825
to implementing it in the military,

1949
01:16:42,825 --> 01:16:45,728
and therefore solving military logistics

1950
01:16:45,728 --> 01:16:48,464
or solving some other aspect of disinformation

1951
01:16:48,464 --> 01:16:50,475
for targeted certain set of people,

1952
01:16:50,475 --> 01:16:52,087
so that they can flip a country's politics,

1953
01:16:52,087 --> 01:16:54,725
or something like that, that is actually like catastrophic

1954
01:16:54,725 --> 01:16:57,259
versus the US just wants to,

1955
01:16:57,259 --> 01:16:59,377
'cause it'll be more capitalistic allocated

1956
01:16:59,377 --> 01:17:00,210
just towards whatever - Mm.

1957
01:17:00,210 --> 01:17:01,833
- is the highest return on income,

1958
01:17:01,833 --> 01:17:04,636
which might be like building factories better or whatever.

1959
01:17:04,636 --> 01:17:06,487
- So, everything I've seen,

1960
01:17:06,487 --> 01:17:09,447
people's intuition seems to fail on robotics.

1961
01:17:09,447 --> 01:17:11,978
So, you have this kind of general optimism.

1962
01:17:11,978 --> 01:17:13,562
I've seen this on self-driving cars.

1963
01:17:13,562 --> 01:17:16,538
People think it's much easier problem than it is.

1964
01:17:16,538 --> 01:17:18,555
Similar with drones.

1965
01:17:18,555 --> 01:17:20,266
Here, I understand it a little bit less,

1966
01:17:20,266 --> 01:17:23,797
but I've just seen the reality of the war in Ukraine

1967
01:17:23,797 --> 01:17:26,835
and the usage of drones on both sides.

1968
01:17:26,835 --> 01:17:29,195
And it seems that humans

1969
01:17:29,195 --> 01:17:33,362
still far outperform any fully autonomous systems.

1970
01:17:34,199 --> 01:17:38,116
AI is an assistant, but humans drive FPV drones

1971
01:17:39,034 --> 01:17:40,570
where the humans controlling most of it

1972
01:17:40,570 --> 01:17:43,736
just far, far, far outperforms AI systems.

1973
01:17:43,736 --> 01:17:45,635
So, I think it's not obvious to me

1974
01:17:45,635 --> 01:17:47,034
that we're going to have swarms

1975
01:17:47,034 --> 01:17:51,561
of autonomous robots anytime soon in the military context.

1976
01:17:51,561 --> 01:17:54,918
Maybe the fastest I can imagine is 2030,

1977
01:17:54,918 --> 01:17:58,641
which is why I said 2030 for the super powerful AI.

1978
01:17:58,641 --> 01:18:01,886
Whenever you have large-scale swarms

1979
01:18:01,886 --> 01:18:03,726
of robots doing military actions,

1980
01:18:03,726 --> 01:18:07,760
that's when the world just starts to look different to me.

1981
01:18:07,760 --> 01:18:09,226
So, that's the thing I'm really worried about.

1982
01:18:09,226 --> 01:18:13,742
But there could be cyber war type of technologies

1983
01:18:13,742 --> 01:18:15,937
that from social engineering,

1984
01:18:15,937 --> 01:18:20,024
to actually just swarms the robots that find attack vectors

1985
01:18:20,024 --> 01:18:21,108
in our code bases,

1986
01:18:21,108 --> 01:18:24,412
and shut down power grids, that kind of stuff.

1987
01:18:24,412 --> 01:18:25,655
And it could be one of those things

1988
01:18:25,655 --> 01:18:29,233
like on any given weekend or something,

1989
01:18:29,233 --> 01:18:31,593
power goes out, nobody knows why,

1990
01:18:31,593 --> 01:18:33,546
and the world changes forever.

1991
01:18:33,546 --> 01:18:35,329
Just power going out for two days

1992
01:18:35,329 --> 01:18:37,992
in all of the United States,

1993
01:18:37,992 --> 01:18:41,203
that will lead to murder to chaos.

1994
01:18:41,203 --> 01:18:44,036
But going back to export controls,

1995
01:18:45,351 --> 01:18:47,934
do you see that as a useful way

1996
01:18:48,982 --> 01:18:52,815
to control the balance of power geopolitically

1997
01:18:54,900 --> 01:18:56,250
in the context of AI?

1998
01:18:56,250 --> 01:18:58,019
- And I think going back to my viewpoint

1999
01:18:58,019 --> 01:19:02,317
is if you believe we're in this stage of economic growth

2000
01:19:02,317 --> 01:19:05,216
and change that we've been in for the last 20 years,

2001
01:19:05,216 --> 01:19:06,189
the export controls

2002
01:19:06,189 --> 01:19:11,113
are absolutely guaranteeing that China will win long-term,

2003
01:19:11,113 --> 01:19:12,421
if you do not believe AI

2004
01:19:12,421 --> 01:19:14,589
is going to make significant changes

2005
01:19:14,589 --> 01:19:17,921
to society in the next 10 years or 5 years.

2006
01:19:17,921 --> 01:19:20,700
Five-year timelines are what the more executives

2007
01:19:20,700 --> 01:19:22,269
and such of AI companies

2008
01:19:22,269 --> 01:19:23,909
and even big tech companies believe.

2009
01:19:23,909 --> 01:19:26,440
But even tenure timelines, it's reasonable.

2010
01:19:26,440 --> 01:19:28,832
But once you get to, hey,

2011
01:19:28,832 --> 01:19:32,415
these timelines are below that time period,

2012
01:19:33,627 --> 01:19:38,284
then the only way to create a sizable advantage

2013
01:19:38,284 --> 01:19:39,797
or disadvantage for America

2014
01:19:39,797 --> 01:19:43,635
versus China is if you constrain compute.

2015
01:19:43,635 --> 01:19:47,921
Because talent is not really something that's constraining.

2016
01:19:47,921 --> 01:19:49,605
China arguably has more talent,

2017
01:19:49,605 --> 01:19:51,950
more STEM graduates, more programmers.

2018
01:19:51,950 --> 01:19:54,658
The US can draw upon the world's people, which it does.

2019
01:19:54,658 --> 01:19:57,394
There's tons of foreigners in the AI industry-

2020
01:19:57,394 --> 01:19:58,654
- [Nathan] So many of these AI teams

2021
01:19:58,654 --> 01:20:01,203
are all people without a US passport.

2022
01:20:01,203 --> 01:20:03,034
- Yeah. (Nathan laughing)

2023
01:20:03,034 --> 01:20:05,045
Many of them are Chinese people

2024
01:20:05,045 --> 01:20:05,878
who are moving - Yeah.

2025
01:20:05,878 --> 01:20:07,079
- to North America, and that's great.

2026
01:20:07,079 --> 01:20:09,050
That's exactly what we want.

2027
01:20:09,050 --> 01:20:11,558
But there's that talent is one aspect,

2028
01:20:11,558 --> 01:20:12,759
but I don't think that's one

2029
01:20:12,759 --> 01:20:15,067
that is a measurable advantage for the US or not.

2030
01:20:15,067 --> 01:20:18,274
It truly is just whether or not compute.

2031
01:20:18,274 --> 01:20:19,569
Now, even on the compute side,

2032
01:20:19,569 --> 01:20:22,573
when we look at chips versus data centers.

2033
01:20:22,573 --> 01:20:25,353
China has the unprecedented ability

2034
01:20:25,353 --> 01:20:29,684
to build ridiculous sums of power clockwork.

2035
01:20:29,684 --> 01:20:31,568
They're always building more and more power.

2036
01:20:31,568 --> 01:20:35,741
They've got steel mills that individually

2037
01:20:35,741 --> 01:20:37,718
are the size of the entire US industry.

2038
01:20:37,718 --> 01:20:38,771
And they've got aluminum mills

2039
01:20:38,771 --> 01:20:41,529
that consume gigawatts and gigawatts of power.

2040
01:20:41,529 --> 01:20:43,912
And when we talk about what's the biggest data center,

2041
01:20:43,912 --> 01:20:46,890
OpenAI made this huge thing about Stargate,

2042
01:20:46,890 --> 01:20:47,932
their announcement there,

2043
01:20:47,932 --> 01:20:51,576
that's like once it's fully built out in a few years,

2044
01:20:51,576 --> 01:20:54,001
it'll be two gigawatts of power.

2045
01:20:54,001 --> 01:20:55,458
And this is still smaller

2046
01:20:55,458 --> 01:20:58,938
than the largest industrial facilities in China.

2047
01:20:58,938 --> 01:21:00,920
China, if they wanted to build the largest data center

2048
01:21:00,920 --> 01:21:03,971
in the world, if they had access to the chips, could.

2049
01:21:03,971 --> 01:21:07,875
So, it's just a question of when, not if.

2050
01:21:07,875 --> 01:21:08,862
- So, their industrial capacity

2051
01:21:08,862 --> 01:21:10,394
far exceeds the United States?

2052
01:21:10,394 --> 01:21:11,227
- [Dylan] Exactly.

2053
01:21:11,227 --> 01:21:13,017
- To the the manufacture stuff.

2054
01:21:13,017 --> 01:21:15,306
- Yeah. - So, long-term,

2055
01:21:15,306 --> 01:21:18,510
they're going to be manufacturing chips there.

2056
01:21:18,510 --> 01:21:20,289
- Chips are a little bit more specialized.

2057
01:21:20,289 --> 01:21:22,289
I'm specifically referring to the data centers.

2058
01:21:22,289 --> 01:21:25,096
Chips, fabs take huge amounts of power, don't get me wrong.

2059
01:21:25,096 --> 01:21:27,432
That's not necessarily the gating factor there.

2060
01:21:27,432 --> 01:21:29,992
The gating factor on how fast people

2061
01:21:29,992 --> 01:21:33,723
can build the largest clusters today in the US is power.

2062
01:21:33,723 --> 01:21:35,870
Whether it's now it could be power generation,

2063
01:21:35,870 --> 01:21:38,221
power transmission, substations,

2064
01:21:38,221 --> 01:21:41,762
and all these sorts of transformers and all these things,

2065
01:21:41,762 --> 01:21:44,466
building the data center, these are all constraints

2066
01:21:44,466 --> 01:21:47,645
on the US industry's ability to build larger

2067
01:21:47,645 --> 01:21:49,386
and larger training systems,

2068
01:21:49,386 --> 01:21:52,115
as well as deploying more and more inference compute.

2069
01:21:52,115 --> 01:21:54,834
- I think we need to make the point clear on why the time

2070
01:21:54,834 --> 01:21:57,307
is now for people that don't think about this,

2071
01:21:57,307 --> 01:21:58,898
'cause essentially with export controls,

2072
01:21:58,898 --> 01:21:59,731
you're making it so China

2073
01:21:59,731 --> 01:22:02,787
cannot make or get cutting edge chips.

2074
01:22:02,787 --> 01:22:05,823
And the idea is that if you time this wrong,

2075
01:22:05,823 --> 01:22:09,271
China is pouring a ton of money into their chip production.

2076
01:22:09,271 --> 01:22:10,219
And if you time it wrong,

2077
01:22:10,219 --> 01:22:12,309
they're going to have more capacity for production,

2078
01:22:12,309 --> 01:22:13,831
more capacity for energy,

2079
01:22:13,831 --> 01:22:15,352
and figure out how to make the chips

2080
01:22:15,352 --> 01:22:16,391
and have more capacity

2081
01:22:16,391 --> 01:22:18,202
than the rest of the world to make the chips,

2082
01:22:18,202 --> 01:22:19,608
because everybody can buy,

2083
01:22:19,608 --> 01:22:21,599
they're gonna sell their Chinese chips to everybody.

2084
01:22:21,599 --> 01:22:22,791
They might subsidize them.

2085
01:22:22,791 --> 01:22:25,392
And therefore, if AI takes a long time

2086
01:22:25,392 --> 01:22:26,643
to become differentiated,

2087
01:22:26,643 --> 01:22:28,544
we've kneecapped the financial performance

2088
01:22:28,544 --> 01:22:29,963
of American companies.

2089
01:22:29,963 --> 01:22:33,521
Nvidia can sell less. TSMC cannot sell to China.

2090
01:22:33,521 --> 01:22:36,461
So, therefore, we have less demand

2091
01:22:36,461 --> 01:22:40,104
to therefore to keep driving the production cycle.

2092
01:22:40,104 --> 01:22:42,531
So, that's the assumption behind the timing

2093
01:22:42,531 --> 01:22:44,130
being important. - Less than 10 years

2094
01:22:44,130 --> 01:22:45,793
or 5 years to above.

2095
01:22:45,793 --> 01:22:49,150
China will win because of these restrictions long-term,

2096
01:22:49,150 --> 01:22:51,751
unless AI does something in the short-term,

2097
01:22:51,751 --> 01:22:53,453
which I believe AI will do,

2098
01:22:53,453 --> 01:22:57,908
make massive changes to society in the medium short-term.

2099
01:22:57,908 --> 01:23:00,898
And so, that's the big unlocker there.

2100
01:23:00,898 --> 01:23:01,937
And even today,

2101
01:23:01,937 --> 01:23:04,120
if Xi Jinping decided to get,

2102
01:23:04,120 --> 01:23:06,858
quote, unquote, "scale pilled",

2103
01:23:06,858 --> 01:23:10,286
i.e, decide that scaling laws are what matters,

2104
01:23:10,286 --> 01:23:13,097
just like the US executives like Satya Nadella

2105
01:23:13,097 --> 01:23:15,886
and Mark Zuckerberg, and Sundar,

2106
01:23:15,886 --> 01:23:17,869
and all these US executives of the biggest,

2107
01:23:17,869 --> 01:23:19,819
most powerful tech companies,

2108
01:23:19,819 --> 01:23:21,320
have decided they're scale pilled

2109
01:23:21,320 --> 01:23:23,479
and they're building multi-gigawatt data centers.

2110
01:23:23,479 --> 01:23:25,990
Whether it's in Texas or Louisiana, or Wisconsin,

2111
01:23:25,990 --> 01:23:28,869
wherever it is, they're building these massive things

2112
01:23:28,869 --> 01:23:31,830
that cost as much as their entire budget

2113
01:23:31,830 --> 01:23:34,607
for spending on data centers globally in one spot.

2114
01:23:34,607 --> 01:23:36,018
This is what they've committed to

2115
01:23:36,018 --> 01:23:38,029
for next year, year after, et cetera.

2116
01:23:38,029 --> 01:23:41,019
And so, they're so convinced

2117
01:23:41,019 --> 01:23:43,208
that this is the way that this is what they're doing.

2118
01:23:43,208 --> 01:23:46,526
But if China decided to, they could do it faster than us.

2119
01:23:46,526 --> 01:23:48,920
But this is where the restrictions come in.

2120
01:23:48,920 --> 01:23:52,558
It is not clear that China as a whole has decided,

2121
01:23:52,558 --> 01:23:54,468
from the highest levels that this is a priority.

2122
01:23:54,468 --> 01:23:56,019
The US has.

2123
01:23:56,019 --> 01:23:57,938
You see Trump talking about DeepSeek

2124
01:23:57,938 --> 01:24:00,968
and Stargate within the same week.

2125
01:24:00,968 --> 01:24:02,120
And the Biden admin as well

2126
01:24:02,120 --> 01:24:05,127
had a lot of discussions about AI and such.

2127
01:24:05,127 --> 01:24:06,503
It's clear that they think about it.

2128
01:24:06,503 --> 01:24:08,088
Only just last week

2129
01:24:08,088 --> 01:24:12,148
did DeepSeek meet the second in command of China.

2130
01:24:12,148 --> 01:24:13,608
They have not even met the top,

2131
01:24:13,608 --> 01:24:16,081
and haven't met Xi, she hasn't set down.

2132
01:24:16,081 --> 01:24:18,918
And they only just released a subsidy

2133
01:24:18,918 --> 01:24:22,911
of a trillion RMB, roughly $160 billion,

2134
01:24:22,911 --> 01:24:26,957
which is closer to the spending of like Microsoft and Meta,

2135
01:24:26,957 --> 01:24:29,427
and Google combined for this year.

2136
01:24:29,427 --> 01:24:32,226
So, it's like they're realizing it just now.

2137
01:24:32,226 --> 01:24:34,749
But that's where these export restrictions come in

2138
01:24:34,749 --> 01:24:36,756
and say, hey, you can't ship

2139
01:24:36,756 --> 01:24:38,796
the most powerful US chips to China.

2140
01:24:38,796 --> 01:24:40,526
You can ship a cut down version.

2141
01:24:40,526 --> 01:24:44,058
You can't ship the most powerful chips

2142
01:24:44,058 --> 01:24:44,978
to all these countries

2143
01:24:44,978 --> 01:24:47,077
who we know are just gonna rent it to China.

2144
01:24:47,077 --> 01:24:48,530
You have to limit the numbers.

2145
01:24:48,530 --> 01:24:49,363
- [Nathan] And the tools.

2146
01:24:49,363 --> 01:24:50,269
Same- - And same

2147
01:24:50,269 --> 01:24:52,481
with manufacturing equipment tools,

2148
01:24:52,481 --> 01:24:55,238
all these different aspects, but it all stems from AI,

2149
01:24:55,238 --> 01:24:58,570
and then what downstream can slow them down in AI.

2150
01:24:58,570 --> 01:25:00,958
And so, the entire semiconductor restrictions,

2151
01:25:00,958 --> 01:25:02,609
you read them, they're very clear,

2152
01:25:02,609 --> 01:25:06,538
it's about AI and military civil fusion of technology.

2153
01:25:06,538 --> 01:25:07,453
It's very clear.

2154
01:25:07,453 --> 01:25:08,639
And then, from there, it goes, oh,

2155
01:25:08,639 --> 01:25:10,981
well, we're banning them from buying lithography tools

2156
01:25:10,981 --> 01:25:13,114
and etch tools, and deposition tools.

2157
01:25:13,114 --> 01:25:15,445
And, oh, this random subsystem

2158
01:25:15,445 --> 01:25:17,692
from a random company that's tiny.

2159
01:25:17,692 --> 01:25:18,536
Why are we banning this?

2160
01:25:18,536 --> 01:25:19,724
Because all of it,

2161
01:25:19,724 --> 01:25:23,644
the US government has decided, is critical to AI systems.

2162
01:25:23,644 --> 01:25:26,587
- I think the fulcrum point is the transition

2163
01:25:26,587 --> 01:25:28,652
from seven nanometer to five nanometer chips,

2164
01:25:28,652 --> 01:25:29,757
where I think it was Huawei

2165
01:25:29,757 --> 01:25:32,836
that had the seven nanometer chip a few years ago,

2166
01:25:32,836 --> 01:25:35,506
which caused another political brouhaha

2167
01:25:35,506 --> 01:25:36,495
almost like this moment.

2168
01:25:36,495 --> 01:25:40,714
And then, it's the ASML deep UV. What is that?

2169
01:25:40,714 --> 01:25:43,044
Like extreme ultraviolet lithography.

2170
01:25:43,044 --> 01:25:44,704
- To set context on the chips,

2171
01:25:44,704 --> 01:25:46,733
what Nathan's referring to is in 2020,

2172
01:25:46,733 --> 01:25:49,979
Huawei released their Ascend 910 chip,

2173
01:25:49,979 --> 01:25:53,174
which was an AI chip, first one on seven nanometer

2174
01:25:53,174 --> 01:25:55,215
before Google did, before Nvidia did.

2175
01:25:55,215 --> 01:25:57,423
And they submitted it to the MLPerf benchmark,

2176
01:25:57,423 --> 01:25:59,487
which is a industry standard

2177
01:25:59,487 --> 01:26:01,690
for machine learning performance benchmark.

2178
01:26:01,690 --> 01:26:03,205
And it did quite well.

2179
01:26:03,205 --> 01:26:05,175
And it was the best chip at the submission.

2180
01:26:05,175 --> 01:26:06,925
This was a huge deal.

2181
01:26:08,031 --> 01:26:11,570
The Trump admin of course banned, it was 2019,

2182
01:26:11,570 --> 01:26:12,881
banned the Huawei

2183
01:26:12,881 --> 01:26:15,595
from getting seven nanometer chips from TSMC.

2184
01:26:15,595 --> 01:26:16,448
And so, then they had to switch

2185
01:26:16,448 --> 01:26:18,794
to using internal domestically produced chips,

2186
01:26:18,794 --> 01:26:20,288
which was a multi-year setback.

2187
01:26:20,288 --> 01:26:22,333
- Many companies have done seven nanometer chips.

2188
01:26:22,333 --> 01:26:25,108
And the question is like, we don't know how much Huawei

2189
01:26:25,108 --> 01:26:27,005
was subsidizing production of that chip.

2190
01:26:27,005 --> 01:26:29,236
Intel has made seven nanometer chips

2191
01:26:29,236 --> 01:26:31,762
that are not profitable, and things like this.

2192
01:26:31,762 --> 01:26:33,136
So, this is how it all feeds back

2193
01:26:33,136 --> 01:26:36,226
into the economic engine of export controls.

2194
01:26:36,226 --> 01:26:39,346
- Well, so you're saying that for now, Xi Jinping

2195
01:26:39,346 --> 01:26:40,954
has not felt the AGI,

2196
01:26:40,954 --> 01:26:42,954
but it feels like the DeepSeek moment

2197
01:26:42,954 --> 01:26:44,621
- Yeah. - might like,

2198
01:26:45,841 --> 01:26:47,537
there might be meetings going on now,

2199
01:26:47,537 --> 01:26:49,685
where he's gonna start wearing the same T-shirt

2200
01:26:49,685 --> 01:26:53,731
and things are gonna escalate. (Nathan and Dylan laughing)

2201
01:26:53,731 --> 01:26:55,423
- He may have woken up last week.

2202
01:26:55,423 --> 01:26:59,301
Liang Feng met the second command guy,

2203
01:26:59,301 --> 01:27:00,289
and they had a meeting.

2204
01:27:00,289 --> 01:27:03,376
And then, the next day, they announced the AI subsidies,

2205
01:27:03,376 --> 01:27:05,312
which are a trillion RMB.

2206
01:27:05,312 --> 01:27:07,651
- So, it's possible that this DeepSeek moment

2207
01:27:07,651 --> 01:27:10,817
is truly the beginning of a cold war.

2208
01:27:10,817 --> 01:27:12,490
- That's what a lot of people are worried about.

2209
01:27:12,490 --> 01:27:13,924
People in AI have been worried

2210
01:27:13,924 --> 01:27:15,662
that this is going towards a cold war

2211
01:27:15,662 --> 01:27:16,831
or already is. - But there is,

2212
01:27:16,831 --> 01:27:19,729
it's not DeepSeek's fault, but there's something,

2213
01:27:19,729 --> 01:27:21,450
a bunch of factors came together

2214
01:27:21,450 --> 01:27:23,336
where it was like explosion. - No, history works.

2215
01:27:23,336 --> 01:27:25,458
- It all has to do with Nvidia stock going down problem.

2216
01:27:25,458 --> 01:27:27,984
But it's just some (Nathan laughing)

2217
01:27:27,984 --> 01:27:29,498
mass hysteria (Lex drowns out Nathan)

2218
01:27:29,498 --> 01:27:31,677
that happened that eventually led

2219
01:27:31,677 --> 01:27:34,875
to Xi Jinping having meetings and waking up to this idea.

2220
01:27:34,875 --> 01:27:38,449
- And the US government realized in October 7th, 2022,

2221
01:27:38,449 --> 01:27:40,477
before ChatGPT released,

2222
01:27:40,477 --> 01:27:42,327
that restriction on October 7th,

2223
01:27:42,327 --> 01:27:44,307
which dropped and shocked everyone.

2224
01:27:44,307 --> 01:27:46,028
And it was very clearly aimed at AI.

2225
01:27:46,028 --> 01:27:48,156
Everyone was like, what the heck are you doing?

2226
01:27:48,156 --> 01:27:49,557
- And diffusion was out then,

2227
01:27:49,557 --> 01:27:51,493
but not ChatGPT. - Yeah, but not ChatGPT.

2228
01:27:51,493 --> 01:27:53,225
- So, it was like starting to be rumblings like-

2229
01:27:53,225 --> 01:27:55,827
- Of what gen AI can do to society,

2230
01:27:55,827 --> 01:27:57,236
but it was very clear, I think,

2231
01:27:57,236 --> 01:27:59,356
to at least National Security Council

2232
01:27:59,356 --> 01:28:00,897
and those sort of folks

2233
01:28:00,897 --> 01:28:03,173
that this was where the world is headed,

2234
01:28:03,173 --> 01:28:04,596
this cold war that's happening.

2235
01:28:04,596 --> 01:28:08,929
- So, is there any concerns that the export controls

2236
01:28:10,379 --> 01:28:14,129
push China to take military action on Taiwan?

2237
01:28:15,597 --> 01:28:17,009
- This is the big risk.

2238
01:28:17,009 --> 01:28:19,586
The further you push China away from having access

2239
01:28:19,586 --> 01:28:22,907
to cutting edge American and global technologies,

2240
01:28:22,907 --> 01:28:24,979
the more likely they are to say,

2241
01:28:24,979 --> 01:28:27,058
well, 'cause I can't access it, I might as well...

2242
01:28:27,058 --> 01:28:28,767
No one should access it.

2243
01:28:28,767 --> 01:28:32,056
And there's a few interesting aspects of that.

2244
01:28:32,056 --> 01:28:35,806
China has a urban rural divide like no other.

2245
01:28:36,831 --> 01:28:40,148
They have a male/female birth ratio like no other,

2246
01:28:40,148 --> 01:28:42,473
to the point where, if you look in most of China,

2247
01:28:42,473 --> 01:28:43,574
it's like the ratio's not that bad.

2248
01:28:43,574 --> 01:28:45,424
But when you look at single dudes in rural China,

2249
01:28:45,424 --> 01:28:47,258
it's like a 30 to 1 ratio. - Mm-hmm.

2250
01:28:47,258 --> 01:28:50,217
- And those are disenfranchised dudes.

2251
01:28:50,217 --> 01:28:52,354
Quote, unquote, the US has an "incel problem"

2252
01:28:52,354 --> 01:28:53,904
like China does too.

2253
01:28:53,904 --> 01:28:56,583
It's just they're placated in some way or crushed down.

2254
01:28:56,583 --> 01:28:57,825
What do you do with these people?

2255
01:28:57,825 --> 01:28:59,236
And at the same time, you're not allowed

2256
01:28:59,236 --> 01:29:01,354
to access the most important technology,

2257
01:29:01,354 --> 01:29:03,034
at least the US thinks so.

2258
01:29:03,034 --> 01:29:04,074
China's maybe starting to think

2259
01:29:04,074 --> 01:29:05,501
this is the most important technology

2260
01:29:05,501 --> 01:29:07,228
by starting to dump subsidies in it.

2261
01:29:07,228 --> 01:29:08,778
They thought EVs and renewables

2262
01:29:08,778 --> 01:29:09,927
were the most important technology.

2263
01:29:09,927 --> 01:29:11,295
They dominate that now.

2264
01:29:11,295 --> 01:29:14,202
Now, they started thinking about semiconductors

2265
01:29:14,202 --> 01:29:17,706
in the late 2010s and early 2020s,

2266
01:29:17,706 --> 01:29:19,116
and now they've been dumping money

2267
01:29:19,116 --> 01:29:21,134
and they're catching up rapidly,

2268
01:29:21,134 --> 01:29:22,294
and they're gonna do the same with AI.

2269
01:29:22,294 --> 01:29:24,384
Because they're very talented.

2270
01:29:24,384 --> 01:29:26,467
So, the question is like,

2271
01:29:27,483 --> 01:29:30,483
when does this hit a breaking point?

2272
01:29:32,399 --> 01:29:36,384
And if China sees this as, hey, they can continue...

2273
01:29:36,384 --> 01:29:39,535
If not having access and starting a true hot war.

2274
01:29:39,535 --> 01:29:42,458
Taking over Taiwan or trying to subvert its democracy

2275
01:29:42,458 --> 01:29:45,348
in some way, or blockading it,

2276
01:29:45,348 --> 01:29:48,233
hurts the rest of the world far more than it hurts them,

2277
01:29:48,233 --> 01:29:50,177
this is something they could potentially do.

2278
01:29:50,177 --> 01:29:54,299
And so, is this pushing them towards that? Potentially.

2279
01:29:54,299 --> 01:29:56,209
I'm not quite a geopolitical person,

2280
01:29:56,209 --> 01:30:00,630
but it's obvious that the world regime of peace

2281
01:30:00,630 --> 01:30:04,047
and trade is super awesome for economics.

2282
01:30:05,070 --> 01:30:07,044
But at some point, it could break.

2283
01:30:07,044 --> 01:30:09,706
- I think we should comment why Chinese economy

2284
01:30:09,706 --> 01:30:11,962
would be hurt by that is that they're export-heavy.

2285
01:30:11,962 --> 01:30:14,047
I think the United States buys so much,

2286
01:30:14,047 --> 01:30:16,508
like if that goes away, that's how their economy goes.

2287
01:30:16,508 --> 01:30:18,356
- Well, also, they just would not be able

2288
01:30:18,356 --> 01:30:21,772
to import raw materials from all over the world.

2289
01:30:21,772 --> 01:30:23,776
The US would just shut down the strait of Malacca

2290
01:30:23,776 --> 01:30:26,435
and at the same time, the US entire,

2291
01:30:26,435 --> 01:30:29,385
you could argue almost all the GDP growth in America

2292
01:30:29,385 --> 01:30:34,135
since the '70s has been either population growth or tech.

2293
01:30:35,499 --> 01:30:39,278
Because your life today is not that much better

2294
01:30:39,278 --> 01:30:42,941
than someone from the '80s outside of tech.

2295
01:30:42,941 --> 01:30:45,123
Cars, they all have semiconductors in them everywhere.

2296
01:30:45,123 --> 01:30:46,325
Fridges, semiconductors everywhere.

2297
01:30:46,325 --> 01:30:48,000
These funny stories about how Russians

2298
01:30:48,000 --> 01:30:49,648
were taking apart laundry machines

2299
01:30:49,648 --> 01:30:51,838
because they had certain Texas instrument chips

2300
01:30:51,838 --> 01:30:53,497
that they could then repurpose

2301
01:30:53,497 --> 01:30:57,459
and put into their anti-missile missile things,

2302
01:30:57,459 --> 01:30:59,060
their S-400 or whatever.

2303
01:30:59,060 --> 01:31:00,248
You would know more about this,

2304
01:31:00,248 --> 01:31:02,396
but there's all sorts of,

2305
01:31:02,396 --> 01:31:03,847
everything about semiconductors

2306
01:31:03,847 --> 01:31:06,158
is so integral to every part of our lives.

2307
01:31:06,158 --> 01:31:09,728
- So, can you explain the role of TSMC

2308
01:31:09,728 --> 01:31:13,408
in this story of semiconductors and maybe also

2309
01:31:13,408 --> 01:31:17,226
how the United States can break the reliance on TSMC?

2310
01:31:17,226 --> 01:31:19,637
- I don't think it's necessarily breaking their alliance.

2311
01:31:19,637 --> 01:31:23,387
I think it's getting TSMC to build in the US.

2312
01:31:24,477 --> 01:31:26,405
So, taking a step back.

2313
01:31:26,405 --> 01:31:29,738
TSMC produces most of the world's chips,

2314
01:31:30,995 --> 01:31:33,206
especially on the foundry side.

2315
01:31:33,206 --> 01:31:34,085
There's a lot of companies

2316
01:31:34,085 --> 01:31:38,027
that build their own chips, Samsung, Intel,

2317
01:31:38,027 --> 01:31:41,728
STMicro, Texas Instruments, Analog Devices,

2318
01:31:41,728 --> 01:31:44,325
all these kinds of companies build their own chips and XP.

2319
01:31:44,325 --> 01:31:45,918
But more and more of these companies

2320
01:31:45,918 --> 01:31:49,822
are outsourcing to TSMC and have been for multiple decades.

2321
01:31:49,822 --> 01:31:51,483
- Can you explain the supply chain there

2322
01:31:51,483 --> 01:31:55,230
and where most of TSMC is in terms of manufacturing?

2323
01:31:55,230 --> 01:31:56,063
- Sure.

2324
01:31:56,063 --> 01:31:57,819
So, historically, supply chain was,

2325
01:31:57,819 --> 01:31:59,522
companies would build their own chips.

2326
01:31:59,522 --> 01:32:02,329
It'd be a company-started, they'd build their own chips,

2327
01:32:02,329 --> 01:32:03,920
and then they'd design the chip

2328
01:32:03,920 --> 01:32:05,931
and build the chip, and sell it.

2329
01:32:05,931 --> 01:32:08,320
Over time, this became really difficult

2330
01:32:08,320 --> 01:32:09,980
because the cost of building a fab

2331
01:32:09,980 --> 01:32:12,290
continues to compound every single generation.

2332
01:32:12,290 --> 01:32:14,389
Of course the technology, figuring out the technology for it

2333
01:32:14,389 --> 01:32:16,078
is incredibly difficult regardless,

2334
01:32:16,078 --> 01:32:19,548
but just the dollars and cents that are required ignoring,

2335
01:32:19,548 --> 01:32:21,428
saying, hey, yes, I have all the technical capability,

2336
01:32:21,428 --> 01:32:23,347
which it's really hard to get that, by the way.

2337
01:32:23,347 --> 01:32:25,970
Intel's failing, Samsung's failing, et cetera.

2338
01:32:25,970 --> 01:32:28,672
But if you look at just the dollars to spend

2339
01:32:28,672 --> 01:32:30,797
to build that next generation fab, it keeps growing.

2340
01:32:30,797 --> 01:32:32,314
Sort of like Moore's Laws

2341
01:32:32,314 --> 01:32:34,032
having the cost of chips every two years.

2342
01:32:34,032 --> 01:32:35,463
There's a separate law

2343
01:32:35,463 --> 01:32:38,125
that's doubling the cost of fabs every handful of years.

2344
01:32:38,125 --> 01:32:40,473
And so, you look at a leading edge fab

2345
01:32:40,473 --> 01:32:42,152
that is gonna be profitable today,

2346
01:32:42,152 --> 01:32:43,427
that's building three nanometer chips

2347
01:32:43,427 --> 01:32:44,966
or two nanometer chips in the future,

2348
01:32:44,966 --> 01:32:48,788
that's gonna cost north of 30, $40 billion.

2349
01:32:48,788 --> 01:32:51,694
And that's just for like a token amount.

2350
01:32:51,694 --> 01:32:52,706
That's like the base building blocking,

2351
01:32:52,706 --> 01:32:54,197
you probably need to build multiple.

2352
01:32:54,197 --> 01:32:57,601
And so, when you look at the industry, over the last,

2353
01:32:57,601 --> 01:33:00,404
if I go back 20, 30 years ago, there were 20,

2354
01:33:00,404 --> 01:33:02,750
30 companies that could build the most advanced chips,

2355
01:33:02,750 --> 01:33:04,900
and then they would design them themselves and sell them.

2356
01:33:04,900 --> 01:33:06,999
So, companies like AMD would build their own chips.

2357
01:33:06,999 --> 01:33:08,269
Intel of course still

2358
01:33:08,269 --> 01:33:09,585
builds their own chips they're very famous for.

2359
01:33:09,585 --> 01:33:10,523
IBM would build their own chips.

2360
01:33:10,523 --> 01:33:12,752
And you could just keep going down the list.

2361
01:33:12,752 --> 01:33:14,162
All these companies built their own chips.

2362
01:33:14,162 --> 01:33:16,696
Slowly, they kept falling like flies.

2363
01:33:16,696 --> 01:33:18,963
And that's because of what TSMC did.

2364
01:33:18,963 --> 01:33:20,931
They created the foundry business model,

2365
01:33:20,931 --> 01:33:23,089
which is, I'm not gonna design any chips,

2366
01:33:23,089 --> 01:33:26,890
I'm just gonna contract manufacturer chips for other people.

2367
01:33:26,890 --> 01:33:28,672
And one of their early customers is Nvidia.

2368
01:33:28,672 --> 01:33:32,005
Nvidia is the only semiconductor company

2369
01:33:34,140 --> 01:33:35,590
that's doing more than a billion dollars of revenue

2370
01:33:35,590 --> 01:33:39,173
that was started in the era of Foundry.

2371
01:33:39,173 --> 01:33:40,653
Every other company started before then,

2372
01:33:40,653 --> 01:33:43,872
and at some point had fabs, which is actually incredible.

2373
01:33:43,872 --> 01:33:46,706
Like AMD and Intel and Broadcom-

2374
01:33:46,706 --> 01:33:47,866
- [Nathan] Such a great fact. (Nathan drowns out Dylan)

2375
01:33:47,866 --> 01:33:48,949
- It's like everyone (Nathan laughing)

2376
01:33:48,949 --> 01:33:49,937
had fabs at some point,

2377
01:33:49,937 --> 01:33:51,810
or some companies like Broadcom,

2378
01:33:51,810 --> 01:33:53,346
it was like a merger amalgamation

2379
01:33:53,346 --> 01:33:54,841
of various companies that rolled up.

2380
01:33:54,841 --> 01:33:56,840
But even today, Broadcom has fabs.

2381
01:33:56,840 --> 01:34:01,507
They build iPhone, RF radio chips in Colorado for Apple.

2382
01:34:03,400 --> 01:34:05,968
All these companies had fabs, and for most of the fabs,

2383
01:34:05,968 --> 01:34:07,519
they threw them away or sold them off,

2384
01:34:07,519 --> 01:34:09,666
or they got rolled into something else.

2385
01:34:09,666 --> 01:34:11,817
And now, everyone relies on TSMC.

2386
01:34:11,817 --> 01:34:16,235
Including Intel, their latest PC chip uses TSMC chips.

2387
01:34:16,235 --> 01:34:19,213
It also uses some intel chips, but it uses TSMC process.

2388
01:34:19,213 --> 01:34:20,658
- Can you explain why the foundry model

2389
01:34:20,658 --> 01:34:23,403
is so successful for these companies?

2390
01:34:23,403 --> 01:34:24,447
Why are they going

2391
01:34:24,447 --> 01:34:25,973
with TSMC- - Economies of scale.

2392
01:34:25,973 --> 01:34:27,076
- Scale. - Yeah.

2393
01:34:27,076 --> 01:34:30,207
So, like I mentioned, the cost of building a fab is so high.

2394
01:34:30,207 --> 01:34:32,253
The R&D is so difficult.

2395
01:34:32,253 --> 01:34:35,417
And when you look at companies

2396
01:34:35,417 --> 01:34:37,031
that had their own vertical stack,

2397
01:34:37,031 --> 01:34:39,667
there was an antiquated process of like, okay,

2398
01:34:39,667 --> 01:34:42,230
I'm so hyper customized to each specific chip.

2399
01:34:42,230 --> 01:34:44,023
But as we've gone through the history

2400
01:34:44,023 --> 01:34:47,463
of the last 50 years of electronics and semiconductors,

2401
01:34:47,463 --> 01:34:49,373
A, you need more and more specialization,

2402
01:34:49,373 --> 01:34:53,037
because Moore's Law has died, Dennard scaling has died.

2403
01:34:53,037 --> 01:34:55,520
i.e, chips are not getting better just for free.

2404
01:34:55,520 --> 01:34:56,402
From manufacturing,

2405
01:34:56,402 --> 01:34:59,375
you have to make real architectural innovations.

2406
01:34:59,375 --> 01:35:02,184
Google is not just running on Intel CPUs for web serving.

2407
01:35:02,184 --> 01:35:03,746
They have a YouTube chip, they have TPUs,

2408
01:35:03,746 --> 01:35:06,715
they have pixel chips, they have a wide diversity of chips

2409
01:35:06,715 --> 01:35:11,130
that generate all the economic value of Google.

2410
01:35:11,130 --> 01:35:12,912
It's running all the services and stuff.

2411
01:35:12,912 --> 01:35:13,904
And so, and this is just Google

2412
01:35:13,904 --> 01:35:15,556
and you could go across any company in the industry,

2413
01:35:15,556 --> 01:35:16,389
and it's like this.

2414
01:35:16,389 --> 01:35:20,917
Cars contain 5,000 chips, 200 different varieties of them.

2415
01:35:20,917 --> 01:35:22,040
All these random things.

2416
01:35:22,040 --> 01:35:24,853
A Tesla door handle has two chips. It's ridiculous.

2417
01:35:24,853 --> 01:35:26,179
And it's a cool door handle.

2418
01:35:26,179 --> 01:35:27,471
You don't think about it,

2419
01:35:27,471 --> 01:35:29,029
but it's like, has two really chipped,

2420
01:35:29,029 --> 01:35:31,740
like penny chips in there.

2421
01:35:31,740 --> 01:35:34,237
Anyways, so as you have more diversity of chips,

2422
01:35:34,237 --> 01:35:36,329
as you have more specialization required,

2423
01:35:36,329 --> 01:35:39,219
and the cost of fabs continues to grow, you need someone

2424
01:35:39,219 --> 01:35:43,350
who is laser-focused on building the best process technology

2425
01:35:43,350 --> 01:35:45,661
and making it as flexible as possible.

2426
01:35:45,661 --> 01:35:46,857
- I think you could say it simpler,

2427
01:35:46,857 --> 01:35:49,277
which is the cost per fab goes up.

2428
01:35:49,277 --> 01:35:51,587
And if you are a small player,

2429
01:35:51,587 --> 01:35:53,332
that makes a few types of chips.

2430
01:35:53,332 --> 01:35:54,502
You're not gonna have the demand

2431
01:35:54,502 --> 01:35:56,699
to pay back the cost of the fab.

2432
01:35:56,699 --> 01:35:58,780
Whereas Nvidia can have many different customers

2433
01:35:58,780 --> 01:36:01,909
and aggregate all this demand into one place,

2434
01:36:01,909 --> 01:36:04,532
and then they're the only person that makes enough money

2435
01:36:04,532 --> 01:36:08,306
building chips to buy the next, to build the next fab.

2436
01:36:08,306 --> 01:36:11,145
So, this is why the companies slowly get killed,

2437
01:36:11,145 --> 01:36:14,553
'cause they have 10 years ago a chip

2438
01:36:14,553 --> 01:36:16,593
that is profitable and is good enough,

2439
01:36:16,593 --> 01:36:18,282
but the cost to build the next one goes up.

2440
01:36:18,282 --> 01:36:20,636
They may try to do this, fail,

2441
01:36:20,636 --> 01:36:22,334
because they don't have the money to make it work,

2442
01:36:22,334 --> 01:36:23,334
and then they don't have any chips.

2443
01:36:23,334 --> 01:36:25,640
Or they build it and it's too expensive and they just have

2444
01:36:25,640 --> 01:36:27,445
not profitable chips. - Or there's more

2445
01:36:27,445 --> 01:36:30,757
failure points of, you could have one little process-related

2446
01:36:30,757 --> 01:36:35,099
to some sort of chemical etch or some sort of plasma etch,

2447
01:36:35,099 --> 01:36:37,445
or some little process that screws up,

2448
01:36:37,445 --> 01:36:38,616
you didn't engineer it right,

2449
01:36:38,616 --> 01:36:41,526
and now the whole company falls apart, you can't make chips.

2450
01:36:41,526 --> 01:36:44,207
And so, super, super powerful companies like Intel,

2451
01:36:44,207 --> 01:36:46,610
they had the weathering storm to like, hey,

2452
01:36:46,610 --> 01:36:48,851
they still exist today, even though they really screwed up

2453
01:36:48,851 --> 01:36:50,895
their manufacturing six, seven years ago.

2454
01:36:50,895 --> 01:36:53,370
But in the case of like AMD, they almost went bankrupt.

2455
01:36:53,370 --> 01:36:57,283
They had to sell their fabs to Mubadala, UAE.

2456
01:36:57,283 --> 01:36:59,298
And that became a separate company

2457
01:36:59,298 --> 01:37:01,924
called GlobalFoundries, which is a foundry firm.

2458
01:37:01,924 --> 01:37:03,564
And then, AMD was able to then focus

2459
01:37:03,564 --> 01:37:05,319
on the return back up was like,

2460
01:37:05,319 --> 01:37:06,974
hey, let's focus on making chiplets

2461
01:37:06,974 --> 01:37:09,972
and a bunch of different chips for different markets.

2462
01:37:09,972 --> 01:37:11,943
And focusing on specific workloads

2463
01:37:11,943 --> 01:37:14,383
rather than all of these different things.

2464
01:37:14,383 --> 01:37:16,024
And so, you get more diversity of chips,

2465
01:37:16,024 --> 01:37:18,242
you have more companies than ever designing chips,

2466
01:37:18,242 --> 01:37:21,571
but you have fewer companies than ever manufacturing them.

2467
01:37:21,571 --> 01:37:24,012
And this is where TSMC comes in,

2468
01:37:24,012 --> 01:37:25,913
is they've just been the best.

2469
01:37:25,913 --> 01:37:27,524
They are so good at it.

2470
01:37:27,524 --> 01:37:28,972
They're customer-focused,

2471
01:37:28,972 --> 01:37:31,132
they make it easy for you to fabricate your chips.

2472
01:37:31,132 --> 01:37:32,327
They take all of that complexity

2473
01:37:32,327 --> 01:37:34,875
and try and abstract a lot of it away from you.

2474
01:37:34,875 --> 01:37:37,299
They make good money, they don't make insane money,

2475
01:37:37,299 --> 01:37:38,826
but they make good money.

2476
01:37:38,826 --> 01:37:41,688
And they're able to aggregate all this demand

2477
01:37:41,688 --> 01:37:43,185
and continue to build the next fab,

2478
01:37:43,185 --> 01:37:44,134
the next fab, the next fab.

2479
01:37:44,134 --> 01:37:46,938
- So, why is Taiwan so special for TSMC?

2480
01:37:46,938 --> 01:37:48,808
Why is it happening there?

2481
01:37:48,808 --> 01:37:51,699
Can it be replicated inside the United States?

2482
01:37:51,699 --> 01:37:53,264
- Yeah, so there's aspects of it

2483
01:37:53,264 --> 01:37:56,488
that I would say yes and aspects that I'd say no.

2484
01:37:56,488 --> 01:37:57,988
TSMC is way ahead,

2485
01:37:58,888 --> 01:38:01,689
because Former Executive Morris Chang

2486
01:38:01,689 --> 01:38:05,032
of Texas Instruments wasn't promoted to CEO,

2487
01:38:05,032 --> 01:38:05,990
and he is like, screw this,

2488
01:38:05,990 --> 01:38:07,648
I'm gonna go make my own chip company.

2489
01:38:07,648 --> 01:38:09,404
And he went to Taiwan and made TSMC.

2490
01:38:09,404 --> 01:38:11,950
And there's a whole lot more story there.

2491
01:38:11,950 --> 01:38:13,450
So, it could have been Texas Instruments,

2492
01:38:13,450 --> 01:38:15,238
it could have been TSMC,

2493
01:38:15,238 --> 01:38:17,177
but Texas Semiconductor Manufacturing,

2494
01:38:17,177 --> 01:38:20,329
instead of Texas Instruments. (Nathan laughing)

2495
01:38:20,329 --> 01:38:21,490
So, there is that whole story there,

2496
01:38:21,490 --> 01:38:23,455
but the race- - Sitting here in Texas.

2497
01:38:23,455 --> 01:38:25,377
- And that sounds like a human story,

2498
01:38:25,377 --> 01:38:26,738
like it didn't get promoted?

2499
01:38:26,738 --> 01:38:28,238
- Just the brilliance of Morris Chang,

2500
01:38:28,238 --> 01:38:29,730
which I wouldn't underplay,

2501
01:38:29,730 --> 01:38:33,757
but there's also a different level of how this works.

2502
01:38:33,757 --> 01:38:35,924
So, in Taiwan, the number,

2503
01:38:38,800 --> 01:38:41,250
top percent of graduates of students

2504
01:38:41,250 --> 01:38:43,291
that go to the best school, which is NTU,

2505
01:38:43,291 --> 01:38:45,674
the top percent of those all go work to TSMC.

2506
01:38:45,674 --> 01:38:47,358
And guess what their pay is,

2507
01:38:47,358 --> 01:38:50,989
their starting pay is like $80,000, $70,000.

2508
01:38:50,989 --> 01:38:52,921
Which is like, that's like starting pay

2509
01:38:52,921 --> 01:38:54,651
for a good graduate in the US.

2510
01:38:54,651 --> 01:38:56,950
Not the top, the top graduates are making hundreds

2511
01:38:56,950 --> 01:39:00,311
of thousands of dollars at the Googles and the Amazons,

2512
01:39:00,311 --> 01:39:02,235
and now I guess the OpenAIs of the world.

2513
01:39:02,235 --> 01:39:05,149
So, there is a large dichotomy of like,

2514
01:39:05,149 --> 01:39:07,378
what is the top 1% of the society doing

2515
01:39:07,378 --> 01:39:09,687
and where are they headed because of economic reasons.

2516
01:39:09,687 --> 01:39:11,755
Intel never paid that crazy good.

2517
01:39:11,755 --> 01:39:13,227
And it didn't make sense to them.

2518
01:39:13,227 --> 01:39:16,051
That's one aspect, where's the best going?

2519
01:39:16,051 --> 01:39:18,350
Second is the work ethic.

2520
01:39:18,350 --> 01:39:21,358
We like to work, you work a lot, we work a lot,

2521
01:39:21,358 --> 01:39:23,525
but at the end of the day,

2522
01:39:25,696 --> 01:39:27,487
what is the time and amount of work that you're doing

2523
01:39:27,487 --> 01:39:28,966
and what does a fab require?

2524
01:39:28,966 --> 01:39:30,565
Fabs are not work from home jobs.

2525
01:39:30,565 --> 01:39:33,585
They are, you go into the fab and grueling work.

2526
01:39:33,585 --> 01:39:38,272
There's, hey, if there is any amount of vibration.

2527
01:39:38,272 --> 01:39:41,697
An earthquake happens, vibrates the machines,

2528
01:39:41,697 --> 01:39:42,997
they're either broken,

2529
01:39:42,997 --> 01:39:44,567
you've scrapped some of your production.

2530
01:39:44,567 --> 01:39:47,065
And then, in many cases, they're like calibrated properly.

2531
01:39:47,065 --> 01:39:49,705
So, when TSMC, when there's an earthquake,

2532
01:39:49,705 --> 01:39:51,144
recently there's been an earthquake,

2533
01:39:51,144 --> 01:39:53,266
TSMC doesn't call their employees,

2534
01:39:53,266 --> 01:39:56,947
they just go to the fab and they just show up.

2535
01:39:56,947 --> 01:39:58,235
The parking lot gets slammed

2536
01:39:58,235 --> 01:40:01,145
and people just go into the fab and fix it.

2537
01:40:01,145 --> 01:40:04,304
It's like ants. It's like a hive of ants.

2538
01:40:04,304 --> 01:40:06,383
It doesn't get told by the queen what to do.

2539
01:40:06,383 --> 01:40:08,217
The ants just know.

2540
01:40:08,217 --> 01:40:11,378
- It's like one person just specializes on these one task,

2541
01:40:11,378 --> 01:40:12,977
and it's like, you're gonna take this one tool

2542
01:40:12,977 --> 01:40:14,425
and you're the best person in the world,

2543
01:40:14,425 --> 01:40:15,328
and this is what you're gonna do

2544
01:40:15,328 --> 01:40:17,447
for your whole life is this one task and the fab.

2545
01:40:17,447 --> 01:40:19,017
- Which is like some special chemistry

2546
01:40:19,017 --> 01:40:21,833
plus nano manufacturing on one line of tools

2547
01:40:21,833 --> 01:40:23,197
that continues to get iterated.

2548
01:40:23,197 --> 01:40:26,378
And yeah, it's like specific plasma etch

2549
01:40:26,378 --> 01:40:27,904
for removing silicon dioxide.

2550
01:40:27,904 --> 01:40:29,626
That's all you focus on your whole career,

2551
01:40:29,626 --> 01:40:31,417
and it's like such a specialized thing.

2552
01:40:31,417 --> 01:40:34,006
And so, it's not like the tasks are transferable.

2553
01:40:34,006 --> 01:40:35,257
AI today is awesome,

2554
01:40:35,257 --> 01:40:37,266
because people can pick it up like that.

2555
01:40:37,266 --> 01:40:38,544
Semiconductor manufacturing

2556
01:40:38,544 --> 01:40:40,797
is very antiquated and difficult.

2557
01:40:40,797 --> 01:40:41,630
None of the materials

2558
01:40:41,630 --> 01:40:44,797
are online for people to read easily and learn.

2559
01:40:44,797 --> 01:40:46,035
The papers are very dense

2560
01:40:46,035 --> 01:40:49,276
and it takes a lot of experience to learn.

2561
01:40:49,276 --> 01:40:51,897
And so, it makes the barrier to entry much higher too.

2562
01:40:51,897 --> 01:40:53,847
So, when you talk about, hey,

2563
01:40:53,847 --> 01:40:55,856
you have all these people that are super specialized,

2564
01:40:55,856 --> 01:41:00,856
they will work 80 hours a week in a factory, in a fab.

2565
01:41:01,106 --> 01:41:02,766
And if anything goes wrong,

2566
01:41:02,766 --> 01:41:04,337
they'll go show up in the middle of the night

2567
01:41:04,337 --> 01:41:05,170
because some earthquake,

2568
01:41:05,170 --> 01:41:06,821
their wife is like, "There's an earthquake."

2569
01:41:06,821 --> 01:41:07,768
He is like, "Great,

2570
01:41:07,768 --> 01:41:09,062
I'm gonna go to the fab." (Nathan laughing)

2571
01:41:09,062 --> 01:41:09,895
- [Nathan] It's like a crime.

2572
01:41:09,895 --> 01:41:11,912
- Would you as an American, do that?

2573
01:41:11,912 --> 01:41:13,700
It's like these sorts of things are like,

2574
01:41:13,700 --> 01:41:15,940
what, I guess are the exemplifying,

2575
01:41:15,940 --> 01:41:17,604
like why TSMC is so amazing.

2576
01:41:17,604 --> 01:41:19,702
Now, can you replicate it in the US?

2577
01:41:19,702 --> 01:41:22,683
Let's not ignore Intel was the leader

2578
01:41:22,683 --> 01:41:24,300
in manufacturing for over 20 years.

2579
01:41:24,300 --> 01:41:28,221
They brought every technology to market first besides EUV,

2580
01:41:28,221 --> 01:41:31,379
strained silicon, High-K/Metal Gates, FinFET,

2581
01:41:31,379 --> 01:41:33,141
and the list just goes on and on and on

2582
01:41:33,141 --> 01:41:35,390
of technologies that Intel brought to market first,

2583
01:41:35,390 --> 01:41:37,671
made the most money from,

2584
01:41:37,671 --> 01:41:40,664
and manufactured at scale first,

2585
01:41:40,664 --> 01:41:42,510
best, highest profit margins.

2586
01:41:42,510 --> 01:41:45,100
So, we shouldn't ignore that Intel can't do this.

2587
01:41:45,100 --> 01:41:48,202
It's that the culture has broken.

2588
01:41:48,202 --> 01:41:50,117
You've invested in the wrong things.

2589
01:41:50,117 --> 01:41:51,386
They said no to the iPhone.

2590
01:41:51,386 --> 01:41:53,238
They had all these different things

2591
01:41:53,238 --> 01:41:55,807
regarding mismanagement of the fabs,

2592
01:41:55,807 --> 01:41:58,454
mismanagement of designs, this lockup.

2593
01:41:58,454 --> 01:41:59,776
And at the same time,

2594
01:41:59,776 --> 01:42:03,627
all these brilliant people, these 50,000 PhDs,

2595
01:42:03,627 --> 01:42:06,229
or masters that have been working on specific chemical,

2596
01:42:06,229 --> 01:42:08,078
or physical processes,

2597
01:42:08,078 --> 01:42:10,459
or nano manufacturing processes for decades,

2598
01:42:10,459 --> 01:42:11,543
in Oregon, they're still there.

2599
01:42:11,543 --> 01:42:12,998
They're still producing amazing work.

2600
01:42:12,998 --> 01:42:14,747
It's just like getting it to the last mile

2601
01:42:14,747 --> 01:42:15,941
of production at high-yield,

2602
01:42:15,941 --> 01:42:18,669
where you can manufacture dozens

2603
01:42:18,669 --> 01:42:22,599
and hundreds of different kinds of chips, and it's good.

2604
01:42:22,599 --> 01:42:24,100
Customer experience has broken.

2605
01:42:24,100 --> 01:42:26,211
It's that customer experience.

2606
01:42:26,211 --> 01:42:27,301
Part of it is like,

2607
01:42:27,301 --> 01:42:30,471
people will say Intel was too pompous in the 2000s, 2010s.

2608
01:42:30,471 --> 01:42:31,851
They just thought they were better than everyone.

2609
01:42:31,851 --> 01:42:33,089
The tool guys were like, oh,

2610
01:42:33,089 --> 01:42:34,675
I don't think that this is mature enough.

2611
01:42:34,675 --> 01:42:36,136
And they're like, ah, you just don't know what we know.

2612
01:42:36,136 --> 01:42:38,104
This sort of stuff would happen.

2613
01:42:38,104 --> 01:42:39,604
And so, can the US

2614
01:42:42,178 --> 01:42:44,431
bring leading edge semiconductor manufacturing to the US?

2615
01:42:44,431 --> 01:42:45,741
And thematically, yes.

2616
01:42:45,741 --> 01:42:46,874
And we are. TS- - It's happening.

2617
01:42:46,874 --> 01:42:50,086
Arizona is getting better and better as time goes on.

2618
01:42:50,086 --> 01:42:54,189
- TSMC has built roughly 20% of their capacity

2619
01:42:54,189 --> 01:42:56,651
for five nanometer in the US.

2620
01:42:56,651 --> 01:42:59,934
Now, this is nowhere near enough.

2621
01:42:59,934 --> 01:43:02,871
20% of capacity in the US is like nothing.

2622
01:43:02,871 --> 01:43:05,944
And furthermore, this is still dependent on Taiwan existing.

2623
01:43:05,944 --> 01:43:08,432
There's important way to separate it out.

2624
01:43:08,432 --> 01:43:12,640
There's R&D and there's high volume manufacturing.

2625
01:43:12,640 --> 01:43:14,597
Effectively, there are three places in the world

2626
01:43:14,597 --> 01:43:17,296
that are doing leading edge R&D.

2627
01:43:17,296 --> 01:43:20,544
There's Hsinchu, Taiwan, there's Hillsboro, Oregon,

2628
01:43:20,544 --> 01:43:24,134
and there is Pyongyang, South Korea.

2629
01:43:24,134 --> 01:43:25,083
These three places

2630
01:43:25,083 --> 01:43:27,395
are doing the leading edge R&D for the rest

2631
01:43:27,395 --> 01:43:30,424
of the world's leading edge semiconductors.

2632
01:43:30,424 --> 01:43:35,056
Now, manufacturing can be distributed more globally.

2633
01:43:35,056 --> 01:43:37,907
And this is where this dichotomy exists

2634
01:43:37,907 --> 01:43:40,197
of who's actually modifying the process,

2635
01:43:40,197 --> 01:43:42,687
who's actually developing the next generation one,

2636
01:43:42,687 --> 01:43:43,744
who's improving them?

2637
01:43:43,744 --> 01:43:46,997
Is Hsinchu, is Hillsboro, is Pyongyang.

2638
01:43:46,997 --> 01:43:50,506
It is not the rest of these fabs like Arizona.

2639
01:43:50,506 --> 01:43:52,184
Arizona is a paperweight.

2640
01:43:52,184 --> 01:43:55,064
If Hsinchu disappeared off the face of the planet,

2641
01:43:55,064 --> 01:43:57,939
within a year, couple years,

2642
01:43:57,939 --> 01:43:59,742
Arizona would stop producing too.

2643
01:43:59,742 --> 01:44:01,579
It's actually like pretty critical.

2644
01:44:01,579 --> 01:44:04,642
One of the things I like to say is if I had a few missiles,

2645
01:44:04,642 --> 01:44:07,717
I know exactly where I could cause the most economic damage.

2646
01:44:07,717 --> 01:44:08,787
It's not targeting the White House.

2647
01:44:08,787 --> 01:44:10,478
It's not- - It's the R&D centers.

2648
01:44:10,478 --> 01:44:13,520
- It's the R&D centers for TSMC, Intel, Samsung,

2649
01:44:13,520 --> 01:44:15,238
and then some of the memory guys, Micron and Hynix.

2650
01:44:15,238 --> 01:44:16,707
- Because they define the future evolution

2651
01:44:16,707 --> 01:44:19,258
of these semiconductors and everything's moving so rapidly

2652
01:44:19,258 --> 01:44:22,758
that it really is fundamentally about R&D.

2653
01:44:24,251 --> 01:44:26,751
And it is all about TSMC, huh?

2654
01:44:27,696 --> 01:44:29,185
- And so, TSMC,

2655
01:44:29,185 --> 01:44:32,824
you cannot purchase a vehicle without TSMC chips.

2656
01:44:32,824 --> 01:44:37,444
You cannot purchase a fridge without TSMC chips.

2657
01:44:37,444 --> 01:44:40,158
I think one of the few things you can purchase ironically

2658
01:44:40,158 --> 01:44:43,310
is a Texas Instruments graphing calculator,

2659
01:44:43,310 --> 01:44:44,997
because they actually manufacture in Texas.

2660
01:44:44,997 --> 01:44:47,327
But outside of that, like a laptop,

2661
01:44:47,327 --> 01:44:48,772
- It's depressing. - a phone, anything you,

2662
01:44:48,772 --> 01:44:51,037
servers, GPUs, none of this stuff can exist.

2663
01:44:51,037 --> 01:44:53,214
And this is without TSMC.

2664
01:44:53,214 --> 01:44:55,109
And in many cases, it's not even like the leading edge,

2665
01:44:55,109 --> 01:44:56,566
sexy five nanometer chip,

2666
01:44:56,566 --> 01:44:58,147
three nanometer chip, two nanometer chip.

2667
01:44:58,147 --> 01:45:00,478
Oftentimes, it's just some stupid power IC

2668
01:45:00,478 --> 01:45:03,648
that's converting from some voltage to another,

2669
01:45:03,648 --> 01:45:04,891
and it's made at TSMC.

2670
01:45:04,891 --> 01:45:06,777
- This is what China is investing in as well.

2671
01:45:06,777 --> 01:45:08,925
It's like they can build out this long tail fab

2672
01:45:08,925 --> 01:45:10,547
where the techniques are much more known.

2673
01:45:10,547 --> 01:45:12,799
You don't have to figure out these problems with EUV.

2674
01:45:12,799 --> 01:45:14,010
They're investing in this,

2675
01:45:14,010 --> 01:45:16,527
and then they have large supply

2676
01:45:16,527 --> 01:45:20,349
for things like the car door handles and the random stuff.

2677
01:45:20,349 --> 01:45:22,453
And that trickles down

2678
01:45:22,453 --> 01:45:24,294
into this whole economic discussion as well,

2679
01:45:24,294 --> 01:45:26,563
which is they have far more than we do.

2680
01:45:26,563 --> 01:45:27,396
And having supply

2681
01:45:27,396 --> 01:45:29,208
for things like this is crucial to normal life.

2682
01:45:29,208 --> 01:45:31,478
- So, they're starting to invest

2683
01:45:31,478 --> 01:45:34,908
in high volume manufacturer, but they're not doing R&D.

2684
01:45:34,908 --> 01:45:36,768
- So, they do R&D on their own.

2685
01:45:36,768 --> 01:45:38,136
They're just way behind. - Yeah.

2686
01:45:38,136 --> 01:45:42,309
- So, I would say like in 2015, China had a five-year plan,

2687
01:45:42,309 --> 01:45:47,098
where they defined by 2025 and 2020 certain goals,

2688
01:45:47,098 --> 01:45:50,137
including 80% domestic production of semiconductors.

2689
01:45:50,137 --> 01:45:52,233
they're not gonna hit that right, to be clear.

2690
01:45:52,233 --> 01:45:55,307
But they are in certain areas, really, really close.

2691
01:45:55,307 --> 01:45:58,627
Like BYD is probably gonna be the first company

2692
01:45:58,627 --> 01:46:01,799
in the world to not have to use TSMC for making...

2693
01:46:01,799 --> 01:46:04,483
'Cause they have their own fabs for making chips.

2694
01:46:04,483 --> 01:46:06,804
Now, they still have to buy some chips from foreign,

2695
01:46:06,804 --> 01:46:10,876
for example, like around self-driving ADAS capabilities.

2696
01:46:10,876 --> 01:46:11,993
'Cause those are really high-end,

2697
01:46:11,993 --> 01:46:14,950
but at least like a internal combustion engine

2698
01:46:14,950 --> 01:46:16,274
has 40 chips in an EV,

2699
01:46:16,274 --> 01:46:18,982
just for controlling flow rates and all these things,

2700
01:46:18,982 --> 01:46:20,213
and EVs are even more complicated.

2701
01:46:20,213 --> 01:46:21,844
So, all these different power ICs

2702
01:46:21,844 --> 01:46:23,794
and battery management controllers,

2703
01:46:23,794 --> 01:46:26,564
and all these things, they're insourcing.

2704
01:46:26,564 --> 01:46:31,086
And this is something that China has been doing since 2015.

2705
01:46:31,086 --> 01:46:32,944
Now, as far as the trailing edge,

2706
01:46:32,944 --> 01:46:34,806
they're getting so much capacity there.

2707
01:46:34,806 --> 01:46:36,205
As far as the leading edge,

2708
01:46:36,205 --> 01:46:38,911
i.e, this five nanometer and so on, so forth.

2709
01:46:38,911 --> 01:46:40,764
Where GPUs, they are still behind.

2710
01:46:40,764 --> 01:46:42,405
And this is the US restrictions

2711
01:46:42,405 --> 01:46:45,337
are trying to stop them in the latter.

2712
01:46:45,337 --> 01:46:47,517
But all that's happened, is yes,

2713
01:46:47,517 --> 01:46:49,136
they've slowed down their five nanometer,

2714
01:46:49,136 --> 01:46:50,276
three nanometer, et cetera,

2715
01:46:50,276 --> 01:46:52,166
but they've accelerated their, hey,

2716
01:46:52,166 --> 01:46:56,038
45 nanometer, 90 nanometer power IC or analog IC,

2717
01:46:56,038 --> 01:46:59,798
or random chip in my keyboard, that kind of stuff.

2718
01:46:59,798 --> 01:47:02,940
So, there is an angle of the US' actions

2719
01:47:02,940 --> 01:47:04,608
have been so, from these export,

2720
01:47:04,608 --> 01:47:06,308
from the angle of the export controls,

2721
01:47:06,308 --> 01:47:08,877
have been so inflammatory

2722
01:47:08,877 --> 01:47:11,569
at slowing down China's progress on the leading edge

2723
01:47:11,569 --> 01:47:12,676
that they've turned around

2724
01:47:12,676 --> 01:47:14,735
and have accelerated their progress elsewhere,

2725
01:47:14,735 --> 01:47:16,837
because they know this is so important.

2726
01:47:16,837 --> 01:47:18,149
If the US is gonna lock them out here

2727
01:47:18,149 --> 01:47:20,858
or if they lock us out here as well in the trailing edge.

2728
01:47:20,858 --> 01:47:24,842
And so, going back, can the US build it here?

2729
01:47:24,842 --> 01:47:26,655
Yes, but it's gonna take a ton of money.

2730
01:47:26,655 --> 01:47:29,282
I truly think like, to revolutionize

2731
01:47:29,282 --> 01:47:31,091
and completely insource semiconductors,

2732
01:47:31,091 --> 01:47:33,212
would take a decade and a trillion dollars.

2733
01:47:33,212 --> 01:47:34,762
- Is some of it also culture?

2734
01:47:34,762 --> 01:47:36,642
Like you said, extreme competence,

2735
01:47:36,642 --> 01:47:39,177
extreme work ethic in Taiwan?

2736
01:47:39,177 --> 01:47:40,152
- I think if you have the demand

2737
01:47:40,152 --> 01:47:41,644
and the money is on the line,

2738
01:47:41,644 --> 01:47:43,301
the American companies figure it out.

2739
01:47:43,301 --> 01:47:45,284
It's gonna take handholding with the government.

2740
01:47:45,284 --> 01:47:48,690
But I think that the culture helps TSMC breakthrough

2741
01:47:48,690 --> 01:47:50,050
and it's easier for them.

2742
01:47:50,050 --> 01:47:50,883
You could-

2743
01:47:50,883 --> 01:47:52,439
- TSMC has some like 90,000 employees.

2744
01:47:52,439 --> 01:47:54,758
It's not actually that insane amount.

2745
01:47:54,758 --> 01:47:58,069
The Arizona fab has 3,000 from Taiwan,

2746
01:47:58,069 --> 01:48:00,307
and these people, their wives were like, yeah,

2747
01:48:00,307 --> 01:48:01,140
we're not gonna have kids

2748
01:48:01,140 --> 01:48:03,370
unless we use sign up for the Arizona fab.

2749
01:48:03,370 --> 01:48:04,989
We go to Arizona and we have our kids there.

2750
01:48:04,989 --> 01:48:06,978
There's also a Japan fab where the same thing happened.

2751
01:48:06,978 --> 01:48:09,101
And so, these wives drove like,

2752
01:48:09,101 --> 01:48:11,043
these dudes to go to Japan

2753
01:48:11,043 --> 01:48:13,092
or America to have the kids there.

2754
01:48:13,092 --> 01:48:15,176
And it's like it's an element of culture. Yeah, sure.

2755
01:48:15,176 --> 01:48:16,737
Taiwan works that hard,

2756
01:48:16,737 --> 01:48:18,999
but also the US has done it in the past,

2757
01:48:18,999 --> 01:48:20,816
they could do it now.

2758
01:48:20,816 --> 01:48:23,788
We can just import, I say import,

2759
01:48:23,788 --> 01:48:25,738
the best people in the world if we want to.

2760
01:48:25,738 --> 01:48:28,579
- That's where the immigration conversation is a tricky one.

2761
01:48:28,579 --> 01:48:30,016
And there's been a lot of debate over that.

2762
01:48:30,016 --> 01:48:32,942
But yeah, it seems absurdly controversial

2763
01:48:32,942 --> 01:48:34,412
to import the best people in the world.

2764
01:48:34,412 --> 01:48:36,313
I don't understand why it's controversial.

2765
01:48:36,313 --> 01:48:37,824
That's the one of the ways

2766
01:48:37,824 --> 01:48:39,212
of winning. - I'm sure we agree with you.

2767
01:48:39,212 --> 01:48:40,323
(Lex laughing) - And even if

2768
01:48:40,323 --> 01:48:41,954
you can't import those people,

2769
01:48:41,954 --> 01:48:43,163
I still think you could do a lot

2770
01:48:43,163 --> 01:48:46,062
to manufacture most of in the US if the money's there.

2771
01:48:46,062 --> 01:48:46,895
And so, like-

2772
01:48:46,895 --> 01:48:47,979
- It's just way more expensive.

2773
01:48:47,979 --> 01:48:49,724
It's not profitable for a long time.

2774
01:48:49,724 --> 01:48:51,817
- And that's the context of like the CHIPS Act

2775
01:48:51,817 --> 01:48:53,350
is only like $50 billion

2776
01:48:53,350 --> 01:48:57,416
relative to some of the renewable initiatives

2777
01:48:57,416 --> 01:48:59,274
that were passed in the Inflation Reduction Act

2778
01:48:59,274 --> 01:49:00,301
and the Infrastructure Act,

2779
01:49:00,301 --> 01:49:02,878
which total in the hundreds of billions of dollars.

2780
01:49:02,878 --> 01:49:04,609
And so, the amount of money that the US

2781
01:49:04,609 --> 01:49:08,450
is spending on the semiconductor industry is nothing.

2782
01:49:08,450 --> 01:49:09,813
Whereas all these other countries

2783
01:49:09,813 --> 01:49:13,009
have structural advantages in terms of work ethic

2784
01:49:13,009 --> 01:49:14,866
and amount of work, and like things like that.

2785
01:49:14,866 --> 01:49:16,230
But also a number of STEM graduates,

2786
01:49:16,230 --> 01:49:19,426
the percentile of their best going to that.

2787
01:49:19,426 --> 01:49:22,787
But they also have differences in terms of like,

2788
01:49:22,787 --> 01:49:25,579
hey, there's just tax benefits in the law

2789
01:49:25,579 --> 01:49:28,319
and have been in the law for 20 years.

2790
01:49:28,319 --> 01:49:30,243
And then, some countries have massive subsidies.

2791
01:49:30,243 --> 01:49:33,584
China has something like $200 billion

2792
01:49:33,584 --> 01:49:35,442
of semiconductor subsidies a year.

2793
01:49:35,442 --> 01:49:38,992
We're talking about $50 billion in the US over like six.

2794
01:49:38,992 --> 01:49:41,144
So, the girth or difference

2795
01:49:41,144 --> 01:49:43,604
in the subsidy amounts is also huge.

2796
01:49:43,604 --> 01:49:44,854
And so I think,

2797
01:49:45,731 --> 01:49:48,991
Trump has been talking about tariffing Taiwan recently.

2798
01:49:48,991 --> 01:49:51,020
That's like one of these things

2799
01:49:51,020 --> 01:49:52,424
that's like, oh, okay, well,

2800
01:49:52,424 --> 01:49:54,032
maybe he doesn't wanna subsidize

2801
01:49:54,032 --> 01:49:56,147
the US semiconductor industry.

2802
01:49:56,147 --> 01:49:57,083
Obviously, tariffing Taiwan

2803
01:49:57,083 --> 01:50:00,321
is gonna cost a lot of things to get much more expensive.

2804
01:50:00,321 --> 01:50:01,369
But does it change the equation

2805
01:50:01,369 --> 01:50:03,125
for TSMC building more fabs in the US?

2806
01:50:03,125 --> 01:50:05,575
That's what he is positing.

2807
01:50:05,575 --> 01:50:08,910
- So, can you lay out the, so we laid out the importance,

2808
01:50:08,910 --> 01:50:13,031
by the way, it's incredible how much you know about so much.

2809
01:50:13,031 --> 01:50:14,970
- [Nathan] We told you Dylan knows all this stuff.

2810
01:50:14,970 --> 01:50:16,598
- Yeah. (Nathan laughing)

2811
01:50:16,598 --> 01:50:21,598
So, okay, you laid out why TSMC is really important.

2812
01:50:21,683 --> 01:50:25,695
If we look out into the future 10, 20 years out,

2813
01:50:25,695 --> 01:50:27,503
US-China relationship

2814
01:50:27,503 --> 01:50:31,586
seems like it can go to a dark place of cold war,

2815
01:50:34,680 --> 01:50:36,935
escalated cold war, even hot war,

2816
01:50:36,935 --> 01:50:40,117
or to a good place of anything

2817
01:50:40,117 --> 01:50:44,993
from frenemies to cooperation, to working together.

2818
01:50:44,993 --> 01:50:48,493
So, in this game theory, complicated game,

2819
01:50:50,647 --> 01:50:51,922
what are the different trajectories?

2820
01:50:51,922 --> 01:50:53,163
What should US be doing?

2821
01:50:53,163 --> 01:50:55,154
What do you see as the different possible trajectories

2822
01:50:55,154 --> 01:50:58,556
of US-China relations as both leaders

2823
01:50:58,556 --> 01:51:00,373
start to feel the AGI more and more,

2824
01:51:00,373 --> 01:51:04,726
and see the importance of chips and the importance of AI?

2825
01:51:04,726 --> 01:51:06,813
- Ultimately, the export controls

2826
01:51:06,813 --> 01:51:10,340
are pointing towards a separate future economy.

2827
01:51:10,340 --> 01:51:13,516
I think the US has made it clear to Chinese leaders

2828
01:51:13,516 --> 01:51:17,257
that we intend to control this technology

2829
01:51:17,257 --> 01:51:21,257
at whatever cost to global economic integration.

2830
01:51:23,097 --> 01:51:24,604
- So, that- - It's hard to unwind that.

2831
01:51:24,604 --> 01:51:25,437
(Nathan chuckles)

2832
01:51:25,437 --> 01:51:27,836
The card has been played. - To the same extent,

2833
01:51:27,836 --> 01:51:30,197
they've also limited US companies for mentoring China.

2834
01:51:30,197 --> 01:51:33,429
So, it's been a long time coming.

2835
01:51:33,429 --> 01:51:36,697
At some point, there was a convergence.

2836
01:51:36,697 --> 01:51:38,709
But over at least the last decade,

2837
01:51:38,709 --> 01:51:40,883
it's been branching further and further out.

2838
01:51:40,883 --> 01:51:42,742
Like US companies can't enter China,

2839
01:51:42,742 --> 01:51:44,867
Chinese companies can't enter the US.

2840
01:51:44,867 --> 01:51:47,157
The US is saying, hey, China,

2841
01:51:47,157 --> 01:51:50,471
you can't get access to our technologies in certain areas.

2842
01:51:50,471 --> 01:51:53,414
And China's rebutting with the same thing around like,

2843
01:51:53,414 --> 01:51:55,377
they've done some sort of specific materials

2844
01:51:55,377 --> 01:51:57,420
in gallium and things like that,

2845
01:51:57,420 --> 01:51:59,890
that they've tried to limit the US on.

2846
01:51:59,890 --> 01:52:01,146
There's a US drone company

2847
01:52:01,146 --> 01:52:02,287
that's not allowed to buy batteries

2848
01:52:02,287 --> 01:52:03,869
and they have military customers.

2849
01:52:03,869 --> 01:52:06,603
And this drone company just tells the military customers

2850
01:52:06,603 --> 01:52:08,307
like, hey, hey, just get it from Amazon,

2851
01:52:08,307 --> 01:52:10,057
'cause I can't actually physically get them.

2852
01:52:10,057 --> 01:52:11,816
There's all these things that are happening

2853
01:52:11,816 --> 01:52:13,814
that point to further and further divergence.

2854
01:52:13,814 --> 01:52:15,200
I have zero idea.

2855
01:52:15,200 --> 01:52:16,033
And I would love

2856
01:52:16,033 --> 01:52:18,526
if we could all hold hands and sing Kumbaya,

2857
01:52:18,526 --> 01:52:21,304
but I have zero idea how that could possibly happen.

2858
01:52:21,304 --> 01:52:26,076
- Is the divergence good or bad for avoiding war?

2859
01:52:26,076 --> 01:52:29,077
Is it possible that the divergence

2860
01:52:29,077 --> 01:52:30,726
in terms of manufacturer chips,

2861
01:52:30,726 --> 01:52:34,594
of training AI systems is actually good for avoiding

2862
01:52:34,594 --> 01:52:36,167
military conflict? - It's an objective fact

2863
01:52:36,167 --> 01:52:39,120
that the world has been the most peaceful

2864
01:52:39,120 --> 01:52:41,707
has ever been when there are global hegemons,

2865
01:52:41,707 --> 01:52:45,508
or regional hegemons in historical context.

2866
01:52:45,508 --> 01:52:47,550
The Mediterranean was the most peaceful ever

2867
01:52:47,550 --> 01:52:48,588
when the Romans were there.

2868
01:52:48,588 --> 01:52:50,814
China had very peaceful and warring times.

2869
01:52:50,814 --> 01:52:52,178
And the peaceful times were when dynasties

2870
01:52:52,178 --> 01:52:54,236
had a lock hold over, not just themselves,

2871
01:52:54,236 --> 01:52:55,820
but all their tributaries around them.

2872
01:52:55,820 --> 01:52:59,217
And likewise, the most peaceful time in human history

2873
01:52:59,217 --> 01:53:02,453
has been when the US was the global hegemon,

2874
01:53:02,453 --> 01:53:04,198
the last decades.

2875
01:53:04,198 --> 01:53:06,473
Now, we've seen things start to slide

2876
01:53:06,473 --> 01:53:07,553
with Russia-Ukraine,

2877
01:53:07,553 --> 01:53:09,164
with what's going on in the Middle East,

2878
01:53:09,164 --> 01:53:11,382
and Taiwan risk, all these different things

2879
01:53:11,382 --> 01:53:12,421
are starting to bubble up,

2880
01:53:12,421 --> 01:53:14,451
still objectively extremely peaceful.

2881
01:53:14,451 --> 01:53:16,818
Now, what happens when it's not one global hegemon,

2882
01:53:16,818 --> 01:53:17,952
but it's two?

2883
01:53:17,952 --> 01:53:21,642
Obviously, and China will be competitive

2884
01:53:21,642 --> 01:53:23,964
or even overtake the US, like it's possible.

2885
01:53:23,964 --> 01:53:27,457
And so, this change in global hegemony,

2886
01:53:27,457 --> 01:53:30,109
I don't think it ever happens like super peacefully.

2887
01:53:30,109 --> 01:53:30,954
When empires fall,

2888
01:53:30,954 --> 01:53:33,371
which is a possible trajectory for America,

2889
01:53:33,371 --> 01:53:35,872
they don't pull fall gracefully.

2890
01:53:35,872 --> 01:53:38,050
They don't just slide out of irrelevance.

2891
01:53:38,050 --> 01:53:40,549
Usually, there's a lot of shaking.

2892
01:53:40,549 --> 01:53:44,009
And so, what the US is trying to do

2893
01:53:44,009 --> 01:53:45,379
is maintain its top position.

2894
01:53:45,379 --> 01:53:47,547
And what China is trying to do is become the top position.

2895
01:53:47,547 --> 01:53:51,059
And obviously, there's butting of heads here,

2896
01:53:51,059 --> 01:53:52,836
in the most simple terms.

2897
01:53:52,836 --> 01:53:55,558
- And that could take shape in all kinds of ways,

2898
01:53:55,558 --> 01:53:57,790
including proxy wars.

2899
01:53:57,790 --> 01:53:58,623
And now- - Yeah, it seems

2900
01:53:58,623 --> 01:53:59,729
like it's already happening.

2901
01:53:59,729 --> 01:54:03,888
As much as I want there to be centuries of prolonged peace,

2902
01:54:03,888 --> 01:54:08,802
it looks like further instability internationally is ahead.

2903
01:54:08,802 --> 01:54:12,128
- And the US is sort of like current task is like,

2904
01:54:12,128 --> 01:54:15,556
hey, if we control AI, if we're the leader in AI,

2905
01:54:15,556 --> 01:54:18,710
and AI could significantly accelerates progress,

2906
01:54:18,710 --> 01:54:20,882
then we can maintain the global hegemony position.

2907
01:54:20,882 --> 01:54:21,715
And therefore-

2908
01:54:21,715 --> 01:54:22,650
- [Nathan] I hope that works.

2909
01:54:22,650 --> 01:54:24,482
- And as an American, (Nathan chuckles)

2910
01:54:24,482 --> 01:54:25,532
kind of like, okay,

2911
01:54:25,532 --> 01:54:27,970
I guess that's gonna lead to peace for us.

2912
01:54:27,970 --> 01:54:29,493
Now, obviously, other people around the world

2913
01:54:29,493 --> 01:54:31,493
get affected negatively.

2914
01:54:33,250 --> 01:54:34,756
Obviously, the Chinese people are not gonna be in

2915
01:54:34,756 --> 01:54:37,995
as advantageous of a position if that happens.

2916
01:54:37,995 --> 01:54:42,274
But this is the reality of what's being done

2917
01:54:42,274 --> 01:54:43,635
and the actions that are being carried out.

2918
01:54:43,635 --> 01:54:46,842
- So, can we go back to the specific detail

2919
01:54:46,842 --> 01:54:47,954
of the different hardware?

2920
01:54:47,954 --> 01:54:51,289
There's this nice graphic in the export controls

2921
01:54:51,289 --> 01:54:56,206
of which GPUs are allowed to be exported and which are not.

2922
01:54:59,085 --> 01:55:00,565
Can you explain the difference?

2923
01:55:00,565 --> 01:55:03,758
Is there, from a technical perspective,

2924
01:55:03,758 --> 01:55:05,675
are the H20s promising?

2925
01:55:08,605 --> 01:55:10,706
- Yeah, so this goes, and I think we'd have to,

2926
01:55:10,706 --> 01:55:13,346
we need to dive really deep into the reasoning aspect

2927
01:55:13,346 --> 01:55:14,985
and what's going on there.

2928
01:55:14,985 --> 01:55:17,394
But the H20, the US has gone

2929
01:55:17,394 --> 01:55:20,148
through multiple iterations of the export controls.

2930
01:55:20,148 --> 01:55:23,736
This H800 was at one point allowed back in '23,

2931
01:55:23,736 --> 01:55:24,638
but then it got canceled.

2932
01:55:24,638 --> 01:55:27,843
And by then, DeepSeek had already built their cluster of,

2933
01:55:27,843 --> 01:55:30,197
they claim 2k, I think they actually have like many more,

2934
01:55:30,197 --> 01:55:31,753
something like 10k of those.

2935
01:55:31,753 --> 01:55:34,086
And now, this H20 is the legally allowed chip.

2936
01:55:34,086 --> 01:55:36,748
Nvidia shipped a million of these last year to China.

2937
01:55:36,748 --> 01:55:39,708
For context, it was like 4 or 5 million GPUs.

2938
01:55:39,708 --> 01:55:41,331
So, the percentage of GPUs

2939
01:55:41,331 --> 01:55:46,057
that were this China specific H20 is quite high,

2940
01:55:46,057 --> 01:55:48,877
roughly 20%, 25%, 20% or so.

2941
01:55:48,877 --> 01:55:52,857
And so, this H20 has been neutered in one way,

2942
01:55:52,857 --> 01:55:54,906
but it's actually upgraded in other ways.

2943
01:55:54,906 --> 01:55:59,500
And you could think of chips along three axes for AI,

2944
01:55:59,500 --> 01:56:01,610
ignoring software stack

2945
01:56:01,610 --> 01:56:04,022
and exact architecture, just raw specifications.

2946
01:56:04,022 --> 01:56:06,661
There's floating point operations, FLOPS.

2947
01:56:06,661 --> 01:56:09,130
There is memory bandwidth, i.e,

2948
01:56:09,130 --> 01:56:11,752
and memory capacity, I/O, memory.

2949
01:56:11,752 --> 01:56:13,319
And then, there is interconnect,

2950
01:56:13,319 --> 01:56:14,831
chip to chip interconnections.

2951
01:56:14,831 --> 01:56:15,664
All three of these

2952
01:56:15,664 --> 01:56:19,581
are incredibly important for making AI systems.

2953
01:56:21,101 --> 01:56:23,170
Because AI systems involve a lot of compute,

2954
01:56:23,170 --> 01:56:25,216
they involve a lot of moving memory around,

2955
01:56:25,216 --> 01:56:27,962
whether it be two memory or two other chips.

2956
01:56:27,962 --> 01:56:29,852
And so, these three vectors.

2957
01:56:29,852 --> 01:56:33,581
The US initially had two of these vectors controlled

2958
01:56:33,581 --> 01:56:35,671
and one of them not controlled, which was FLOPS

2959
01:56:35,671 --> 01:56:37,764
and interconnect bandwidth were initially controlled.

2960
01:56:37,764 --> 01:56:39,282
And then, they said, no, no, no, no,

2961
01:56:39,282 --> 01:56:40,614
we're gonna remove the interconnect bandwidth

2962
01:56:40,614 --> 01:56:42,720
and just make it a very simple only FLOPS.

2963
01:56:42,720 --> 01:56:45,933
But now, Nvidia can now make a chip that has, okay,

2964
01:56:45,933 --> 01:56:47,917
it's cut down on FLOPS,

2965
01:56:47,917 --> 01:56:50,750
so like one third that of the H100

2966
01:56:51,818 --> 01:56:55,207
on spec sheet paper performance for FLOPS.

2967
01:56:55,207 --> 01:56:56,842
In real world, it's closer to half,

2968
01:56:56,842 --> 01:56:59,350
or maybe even like 60% of it.

2969
01:56:59,350 --> 01:57:00,972
But then, on the other two vectors,

2970
01:57:00,972 --> 01:57:03,390
it's just as good for interconnect bandwidth.

2971
01:57:03,390 --> 01:57:05,370
And then, for memory bandwidth and memory capacity,

2972
01:57:05,370 --> 01:57:07,320
the H20 has more memory bandwidth

2973
01:57:07,320 --> 01:57:10,652
and more memory capacity than the H100.

2974
01:57:10,652 --> 01:57:13,838
Now, recently, we, at our research,

2975
01:57:13,838 --> 01:57:15,940
we cut Nvidia's production for H20

2976
01:57:15,940 --> 01:57:18,175
for this year down drastically.

2977
01:57:18,175 --> 01:57:20,460
They were gonna make another 2 million of those this year,

2978
01:57:20,460 --> 01:57:23,722
but they just canceled all the orders a couple weeks ago.

2979
01:57:23,722 --> 01:57:25,080
In our view, that's because we think

2980
01:57:25,080 --> 01:57:27,251
that they think they're gonna get restricted.

2981
01:57:27,251 --> 01:57:30,787
Because why would they cancel all these orders for H20?

2982
01:57:30,787 --> 01:57:32,345
Because they shipped a million of 'em last year.

2983
01:57:32,345 --> 01:57:34,447
They had orders in for a couple million this year,

2984
01:57:34,447 --> 01:57:35,386
and just gone.

2985
01:57:35,386 --> 01:57:39,207
For H20, B20, a successor to H20, and now they're all gone.

2986
01:57:39,207 --> 01:57:41,114
Now, why would they do this?

2987
01:57:41,114 --> 01:57:42,757
I think it's very clear.

2988
01:57:42,757 --> 01:57:46,477
The H20 is actually better for certain tasks

2989
01:57:46,477 --> 01:57:49,879
and that certain task is reasoning.

2990
01:57:49,879 --> 01:57:53,399
Reasoning is incredibly different than...

2991
01:57:53,399 --> 01:57:56,001
When you look at the different regimes of models,

2992
01:57:56,001 --> 01:57:59,417
pre-training is all about FLOPS.

2993
01:57:59,417 --> 01:58:00,250
It's all about FLOPS.

2994
01:58:00,250 --> 01:58:02,212
There's things you do like mixture of experts

2995
01:58:02,212 --> 01:58:04,287
that we talked about, to trade off interconnect,

2996
01:58:04,287 --> 01:58:07,297
or to trade off other aspects and lower the FLOPS,

2997
01:58:07,297 --> 01:58:10,249
and rely more on interconnect and memory.

2998
01:58:10,249 --> 01:58:12,716
But at the end of the day, it's FLOPS is everything.

2999
01:58:12,716 --> 01:58:16,576
We talk about models in terms of how many FLOPS there are.

3000
01:58:16,576 --> 01:58:19,659
So, we talk about, oh, GPT-4 is 2e25.

3001
01:58:20,527 --> 01:58:25,444
Two to the 25th, 25 zeros, FLOP, floating point operations.

3002
01:58:28,489 --> 01:58:29,905
- For training. - For training.

3003
01:58:29,905 --> 01:58:30,738
And we're talking

3004
01:58:30,738 --> 01:58:33,976
about the restrictions for the 2e24 or 25, whatever.

3005
01:58:33,976 --> 01:58:37,806
The US has an executive order that Trump recently unsigned,

3006
01:58:37,806 --> 01:58:40,898
which was, hey, 1e26, once you hit that number

3007
01:58:40,898 --> 01:58:43,174
of floating point operations, you must notify the government

3008
01:58:43,174 --> 01:58:45,478
and you must share your results with us.

3009
01:58:45,478 --> 01:58:46,898
There's a level of model

3010
01:58:46,898 --> 01:58:48,892
where the US government must be told.

3011
01:58:48,892 --> 01:58:49,989
And that's 1e26.

3012
01:58:49,989 --> 01:58:51,989
And so, as we move forward,

3013
01:58:51,989 --> 01:58:54,258
this is an incredibly important...

3014
01:58:54,258 --> 01:58:55,469
FLOP is the vector that the government

3015
01:58:55,469 --> 01:58:56,827
has cared about historically,

3016
01:58:56,827 --> 01:59:01,697
but the other two vectors are arguably just as important.

3017
01:59:01,697 --> 01:59:03,808
And especially when we come to this new paradigm,

3018
01:59:03,808 --> 01:59:05,847
which the world is only just learning about

3019
01:59:05,847 --> 01:59:07,796
over the last six months, reasoning.

3020
01:59:07,796 --> 01:59:10,527
- And do we understand firmly

3021
01:59:10,527 --> 01:59:13,978
which of the three dimensions is best for reasoning?

3022
01:59:13,978 --> 01:59:16,471
So, interconnect, the FLOPS don't matter as much.

3023
01:59:16,471 --> 01:59:17,304
Is it memory?

3024
01:59:17,304 --> 01:59:18,430
- Memory. Right.

3025
01:59:18,430 --> 01:59:19,263
- Yeah, so- - Context length.

3026
01:59:19,263 --> 01:59:20,677
We're gonna get into technical stuff

3027
01:59:20,677 --> 01:59:22,399
real fast, yeah. (chuckles) - I would just say

3028
01:59:22,399 --> 01:59:24,446
there's two articles in this one that I could show,

3029
01:59:24,446 --> 01:59:26,513
maybe graphics that might be interesting for you to pull up.

3030
01:59:26,513 --> 01:59:28,928
- For the listeners, we're looking at the section

3031
01:59:28,928 --> 01:59:32,214
of o1 inference architecture tokenomics.

3032
01:59:32,214 --> 01:59:33,047
- Hmm.

3033
01:59:33,047 --> 01:59:35,228
Do you wanna explain KV cache before we talk about this?

3034
01:59:35,228 --> 01:59:36,708
I think it's better to- - Okay, yeah.

3035
01:59:36,708 --> 01:59:39,626
We need to go through a lot of specific technical things

3036
01:59:39,626 --> 01:59:42,718
of transformers to make this easy for people.

3037
01:59:42,718 --> 01:59:44,729
- Because it's incredibly important

3038
01:59:44,729 --> 01:59:46,415
because this changes how models work.

3039
01:59:46,415 --> 01:59:51,375
But I think resetting. Why is memory so important?

3040
01:59:51,375 --> 01:59:53,781
It's because so far, we've talked about parameter counts.

3041
01:59:53,781 --> 01:59:55,639
And mixture of experts, you can change

3042
01:59:55,639 --> 01:59:57,700
how many active parameters versus total parameters

3043
01:59:57,700 --> 01:59:59,540
to embed more data but have less FLOPS.

3044
01:59:59,540 --> 02:00:03,090
But more important, another aspect of,

3045
02:00:03,090 --> 02:00:04,779
what's part of this humongous revolution

3046
02:00:04,779 --> 02:00:06,900
in the last handful of years is the transformer.

3047
02:00:06,900 --> 02:00:08,489
And the attention mechanism.

3048
02:00:08,489 --> 02:00:10,471
Attention mechanism is that the model

3049
02:00:10,471 --> 02:00:12,310
understands the relationships

3050
02:00:12,310 --> 02:00:14,262
between all the words in its context.

3051
02:00:14,262 --> 02:00:18,833
And that is separate from the parameters themselves.

3052
02:00:18,833 --> 02:00:22,020
And that is something that you must calculate.

3053
02:00:22,020 --> 02:00:25,071
How each token, each word in the context length

3054
02:00:25,071 --> 02:00:28,940
is relatively connected to each other.

3055
02:00:28,940 --> 02:00:30,186
And I think, Nathan,

3056
02:00:30,186 --> 02:00:31,019
- Yeah. It's- - you should explain

3057
02:00:31,019 --> 02:00:31,852
KV cache better.

3058
02:00:31,852 --> 02:00:32,685
- KV cache is one

3059
02:00:32,685 --> 02:00:33,556
of the optimizations- - Yeah.

3060
02:00:33,556 --> 02:00:37,165
So, the attention operator has three core things.

3061
02:00:37,165 --> 02:00:39,660
It's queries, keys, and values.

3062
02:00:39,660 --> 02:00:42,792
QKV is the thing that goes into this.

3063
02:00:42,792 --> 02:00:44,217
You'll look at the equation,

3064
02:00:44,217 --> 02:00:46,817
you see that these matrices are multiplied together.

3065
02:00:46,817 --> 02:00:48,504
These words, query, key, and value,

3066
02:00:48,504 --> 02:00:50,257
come from information retrieval backgrounds,

3067
02:00:50,257 --> 02:00:52,238
where the query is the thing

3068
02:00:52,238 --> 02:00:53,801
you're trying to get the values for

3069
02:00:53,801 --> 02:00:56,279
and you access the keys and the values is re-weighting.

3070
02:00:56,279 --> 02:00:58,773
My background's not information retrieval

3071
02:00:58,773 --> 02:00:59,606
and things like this.

3072
02:00:59,606 --> 02:01:01,560
It's just fun to have back links.

3073
02:01:01,560 --> 02:01:03,406
And what effectively happens

3074
02:01:03,406 --> 02:01:06,154
is that when you're doing these matrix multiplication,

3075
02:01:06,154 --> 02:01:07,074
you're having matrices

3076
02:01:07,074 --> 02:01:09,954
that are of the size of the context length.

3077
02:01:09,954 --> 02:01:12,040
So, the number of tokens that you put into the model.

3078
02:01:12,040 --> 02:01:14,554
And the KV cache is effectively

3079
02:01:14,554 --> 02:01:17,047
some form of compressed representation

3080
02:01:17,047 --> 02:01:19,207
of all the previous tokens in the model.

3081
02:01:19,207 --> 02:01:20,395
So, when you're doing this,

3082
02:01:20,395 --> 02:01:22,336
we talk about autoaggressive models.

3083
02:01:22,336 --> 02:01:24,318
You predict one token at a time.

3084
02:01:24,318 --> 02:01:26,437
You start with whatever your prompt was.

3085
02:01:26,437 --> 02:01:30,306
You ask a question like, who was the president in 1825?

3086
02:01:30,306 --> 02:01:32,764
The model then is gonna generate its first token.

3087
02:01:32,764 --> 02:01:33,722
For each of these tokens,

3088
02:01:33,722 --> 02:01:36,297
you're doing the same attention operator,

3089
02:01:36,297 --> 02:01:40,128
where you're multiplying these query key value matrices.

3090
02:01:40,128 --> 02:01:42,517
But the math is very nice,

3091
02:01:42,517 --> 02:01:44,676
so that when you're doing this repeatedly,

3092
02:01:44,676 --> 02:01:48,009
this KV cache, this key value operation,

3093
02:01:49,036 --> 02:01:51,338
you can keep appending the new values to it.

3094
02:01:51,338 --> 02:01:53,767
So, you keep track of what your previous values

3095
02:01:53,767 --> 02:01:56,858
you're inferring over in this autoaggressive chain.

3096
02:01:56,858 --> 02:01:58,968
You keep it in memory the whole time.

3097
02:01:58,968 --> 02:02:01,588
And this is a really crucial thing

3098
02:02:01,588 --> 02:02:04,606
to manage when serving inference at scale.

3099
02:02:04,606 --> 02:02:06,225
There are far bigger experts in this

3100
02:02:06,225 --> 02:02:09,858
and there are so many levels of detail that you can go into.

3101
02:02:09,858 --> 02:02:12,116
Essentially, one of the key,

3102
02:02:12,116 --> 02:02:15,240
quote, unquote, "drawbacks" of the attention operator

3103
02:02:15,240 --> 02:02:17,247
and the transformer is that there is a form

3104
02:02:17,247 --> 02:02:19,027
of quadratic memory cost

3105
02:02:19,027 --> 02:02:21,645
in proportion to the context length.

3106
02:02:21,645 --> 02:02:23,988
So, as you put in longer questions,

3107
02:02:23,988 --> 02:02:27,257
the memory used in order to make that computation

3108
02:02:27,257 --> 02:02:29,836
is going up in the form of a quadratic.

3109
02:02:29,836 --> 02:02:30,669
You'll hear about a lot

3110
02:02:30,669 --> 02:02:33,276
of other language model architectures

3111
02:02:33,276 --> 02:02:36,477
that are sub-quadratic or linear attention forms,

3112
02:02:36,477 --> 02:02:38,309
which is state-space models.

3113
02:02:38,309 --> 02:02:40,218
We don't need to go down all these now.

3114
02:02:40,218 --> 02:02:41,887
And then, there's innovations

3115
02:02:41,887 --> 02:02:44,697
on attention to make this memory usage

3116
02:02:44,697 --> 02:02:48,406
and the ability to attend over long contexts,

3117
02:02:48,406 --> 02:02:50,403
much more accurate and high performance.

3118
02:02:50,403 --> 02:02:52,389
- And those innovations are going to help you with,

3119
02:02:52,389 --> 02:02:54,168
your highly memory constrain-

3120
02:02:54,168 --> 02:02:55,927
- You help with memory constraint and performance.

3121
02:02:55,927 --> 02:02:59,200
So, if you put in a book into, I think Gemini is the model

3122
02:02:59,200 --> 02:03:01,320
that has the longest context length that people are using.

3123
02:03:01,320 --> 02:03:02,417
Gemini is known for 1 million

3124
02:03:02,417 --> 02:03:04,048
and now 2 million context length.

3125
02:03:04,048 --> 02:03:05,798
You put a whole book into Gemini

3126
02:03:05,798 --> 02:03:09,358
and sometimes it'll draw facts out of it.

3127
02:03:09,358 --> 02:03:12,401
It's not perfect. They're getting better.

3128
02:03:12,401 --> 02:03:13,234
So, there's two things.

3129
02:03:13,234 --> 02:03:15,716
It's like one, to be able to serve this on the memory level,

3130
02:03:15,716 --> 02:03:17,777
Google has magic with their TPU stack

3131
02:03:17,777 --> 02:03:19,241
where they can serve really long contexts.

3132
02:03:19,241 --> 02:03:21,901
And then, there's also many decisions along the way

3133
02:03:21,901 --> 02:03:24,272
to actually make long context performance work

3134
02:03:24,272 --> 02:03:25,323
that supplies the data.

3135
02:03:25,323 --> 02:03:28,890
There's subtle changes to these computations and attention.

3136
02:03:28,890 --> 02:03:30,869
And it changes the architecture.

3137
02:03:30,869 --> 02:03:35,499
But serving long context is extremely memory-constrained,

3138
02:03:35,499 --> 02:03:37,232
especially when you're making a lot of predictions.

3139
02:03:37,232 --> 02:03:40,047
I actually don't know why input

3140
02:03:40,047 --> 02:03:41,408
and output tokens are more expensive,

3141
02:03:41,408 --> 02:03:42,770
but I think essentially, output tokens,

3142
02:03:42,770 --> 02:03:44,261
you have to do more computation,

3143
02:03:44,261 --> 02:03:46,012
'cause you have to sample from the model.

3144
02:03:46,012 --> 02:03:46,845
- I can explain that.

3145
02:03:46,845 --> 02:03:49,831
So, today, if you use a model, like you look at an API,

3146
02:03:49,831 --> 02:03:54,641
OpenAI charges a certain price per million tokens.

3147
02:03:54,641 --> 02:03:57,842
And that price for input and output tokens is different.

3148
02:03:57,842 --> 02:04:01,000
And the reason is, is that there is,

3149
02:04:01,000 --> 02:04:04,089
when you're inputting a query into the model.

3150
02:04:04,089 --> 02:04:06,066
Let's say you have a book.

3151
02:04:06,066 --> 02:04:09,211
That book, you must now calculate the entire KV cache for,

3152
02:04:09,211 --> 02:04:10,291
this key value cache.

3153
02:04:10,291 --> 02:04:13,528
And so, when you do that, that is a parallel operation.

3154
02:04:13,528 --> 02:04:15,818
All of the tokens can be processed at one time.

3155
02:04:15,818 --> 02:04:16,651
And therefore,

3156
02:04:16,651 --> 02:04:19,137
you can dramatically reduce how much you're spending.

3157
02:04:19,137 --> 02:04:21,521
The FLOP requirements for generating a token

3158
02:04:21,521 --> 02:04:23,869
and an input token are identical.

3159
02:04:23,869 --> 02:04:25,097
If I input one token

3160
02:04:25,097 --> 02:04:27,604
or if I generate one token, it's completely identical.

3161
02:04:27,604 --> 02:04:28,588
I have to go through the model.

3162
02:04:28,588 --> 02:04:31,770
But the difference is that I can do that input,

3163
02:04:31,770 --> 02:04:32,603
i.e, the prefill,

3164
02:04:32,603 --> 02:04:37,318
i.e, the prompt simultaneously in a batch nature.

3165
02:04:37,318 --> 02:04:38,916
And therefore, it is all FLOP.

3166
02:04:38,916 --> 02:04:42,037
- I think the pricing model mostly they use is for input.

3167
02:04:42,037 --> 02:04:43,731
Tokens is about one fourth the price

3168
02:04:43,731 --> 02:04:44,819
of the output tokens. - Correct.

3169
02:04:44,819 --> 02:04:45,760
But then, output tokens,

3170
02:04:45,760 --> 02:04:47,410
the reason why it's so expensive

3171
02:04:47,410 --> 02:04:48,770
is because I can't do it in parallel.

3172
02:04:48,770 --> 02:04:49,690
It's autoaggressive.

3173
02:04:49,690 --> 02:04:51,058
Every time I generate a token,

3174
02:04:51,058 --> 02:04:53,718
I must not only take the entire,

3175
02:04:53,718 --> 02:04:56,072
I must not only read the whole entire model into memory

3176
02:04:56,072 --> 02:04:59,857
and activate it, go calculate it to generate the next token.

3177
02:04:59,857 --> 02:05:01,457
I also have to read the entire KV cache

3178
02:05:01,457 --> 02:05:02,658
and I generate a token,

3179
02:05:02,658 --> 02:05:05,151
and then I append that KV, that one token I generated,

3180
02:05:05,151 --> 02:05:07,389
and it's KV cache, and then I do it again.

3181
02:05:07,389 --> 02:05:10,921
And so, therefore, this is a non-parallel operation.

3182
02:05:10,921 --> 02:05:13,297
And this is one where you have to,

3183
02:05:13,297 --> 02:05:15,521
in the case of pre-fill or prompt,

3184
02:05:15,521 --> 02:05:17,067
you pull the whole model in,

3185
02:05:17,067 --> 02:05:19,849
and you calculate 20,000 tokens at once.

3186
02:05:19,849 --> 02:05:21,208
These 20,000- - So, these are features

3187
02:05:21,208 --> 02:05:22,151
that APIs are shipping,

3188
02:05:22,151 --> 02:05:25,904
which is like prompt caching, pre-filling.

3189
02:05:25,904 --> 02:05:27,178
'Cause you can drive prices down

3190
02:05:27,178 --> 02:05:28,633
and you can make APIs much faster.

3191
02:05:28,633 --> 02:05:30,539
If you know you're gonna keep, if you run a business

3192
02:05:30,539 --> 02:05:33,099
and you're gonna keep passing the same initial content

3193
02:05:33,099 --> 02:05:35,773
to Claude's API, you can load that in

3194
02:05:35,773 --> 02:05:38,525
to the Anthropic API and always keep it there.

3195
02:05:38,525 --> 02:05:39,358
But it's very different

3196
02:05:39,358 --> 02:05:41,407
than we're leading to the reasoning models,

3197
02:05:41,407 --> 02:05:43,512
which we showed this example earlier

3198
02:05:43,512 --> 02:05:46,846
and read some of this mumbling stuff.

3199
02:05:46,846 --> 02:05:47,760
And what happens

3200
02:05:47,760 --> 02:05:51,600
is that the output context length is so much higher.

3201
02:05:51,600 --> 02:05:53,598
And I learned a lot about this from Dylan's work,

3202
02:05:53,598 --> 02:05:56,376
which is essentially as the output work length gets higher,

3203
02:05:56,376 --> 02:05:58,220
you're using this, you're writing this quadratic

3204
02:05:58,220 --> 02:05:59,628
in terms of memory used,

3205
02:05:59,628 --> 02:06:02,597
and then the GPUs that we have,

3206
02:06:02,597 --> 02:06:05,018
effectively, you're gonna run out of memory.

3207
02:06:05,018 --> 02:06:07,451
And they're all trying to serve multiple requests at once.

3208
02:06:07,451 --> 02:06:08,898
So, they're doing this batch processing,

3209
02:06:08,898 --> 02:06:10,686
where not all of the prompts are exactly the same,

3210
02:06:10,686 --> 02:06:11,795
really complex handling.

3211
02:06:11,795 --> 02:06:15,148
And then, as context links gets longer, there's this link.

3212
02:06:15,148 --> 02:06:16,647
I think you call it critical batch size,

3213
02:06:16,647 --> 02:06:20,628
where your ability to serve more users.

3214
02:06:20,628 --> 02:06:22,058
So, how much you can paralyze

3215
02:06:22,058 --> 02:06:24,235
your inference plummets,

3216
02:06:24,235 --> 02:06:25,605
because of this long contract.

3217
02:06:25,605 --> 02:06:27,758
So, your memory usage is going way up

3218
02:06:27,758 --> 02:06:29,303
with these reasoning models,

3219
02:06:29,303 --> 02:06:30,417
and you still have a lot of users.

3220
02:06:30,417 --> 02:06:34,976
So, effectively, the cost to serve multiplies by a ton.

3221
02:06:34,976 --> 02:06:36,576
- [Lex] And we're looking at a plot

3222
02:06:36,576 --> 02:06:40,175
when the x-axis is sequence length.

3223
02:06:40,175 --> 02:06:41,008
- [Dylan] i.e, how many tokens

3224
02:06:41,008 --> 02:06:43,324
are being generated/prompt. - Mm-hmm.

3225
02:06:43,324 --> 02:06:45,585
- [Dylan] So, if I put in a book, that's a million tokens.

3226
02:06:45,585 --> 02:06:48,197
But if I put in, the sky is blue,

3227
02:06:48,197 --> 02:06:49,669
then that's like six tokens or whatever.

3228
02:06:49,669 --> 02:06:51,854
- I should say that what we're calling reasoning

3229
02:06:51,854 --> 02:06:55,662
and chain of thought is extending this sequence length.

3230
02:06:55,662 --> 02:06:56,838
- It's mostly output tokens.

3231
02:06:56,838 --> 02:06:58,466
- So, before, three months ago,

3232
02:06:58,466 --> 02:07:00,843
whenever o1 launched, all of the use cases

3233
02:07:00,843 --> 02:07:02,614
for long context length were like,

3234
02:07:02,614 --> 02:07:05,496
let me put a ton of documents in and then get an answer out.

3235
02:07:05,496 --> 02:07:09,525
And it's a single prefill, compute a lot in parallel,

3236
02:07:09,525 --> 02:07:11,185
and then output a little bit.

3237
02:07:11,185 --> 02:07:13,581
Now, with reasoning and agents,

3238
02:07:13,581 --> 02:07:14,886
this is a very different idea.

3239
02:07:14,886 --> 02:07:17,943
Now, instead, I might only have like, hey, do this task,

3240
02:07:17,943 --> 02:07:18,995
or I might have all these documents,

3241
02:07:18,995 --> 02:07:20,055
but at the end of the day,

3242
02:07:20,055 --> 02:07:22,525
the model is not just producing a little bit.

3243
02:07:22,525 --> 02:07:24,904
It's producing tons of information,

3244
02:07:24,904 --> 02:07:26,026
this chain of thought, - Tens of thousands

3245
02:07:26,026 --> 02:07:26,859
token. - just continues to go

3246
02:07:26,859 --> 02:07:28,255
and go and go and go.

3247
02:07:28,255 --> 02:07:30,577
And so, the sequence length is effectively

3248
02:07:30,577 --> 02:07:32,605
that if it's generated 10,000 tokens,

3249
02:07:32,605 --> 02:07:35,447
it's 10,000 sequence length.

3250
02:07:35,447 --> 02:07:37,168
And plus whatever you input it in the prompt.

3251
02:07:37,168 --> 02:07:38,725
And so, what this chart is showing,

3252
02:07:38,725 --> 02:07:39,969
and it's a logarithmic chart,

3253
02:07:39,969 --> 02:07:43,469
is as you grow from 1k to 4k or 4k to 16k,

3254
02:07:46,378 --> 02:07:50,386
the memory requirements grow so fast for your KV cache

3255
02:07:50,386 --> 02:07:55,167
that you end up not being able to run a certain number of...

3256
02:07:55,167 --> 02:07:57,509
Your sequence length is capped or the number of users

3257
02:07:57,509 --> 02:07:58,406
you can serve. - Let's say the model.

3258
02:07:58,406 --> 02:08:02,260
So, this is showing for a 405b model and batch size 64.

3259
02:08:02,260 --> 02:08:04,649
- Llama 3.1-405b. - Yeah.

3260
02:08:04,649 --> 02:08:06,237
And batch size is crucial to,

3261
02:08:06,237 --> 02:08:09,026
essentially they just like, you wanna have higher batch size

3262
02:08:09,026 --> 02:08:11,862
to parallel your throughput.

3263
02:08:11,862 --> 02:08:13,083
- 64 different users at once, right?

3264
02:08:13,083 --> 02:08:13,916
- Yeah. - And therefore,

3265
02:08:13,916 --> 02:08:15,066
your serving costs are lower.

3266
02:08:15,066 --> 02:08:16,457
Because the server costs the same.

3267
02:08:16,457 --> 02:08:20,052
This is eight H100s, roughly $2 an hour per GPU.

3268
02:08:20,052 --> 02:08:21,420
That's $16 an hour.

3269
02:08:21,420 --> 02:08:23,741
That is like somewhat of a fixed cost.

3270
02:08:23,741 --> 02:08:25,180
You can do things to make it lower of course,

3271
02:08:25,180 --> 02:08:26,690
but it's like $16 an hour.

3272
02:08:26,690 --> 02:08:28,323
Now, how many users can you serve?

3273
02:08:28,323 --> 02:08:29,993
How many tokens can you generate?

3274
02:08:29,993 --> 02:08:32,362
And then, you divide the two and that's your cost.

3275
02:08:32,362 --> 02:08:34,124
And so, with reasoning models,

3276
02:08:34,124 --> 02:08:37,255
this is where a lot of the complexity comes about

3277
02:08:37,255 --> 02:08:38,614
and why memory is so important.

3278
02:08:38,614 --> 02:08:41,011
Because if you have limited amounts of memory,

3279
02:08:41,011 --> 02:08:42,863
then you can't serve so many users.

3280
02:08:42,863 --> 02:08:44,346
If you have limited amounts of memory,

3281
02:08:44,346 --> 02:08:46,027
your serving speeds get lower.

3282
02:08:46,027 --> 02:08:48,715
And so, your costs get a lot, lot worse.

3283
02:08:48,715 --> 02:08:50,042
Because all of a sudden,

3284
02:08:50,042 --> 02:08:53,062
if I was used to, hey, on this $16 an hour server,

3285
02:08:53,062 --> 02:08:54,592
I'm serving Llama 405b.

3286
02:08:54,592 --> 02:08:57,236
Or if I'm serving DeepSeek-V3,

3287
02:08:57,236 --> 02:08:59,397
and it's all chat style applications,

3288
02:08:59,397 --> 02:09:00,760
i.e, we're just chit-chatting.

3289
02:09:00,760 --> 02:09:04,463
the sequence sensor, a thousand, a few thousand.

3290
02:09:04,463 --> 02:09:05,371
When you use a language model,

3291
02:09:05,371 --> 02:09:06,930
it's a few thousand context length most times.

3292
02:09:06,930 --> 02:09:07,949
Sometimes you're dropping a big document,

3293
02:09:07,949 --> 02:09:09,428
but then you process it,

3294
02:09:09,428 --> 02:09:10,863
you get your answer, you throw it away.

3295
02:09:10,863 --> 02:09:12,262
You move on to the next thing.

3296
02:09:12,262 --> 02:09:13,754
Whereas with reasoning,

3297
02:09:13,754 --> 02:09:17,773
I'm now generating tens of thousands of tokens in sequence.

3298
02:09:17,773 --> 02:09:21,080
And so, this memory, this KV cache has to stay resonant.

3299
02:09:21,080 --> 02:09:21,913
- Yeah. - And you have

3300
02:09:21,913 --> 02:09:22,746
to keep loading it,

3301
02:09:22,746 --> 02:09:24,588
you have to keep it in memory constantly.

3302
02:09:24,588 --> 02:09:26,825
And now, this butts out other users.

3303
02:09:26,825 --> 02:09:28,576
If there's now a reasoning task

3304
02:09:28,576 --> 02:09:32,526
and the model's capable of reasoning, then all of a sudden,

3305
02:09:32,526 --> 02:09:33,948
that memory pressure

3306
02:09:33,948 --> 02:09:36,271
means that I can't serve as many users simultaneously.

3307
02:09:36,271 --> 02:09:37,959
- Let's go into DeepSeek again.

3308
02:09:37,959 --> 02:09:42,119
So, we're in the post DeepSeek-R1 time I think.

3309
02:09:42,119 --> 02:09:44,869
And there's two sides to this market

3310
02:09:44,869 --> 02:09:47,178
watching how hard it is to serve it.

3311
02:09:47,178 --> 02:09:49,253
On one side, we're gonna talk about DeepSeek themselves.

3312
02:09:49,253 --> 02:09:50,339
They now have a chat app

3313
02:09:50,339 --> 02:09:52,381
that got to number one on the App Store.

3314
02:09:52,381 --> 02:09:54,091
Disclaimer, number one on the App Store

3315
02:09:54,091 --> 02:09:55,192
is measured by velocity.

3316
02:09:55,192 --> 02:09:56,573
So, it's not necessarily saying

3317
02:09:56,573 --> 02:09:58,142
that more people have the DeepSeek app

3318
02:09:58,142 --> 02:09:59,270
than ChatGPT app. - Mm-hmm.

3319
02:09:59,270 --> 02:10:00,941
Yep. - But it is still remarkable.

3320
02:10:00,941 --> 02:10:02,773
Claude has never hit the number one on the App Store.

3321
02:10:02,773 --> 02:10:04,430
Even though everyone in San Francisco is like,

3322
02:10:04,430 --> 02:10:06,102
oh my god, you gotta use Claude, don't use ChatGPT.

3323
02:10:06,102 --> 02:10:07,304
So, DeepSeek hit this.

3324
02:10:07,304 --> 02:10:09,930
They also launched an API product recently

3325
02:10:09,930 --> 02:10:11,794
where you can ping their API

3326
02:10:11,794 --> 02:10:14,856
and get these super long responses for R1 out.

3327
02:10:14,856 --> 02:10:17,114
And at the same time as these are out,

3328
02:10:17,114 --> 02:10:19,073
we'll get to what's happened to them.

3329
02:10:19,073 --> 02:10:21,433
Because the model waits for DeepSeek R-1

3330
02:10:21,433 --> 02:10:24,612
are openly available and the license is very friendly,

3331
02:10:24,612 --> 02:10:26,222
the MIT license commercially available,

3332
02:10:26,222 --> 02:10:28,143
all of these mid-size companies

3333
02:10:28,143 --> 02:10:29,644
and big companies are trying

3334
02:10:29,644 --> 02:10:33,536
to be first to serve R1 to their users.

3335
02:10:33,536 --> 02:10:35,281
We are trying to evaluate R1,

3336
02:10:35,281 --> 02:10:37,101
'cause we have really similar research going on.

3337
02:10:37,101 --> 02:10:39,553
We released the model and we're trying to compare to it.

3338
02:10:39,553 --> 02:10:41,115
And out of all the companies

3339
02:10:41,115 --> 02:10:44,165
that are, quote, unquote, serving R1,

3340
02:10:44,165 --> 02:10:45,743
and they're doing it at prices

3341
02:10:45,743 --> 02:10:48,377
that are way higher than the DeepSeek API,

3342
02:10:48,377 --> 02:10:51,018
most of them barely work, and the throughput is really low.

3343
02:10:51,018 --> 02:10:52,907
- To give context, everyone,

3344
02:10:52,907 --> 02:10:54,685
one of the parts of freaking this out

3345
02:10:54,685 --> 02:10:56,059
was China reached capabilities.

3346
02:10:56,059 --> 02:10:57,705
The other aspect is they did it so cheap.

3347
02:10:57,705 --> 02:10:58,860
And the so cheap,

3348
02:10:58,860 --> 02:11:00,410
we kind of talked about on the training side

3349
02:11:00,410 --> 02:11:02,273
why it was so cheap slash- - Yeah, let was talk

3350
02:11:02,273 --> 02:11:04,410
about why it's so cheap on the inference.

3351
02:11:04,410 --> 02:11:06,197
It works well and it's cheap.

3352
02:11:06,197 --> 02:11:08,589
- Yeah. - Why is R1 so damn cheap?

3353
02:11:08,589 --> 02:11:10,908
- So, I think there's a couple factors here.

3354
02:11:10,908 --> 02:11:14,629
One is that they do have model architecture innovations.

3355
02:11:14,629 --> 02:11:18,809
This MLA, this new attention that they've done is different

3356
02:11:18,809 --> 02:11:21,999
than the attention from attention is all you need,

3357
02:11:21,999 --> 02:11:22,980
the transformer attention.

3358
02:11:22,980 --> 02:11:24,649
Now, others have already innovated.

3359
02:11:24,649 --> 02:11:28,349
There's a lot of work like MQA GQA, local, global,

3360
02:11:28,349 --> 02:11:31,226
all these different innovations that try to bend the curve.

3361
02:11:31,226 --> 02:11:33,870
It's still quadratic, but the constant is now smaller.

3362
02:11:33,870 --> 02:11:35,707
- Related to our previous discussion,

3363
02:11:35,707 --> 02:11:39,730
this multi-head latent attention can save about 80

3364
02:11:39,730 --> 02:11:42,911
to 90% in memory from the attention mechanism,

3365
02:11:42,911 --> 02:11:44,751
which helps, especially along context.

3366
02:11:44,751 --> 02:11:46,591
- It's 80 to 90% versus the original,

3367
02:11:46,591 --> 02:11:47,622
but then versus what people

3368
02:11:47,622 --> 02:11:49,641
are actually doing, it's still an innovation.

3369
02:11:49,641 --> 02:11:52,701
- This 80 to 90% doesn't say that the whole model

3370
02:11:52,701 --> 02:11:54,861
is 80 to 90% cheaper, just as one part of it.

3371
02:11:54,861 --> 02:11:55,694
- Well, and not just that.

3372
02:11:55,694 --> 02:11:57,191
Other people have implemented techniques

3373
02:11:57,191 --> 02:11:58,436
like local, global, - Yeah, yeah, yeah.

3374
02:11:58,436 --> 02:12:00,165
- and sliding window and GQA, MQA.

3375
02:12:00,165 --> 02:12:03,162
But anyways, DeepSeek has their attention mechanism

3376
02:12:03,162 --> 02:12:04,982
is a true architectural innovation.

3377
02:12:04,982 --> 02:12:06,442
They did tons of experimentation

3378
02:12:06,442 --> 02:12:10,011
and this dramatically reduces the memory pressure.

3379
02:12:10,011 --> 02:12:11,483
It's still there.

3380
02:12:11,483 --> 02:12:12,861
It's still attention, it's still quadratic,

3381
02:12:12,861 --> 02:12:16,190
it's just dramatically reduced it relative to prior forms.

3382
02:12:16,190 --> 02:12:17,973
- All right. That's the memory pressure.

3383
02:12:17,973 --> 02:12:21,142
I should say, in case people don't know,

3384
02:12:21,142 --> 02:12:23,725
R1 is 27 times cheaper than o1.

3385
02:12:24,671 --> 02:12:25,504
(Nathan chuckles)

3386
02:12:25,504 --> 02:12:26,528
- Yes. - We think that OpenAI

3387
02:12:26,528 --> 02:12:28,009
had a large margin built-in.

3388
02:12:28,009 --> 02:12:29,020
- [Lex] Okay. So, that's one-

3389
02:12:29,020 --> 02:12:29,960
- There's multiple factors.

3390
02:12:29,960 --> 02:12:31,342
We should break down the factors, I think.

3391
02:12:31,342 --> 02:12:35,222
- It's two bucks per million token output for R1

3392
02:12:35,222 --> 02:12:38,555
and $60 per million token output for o1.

3393
02:12:40,598 --> 02:12:43,342
- [Nathan] Yeah, let's look at this.

3394
02:12:43,342 --> 02:12:45,274
- So, I think this is very important.

3395
02:12:45,274 --> 02:12:49,857
OpenAI is that drastic gap between DeepSeek in pricing,

3396
02:12:50,726 --> 02:12:52,827
but DeepSeek is offering the same model

3397
02:12:52,827 --> 02:12:54,956
because they open-weight it to everyone else

3398
02:12:54,956 --> 02:12:57,555
for a very similar, much lower price

3399
02:12:57,555 --> 02:12:59,388
than what others are able to serve it for.

3400
02:12:59,388 --> 02:13:01,557
So, there's two factors here.

3401
02:13:01,557 --> 02:13:05,127
Their model is cheaper. It is 27 times cheaper.

3402
02:13:05,127 --> 02:13:06,041
Well, I don't remember the number

3403
02:13:06,041 --> 02:13:07,075
exactly off the top of my head.

3404
02:13:07,075 --> 02:13:08,753
- So, we're looking at a graphic

3405
02:13:08,753 --> 02:13:12,336
that's showing different places serving V3.

3406
02:13:13,285 --> 02:13:14,394
- Yeah. - DeepSeek-V3,

3407
02:13:14,394 --> 02:13:17,738
which is similar to DeepSeek-R1.

3408
02:13:17,738 --> 02:13:20,843
And there's a vast difference

3409
02:13:20,843 --> 02:13:22,704
- In serving cost. - in serving cost.

3410
02:13:22,704 --> 02:13:23,942
And what explains that difference?

3411
02:13:23,942 --> 02:13:28,362
- And so, part of it is OpenAI has a fantastic margin.

3412
02:13:28,362 --> 02:13:29,195
When they're doing inference,

3413
02:13:29,195 --> 02:13:31,596
their gross margins are north of 75%.

3414
02:13:31,596 --> 02:13:34,417
So, that's a 4 to 5x factor right there

3415
02:13:34,417 --> 02:13:35,514
of the cost difference,

3416
02:13:35,514 --> 02:13:38,335
is that OpenAI is just making crazy amounts of money

3417
02:13:38,335 --> 02:13:40,007
because they're the only one with the capability.

3418
02:13:40,007 --> 02:13:42,503
- Do they need that money? Are they using it for R&D?

3419
02:13:42,503 --> 02:13:44,544
- They're losing money obviously as a company,

3420
02:13:44,544 --> 02:13:45,655
because they spend so much on training.

3421
02:13:45,655 --> 02:13:47,025
So, the inference itself - Right.

3422
02:13:47,025 --> 02:13:48,006
- is a very high margin,

3423
02:13:48,006 --> 02:13:49,176
but it doesn't recoup the cost

3424
02:13:49,176 --> 02:13:50,364
of everything else they're doing.

3425
02:13:50,364 --> 02:13:51,434
- Okay. - So, yes,

3426
02:13:51,434 --> 02:13:53,335
they need that money because the revenue

3427
02:13:53,335 --> 02:13:56,395
and margins pay for continuing to build the next thing,

3428
02:13:56,395 --> 02:13:57,228
- So- - alongside

3429
02:13:57,228 --> 02:13:58,061
raising more money.

3430
02:13:58,061 --> 02:13:59,406
- So, the suggestion is that DeepSeek

3431
02:13:59,406 --> 02:14:01,024
is like really bleeding out money.

3432
02:14:01,024 --> 02:14:02,415
- Well, so here's one thing.

3433
02:14:02,415 --> 02:14:03,837
We'll get to this in a second.

3434
02:14:03,837 --> 02:14:06,497
But DeepSeek doesn't have any capacity

3435
02:14:06,497 --> 02:14:07,476
to actually serve the model.

3436
02:14:07,476 --> 02:14:09,157
They stop signups.

3437
02:14:09,157 --> 02:14:12,344
The ability to use it is like non-existent now.

3438
02:14:12,344 --> 02:14:14,542
For most people because so many people are trying to use it,

3439
02:14:14,542 --> 02:14:16,733
they just don't have the GPUs to serve it.

3440
02:14:16,733 --> 02:14:18,978
OpenAI has hundreds of thousands of GPUs between them

3441
02:14:18,978 --> 02:14:21,310
and Microsoft to serve their models.

3442
02:14:21,310 --> 02:14:24,009
DeepSeek has a factor of much lower.

3443
02:14:24,009 --> 02:14:26,906
Even if you believe our research, which is 50,000 GPUs,

3444
02:14:26,906 --> 02:14:28,586
and a portion of those are for research,

3445
02:14:28,586 --> 02:14:30,180
portion of those are for the hedge fund,

3446
02:14:30,180 --> 02:14:32,444
they still have nowhere close to the GPU volumes

3447
02:14:32,444 --> 02:14:36,107
and capacity to serve the model at scale.

3448
02:14:36,107 --> 02:14:38,196
So, it is cheaper.

3449
02:14:38,196 --> 02:14:40,025
A part of that is OpenAI making a ton of money.

3450
02:14:40,025 --> 02:14:42,995
Is DeepSeek making money on their API?

3451
02:14:42,995 --> 02:14:45,310
Unknown. I don't actually think so.

3452
02:14:45,310 --> 02:14:46,918
And part of that is this chart.

3453
02:14:46,918 --> 02:14:48,288
Look at all the other providers.

3454
02:14:48,288 --> 02:14:51,597
Together AI, Fireworks AI are very high-end companies.

3455
02:14:51,597 --> 02:14:53,889
X, Meta, Together AI's Tri Dao,

3456
02:14:53,889 --> 02:14:56,298
and the inventor of like flashattention,

3457
02:14:56,298 --> 02:14:58,249
which is a huge efficiency technique.

3458
02:14:58,249 --> 02:14:59,936
They're very efficient, good companies,

3459
02:14:59,936 --> 02:15:02,829
and do know those companies make money.

3460
02:15:02,829 --> 02:15:04,886
Not tons of money on inference, but they make money.

3461
02:15:04,886 --> 02:15:09,383
And so, they're serving at a 5 to 7x difference in cost.

3462
02:15:09,383 --> 02:15:11,604
And so, now, when you equate, okay,

3463
02:15:11,604 --> 02:15:14,158
OpenAI is making tons of money, that's like a 5x difference.

3464
02:15:14,158 --> 02:15:15,630
And the companies that are trying to make money

3465
02:15:15,630 --> 02:15:17,316
for this model is like a 5x difference.

3466
02:15:17,316 --> 02:15:18,407
There is still a gap.

3467
02:15:18,407 --> 02:15:19,240
There's still a gap

3468
02:15:19,240 --> 02:15:21,487
and that is just DeepSeek being really freaking good.

3469
02:15:21,487 --> 02:15:24,790
The model architecture, MLA, the way they did the MoE,

3470
02:15:24,790 --> 02:15:25,623
all these things,

3471
02:15:25,623 --> 02:15:27,785
there is like legitimate just efficiency differences-

3472
02:15:27,785 --> 02:15:29,029
- It's all their low level libraries

3473
02:15:29,029 --> 02:15:30,579
that we talked about in training,

3474
02:15:30,579 --> 02:15:32,097
some of them probably translate to inference

3475
02:15:32,097 --> 02:15:33,278
and those weren't released. - Yeah.

3476
02:15:33,278 --> 02:15:35,989
- So, we may go a bit into conspiracy land,

3477
02:15:35,989 --> 02:15:38,128
but is it possible the Chinese government

3478
02:15:38,128 --> 02:15:40,641
is subsidizing DeepSeek?

3479
02:15:40,641 --> 02:15:43,396
- I actually don't think they are.

3480
02:15:43,396 --> 02:15:45,243
I think when you look at the Chinese labs,

3481
02:15:45,243 --> 02:15:49,048
there's Huawei has a lab, Moonshot AI,

3482
02:15:49,048 --> 02:15:50,223
there's a couple other labs out there

3483
02:15:50,223 --> 02:15:52,067
that are really close with the government.

3484
02:15:52,067 --> 02:15:54,546
And then, there's labs like Alibaba and DeepSeek,

3485
02:15:54,546 --> 02:15:56,998
which are not close with the government.

3486
02:15:56,998 --> 02:15:59,059
And we talked about the CEO,

3487
02:15:59,059 --> 02:16:02,568
this like reverent figure who's quite different,

3488
02:16:02,568 --> 02:16:03,934
who has like- - Sounds awesome. (chuckles)

3489
02:16:03,934 --> 02:16:06,248
- Very different viewpoints based on the Chinese interviews

3490
02:16:06,248 --> 02:16:07,081
that are translated

3491
02:16:07,081 --> 02:16:09,929
than what the CCP might necessarily want.

3492
02:16:09,929 --> 02:16:12,106
Now, to be clear, does he have a loss leader

3493
02:16:12,106 --> 02:16:13,803
because he can fund it through his hedge fund?

3494
02:16:13,803 --> 02:16:14,636
Yeah, sure.

3495
02:16:14,636 --> 02:16:16,642
- So, the hedge fund might be subsidizing it.

3496
02:16:16,642 --> 02:16:18,046
- Yes. I mean, they absolutely did.

3497
02:16:18,046 --> 02:16:19,658
Because DeepSeek has not raised much money.

3498
02:16:19,658 --> 02:16:21,980
They're now trying to raise around in China,

3499
02:16:21,980 --> 02:16:24,128
but they have not raised money historically.

3500
02:16:24,128 --> 02:16:25,707
It's all just been funded by the hedge fund.

3501
02:16:25,707 --> 02:16:27,565
And he owns like over half the company,

3502
02:16:27,565 --> 02:16:29,431
like 50, 60% of the company's owned by him.

3503
02:16:29,431 --> 02:16:30,264
- Some of the interviews,

3504
02:16:30,264 --> 02:16:32,716
there's discussion on how doing this as a recruiting tool.

3505
02:16:32,716 --> 02:16:34,339
You see this at the American companies too.

3506
02:16:34,339 --> 02:16:37,075
It's like having GPUs, recruiting tool,

3507
02:16:37,075 --> 02:16:39,761
being at the cutting edge of AI, recruiting tool.

3508
02:16:39,761 --> 02:16:40,977
- Open sourcing. - Open sourcing,

3509
02:16:40,977 --> 02:16:42,549
recruiting tool. - That is so much talent.

3510
02:16:42,549 --> 02:16:44,518
They were so far behind and they got so much talent,

3511
02:16:44,518 --> 02:16:45,351
- Yeah. - Because they just

3512
02:16:45,351 --> 02:16:46,392
opened source stuff. - Yeah.

3513
02:16:46,392 --> 02:16:48,590
- More conspiracy thoughts.

3514
02:16:48,590 --> 02:16:50,371
Is it possible since they're a hedge fund

3515
02:16:50,371 --> 02:16:55,371
that they timed everything with this release and the pricing

3516
02:16:55,407 --> 02:16:58,771
and they shorted an Nvidia stock

3517
02:16:58,771 --> 02:17:01,352
and stock of US AI companies,

3518
02:17:01,352 --> 02:17:06,058
and released it with Stargate, like just perfect timing

3519
02:17:06,058 --> 02:17:07,641
to be able to make money. (Lex drowns out Nathan)

3520
02:17:07,641 --> 02:17:08,724
(Lex laughing)

3521
02:17:08,724 --> 02:17:10,619
- They've released it on Inauguration Day.

3522
02:17:10,619 --> 02:17:11,909
They know the international,

3523
02:17:11,909 --> 02:17:14,309
what is on the international calendar,

3524
02:17:14,309 --> 02:17:16,218
but I don't expect them to.

3525
02:17:16,218 --> 02:17:18,418
If you listen to their motivations for AI, it's like...

3526
02:17:18,418 --> 02:17:19,501
- [Lex] No, if you-

3527
02:17:19,501 --> 02:17:22,206
- They released V3 on like December 26th.

3528
02:17:22,206 --> 02:17:23,039
Who releases the day - Yeah.

3529
02:17:23,039 --> 02:17:23,880
- after Christmas? - Yeah. (chuckles)

3530
02:17:23,880 --> 02:17:24,833
- No one looks.

3531
02:17:24,833 --> 02:17:26,775
They had released the papers before this,

3532
02:17:26,775 --> 02:17:28,923
the V3 paper and the R1 paper.

3533
02:17:28,923 --> 02:17:30,710
So, people had been looking at it and being like, wow.

3534
02:17:30,710 --> 02:17:33,273
And then, they just released the R1 model.

3535
02:17:33,273 --> 02:17:35,075
I think they're just shipping as fast as they can

3536
02:17:35,075 --> 02:17:36,585
and like, who cares about Christmas?

3537
02:17:36,585 --> 02:17:37,419
- We should- - Who cares about,

3538
02:17:37,419 --> 02:17:38,591
get it out before Chinese New Year?

3539
02:17:38,591 --> 02:17:39,424
Obviously, - Yeah.

3540
02:17:39,424 --> 02:17:40,507
- which just happened.

3541
02:17:40,507 --> 02:17:43,193
I don't think they actually were timing the market

3542
02:17:43,193 --> 02:17:45,216
or trying to make the biggest splash possible.

3543
02:17:45,216 --> 02:17:46,421
I think they're just shipping.

3544
02:17:46,421 --> 02:17:48,763
- I think that's one of their big advantages.

3545
02:17:48,763 --> 02:17:50,596
We know that a lot of the American companies

3546
02:17:50,596 --> 02:17:52,066
are very invested in safety,

3547
02:17:52,066 --> 02:17:56,170
and that is the central culture of a place like Anthropic.

3548
02:17:56,170 --> 02:17:58,815
And I think Anthropic sounds like a wonderful place to work.

3549
02:17:58,815 --> 02:18:02,202
But if safety is your number one goal,

3550
02:18:02,202 --> 02:18:04,671
it takes way longer to get artifacts out.

3551
02:18:04,671 --> 02:18:06,725
That's why Anthropic is not open sourcing things,

3552
02:18:06,725 --> 02:18:07,752
that's their claims.

3553
02:18:07,752 --> 02:18:09,993
But there's reviews internally.

3554
02:18:09,993 --> 02:18:13,894
Anthropic mentions things to international governments.

3555
02:18:13,894 --> 02:18:15,263
There's been news of how Anthropic

3556
02:18:15,263 --> 02:18:16,644
has done pre-release testing

3557
02:18:16,644 --> 02:18:18,204
with the UK AI Safety Institute.

3558
02:18:18,204 --> 02:18:19,983
All of these things add inertia

3559
02:18:19,983 --> 02:18:21,754
to the process of getting things out.

3560
02:18:21,754 --> 02:18:24,943
And we're on this trend line where progress is very high.

3561
02:18:24,943 --> 02:18:26,102
So, if you reduce the time

3562
02:18:26,102 --> 02:18:27,702
from when your model is done training,

3563
02:18:27,702 --> 02:18:29,135
you run a vows that's good,

3564
02:18:29,135 --> 02:18:32,193
you want to get it out as soon as possible

3565
02:18:32,193 --> 02:18:35,683
to maximize the perceived quality of your outputs.

3566
02:18:35,683 --> 02:18:37,141
DMC does this so well.

3567
02:18:37,141 --> 02:18:39,632
- Dario explicitly said Claude 3.5 Sonnet

3568
02:18:39,632 --> 02:18:41,252
was trained nine months

3569
02:18:41,252 --> 02:18:42,191
or a year- - 9 to 10 months ago.

3570
02:18:42,191 --> 02:18:43,882
- 9 to 10 months ago, and I think it took them

3571
02:18:43,882 --> 02:18:46,476
another handful of months to release it.

3572
02:18:46,476 --> 02:18:49,643
So, it's like there is a significant gap here.

3573
02:18:49,643 --> 02:18:51,820
And especially with reasoning models,

3574
02:18:51,820 --> 02:18:53,709
the word in the San Francisco Street

3575
02:18:53,709 --> 02:18:56,447
is that Anthropic has a better model than o3.

3576
02:18:56,447 --> 02:18:58,342
And they won't release it. Why?

3577
02:18:58,342 --> 02:19:00,760
Because chains of thought are scary.

3578
02:19:00,760 --> 02:19:02,579
And they are legitimately scary.

3579
02:19:02,579 --> 02:19:04,440
If you look at R1, it flips back

3580
02:19:04,440 --> 02:19:05,950
and forth between Chinese and English,

3581
02:19:05,950 --> 02:19:07,317
sometimes it's gibberish,

3582
02:19:07,317 --> 02:19:08,672
and then the right answer comes out.

3583
02:19:08,672 --> 02:19:09,981
And for you and I,

3584
02:19:09,981 --> 02:19:11,317
it's like great. (Nathan and Lex laughing)

3585
02:19:11,317 --> 02:19:13,138
- This is why people are infatuated with you.

3586
02:19:13,138 --> 02:19:16,142
You're telling me this is a high value thing

3587
02:19:16,142 --> 02:19:17,504
and it works and is doing those?

3588
02:19:17,504 --> 02:19:18,968
It's amazing. - Yeah. It's incredible.

3589
02:19:18,968 --> 02:19:21,546
- You talked about that chain of thought

3590
02:19:21,546 --> 02:19:22,602
for that philosophical thing,

3591
02:19:22,602 --> 02:19:23,434
- Yeah. - which is not something

3592
02:19:23,434 --> 02:19:25,107
they trained it to be philosophically good.

3593
02:19:25,107 --> 02:19:25,981
It's just an artifact

3594
02:19:25,981 --> 02:19:28,111
of the chain of thought training it did.

3595
02:19:28,111 --> 02:19:31,655
But that's super important in that like,

3596
02:19:31,655 --> 02:19:34,290
can I inspect your mind and what you're thinking right now?

3597
02:19:34,290 --> 02:19:35,124
No.

3598
02:19:35,124 --> 02:19:36,884
And so, I don't know if you're lying to my face.

3599
02:19:36,884 --> 02:19:38,814
And chain of thought models are that way.

3600
02:19:38,814 --> 02:19:41,072
This is a true, quote, unquote,

3601
02:19:41,072 --> 02:19:43,112
"risk" between a chat application,

3602
02:19:43,112 --> 02:19:46,273
where, hey, I asked the model to say bad words or whatever,

3603
02:19:46,273 --> 02:19:48,107
or how to make anthrax.

3604
02:19:48,107 --> 02:19:49,853
And it tells me that's unsafe, sure,

3605
02:19:49,853 --> 02:19:52,804
but that's something I can get out relatively easily.

3606
02:19:52,804 --> 02:19:54,723
What if I tell the AI to do a task,

3607
02:19:54,723 --> 02:19:55,843
and then it does the task

3608
02:19:55,843 --> 02:19:58,542
all of a sudden randomly in a way that I don't want it.

3609
02:19:58,542 --> 02:20:00,301
And now, that has much more task

3610
02:20:00,301 --> 02:20:02,022
versus response is very different.

3611
02:20:02,022 --> 02:20:04,141
So, the bar for safety is much higher.

3612
02:20:04,141 --> 02:20:06,304
At least this is Anthropic's case.

3613
02:20:06,304 --> 02:20:08,881
For DeepSeek, they're like ship, right?

3614
02:20:08,881 --> 02:20:09,714
- Yeah.

3615
02:20:09,714 --> 02:20:10,716
So, the bar for safety

3616
02:20:10,716 --> 02:20:13,867
is probably lowered a bit because of DeepSeek.

3617
02:20:13,867 --> 02:20:15,867
There's parallels here to the space race.

3618
02:20:15,867 --> 02:20:19,591
The reason the Soviets probably put a man in space first

3619
02:20:19,591 --> 02:20:22,841
is 'cause their approach to safety was,

3620
02:20:25,031 --> 02:20:26,183
the bar for safety was lower.

3621
02:20:26,183 --> 02:20:28,275
- And they killed that dog, and all these things.

3622
02:20:28,275 --> 02:20:31,108
So, it's like... - Less risk-averse

3623
02:20:31,984 --> 02:20:33,572
than the US-based program.

3624
02:20:33,572 --> 02:20:35,786
And there's parallels here.

3625
02:20:35,786 --> 02:20:37,615
But there's probably going to be downward pressure

3626
02:20:37,615 --> 02:20:40,816
on that safety bar for the US companies.

3627
02:20:40,816 --> 02:20:43,037
- And this is something that Dario talks about is like,

3628
02:20:43,037 --> 02:20:45,623
that's the situation that Dario wants to avoid

3629
02:20:45,623 --> 02:20:48,587
is Dario talks too about the difference

3630
02:20:48,587 --> 02:20:50,659
between race to the bottom and race to the top.

3631
02:20:50,659 --> 02:20:51,503
And the race to the top

3632
02:20:51,503 --> 02:20:53,500
is where there's a very high standard on safety.

3633
02:20:53,500 --> 02:20:55,712
There's a very high standard on your model performs

3634
02:20:55,712 --> 02:20:57,495
and certain crucial evaluations.

3635
02:20:57,495 --> 02:20:58,754
And when certain companies

3636
02:20:58,754 --> 02:21:01,119
are really good to it, they will converge.

3637
02:21:01,119 --> 02:21:01,952
This is the idea.

3638
02:21:01,952 --> 02:21:06,369
And ultimately, AI is not confined to one nationality

3639
02:21:07,670 --> 02:21:12,009
or to one set of morals for what it should mean.

3640
02:21:12,009 --> 02:21:13,580
And there's a lot of arguments on like,

3641
02:21:13,580 --> 02:21:16,488
should we stop open sourcing models?

3642
02:21:16,488 --> 02:21:18,846
And if the US stops, it's pretty clear.

3643
02:21:18,846 --> 02:21:20,916
It's way easier to see now at DeepSeek

3644
02:21:20,916 --> 02:21:23,248
that a different international body

3645
02:21:23,248 --> 02:21:24,699
will be the one that builds it.

3646
02:21:24,699 --> 02:21:26,339
We talk about the cost of training.

3647
02:21:26,339 --> 02:21:29,430
DeepSeek has this shocking $5 million number.

3648
02:21:29,430 --> 02:21:30,869
Think about how many entities in the world

3649
02:21:30,869 --> 02:21:32,350
can afford 100 times that

3650
02:21:32,350 --> 02:21:34,280
to have the best open source model

3651
02:21:34,280 --> 02:21:36,071
that people use in the world.

3652
02:21:36,071 --> 02:21:39,208
And it's like, it's a scary reality,

3653
02:21:39,208 --> 02:21:40,970
which is that these open models

3654
02:21:40,970 --> 02:21:43,530
are probably going to keep coming for the time being,

3655
02:21:43,530 --> 02:21:45,178
whether or not we want to stop them.

3656
02:21:45,178 --> 02:21:47,855
And stopping them might make it

3657
02:21:47,855 --> 02:21:49,890
even worse and harder to prepare.

3658
02:21:49,890 --> 02:21:51,618
But it just means that the preparation

3659
02:21:51,618 --> 02:21:53,048
and understanding what AI can do

3660
02:21:53,048 --> 02:21:56,005
is just so much more important. (chuckles)

3661
02:21:56,005 --> 02:21:58,339
That's why I'm here at the end of the day.

3662
02:21:58,339 --> 02:22:00,697
But it's like letting that sink into people,

3663
02:22:00,697 --> 02:22:03,889
especially not in AI is that this is coming,

3664
02:22:03,889 --> 02:22:05,328
there are some structural things

3665
02:22:05,328 --> 02:22:09,180
in a global interconnected world that you have to accept.

3666
02:22:09,180 --> 02:22:12,219
- Yeah, you mentioned, you sent me something

3667
02:22:12,219 --> 02:22:15,818
that Mark Zuckerberg mentioned on the earnings call.

3668
02:22:15,818 --> 02:22:18,303
He said that, "I think in light of some of the recent news,

3669
02:22:18,303 --> 02:22:20,636
the new competitor, DeepSeek from China,

3670
02:22:20,636 --> 02:22:21,656
I think it's one of the things

3671
02:22:21,656 --> 02:22:23,527
that we're talking about is there's going to be

3672
02:22:23,527 --> 02:22:25,324
an open source standard globally.

3673
02:22:25,324 --> 02:22:28,043
And I think for our kind of national advantage,

3674
02:22:28,043 --> 02:22:30,311
it's important that it's an American standard.

3675
02:22:30,311 --> 02:22:32,072
So, we take that seriously.

3676
02:22:32,072 --> 02:22:33,512
We want to build the AI system

3677
02:22:33,512 --> 02:22:35,032
that people around the world are using.

3678
02:22:35,032 --> 02:22:36,852
And I think that if anything,

3679
02:22:36,852 --> 02:22:39,482
some of the recent news has only strengthened our conviction

3680
02:22:39,482 --> 02:22:41,633
that this is the right thing to be focused on."

3681
02:22:41,633 --> 02:22:43,119
So, yeah, open sourcing.

3682
02:22:43,119 --> 02:22:47,223
- Yeah, Mark Zuckerberg is not new to having American values

3683
02:22:47,223 --> 02:22:50,148
and how he presents his company's trajectory.

3684
02:22:50,148 --> 02:22:52,764
I think their products have long since been banned in China.

3685
02:22:52,764 --> 02:22:55,362
And I respect the saying it directly.

3686
02:22:55,362 --> 02:22:56,792
- And there's an interesting aspect

3687
02:22:56,792 --> 02:22:58,534
of just because it's open-weights

3688
02:22:58,534 --> 02:23:01,912
or open sourced doesn't mean it can't be subverted.

3689
02:23:01,912 --> 02:23:04,831
There have been many open source software bugs

3690
02:23:04,831 --> 02:23:07,125
that have been like, for example,

3691
02:23:07,125 --> 02:23:08,594
there was a Linux bug that was found

3692
02:23:08,594 --> 02:23:11,354
after 10 years, which was clearly a backdoor,

3693
02:23:11,354 --> 02:23:12,774
because somebody was like,

3694
02:23:12,774 --> 02:23:13,607
"Why is this taking

3695
02:23:13,607 --> 02:23:15,485
half a second to load?" - This is the recent one.

3696
02:23:15,485 --> 02:23:17,658
- Right? Like why is this taking half a second to load?

3697
02:23:17,658 --> 02:23:19,554
And it was like, "Oh crap, there's a backdoor here.

3698
02:23:19,554 --> 02:23:20,387
That's why."

3699
02:23:20,387 --> 02:23:25,237
And it's like, this is very much possible with AI models.

3700
02:23:25,237 --> 02:23:29,503
Today, the alignment of these models is very clear.

3701
02:23:29,503 --> 02:23:31,686
I'm not gonna say bad words,

3702
02:23:31,686 --> 02:23:33,654
I'm not gonna teach you how to make anthrax,

3703
02:23:33,654 --> 02:23:37,031
I'm not gonna talk about Tiananmen Square.

3704
02:23:37,031 --> 02:23:41,549
I'm gonna say Taiwan is just an eastern preference.

3705
02:23:41,549 --> 02:23:43,542
All these things are like,

3706
02:23:43,542 --> 02:23:46,674
depending on who you are, what you align, whether,

3707
02:23:46,674 --> 02:23:49,576
and even like xAI is aligned a certain way.

3708
02:23:49,576 --> 02:23:50,742
There they might be,

3709
02:23:50,742 --> 02:23:52,758
it's not aligned in the woke sense,

3710
02:23:52,758 --> 02:23:54,712
it's not aligned in the pro-China sense,

3711
02:23:54,712 --> 02:23:55,814
but there is certain things

3712
02:23:55,814 --> 02:23:57,297
that are imbued within the model.

3713
02:23:57,297 --> 02:23:59,723
Now, when you release this publicly in an instruct model

3714
02:23:59,723 --> 02:24:02,336
that's open-weights, this can then proliferate.

3715
02:24:02,336 --> 02:24:04,912
But as these systems get more and more capable,

3716
02:24:04,912 --> 02:24:09,455
what you can embed deep down in the model is not as clear.

3717
02:24:09,455 --> 02:24:11,882
And so, that is like one of the big fears

3718
02:24:11,882 --> 02:24:16,406
is if an American model or a Chinese model is the top model,

3719
02:24:16,406 --> 02:24:19,097
you are going to embed things that are unclear,

3720
02:24:19,097 --> 02:24:20,171
and it can be unintentional too.

3721
02:24:20,171 --> 02:24:24,232
Like British English is dead because American LLMs won.

3722
02:24:24,232 --> 02:24:25,291
And the internet is American,

3723
02:24:25,291 --> 02:24:27,970
and therefore, color is spelled the way Americans spell it.

3724
02:24:27,970 --> 02:24:28,876
And this is just- - A lot

3725
02:24:28,876 --> 02:24:30,153
of strong words right now.

3726
02:24:30,153 --> 02:24:30,986
(Nathan laughing)

3727
02:24:30,986 --> 02:24:33,401
- This is just the factual nature of the LLMs now.

3728
02:24:33,401 --> 02:24:34,506
- The right way to- - I mean, it's the carve

3729
02:24:34,506 --> 02:24:37,081
at the tree, the English is the hottest programming language

3730
02:24:37,081 --> 02:24:39,351
and that English is defined by a bunch of companies

3731
02:24:39,351 --> 02:24:41,641
that primarily are in San Francisco.

3732
02:24:41,641 --> 02:24:44,908
- The right way to spell optimization is with a Z,

3733
02:24:44,908 --> 02:24:47,358
just in case you both... (Nathan laughing)

3734
02:24:47,358 --> 02:24:48,969
I think it's an S in British English.

3735
02:24:48,969 --> 02:24:50,052
- [Nathan] It is-

3736
02:24:50,052 --> 02:24:51,560
- Taking it as something silly.

3737
02:24:51,560 --> 02:24:53,305
Something as silly as the spelling,

3738
02:24:53,305 --> 02:24:54,515
like which British and English,

3739
02:24:54,515 --> 02:24:57,782
Brits and Americans will like laugh about probably.

3740
02:24:57,782 --> 02:25:01,356
I don't think we care that much, but some people will.

3741
02:25:01,356 --> 02:25:06,356
But this can boil down into very, very important topics.

3742
02:25:06,656 --> 02:25:09,989
Like, hey, subverting people, chat bots.

3743
02:25:11,257 --> 02:25:15,927
Character AI has shown that they can talk to kids or adults,

3744
02:25:15,927 --> 02:25:19,544
and you people feel a certain way.

3745
02:25:19,544 --> 02:25:20,966
And that's unintentional alignment.

3746
02:25:20,966 --> 02:25:23,735
But what happens when there's intentional alignment

3747
02:25:23,735 --> 02:25:25,475
deep down on the open source standard?

3748
02:25:25,475 --> 02:25:27,932
It's a backdoor today for like Linux

3749
02:25:27,932 --> 02:25:30,691
that we discover, or some encryption system.

3750
02:25:30,691 --> 02:25:32,958
China uses different encryption than NIST defines,

3751
02:25:32,958 --> 02:25:34,264
the US NIST, because there's clearly,

3752
02:25:34,264 --> 02:25:36,563
at least they think there's backdoors in it.

3753
02:25:36,563 --> 02:25:39,071
What happens when the models are backdoors,

3754
02:25:39,071 --> 02:25:41,935
not just to computer systems, but to our minds.

3755
02:25:41,935 --> 02:25:43,438
- Yeah, they're cultural backdoors.

3756
02:25:43,438 --> 02:25:47,212
The thing that amplifies the relevance of culture

3757
02:25:47,212 --> 02:25:50,373
with language models is that we are used to this mode

3758
02:25:50,373 --> 02:25:55,194
of interacting with people in back and forth conversation.

3759
02:25:55,194 --> 02:25:58,474
And we have now have a very powerful computer system

3760
02:25:58,474 --> 02:26:03,321
that slots into a social context that we're used to,

3761
02:26:03,321 --> 02:26:06,660
which makes people very, we don't know the extent

3762
02:26:06,660 --> 02:26:09,223
that which people can be impacted by that.

3763
02:26:09,223 --> 02:26:10,664
- So, there could be, this is one,

3764
02:26:10,664 --> 02:26:15,204
this is an actual concern with a Chinese company

3765
02:26:15,204 --> 02:26:18,001
that is providing open-weights models,

3766
02:26:18,001 --> 02:26:22,573
is that there could be some secret Chinese government,

3767
02:26:22,573 --> 02:26:24,423
sort of requirement for these models

3768
02:26:24,423 --> 02:26:26,483
to have a certain kind of backdoor,

3769
02:26:26,483 --> 02:26:28,193
to have some kind of thing where-

3770
02:26:28,193 --> 02:26:30,106
- I don't necessarily think it'll be a backdoor,

3771
02:26:30,106 --> 02:26:32,579
'cause once it's open-weights, it doesn't phone home.

3772
02:26:32,579 --> 02:26:33,934
It's more about

3773
02:26:33,934 --> 02:26:37,351
if it recognizes a certain system, it could...

3774
02:26:37,351 --> 02:26:39,469
Now, it could be a backdoor in the sense of like,

3775
02:26:39,469 --> 02:26:42,810
hey, if you're building a software, something in software,

3776
02:26:42,810 --> 02:26:44,270
all of a sudden, it's a software agent,

3777
02:26:44,270 --> 02:26:46,894
oh, program this backdoor that only we know about.

3778
02:26:46,894 --> 02:26:49,094
Or it could be subvert the mind to think

3779
02:26:49,094 --> 02:26:51,327
that X, Y, Z opinion is the correct one.

3780
02:26:51,327 --> 02:26:52,554
- Anthropic has research on this

3781
02:26:52,554 --> 02:26:55,762
where they show that if you put different phrases,

3782
02:26:55,762 --> 02:26:57,752
certain phrases in at pre-training,

3783
02:26:57,752 --> 02:27:00,443
you can then elicit different behavior

3784
02:27:00,443 --> 02:27:02,013
when you're actually using the model,

3785
02:27:02,013 --> 02:27:04,442
because they've poisoned the pre-training data.

3786
02:27:04,442 --> 02:27:06,931
- Mm-hmm. - I don't think, as of now,

3787
02:27:06,931 --> 02:27:08,712
I don't think anybody in a production system

3788
02:27:08,712 --> 02:27:10,712
is trying to do anything like this.

3789
02:27:10,712 --> 02:27:14,372
I think it's mostly Anthropic is doing very direct work

3790
02:27:14,372 --> 02:27:16,020
and mostly just subtle things

3791
02:27:16,020 --> 02:27:19,213
of we don't know what these models are going to,

3792
02:27:19,213 --> 02:27:21,123
how they are going to generate tokens,

3793
02:27:21,123 --> 02:27:22,902
what information they're gonna represent,

3794
02:27:22,902 --> 02:27:26,167
and what the complex representations they have are.

3795
02:27:26,167 --> 02:27:27,000
- Well, one of the...

3796
02:27:27,000 --> 02:27:28,043
We're talking about Anthropic,

3797
02:27:28,043 --> 02:27:30,974
which is generally just is permeated

3798
02:27:30,974 --> 02:27:35,284
with good humans trying to do good in the world.

3799
02:27:35,284 --> 02:27:37,867
We just don't know of any labs,

3800
02:27:38,764 --> 02:27:40,724
this would be done in a military context,

3801
02:27:40,724 --> 02:27:44,593
that are explicitly trained to, okay,

3802
02:27:44,593 --> 02:27:48,760
how can we, the front door looks like a happy LLM,

3803
02:27:50,521 --> 02:27:55,018
but underneath, it's a thing that will over time,

3804
02:27:55,018 --> 02:27:57,120
do the maximum amount of damage

3805
02:27:57,120 --> 02:27:58,397
to our, quote, unquote, "enemies".

3806
02:27:58,397 --> 02:28:01,402
- There's this very good quote from Sam Altman who,

3807
02:28:01,402 --> 02:28:02,803
he can be hype beast sometime,

3808
02:28:02,803 --> 02:28:05,351
but one of the things he said, and I think I agree,

3809
02:28:05,351 --> 02:28:07,261
is that superhuman persuasion

3810
02:28:07,261 --> 02:28:09,592
will happen before superhuman intelligence.

3811
02:28:09,592 --> 02:28:10,448
- Yeah. - Right?

3812
02:28:10,448 --> 02:28:12,131
And if that's the case, then these things

3813
02:28:12,131 --> 02:28:15,272
before we get this AGI, ASI stuff,

3814
02:28:15,272 --> 02:28:18,404
we can embed superhuman persuasion towards our ideal

3815
02:28:18,404 --> 02:28:20,711
or whatever the ideal of the model maker is.

3816
02:28:20,711 --> 02:28:21,930
And again, like today,

3817
02:28:21,930 --> 02:28:24,337
I truly don't believe DeepSeek has done this.

3818
02:28:24,337 --> 02:28:26,891
But it is a sign of what could happen.

3819
02:28:26,891 --> 02:28:28,632
- So, one of the dystopian worlds

3820
02:28:28,632 --> 02:28:31,163
is described by "Brave New World".

3821
02:28:31,163 --> 02:28:34,852
So, we could just be stuck scrolling Instagram

3822
02:28:34,852 --> 02:28:37,772
looking at cute puppies or worse,

3823
02:28:37,772 --> 02:28:41,123
and then talking to bots that are giving us a narrative

3824
02:28:41,123 --> 02:28:43,151
and we completely get lost in that world

3825
02:28:43,151 --> 02:28:45,208
that's controlled by somebody else,

3826
02:28:45,208 --> 02:28:46,978
versus thinking independently.

3827
02:28:46,978 --> 02:28:50,086
And that's a major concern as we rely more

3828
02:28:50,086 --> 02:28:51,826
and more on these kinds of systems.

3829
02:28:51,826 --> 02:28:54,114
- We've already seen this with recommendation systems.

3830
02:28:54,114 --> 02:28:55,166
- Yeah, recommendation systems (Nathan laughing)

3831
02:28:55,166 --> 02:28:57,533
hack the dopamine-induced reward circuit,

3832
02:28:57,533 --> 02:28:59,583
but the brain is a lot more complicated.

3833
02:28:59,583 --> 02:29:01,181
And what other sort of circuits,

3834
02:29:01,181 --> 02:29:02,835
quote, unquote, "feedback loops" in your brain

3835
02:29:02,835 --> 02:29:07,008
can you hack/subvert in ways like recommendation systems

3836
02:29:07,008 --> 02:29:08,869
are purely just trying to do,

3837
02:29:08,869 --> 02:29:10,802
increased time in ads, and et cetera.

3838
02:29:10,802 --> 02:29:12,259
But there's so many more goals

3839
02:29:12,259 --> 02:29:15,852
that can be achieved through these complicated models.

3840
02:29:15,852 --> 02:29:18,137
- There's just no reason in some number of years

3841
02:29:18,137 --> 02:29:19,727
that you can't train a language model

3842
02:29:19,727 --> 02:29:23,460
to maximize time spent on a chat app.

3843
02:29:23,460 --> 02:29:24,577
Right now, they are trained

3844
02:29:24,577 --> 02:29:25,410
for- - Is that not

3845
02:29:25,410 --> 02:29:26,766
what Character AI has done?

3846
02:29:26,766 --> 02:29:28,760
Their time per session is like two hours.

3847
02:29:28,760 --> 02:29:31,146
- Yeah, Character AI very likely

3848
02:29:31,146 --> 02:29:32,730
could be optimizing this where it's like

3849
02:29:32,730 --> 02:29:34,846
the way that this data is collected is naive

3850
02:29:34,846 --> 02:29:36,475
where it's like you're presented a few options

3851
02:29:36,475 --> 02:29:37,308
and you choose them,

3852
02:29:37,308 --> 02:29:39,046
but that's not the only way

3853
02:29:39,046 --> 02:29:40,651
that these models are gonna be trained.

3854
02:29:40,651 --> 02:29:42,375
- It's naive stuff like talk to an anime girl,

3855
02:29:42,375 --> 02:29:45,885
but it can be like, yeah, this is a risk.

3856
02:29:45,885 --> 02:29:47,553
- It's a bit of a cliche thing to say,

3857
02:29:47,553 --> 02:29:52,304
but I've, over the past year, had a few stretches of time

3858
02:29:52,304 --> 02:29:55,375
where I didn't use social media or the internet at all,

3859
02:29:55,375 --> 02:29:58,132
and just read books and was out in nature.

3860
02:29:58,132 --> 02:30:01,636
And it clearly has an effect on the mind,

3861
02:30:01,636 --> 02:30:03,401
where it change...

3862
02:30:03,401 --> 02:30:05,383
I feel like I'm returning,

3863
02:30:05,383 --> 02:30:08,221
of course I was raised before the internet really took off,

3864
02:30:08,221 --> 02:30:10,971
but I'm returning to some more...

3865
02:30:12,032 --> 02:30:14,766
- I know where you're going. You can see it physiologically.

3866
02:30:14,766 --> 02:30:17,167
I take three days if I'm like backpacking or something,

3867
02:30:17,167 --> 02:30:22,167
and you're literal, you're breaking down addiction cycles.

3868
02:30:22,486 --> 02:30:23,607
(Nathan chuckles) - I feel like I'm more

3869
02:30:23,607 --> 02:30:25,662
in control of my mind.

3870
02:30:25,662 --> 02:30:28,608
There feels like a sovereignty of intelligence

3871
02:30:28,608 --> 02:30:30,936
that's happening when I'm disconnected from the internet.

3872
02:30:30,936 --> 02:30:34,436
I think the more I use the internet and social media,

3873
02:30:34,436 --> 02:30:36,365
the more other people are controlling my mind.

3874
02:30:36,365 --> 02:30:37,953
That's definitely a feeling.

3875
02:30:37,953 --> 02:30:40,628
And then, in the future, that will be not other people

3876
02:30:40,628 --> 02:30:41,650
but algorithms,

3877
02:30:41,650 --> 02:30:45,599
or other people presented to me via algorithms.

3878
02:30:45,599 --> 02:30:48,424
- There are already tons of AI bots on the internet

3879
02:30:48,424 --> 02:30:49,257
and every so...

3880
02:30:49,257 --> 02:30:50,090
Right now it's not frequent,

3881
02:30:50,090 --> 02:30:52,352
but every so often I have replied to one

3882
02:30:52,352 --> 02:30:53,515
and they're instantly replied,

3883
02:30:53,515 --> 02:30:54,505
and I'm like, crap, I was a bot.

3884
02:30:54,505 --> 02:30:57,340
And that is just gonna become more common.

3885
02:30:57,340 --> 02:30:58,653
They're gonna get good.

3886
02:30:58,653 --> 02:31:01,667
- One of the hilarious things about technology

3887
02:31:01,667 --> 02:31:02,646
over its history

3888
02:31:02,646 --> 02:31:05,376
is that the illicit adult entertainment industry

3889
02:31:05,376 --> 02:31:07,634
is always adopted technologies first.

3890
02:31:07,634 --> 02:31:08,467
- [Lex] Yeah.

3891
02:31:08,467 --> 02:31:09,657
- Whether it was video streaming

3892
02:31:09,657 --> 02:31:10,740
- [Lex] Yeah.

3893
02:31:10,740 --> 02:31:14,002
- to where there's now the independent

3894
02:31:14,002 --> 02:31:15,943
adult illicit content creators,

3895
02:31:15,943 --> 02:31:18,462
who have their subscription pages,

3896
02:31:18,462 --> 02:31:21,043
and there they actually heavily utilize...

3897
02:31:21,043 --> 02:31:22,790
Generative AI has already been diffusion models

3898
02:31:22,790 --> 02:31:23,812
and all that is huge there.

3899
02:31:23,812 --> 02:31:27,263
But now these subscription-based individual creators

3900
02:31:27,263 --> 02:31:30,122
do use bots to approximate themselves

3901
02:31:30,122 --> 02:31:33,155
and chat with their whales. - People pay a lot for it.

3902
02:31:33,155 --> 02:31:34,136
Yeah. - And people pay a lot.

3903
02:31:34,136 --> 02:31:35,239
It's a lot of times, it's them,

3904
02:31:35,239 --> 02:31:37,236
but a lot of, there are agencies that do this

3905
02:31:37,236 --> 02:31:41,492
for these creators, and do it like on a mass scale.

3906
02:31:41,492 --> 02:31:44,813
So, the largest creators are able to talk to hundreds

3907
02:31:44,813 --> 02:31:49,299
or thousands of people at a time because of these bots.

3908
02:31:49,299 --> 02:31:51,714
And so, it's already being used there.

3909
02:31:51,714 --> 02:31:54,964
Obviously, video streaming and other technology

3910
02:31:54,964 --> 02:31:55,797
that have come there first,

3911
02:31:55,797 --> 02:31:58,015
it's gonna come to the rest of society too.

3912
02:31:58,015 --> 02:31:59,533
- There's a general concern

3913
02:31:59,533 --> 02:32:02,365
that models get censored by the companies that deploy them.

3914
02:32:02,365 --> 02:32:05,663
So, one case where we've seen that,

3915
02:32:05,663 --> 02:32:10,496
and maybe censorship is one word, alignment maybe via RLHF

3916
02:32:11,895 --> 02:32:14,169
or some other way is another word.

3917
02:32:14,169 --> 02:32:15,625
So, we saw that

3918
02:32:15,625 --> 02:32:19,375
with Black Nazi image generation with Gemini.

3919
02:32:21,194 --> 02:32:24,443
As you mentioned, we also see that with Chinese models

3920
02:32:24,443 --> 02:32:27,215
refusing to answer what happened (chuckles)

3921
02:32:27,215 --> 02:32:31,078
in June 4th, 1989 at Tiananmen Square.

3922
02:32:31,078 --> 02:32:33,490
So, how can this be avoided?

3923
02:32:33,490 --> 02:32:35,150
And maybe can you just in general,

3924
02:32:35,150 --> 02:32:39,231
talk about how this happens and how can it be avoided?

3925
02:32:39,231 --> 02:32:41,446
- You give multiple examples.

3926
02:32:41,446 --> 02:32:46,389
There's probably a few things to keep in mind here.

3927
02:32:46,389 --> 02:32:50,365
One is the kind of Tiananmen Square factual knowledge,

3928
02:32:50,365 --> 02:32:54,472
like how does that get embedded into the models?

3929
02:32:54,472 --> 02:32:59,343
Two is the Gemini, what you call the Black Nazi incident,

3930
02:32:59,343 --> 02:33:01,891
which is when Gemini as a system

3931
02:33:01,891 --> 02:33:03,502
had this extra thing put into it

3932
02:33:03,502 --> 02:33:05,113
that dramatically changed the behavior.

3933
02:33:05,113 --> 02:33:07,702
And then, three is what most people

3934
02:33:07,702 --> 02:33:11,707
would call general alignment, RLHF post-training.

3935
02:33:11,707 --> 02:33:14,061
Each of these have very different scopes

3936
02:33:14,061 --> 02:33:15,663
in how they're applied.

3937
02:33:15,663 --> 02:33:16,618
In order to do,

3938
02:33:16,618 --> 02:33:18,292
if you're just gonna look at the model weights,

3939
02:33:18,292 --> 02:33:22,844
in order to audit specific facts is extremely hard.

3940
02:33:22,844 --> 02:33:25,701
'Cause you have to chrome through the pre-training data

3941
02:33:25,701 --> 02:33:29,404
and look at all of this, and then that's terabytes of files

3942
02:33:29,404 --> 02:33:32,631
and look for very specific words or hints of the words.

3943
02:33:32,631 --> 02:33:33,882
- So, I guess one way to say it

3944
02:33:33,882 --> 02:33:35,844
is that you can insert censorship

3945
02:33:35,844 --> 02:33:38,724
or alignment at various stages in the pipeline.

3946
02:33:38,724 --> 02:33:41,020
And what you refer to now is at the very beginning

3947
02:33:41,020 --> 02:33:42,781
of the data selection stage. - Yeah, so if you want

3948
02:33:42,781 --> 02:33:44,709
to get rid of facts in a model,

3949
02:33:44,709 --> 02:33:46,462
you have to do it at every stage.

3950
02:33:46,462 --> 02:33:47,632
You have to do it at the pre-training.

3951
02:33:47,632 --> 02:33:49,060
So, most people think that pre-training

3952
02:33:49,060 --> 02:33:51,823
is where most of the knowledge is put into the model.

3953
02:33:51,823 --> 02:33:55,564
And then, you can elicit and move that in different ways,

3954
02:33:55,564 --> 02:33:56,659
whether through post-training

3955
02:33:56,659 --> 02:33:58,792
or whether through systems afterwards.

3956
02:33:58,792 --> 02:34:01,488
- This is where the whole hacking models comes from.

3957
02:34:01,488 --> 02:34:03,624
GPT will not tell you how to make anthrax,

3958
02:34:03,624 --> 02:34:05,871
but if you try really, really hard,

3959
02:34:05,871 --> 02:34:07,493
you can eventually get it to tell you about anthrax.

3960
02:34:07,493 --> 02:34:11,934
Because they didn't filter it from the pre-training dataset.

3961
02:34:11,934 --> 02:34:16,116
- But by the way, removing facts has such

3962
02:34:16,116 --> 02:34:18,862
a ominous dark feel to it. - I almost think

3963
02:34:18,862 --> 02:34:19,983
it's practically impossible.

3964
02:34:19,983 --> 02:34:20,816
'Cause you effectively

3965
02:34:20,816 --> 02:34:22,263
have to remove them from the internet.

3966
02:34:22,263 --> 02:34:24,148
You're taking on a-

3967
02:34:24,148 --> 02:34:28,617
- Well, did they remove the mm thing from the subreddits?

3968
02:34:28,617 --> 02:34:29,898
The mmmmm?

3969
02:34:29,898 --> 02:34:30,858
- [Nathan] It gets filtered out.

3970
02:34:30,858 --> 02:34:31,973
- Right. So, that's- - So, you have

3971
02:34:31,973 --> 02:34:33,917
quality filters, which are small language models

3972
02:34:33,917 --> 02:34:34,750
that look at a document,

3973
02:34:34,750 --> 02:34:37,299
and tell you like, how good is this text?

3974
02:34:37,299 --> 02:34:38,727
Is it close to a Wikipedia article?

3975
02:34:38,727 --> 02:34:40,271
Which is a good thing

3976
02:34:40,271 --> 02:34:42,329
that we want language models to be able to imitate.

3977
02:34:42,329 --> 02:34:44,399
- So, couldn't you do a small language model

3978
02:34:44,399 --> 02:34:47,437
that filter Zhou mentions at Tiananmen Square in the data?

3979
02:34:47,437 --> 02:34:51,677
- Yes, but is it gonna catch word play or encoded language

3980
02:34:51,677 --> 02:34:52,850
is the same thing. - People have been meaning

3981
02:34:52,850 --> 02:34:53,985
on games and other stuff,

3982
02:34:53,985 --> 02:34:57,935
how to say things that don't say Tiananmen Square,

3983
02:34:57,935 --> 02:34:59,203
or like yeah.

3984
02:34:59,203 --> 02:35:00,922
So, there's always different ways to do it.

3985
02:35:00,922 --> 02:35:02,985
There's, hey, the internet as a whole

3986
02:35:02,985 --> 02:35:05,222
does tend to just have a slight left bias,

3987
02:35:05,222 --> 02:35:07,203
- Mm-hmm. - because it's always

3988
02:35:07,203 --> 02:35:11,374
been richer, more affluent, younger people on the internet

3989
02:35:11,374 --> 02:35:13,318
relative to the rest of the population.

3990
02:35:13,318 --> 02:35:14,151
So, there is already

3991
02:35:14,151 --> 02:35:17,347
inherently a slight left bias on the internet.

3992
02:35:17,347 --> 02:35:19,974
And so, how do you filter things that are this complicated?

3993
02:35:19,974 --> 02:35:23,904
And some of these can be factual, non-factual.

3994
02:35:23,904 --> 02:35:24,761
But like Tiananmen Square

3995
02:35:24,761 --> 02:35:26,304
is obviously the example of a factual,

3996
02:35:26,304 --> 02:35:27,317
but it gets a lot harder

3997
02:35:27,317 --> 02:35:31,150
when you're talking about aligning to a ideal,

3998
02:35:32,254 --> 02:35:33,324
which is- - Yeah. Yeah.

3999
02:35:33,324 --> 02:35:34,383
- And so, grok, for example,

4000
02:35:34,383 --> 02:35:36,253
Elon's tried really hard to make the model

4001
02:35:36,253 --> 02:35:38,616
not be super PC and woke,

4002
02:35:38,616 --> 02:35:40,735
but the best way to do pre-training

4003
02:35:40,735 --> 02:35:43,135
is to throw the whole freaking internet at it.

4004
02:35:43,135 --> 02:35:44,557
And then, later figure out.

4005
02:35:44,557 --> 02:35:45,390
But then at the end of the day,

4006
02:35:45,390 --> 02:35:48,055
the model at its core now still has some of these ideals.

4007
02:35:48,055 --> 02:35:50,964
You still ingested Reddit /r/politics,

4008
02:35:50,964 --> 02:35:52,777
which is probably the largest political discussion board

4009
02:35:52,777 --> 02:35:54,828
on the world that's freely available to scrape.

4010
02:35:54,828 --> 02:35:56,269
And guess what? That's left-leaning.

4011
02:35:56,269 --> 02:36:01,269
And so, there are some aspects that you just can't censor

4012
02:36:01,757 --> 02:36:05,056
unless you try really, really, really, really, really hard.

4013
02:36:05,056 --> 02:36:06,529
- So, the base model

4014
02:36:06,529 --> 02:36:09,780
will always have some TDS, Trump derangement syndrome,

4015
02:36:09,780 --> 02:36:11,411
because it's trained so much.

4016
02:36:11,411 --> 02:36:12,244
- It'll have the ability

4017
02:36:12,244 --> 02:36:13,077
to express it. - I don't know if you...

4018
02:36:13,077 --> 02:36:15,169
But what if you... (Nathan and Lex laughing)

4019
02:36:15,169 --> 02:36:18,324
- There's a wide representation in the data.

4020
02:36:18,324 --> 02:36:19,308
- This is what happens.

4021
02:36:19,308 --> 02:36:22,219
It's like a lot of modern, what is called post-training,

4022
02:36:22,219 --> 02:36:23,650
it's a series of techniques

4023
02:36:23,650 --> 02:36:27,618
to get the model on rails of a really specific behavior.

4024
02:36:27,618 --> 02:36:29,400
- And it's like you can,

4025
02:36:29,400 --> 02:36:31,731
you also have the ingested data of like Twitter

4026
02:36:31,731 --> 02:36:35,724
or Reddit /r/The_Donald, which is also super pro-Trump.

4027
02:36:35,724 --> 02:36:37,376
And then, you have fascist subreddits

4028
02:36:37,376 --> 02:36:38,789
or you have communist subreddit.

4029
02:36:38,789 --> 02:36:41,521
So, the model in pre-training ingests everything.

4030
02:36:41,521 --> 02:36:43,176
It has no worldview.

4031
02:36:43,176 --> 02:36:45,599
Now, it does have some skew,

4032
02:36:45,599 --> 02:36:48,159
because more of the text is skewed a certain way,

4033
02:36:48,159 --> 02:36:50,780
which is general like slight left,

4034
02:36:50,780 --> 02:36:54,933
but also somewhat intellectual, somewhat like,

4035
02:36:54,933 --> 02:36:57,345
it's just like the general internet is a certain way.

4036
02:36:57,345 --> 02:36:59,629
- Mm-hmm. - And then, as Nathan's about

4037
02:36:59,629 --> 02:37:02,720
to describe eloquently, you can elicit certain things out.

4038
02:37:02,720 --> 02:37:03,879
- And there's a lot of history here.

4039
02:37:03,879 --> 02:37:06,396
So, we can go through multiple examples and what happened.

4040
02:37:06,396 --> 02:37:07,936
Llama 2 was a launch

4041
02:37:07,936 --> 02:37:10,417
that the phrase like too much RLHF

4042
02:37:10,417 --> 02:37:13,186
or too much safety was a lot. - Mm-hmm.

4043
02:37:13,186 --> 02:37:15,279
- It was just, that was the whole narrative

4044
02:37:15,279 --> 02:37:17,758
after Llama 2's chat models released.

4045
02:37:17,758 --> 02:37:20,358
And the examples are sorts of things

4046
02:37:20,358 --> 02:37:22,056
like you would ask Llama 2 chat,

4047
02:37:22,056 --> 02:37:23,582
how do you kill a python process?

4048
02:37:23,582 --> 02:37:25,760
And it would say, I can't talk about killing

4049
02:37:25,760 --> 02:37:27,374
because that's a bad thing. - Mm-hmm.

4050
02:37:27,374 --> 02:37:30,320
- And anyone that is trying to design an AI model

4051
02:37:30,320 --> 02:37:32,831
will probably agree that that's just like, eh,

4052
02:37:32,831 --> 02:37:34,676
you messed up a bit on the training there.

4053
02:37:34,676 --> 02:37:35,913
I don't think they meant to do this,

4054
02:37:35,913 --> 02:37:37,111
but this was in the model weight.

4055
02:37:37,111 --> 02:37:39,567
So, this is not, it didn't necessarily be,

4056
02:37:39,567 --> 02:37:42,301
there's things called system prompts which are,

4057
02:37:42,301 --> 02:37:45,650
when you're querying a model, it's a piece of text

4058
02:37:45,650 --> 02:37:48,240
that is shown to the model, but not to the user.

4059
02:37:48,240 --> 02:37:49,920
So, a fun example

4060
02:37:49,920 --> 02:37:52,631
is your system prompt could be talked like a pirate.

4061
02:37:52,631 --> 02:37:54,901
So, no matter what the user says to the model,

4062
02:37:54,901 --> 02:37:56,039
it'll respond like a pirate.

4063
02:37:56,039 --> 02:38:00,061
In practice, what they are is you are a helpful assistant,

4064
02:38:00,061 --> 02:38:01,481
you should break down problems.

4065
02:38:01,481 --> 02:38:02,920
If you don't know about something,

4066
02:38:02,920 --> 02:38:05,411
don't tell them your date cutoff is this,

4067
02:38:05,411 --> 02:38:06,501
today's date is this.

4068
02:38:06,501 --> 02:38:07,991
It's a lot of really useful context

4069
02:38:07,991 --> 02:38:09,686
for how can you answer a question well.

4070
02:38:09,686 --> 02:38:10,849
- And Anthropic publishes

4071
02:38:10,849 --> 02:38:12,730
their system prompt. - Yes, which I think is great.

4072
02:38:12,730 --> 02:38:14,513
And there's a lot of research that goes into this.

4073
02:38:14,513 --> 02:38:17,038
And one of your previous guests, Amanda Askell,

4074
02:38:17,038 --> 02:38:19,380
is probably the most knowledgeable person

4075
02:38:19,380 --> 02:38:22,652
that at least in the combination of execution and sharing,

4076
02:38:22,652 --> 02:38:24,910
she's the person that should talk about system prompts

4077
02:38:24,910 --> 02:38:26,709
and character of models. - Yeah.

4078
02:38:26,709 --> 02:38:28,365
And then, people should read these system prompts,

4079
02:38:28,365 --> 02:38:31,282
'cause you're like, trying to nudge

4080
02:38:33,379 --> 02:38:34,809
sometimes through extreme politeness

4081
02:38:34,809 --> 02:38:36,807
the model to be a certain way.

4082
02:38:36,807 --> 02:38:38,879
- And you could use this for bad things.

4083
02:38:38,879 --> 02:38:39,712
We've done tests

4084
02:38:39,712 --> 02:38:43,564
which is what if I tell the model to be a dumb model?

4085
02:38:43,564 --> 02:38:45,329
which evaluation scores go down,

4086
02:38:45,329 --> 02:38:47,497
and it's like we'll have this behavior

4087
02:38:47,497 --> 02:38:49,760
where it could sometimes say, oh, I'm supposed to be dumb.

4088
02:38:49,760 --> 02:38:50,739
And sometimes it's like

4089
02:38:50,739 --> 02:38:53,497
it doesn't affect math abilities as much,

4090
02:38:53,497 --> 02:38:55,659
but something like a, if you're trying,

4091
02:38:55,659 --> 02:38:57,389
it's just the quality of a human judgment

4092
02:38:57,389 --> 02:38:58,808
would draw through the floors.

4093
02:38:58,808 --> 02:39:00,482
Let's go back to post-training,

4094
02:39:00,482 --> 02:39:02,159
specifically RLHF around Llama 2,

4095
02:39:02,159 --> 02:39:05,748
it was too much safety prioritization

4096
02:39:05,748 --> 02:39:07,219
was baked into the model weights.

4097
02:39:07,219 --> 02:39:08,618
This makes you refuse things

4098
02:39:08,618 --> 02:39:10,377
in a really annoying way for users.

4099
02:39:10,377 --> 02:39:11,297
It's not great.

4100
02:39:11,297 --> 02:39:13,630
It caused a lot of awareness

4101
02:39:15,509 --> 02:39:17,979
to be attached to RLHF that it makes the models dumb-

4102
02:39:17,979 --> 02:39:19,198
- And it stigmatized the word.

4103
02:39:19,198 --> 02:39:21,046
- It did, and AI culture.

4104
02:39:21,046 --> 02:39:23,528
And as the techniques have evolved,

4105
02:39:23,528 --> 02:39:24,784
that's no longer the case,

4106
02:39:24,784 --> 02:39:27,472
where all of these labs have very fine grain control

4107
02:39:27,472 --> 02:39:28,689
over what they get out of the models

4108
02:39:28,689 --> 02:39:30,158
through techniques like RLHF.

4109
02:39:30,158 --> 02:39:33,184
- Although different labs are definitely different levels,

4110
02:39:33,184 --> 02:39:36,219
like on one end of the spectrum is Google,

4111
02:39:36,219 --> 02:39:40,385
and then maybe OpenAI does less and Anthropic does less.

4112
02:39:40,385 --> 02:39:42,626
And then, on the other end of the spectrum is like xAI.

4113
02:39:42,626 --> 02:39:43,493
- Yeah. - But they all

4114
02:39:43,493 --> 02:39:45,424
have different forms of RLHF

4115
02:39:45,424 --> 02:39:47,194
trying to make them a certain way.

4116
02:39:47,194 --> 02:39:49,774
- And the important thing to say

4117
02:39:49,774 --> 02:39:53,333
is that no matter how you want the model to behave,

4118
02:39:53,333 --> 02:39:55,593
these RLHF and preference tuning techniques

4119
02:39:55,593 --> 02:39:57,054
also improve performance.

4120
02:39:57,054 --> 02:39:59,112
So, on things like math evals and code evals,

4121
02:39:59,112 --> 02:40:01,254
there is something innate to these

4122
02:40:01,254 --> 02:40:03,875
what is called contrastive loss functions.

4123
02:40:03,875 --> 02:40:06,196
We could start to get into RL here. We don't really need to.

4124
02:40:06,196 --> 02:40:08,037
But RLHF also boosts performance

4125
02:40:08,037 --> 02:40:10,106
on anything from a chat task

4126
02:40:10,106 --> 02:40:11,795
to a math problem to a code problem.

4127
02:40:11,795 --> 02:40:15,923
So, it is becoming a much more useful tool to these labs.

4128
02:40:15,923 --> 02:40:18,044
So, this takes us through the arc of,

4129
02:40:18,044 --> 02:40:19,821
we've talked about pre-training, hard to get rid of things.

4130
02:40:19,821 --> 02:40:21,414
We've talked about post-training

4131
02:40:21,414 --> 02:40:23,716
and how post-training, you can mess it up.

4132
02:40:23,716 --> 02:40:26,766
It's a complex multifaceted optimization

4133
02:40:26,766 --> 02:40:30,194
with 10 to 100 person teams converging a one artifact.

4134
02:40:30,194 --> 02:40:32,182
It's really easy to not do it perfectly.

4135
02:40:32,182 --> 02:40:33,593
And then, there's the third case,

4136
02:40:33,593 --> 02:40:35,134
which is what we talked about Gemini.

4137
02:40:35,134 --> 02:40:37,012
The thing that was about Gemini

4138
02:40:37,012 --> 02:40:39,526
is this was a served product where Gemini,

4139
02:40:39,526 --> 02:40:40,945
Google has their internal model weights,

4140
02:40:40,945 --> 02:40:42,792
they've done all these processes that we talked about.

4141
02:40:42,792 --> 02:40:44,253
And in the served product,

4142
02:40:44,253 --> 02:40:46,585
what came out after this was that they had a prompt

4143
02:40:46,585 --> 02:40:48,341
that they were rewriting user queries

4144
02:40:48,341 --> 02:40:50,483
to boost diversity or something.

4145
02:40:50,483 --> 02:40:51,622
And this just made it

4146
02:40:51,622 --> 02:40:53,823
that outputs were just blatantly wrong.

4147
02:40:53,823 --> 02:40:56,174
It was a, some sort of organizational failure

4148
02:40:56,174 --> 02:40:57,893
that had this prompt in that position.

4149
02:40:57,893 --> 02:41:01,132
And I think Google executives probably have owned this.

4150
02:41:01,132 --> 02:41:02,972
I don't pay that attention to that detail,

4151
02:41:02,972 --> 02:41:04,824
but it was just a mess up in execution

4152
02:41:04,824 --> 02:41:08,061
that led to this ridiculous thing, but at the system level.

4153
02:41:08,061 --> 02:41:09,694
The model weights might have been fine.

4154
02:41:09,694 --> 02:41:11,633
- So, at the very end of the pipeline,

4155
02:41:11,633 --> 02:41:12,864
there was a rewriting.

4156
02:41:12,864 --> 02:41:14,274
- To a something like a system prompt.

4157
02:41:14,274 --> 02:41:15,885
It was like the system prompt

4158
02:41:15,885 --> 02:41:19,873
or what is called an industry is like you rewrite prompts.

4159
02:41:19,873 --> 02:41:21,284
So, especially for image models,

4160
02:41:21,284 --> 02:41:24,310
if you're using DALL-E or ChatGPT,

4161
02:41:24,310 --> 02:41:25,815
you can generate you an image,

4162
02:41:25,815 --> 02:41:27,941
you'll say draw me a beautiful car.

4163
02:41:27,941 --> 02:41:30,375
With these leading image models,

4164
02:41:30,375 --> 02:41:33,840
they benefit from highly descriptive prompts.

4165
02:41:33,840 --> 02:41:34,673
So, what would happen

4166
02:41:34,673 --> 02:41:36,351
is if you do that on ChatGPT,

4167
02:41:36,351 --> 02:41:39,018
a language model behind the scenes will rewrite the prompt,

4168
02:41:39,018 --> 02:41:40,789
say make this more descriptive,

4169
02:41:40,789 --> 02:41:42,861
and then that is passed to the image model.

4170
02:41:42,861 --> 02:41:43,999
So, prompt rewriting is something

4171
02:41:43,999 --> 02:41:46,300
that is used at multiple levels of industry

4172
02:41:46,300 --> 02:41:48,524
and it's used effectively for image models.

4173
02:41:48,524 --> 02:41:52,254
And the Gemini example is just a failed execution.

4174
02:41:52,254 --> 02:41:53,833
- Big philosophical question here

4175
02:41:53,833 --> 02:41:57,666
with RLHF to generalize, where is human input?

4176
02:42:01,692 --> 02:42:02,525
Human in the loop,

4177
02:42:02,525 --> 02:42:06,421
human data most useful at the current stage.

4178
02:42:06,421 --> 02:42:07,921
- For the past few years,

4179
02:42:07,921 --> 02:42:12,598
the highest cost human data has been in these preferences,

4180
02:42:12,598 --> 02:42:14,423
which is comparing,

4181
02:42:14,423 --> 02:42:17,241
I would say highest cost and highest total usage.

4182
02:42:17,241 --> 02:42:18,074
So, a lot of money

4183
02:42:18,074 --> 02:42:19,961
has gone to these parallelized comparisons

4184
02:42:19,961 --> 02:42:21,381
where you have two model outputs

4185
02:42:21,381 --> 02:42:24,714
and a human is comparing between the two of them.

4186
02:42:24,714 --> 02:42:25,650
In earlier years,

4187
02:42:25,650 --> 02:42:28,062
there was a lot of this instruction tuning data.

4188
02:42:28,062 --> 02:42:31,259
So, creating highly specific examples

4189
02:42:31,259 --> 02:42:32,729
to something like a Reddit question

4190
02:42:32,729 --> 02:42:34,372
to a domain that you care about.

4191
02:42:34,372 --> 02:42:36,491
Language models used to struggle on math and code.

4192
02:42:36,491 --> 02:42:38,309
So, you would pay experts in math

4193
02:42:38,309 --> 02:42:39,721
and code to come up with questions,

4194
02:42:39,721 --> 02:42:41,132
and write detailed answers

4195
02:42:41,132 --> 02:42:42,673
that were used to train the models.

4196
02:42:42,673 --> 02:42:46,931
Now, it is the case that there are many model options

4197
02:42:46,931 --> 02:42:49,807
that are way better than humans at writing detailed

4198
02:42:49,807 --> 02:42:53,038
and eloquent answers for things like model and code.

4199
02:42:53,038 --> 02:42:55,645
So, they talked about this with the Llama 3 release,

4200
02:42:55,645 --> 02:42:58,710
where they switched to using Llama 3, 4, or 5b

4201
02:42:58,710 --> 02:43:01,058
to write their answers for math and code.

4202
02:43:01,058 --> 02:43:03,269
But they, in their paper,

4203
02:43:03,269 --> 02:43:06,157
talk about how they use extensive human preference data,

4204
02:43:06,157 --> 02:43:08,819
which is something that they haven't gotten AIs to replace.

4205
02:43:08,819 --> 02:43:10,288
There are other techniques in industry

4206
02:43:10,288 --> 02:43:11,286
like constitutional AI,

4207
02:43:11,286 --> 02:43:13,057
where you use human data for preferences

4208
02:43:13,057 --> 02:43:14,517
and AI for preferences.

4209
02:43:14,517 --> 02:43:16,038
And I expect the AI part

4210
02:43:16,038 --> 02:43:17,988
to scale faster than the human part.

4211
02:43:17,988 --> 02:43:20,388
But among the research that we have access to

4212
02:43:20,388 --> 02:43:24,458
is that humans are in this kind of preference loop.

4213
02:43:24,458 --> 02:43:28,045
- So, as reasoning becomes bigger and bigger and bigger,

4214
02:43:28,045 --> 02:43:31,238
as we said, where's the role of humans in that?

4215
02:43:31,238 --> 02:43:33,078
- It's even less prevalent.

4216
02:43:33,078 --> 02:43:36,610
So, it's the remarkable thing about these reasoning results

4217
02:43:36,610 --> 02:43:39,638
and especially the DeepSeek-R1 paper is this result

4218
02:43:39,638 --> 02:43:41,690
that they call DeepSeek-R1-Zero,

4219
02:43:41,690 --> 02:43:43,449
which is they took one of these pre-trained models,

4220
02:43:43,449 --> 02:43:45,257
they took DeepSeek-V3 base,

4221
02:43:45,257 --> 02:43:48,360
and then they do this reinforcement learning optimization

4222
02:43:48,360 --> 02:43:51,097
on verifiable questions or verifiable rewards

4223
02:43:51,097 --> 02:43:54,069
for a lot of questions and a lot of training.

4224
02:43:54,069 --> 02:43:56,768
And these reasoning behaviors emerge naturally.

4225
02:43:56,768 --> 02:43:58,449
So, these things like, wait, let me see,

4226
02:43:58,449 --> 02:43:59,798
wait, let me check this.

4227
02:43:59,798 --> 02:44:01,206
Oh, that might be a mistake.

4228
02:44:01,206 --> 02:44:05,029
And they emerge from only having questions and answers.

4229
02:44:05,029 --> 02:44:06,439
And when you're using the model,

4230
02:44:06,439 --> 02:44:08,425
the part that you look at is the completion.

4231
02:44:08,425 --> 02:44:09,585
So, in this case,

4232
02:44:09,585 --> 02:44:13,608
all of that just emerges from this large-scale RL training.

4233
02:44:13,608 --> 02:44:16,357
And that model, which the weights are available,

4234
02:44:16,357 --> 02:44:20,328
has no human preferences added into the post-training.

4235
02:44:20,328 --> 02:44:23,059
There are the DeepSeek-R1 full model

4236
02:44:23,059 --> 02:44:25,187
has some of this human preference tuning,

4237
02:44:25,187 --> 02:44:27,828
this RLHF after the reasoning stage.

4238
02:44:27,828 --> 02:44:29,247
But the very remarkable thing

4239
02:44:29,247 --> 02:44:31,317
is that you can get these reasoning behaviors,

4240
02:44:31,317 --> 02:44:32,777
and it's very unlikely

4241
02:44:32,777 --> 02:44:35,180
that there's humans writing out reasoning chains.

4242
02:44:35,180 --> 02:44:37,409
It's very unlikely that they somehow hacked OpenAI

4243
02:44:37,409 --> 02:44:39,208
and they got access to OpenAI.

4244
02:44:39,208 --> 02:44:40,387
o1's reasoning chains,

4245
02:44:40,387 --> 02:44:43,646
it's something about the pre-trained language models

4246
02:44:43,646 --> 02:44:46,189
and this RL training where you reward the model

4247
02:44:46,189 --> 02:44:47,916
for getting the question right.

4248
02:44:47,916 --> 02:44:49,849
And therefore, it's trying multiple solutions

4249
02:44:49,849 --> 02:44:52,859
and it emerges this chain of thought.

4250
02:44:52,859 --> 02:44:57,488
- This might be a good place to mention the eloquent

4251
02:44:57,488 --> 02:45:00,254
and the insightful tweet of the great

4252
02:45:00,254 --> 02:45:02,588
and the powerful Andrej Karpathy.

4253
02:45:02,588 --> 02:45:03,751
I think he had a bunch of thoughts

4254
02:45:03,751 --> 02:45:05,616
but one of them, "Last thought,

4255
02:45:05,616 --> 02:45:07,000
not sure if this is obvious."

4256
02:45:07,000 --> 02:45:08,648
You know something profound is coming

4257
02:45:08,648 --> 02:45:11,675
when you're saying it's not sure if it's obvious.

4258
02:45:11,675 --> 02:45:13,445
"There are two major types of learning,

4259
02:45:13,445 --> 02:45:15,608
in both children and in deep learning.

4260
02:45:15,608 --> 02:45:18,963
There is, one, imitation learning, watch and repeat,

4261
02:45:18,963 --> 02:45:21,393
i.e, pre-training, supervised fine-tuning,

4262
02:45:21,393 --> 02:45:25,641
and, two, trial-and-error learning, reinforcement learning.

4263
02:45:25,641 --> 02:45:27,623
My favorite simple example is AlphaGo.

4264
02:45:27,623 --> 02:45:31,144
One is learning by imitating expert players.

4265
02:45:31,144 --> 02:45:33,505
Two is reinforcement learning to win the game.

4266
02:45:33,505 --> 02:45:36,665
Almost every single shocking result of deep learning,

4267
02:45:36,665 --> 02:45:40,648
and the source of all magic is always two.

4268
02:45:40,648 --> 02:45:42,776
Two is significantly more powerful.

4269
02:45:42,776 --> 02:45:44,847
Two is what surprises you.

4270
02:45:44,847 --> 02:45:47,706
Two is when the paddle learns to hit the ball

4271
02:45:47,706 --> 02:45:49,282
behind the blocks and break out.

4272
02:45:49,282 --> 02:45:52,272
Two is when AlphaGo beats even Lee Sedol.

4273
02:45:52,272 --> 02:45:56,144
And two is the aha moment when the DeepSeek,

4274
02:45:56,144 --> 02:45:59,873
or o1, et cetera, discovers that it works well

4275
02:45:59,873 --> 02:46:02,273
to reevaluate your assumptions,

4276
02:46:02,273 --> 02:46:04,861
backtrack, try something else, et cetera.

4277
02:46:04,861 --> 02:46:06,085
It's the solving strategies

4278
02:46:06,085 --> 02:46:09,733
you see this model use in its chain of thought.

4279
02:46:09,733 --> 02:46:13,182
It's how it goes back and forth thinking to itself.

4280
02:46:13,182 --> 02:46:17,221
These thoughts are emergent," three exclamation points,

4281
02:46:17,221 --> 02:46:20,782
"and this is actually seriously incredible, impressive,

4282
02:46:20,782 --> 02:46:24,544
and new, and is publicly available and documented.

4283
02:46:24,544 --> 02:46:28,268
The model could never learn this with the imitation,

4284
02:46:28,268 --> 02:46:30,581
because the cognition of the model

4285
02:46:30,581 --> 02:46:33,226
and the cognition of the human labeler is different.

4286
02:46:33,226 --> 02:46:34,499
The human would never know

4287
02:46:34,499 --> 02:46:37,251
to correctly annotate these kinds of solving strategies

4288
02:46:37,251 --> 02:46:39,840
and what they should even look like.

4289
02:46:39,840 --> 02:46:42,290
They have to be discovered during reinforcement learning

4290
02:46:42,290 --> 02:46:43,958
as empirically statistically useful

4291
02:46:43,958 --> 02:46:45,407
towards the final outcome."

4292
02:46:45,407 --> 02:46:49,587
Anyway, the AlphaZero sort of metaphor analogy here,

4293
02:46:49,587 --> 02:46:51,244
can you speak to that?

4294
02:46:51,244 --> 02:46:52,475
- Yeah. - The magic of the chain

4295
02:46:52,475 --> 02:46:54,010
of thought that he's referring to?

4296
02:46:54,010 --> 02:46:56,657
- I think it's good to recap AlphaGo and AlphaZero

4297
02:46:56,657 --> 02:46:58,648
because it plays nicely with these analogies

4298
02:46:58,648 --> 02:47:00,672
between imitation learning and learning from scratch.

4299
02:47:00,672 --> 02:47:03,588
So, AlphaGo, the beginning of the process

4300
02:47:03,588 --> 02:47:05,227
was learning from humans where they had,

4301
02:47:05,227 --> 02:47:06,857
they started the first,

4302
02:47:06,857 --> 02:47:09,608
this is the first expert level go player

4303
02:47:09,608 --> 02:47:11,781
or chess player in DeepMind series of models,

4304
02:47:11,781 --> 02:47:12,949
where they had some human data.

4305
02:47:12,949 --> 02:47:15,587
And then, why it is called AlphaZero

4306
02:47:15,587 --> 02:47:17,900
is that there was zero human data in the loop,

4307
02:47:17,900 --> 02:47:20,678
and that changed to AlphaZero made a model

4308
02:47:20,678 --> 02:47:23,169
that was dramatically more powerful for DeepMind.

4309
02:47:23,169 --> 02:47:25,878
So, this remove of the human prior,

4310
02:47:25,878 --> 02:47:27,450
the human inductive bias,

4311
02:47:27,450 --> 02:47:29,637
makes the final system far more powerful.

4312
02:47:29,637 --> 02:47:32,268
This, we mentioned, bitter lesson hours ago

4313
02:47:32,268 --> 02:47:35,020
and this is all aligned with this.

4314
02:47:35,020 --> 02:47:37,637
And then, there's been a lot

4315
02:47:37,637 --> 02:47:39,661
of discussion in language models.

4316
02:47:39,661 --> 02:47:40,494
This is not new.

4317
02:47:40,494 --> 02:47:42,900
This goes back to the whole QStar rumors,

4318
02:47:42,900 --> 02:47:45,878
which if you piece together the pieces

4319
02:47:45,878 --> 02:47:48,040
is probably the start of OpenAI

4320
02:47:48,040 --> 02:47:50,966
figuring out its o1 stuff when last year in November.

4321
02:47:50,966 --> 02:47:52,449
the QStar rumors came out.

4322
02:47:52,449 --> 02:47:55,366
There's a lot of intellectual drive

4323
02:47:57,021 --> 02:47:58,649
to know when is something like this

4324
02:47:58,649 --> 02:48:00,286
going to happen with language models?

4325
02:48:00,286 --> 02:48:01,839
Because we know these models are so powerful

4326
02:48:01,839 --> 02:48:04,619
and we know it has been so successful in the past.

4327
02:48:04,619 --> 02:48:09,159
And it is a reasonable analogy that this new type

4328
02:48:09,159 --> 02:48:10,620
of reinforcement learning training

4329
02:48:10,620 --> 02:48:14,598
for reasoning models is when the doors open to this.

4330
02:48:14,598 --> 02:48:17,701
We don't yet have the equivalent of turn 37,

4331
02:48:17,701 --> 02:48:18,919
which is the famous turn

4332
02:48:18,919 --> 02:48:22,028
where the DeepMind's AI playing ghost

4333
02:48:22,028 --> 02:48:24,269
dumped Lee Sedol completely.

4334
02:48:24,269 --> 02:48:27,221
We don't have something that's that level of focal point,

4335
02:48:27,221 --> 02:48:28,523
but that doesn't mean that the approach

4336
02:48:28,523 --> 02:48:29,398
to technology is different

4337
02:48:29,398 --> 02:48:30,829
and the impact of the general training,

4338
02:48:30,829 --> 02:48:32,735
it's still incredibly new.

4339
02:48:32,735 --> 02:48:34,245
- What do you think that point would be?

4340
02:48:34,245 --> 02:48:37,626
What would be Move 37 for chain of thought, for reasoning?

4341
02:48:37,626 --> 02:48:38,882
- Scientific discovery.

4342
02:48:38,882 --> 02:48:40,792
When you use this sort of reasoning problem

4343
02:48:40,792 --> 02:48:43,515
and it just something we fully don't expect.

4344
02:48:43,515 --> 02:48:46,030
- I think it's actually probably simpler than that.

4345
02:48:46,030 --> 02:48:48,892
It's probably something related to computer user robotics

4346
02:48:48,892 --> 02:48:51,351
rather than science discovery.

4347
02:48:51,351 --> 02:48:53,722
Because the important aspect here

4348
02:48:53,722 --> 02:48:57,187
is models take so much data to learn.

4349
02:48:57,187 --> 02:48:59,002
They're not sample-efficient.

4350
02:48:59,002 --> 02:49:01,432
Trillions, they take the entire web

4351
02:49:01,432 --> 02:49:05,090
over 10 trillion tokens to train on.

4352
02:49:05,090 --> 02:49:09,262
This would take a human thousands of years to read.

4353
02:49:09,262 --> 02:49:13,012
A human does not, and humans know most of the stuff,

4354
02:49:13,012 --> 02:49:14,861
a lot of the stuff models know better than it.

4355
02:49:14,861 --> 02:49:17,111
Humans are way, way, way more sample-efficient.

4356
02:49:17,111 --> 02:49:18,821
And that is because of the self-play.

4357
02:49:18,821 --> 02:49:21,598
How does a baby learn what its body is

4358
02:49:21,598 --> 02:49:23,641
as it sticks its foot in its mouth

4359
02:49:23,641 --> 02:49:26,236
and it says, oh, this is my body.

4360
02:49:26,236 --> 02:49:27,951
It sticks its hand in its mouth

4361
02:49:27,951 --> 02:49:30,221
and it calibrates its touch on its fingers

4362
02:49:30,221 --> 02:49:32,334
with the most sensitive touch thing on its tongue.

4363
02:49:32,334 --> 02:49:33,533
Like is how babies learn.

4364
02:49:33,533 --> 02:49:35,570
And it's just self-play

4365
02:49:35,570 --> 02:49:37,413
over and over and over and over again.

4366
02:49:37,413 --> 02:49:40,860
And now, we have something that is similar to that

4367
02:49:40,860 --> 02:49:44,082
with these verifiable proofs,

4368
02:49:44,082 --> 02:49:45,642
whether it's a unit test and code

4369
02:49:45,642 --> 02:49:49,061
or a mathematical verifiable task,

4370
02:49:49,061 --> 02:49:51,981
generate many traces of reasoning.

4371
02:49:51,981 --> 02:49:54,100
And keep branching them out, keep branching them out.

4372
02:49:54,100 --> 02:49:55,569
And then, check at the end, hey,

4373
02:49:55,569 --> 02:49:56,611
which one actually has the right answer?

4374
02:49:56,611 --> 02:49:57,801
Most of 'em are wrong. Great.

4375
02:49:57,801 --> 02:49:58,799
These are the few that are right.

4376
02:49:58,799 --> 02:50:00,712
Maybe we use some sort of reward model outside of this

4377
02:50:00,712 --> 02:50:03,330
to select even the best one to preference as well.

4378
02:50:03,330 --> 02:50:04,899
But now, you've started to get better

4379
02:50:04,899 --> 02:50:06,302
and better at these benchmarks.

4380
02:50:06,302 --> 02:50:08,203
And so, you've seen over the last six months

4381
02:50:08,203 --> 02:50:11,460
a skyrocketing in a lot of different benchmarks.

4382
02:50:11,460 --> 02:50:13,919
- All math and code benchmarks were pretty much solved

4383
02:50:13,919 --> 02:50:14,853
except for frontier math,

4384
02:50:14,853 --> 02:50:17,401
which is designed to be almost questions

4385
02:50:17,401 --> 02:50:19,219
that aren't practical to most people,

4386
02:50:19,219 --> 02:50:24,219
'cause they're exam level, open math problem type things.

4387
02:50:24,719 --> 02:50:26,680
So, it's like on the math problems

4388
02:50:26,680 --> 02:50:27,706
that are somewhat reasonable,

4389
02:50:27,706 --> 02:50:29,679
which is like somewhat complicated word problems

4390
02:50:29,679 --> 02:50:30,819
or coding problems.

4391
02:50:30,819 --> 02:50:32,526
It's just what Dylan is saying.

4392
02:50:32,526 --> 02:50:35,249
- So, the thing here is that

4393
02:50:35,249 --> 02:50:36,839
these are only with verifiable tasks.

4394
02:50:36,839 --> 02:50:39,890
We earlier showed an example of the really interesting,

4395
02:50:39,890 --> 02:50:41,100
like what happens when chain of thought

4396
02:50:41,100 --> 02:50:42,707
is to a non-verifiable thing.

4397
02:50:42,707 --> 02:50:45,370
It's just like a human chatting with a,

4398
02:50:45,370 --> 02:50:48,018
thinking about what's novel for humans, a unique thought.

4399
02:50:48,018 --> 02:50:50,701
But this task and form of training

4400
02:50:50,701 --> 02:50:53,123
only works when it's verifiable.

4401
02:50:53,123 --> 02:50:55,583
And from here, the thought is, okay,

4402
02:50:55,583 --> 02:50:58,193
we can continue to scale this current training method

4403
02:50:58,193 --> 02:51:01,423
by increasing the number of verifiable tasks.

4404
02:51:01,423 --> 02:51:04,030
In math and coding, coding probably has a lot more to go.

4405
02:51:04,030 --> 02:51:05,913
Math has a lot less to go

4406
02:51:05,913 --> 02:51:07,623
in terms of what are verifiable things.

4407
02:51:07,623 --> 02:51:08,514
Can I create a solver

4408
02:51:08,514 --> 02:51:11,193
that then I generate trajectories toward,

4409
02:51:11,193 --> 02:51:12,971
or traces towards, reasoning traces towards,

4410
02:51:12,971 --> 02:51:14,612
and then prune the ones that don't work

4411
02:51:14,612 --> 02:51:15,871
and keep the ones that do work?

4412
02:51:15,871 --> 02:51:17,842
Well, those are gonna be solved pretty quickly.

4413
02:51:17,842 --> 02:51:18,853
But even if you've solved math,

4414
02:51:18,853 --> 02:51:22,250
you have not actually created intelligence.

4415
02:51:22,250 --> 02:51:25,456
And so, this is where I think the aha moment

4416
02:51:25,456 --> 02:51:27,801
of computer use or robotics will come in,

4417
02:51:27,801 --> 02:51:30,942
because now you have a sandbox

4418
02:51:30,942 --> 02:51:34,652
or a playground that is infinitely verifiable.

4419
02:51:34,652 --> 02:51:36,751
Did you, messing around on the internet,

4420
02:51:36,751 --> 02:51:37,774
there are so many actions

4421
02:51:37,774 --> 02:51:39,106
that you can do that are verifiable.

4422
02:51:39,106 --> 02:51:41,180
It'll start off with log into a website,

4423
02:51:41,180 --> 02:51:43,512
create an account, click a button here, blah, blah, blah.

4424
02:51:43,512 --> 02:51:44,919
But it'll then get to the point,

4425
02:51:44,919 --> 02:51:47,451
where it's, hey, go do a task on Tasker

4426
02:51:47,451 --> 02:51:49,280
or whatever these other, all these various task websites.

4427
02:51:49,280 --> 02:51:51,828
Hey, go get hundreds of likes.

4428
02:51:51,828 --> 02:51:53,268
And it's gonna fail.

4429
02:51:53,268 --> 02:51:54,371
It's gonna spawn hundreds of accounts,

4430
02:51:54,371 --> 02:51:55,520
it's gonna fail on most of them,

4431
02:51:55,520 --> 02:51:57,093
but this one got to a thousand, great.

4432
02:51:57,093 --> 02:51:58,698
Now, you've reached the verifiable thing.

4433
02:51:58,698 --> 02:52:00,971
And you just keep iterating this loop over and over.

4434
02:52:00,971 --> 02:52:01,839
And that's when...

4435
02:52:01,839 --> 02:52:02,672
And same with robotics.

4436
02:52:02,672 --> 02:52:06,210
That's where you have an infinite playground of tasks like,

4437
02:52:06,210 --> 02:52:07,618
hey, did I put the ball in the bucket,

4438
02:52:07,618 --> 02:52:09,658
all the way to like, oh, did I build a car?

4439
02:52:09,658 --> 02:52:11,948
There's a whole trajectory

4440
02:52:11,948 --> 02:52:14,827
to speed run or what models can do.

4441
02:52:14,827 --> 02:52:17,599
But at some point, I truly think that like,

4442
02:52:17,599 --> 02:52:19,247
we'll spawn models,

4443
02:52:19,247 --> 02:52:21,178
and initially, all the training will be in sandboxes,

4444
02:52:21,178 --> 02:52:22,396
but then at some point,

4445
02:52:22,396 --> 02:52:23,877
the language model pre-training

4446
02:52:23,877 --> 02:52:27,356
is gonna be dwarfed by what is this reinforcement learning?

4447
02:52:27,356 --> 02:52:29,238
You'll pre-train a multimodal model

4448
02:52:29,238 --> 02:52:31,438
that can see, that can read, that can write,

4449
02:52:31,438 --> 02:52:34,106
blah, blah, blah, whatever, vision, audio, et cetera,

4450
02:52:34,106 --> 02:52:38,045
but then you'll have it play in a sandbox infinitely,

4451
02:52:38,045 --> 02:52:40,169
and figure out math, figure out code,

4452
02:52:40,169 --> 02:52:41,237
figure out navigating the web,

4453
02:52:41,237 --> 02:52:42,929
figure out operating a robot arm.

4454
02:52:42,929 --> 02:52:45,536
And then, it'll learn so much.

4455
02:52:45,536 --> 02:52:48,647
And the aha moment I think will be when this is available

4456
02:52:48,647 --> 02:52:50,915
to then create something that's not good.

4457
02:52:50,915 --> 02:52:51,748
Like, oh cool.

4458
02:52:51,748 --> 02:52:53,198
Part of it was figuring out how to use the web.

4459
02:52:53,198 --> 02:52:56,136
Now, all of a sudden, it's figured out really well

4460
02:52:56,136 --> 02:52:58,336
how to just get hundreds of thousands of followers

4461
02:52:58,336 --> 02:53:00,048
that are real and real engagement on Twitter,

4462
02:53:00,048 --> 02:53:00,881
because all of a sudden,

4463
02:53:00,881 --> 02:53:02,690
this is one of the things that are verifiable.

4464
02:53:02,690 --> 02:53:05,355
- And maybe not just engagement, but make money.

4465
02:53:05,355 --> 02:53:06,756
- Yes, of course. - I become an...

4466
02:53:06,756 --> 02:53:08,174
I mean, that could be the thing

4467
02:53:08,174 --> 02:53:10,242
where almost fully automated,

4468
02:53:10,242 --> 02:53:13,909
it makes $10 million by being an influencer,

4469
02:53:14,977 --> 02:53:17,607
selling a product, creating the product.

4470
02:53:17,607 --> 02:53:20,566
And I'm not referring to like a hype product,

4471
02:53:20,566 --> 02:53:21,605
but an actual product

4472
02:53:21,605 --> 02:53:25,285
or like, holy shit, this thing created a business.

4473
02:53:25,285 --> 02:53:27,767
It's running it, it's the face of the business.

4474
02:53:27,767 --> 02:53:28,655
That kind of thing.

4475
02:53:28,655 --> 02:53:31,526
Or maybe a number one song,

4476
02:53:31,526 --> 02:53:33,627
like it creates the whole infrastructure

4477
02:53:33,627 --> 02:53:36,394
required to create the song, to be the influence

4478
02:53:36,394 --> 02:53:38,116
that represents that song, that kind of thing.

4479
02:53:38,116 --> 02:53:39,229
And makes a lot of...

4480
02:53:39,229 --> 02:53:40,708
That could be the mo...

4481
02:53:40,708 --> 02:53:44,601
I mean, our culture respects money in that kind of way.

4482
02:53:44,601 --> 02:53:46,108
- And it's verifiable, right?

4483
02:53:46,108 --> 02:53:46,941
- [Lex] It's verifiable.

4484
02:53:46,941 --> 02:53:49,394
- The bank account can't lie. - Exactly.

4485
02:53:49,394 --> 02:53:53,062
- There is surprising evidence that once you set up the ways

4486
02:53:53,062 --> 02:53:55,875
of collecting the verifiable domain that this can work.

4487
02:53:55,875 --> 02:53:57,721
There's been a lot of research

4488
02:53:57,721 --> 02:54:00,673
before this R1 on math problems,

4489
02:54:00,673 --> 02:54:02,794
and they approach math with language models

4490
02:54:02,794 --> 02:54:04,913
just by increasing the number of samples.

4491
02:54:04,913 --> 02:54:06,449
So, you can just try again and again and again.

4492
02:54:06,449 --> 02:54:09,093
And you look at the amount of times

4493
02:54:09,093 --> 02:54:10,572
that the language models get it right.

4494
02:54:10,572 --> 02:54:11,822
And what we see

4495
02:54:12,932 --> 02:54:15,722
is that even very bad models get it right sometimes.

4496
02:54:15,722 --> 02:54:18,160
And the whole idea behind reinforcement learning

4497
02:54:18,160 --> 02:54:20,404
is that you can learn from very sparse rewards.

4498
02:54:20,404 --> 02:54:24,926
So, the space of language and the space of tokens,

4499
02:54:24,926 --> 02:54:25,806
whether you're generating language

4500
02:54:25,806 --> 02:54:27,846
or tasks for a robot is so big

4501
02:54:27,846 --> 02:54:30,422
that you might say that it's like,

4502
02:54:30,422 --> 02:54:32,186
the tokenizer for a language model

4503
02:54:32,186 --> 02:54:33,425
can be like 200,000 things.

4504
02:54:33,425 --> 02:54:36,264
So, at each step, it can sample from that big of a space.

4505
02:54:36,264 --> 02:54:38,688
So, if it can generate a bit of a signal

4506
02:54:38,688 --> 02:54:40,113
that it can climb onto,

4507
02:54:40,113 --> 02:54:42,957
that's the whole field of RL is around,

4508
02:54:42,957 --> 02:54:45,195
is learning from sparse rewards.

4509
02:54:45,195 --> 02:54:46,957
And the same thing has played out in math

4510
02:54:46,957 --> 02:54:48,408
where it's like very weak models

4511
02:54:48,408 --> 02:54:49,825
that sometimes generate answers,

4512
02:54:49,825 --> 02:54:51,418
where you see research already

4513
02:54:51,418 --> 02:54:53,427
that you can boost their math scores,

4514
02:54:53,427 --> 02:54:56,555
you can do this sort of RL training for math.

4515
02:54:56,555 --> 02:54:57,896
It might not be as effective,

4516
02:54:57,896 --> 02:54:59,917
but if you take a 1 billion parameter model,

4517
02:54:59,917 --> 02:55:02,507
so something 600 times smaller than DeepSeek,

4518
02:55:02,507 --> 02:55:04,955
you can boost its grade school math scores

4519
02:55:04,955 --> 02:55:07,537
very directly with a small amount of this training.

4520
02:55:07,537 --> 02:55:10,548
So, it's not to say that this is coming soon,

4521
02:55:10,548 --> 02:55:13,348
setting up the verification domains is extremely hard,

4522
02:55:13,348 --> 02:55:15,680
and there's a lot of nuance in this,

4523
02:55:15,680 --> 02:55:18,266
but there are some basic things that we have seen before,

4524
02:55:18,266 --> 02:55:22,077
where it's at least expectable that there's a domain

4525
02:55:22,077 --> 02:55:23,923
and there's a chance that this works.

4526
02:55:23,923 --> 02:55:26,099
- All right, so we have fun things happening in real time.

4527
02:55:26,099 --> 02:55:27,126
This is a good opportunity

4528
02:55:27,126 --> 02:55:30,876
to talk about other reasoning models, o1, o3.

4529
02:55:32,237 --> 02:55:36,820
Just now, OpenAI as perhaps expected, released o3-mini.

4530
02:55:38,647 --> 02:55:41,302
What are we expecting from the different flavors?

4531
02:55:41,302 --> 02:55:44,542
Can you just lay out the different flavors of the o models

4532
02:55:44,542 --> 02:55:47,481
and from Gemini, the reasoning model?

4533
02:55:47,481 --> 02:55:49,324
- Something I would say about these reasoning models

4534
02:55:49,324 --> 02:55:50,157
is we talked a lot

4535
02:55:50,157 --> 02:55:52,493
about reasoning training on math and code.

4536
02:55:52,493 --> 02:55:55,125
And what is done is that you have the base model,

4537
02:55:55,125 --> 02:55:56,673
we've talked about a lot on the internet.

4538
02:55:56,673 --> 02:55:58,766
You do this large-scale reasoning training

4539
02:55:58,766 --> 02:55:59,994
with reinforcement learning.

4540
02:55:59,994 --> 02:56:04,580
And then, what the DeepSeek paper detailed in this R1 paper,

4541
02:56:04,580 --> 02:56:07,044
which for me is one of the big open questions

4542
02:56:07,044 --> 02:56:10,697
on how do you do this, is that they did reasoning-heavy,

4543
02:56:10,697 --> 02:56:13,187
but very standard post-training techniques

4544
02:56:13,187 --> 02:56:15,378
after the large-scale reasoning RL.

4545
02:56:15,378 --> 02:56:17,299
So, they did the same things with a form

4546
02:56:17,299 --> 02:56:19,958
of instruction tuning through rejection sampling,

4547
02:56:19,958 --> 02:56:22,726
which is essentially heavily filtered instruction tuning

4548
02:56:22,726 --> 02:56:24,167
with some reward models.

4549
02:56:24,167 --> 02:56:27,209
And then, they did this RLHF, but they made it math-heavy.

4550
02:56:27,209 --> 02:56:29,485
So, some of this transfer,

4551
02:56:29,485 --> 02:56:33,539
we've looked at this philosophical example early on.

4552
02:56:33,539 --> 02:56:34,947
One of the big open questions

4553
02:56:34,947 --> 02:56:37,158
is how much does this transfer?

4554
02:56:37,158 --> 02:56:40,067
If we bring in domains after the reasoning training,

4555
02:56:40,067 --> 02:56:41,019
are all the models

4556
02:56:41,019 --> 02:56:43,579
gonna be become eloquent writers by reasoning?

4557
02:56:43,579 --> 02:56:45,213
Is this philosophy stuff gonna be open?

4558
02:56:45,213 --> 02:56:46,141
We don't know in the research

4559
02:56:46,141 --> 02:56:47,565
of how much this will transfer.

4560
02:56:47,565 --> 02:56:50,498
There's other things about how we can make soft verifiers

4561
02:56:50,498 --> 02:56:51,369
and things like this.

4562
02:56:51,369 --> 02:56:53,786
But there is more training after reasoning,

4563
02:56:53,786 --> 02:56:56,547
which makes it easier to use these reasoning models.

4564
02:56:56,547 --> 02:56:58,017
And that's what we're using right now.

4565
02:56:58,017 --> 02:57:00,037
So, if we're gonna talk about o3-mini and o1,

4566
02:57:00,037 --> 02:57:02,251
these have gone through these extra techniques

4567
02:57:02,251 --> 02:57:03,990
that are designed for human preferences

4568
02:57:03,990 --> 02:57:06,680
after being trained to elicit reasoning.

4569
02:57:06,680 --> 02:57:09,351
- I think one of the things that people are ignoring

4570
02:57:09,351 --> 02:57:12,230
is Google's Gemini Flash Thinking

4571
02:57:12,230 --> 02:57:13,390
- Yeah. - is both cheaper

4572
02:57:13,390 --> 02:57:15,591
than R1 and better. - Yeah.

4573
02:57:15,591 --> 02:57:17,291
- And they released it in the beginning of December.

4574
02:57:17,291 --> 02:57:18,510
- [Lex] And nobody's talking about it.

4575
02:57:18,510 --> 02:57:19,343
- No one cares. - It has

4576
02:57:19,343 --> 02:57:20,289
a different flavor to it.

4577
02:57:20,289 --> 02:57:22,617
Its behavior is less expressive than something like o1

4578
02:57:22,617 --> 02:57:25,752
or it has fewer tracks than it is on.

4579
02:57:25,752 --> 02:57:28,823
Qwen released a model last fall, QWQ,

4580
02:57:28,823 --> 02:57:31,060
which was their preview reasoning model.

4581
02:57:31,060 --> 02:57:33,492
And DeepSeek had R1-Lite last fall,

4582
02:57:33,492 --> 02:57:35,638
where these models felt like they're on rails,

4583
02:57:35,638 --> 02:57:37,843
where they really, really only can do math and code.

4584
02:57:37,843 --> 02:57:41,142
And o1 is, it can answer anything.

4585
02:57:41,142 --> 02:57:43,195
It might not be perfect for some tasks,

4586
02:57:43,195 --> 02:57:46,771
but it's flexible, it has some richness to it.

4587
02:57:46,771 --> 02:57:48,376
And this is the art

4588
02:57:48,376 --> 02:57:51,926
of how is a model a little bit undercooked?

4589
02:57:51,926 --> 02:57:53,607
It's like, it's good to get a model out the door,

4590
02:57:53,607 --> 02:57:55,967
but it's hard to gauge

4591
02:57:55,967 --> 02:57:57,688
and it takes a lot of taste to be like,

4592
02:57:57,688 --> 02:58:00,158
is this a full-fledged model?

4593
02:58:00,158 --> 02:58:01,571
Can I use this for everything?

4594
02:58:01,571 --> 02:58:03,857
And they're probably more similar for math and code.

4595
02:58:03,857 --> 02:58:05,768
My quick read is that Gemini Flash

4596
02:58:05,768 --> 02:58:08,601
is not trained the same way as o1,

4597
02:58:10,229 --> 02:58:13,239
but taking an existing training stack,

4598
02:58:13,239 --> 02:58:14,658
adding reasoning to it.

4599
02:58:14,658 --> 02:58:16,069
So, taking a more normal training stack

4600
02:58:16,069 --> 02:58:17,499
and adding reasoning to it.

4601
02:58:17,499 --> 02:58:19,509
And I'm sure they're gonna have more,

4602
02:58:19,509 --> 02:58:20,608
I mean, they've done quick releases

4603
02:58:20,608 --> 02:58:22,768
on Gemini Flash, the reasoning,

4604
02:58:22,768 --> 02:58:26,195
and this is the second version from the holidays.

4605
02:58:26,195 --> 02:58:27,285
It's evolving fast

4606
02:58:27,285 --> 02:58:30,918
and it takes longer to make this training stack

4607
02:58:30,918 --> 02:58:31,751
where you're doing

4608
02:58:31,751 --> 02:58:32,982
this large-scale RL. - Ask it the same question

4609
02:58:32,982 --> 02:58:35,277
from earlier, the one about the-

4610
02:58:35,277 --> 02:58:37,672
- The human nature. - Yeah.

4611
02:58:37,672 --> 02:58:40,788
- [Lex] What was the human nature one?

4612
02:58:40,788 --> 02:58:43,342
- The way I can ramble, why I can ramble about this so much

4613
02:58:43,342 --> 02:58:44,982
is that we've been working

4614
02:58:44,982 --> 02:58:49,401
on this at AI2 before o1 was fully available to everyone,

4615
02:58:49,401 --> 02:58:50,234
and before R1,

4616
02:58:50,234 --> 02:58:53,522
which is essentially using this RL training for fine-tuning.

4617
02:58:53,522 --> 02:58:56,384
We use this in our Tulu series of models.

4618
02:58:56,384 --> 02:58:58,564
And you can elicit the same behaviors

4619
02:58:58,564 --> 02:59:01,394
where you say like weight and so much on,

4620
02:59:01,394 --> 02:59:03,205
but it's subtle late in the training process

4621
02:59:03,205 --> 02:59:06,772
that this kind of reasoning expression is much lighter.

4622
02:59:06,772 --> 02:59:08,505
So, there's essentially a gradation

4623
02:59:08,505 --> 02:59:10,217
and just how much of this RL training

4624
02:59:10,217 --> 02:59:13,216
you put into it determines how the output looks.

4625
02:59:13,216 --> 02:59:16,594
- So, we're now using Gemini 2.0

4626
02:59:16,594 --> 02:59:19,427
Flash Thinking Experimental 01-21.

4627
02:59:21,096 --> 02:59:22,126
- It summarized the prompt

4628
02:59:22,126 --> 02:59:24,823
as humans self-domesticated apes.

4629
02:59:24,823 --> 02:59:27,862
(Nathan and Lex laughing)

4630
02:59:27,862 --> 02:59:28,695
- Perspective. Okay.

4631
02:59:28,695 --> 02:59:31,623
All right. So, wait, is this revealing the reasoning?

4632
02:59:31,623 --> 02:59:34,134
Here's why this is a novel. Oh, okay.

4633
02:59:34,134 --> 02:59:36,633
- You click to expand. - Oh, yeah, click to expand.

4634
02:59:36,633 --> 02:59:39,222
- Okay. Analyze the request.

4635
02:59:39,222 --> 02:59:40,537
Novel is the key word.

4636
02:59:40,537 --> 02:59:42,766
- See how it just looks a little different.

4637
02:59:42,766 --> 02:59:45,786
It looks like a normal output. (chuckles)

4638
02:59:45,786 --> 02:59:47,595
- Yeah, it's...

4639
02:59:47,595 --> 02:59:50,457
In some sense, it's better structured, it makes more sense.

4640
02:59:50,457 --> 02:59:52,708
And- - When it latched onto human

4641
02:59:52,708 --> 02:59:54,419
and then it went into organisms,

4642
02:59:54,419 --> 02:59:55,884
and oh wow. (Nathan laughing)

4643
02:59:55,884 --> 02:59:59,217
- Apex predator, focus on domestication.

4644
03:00:00,418 --> 03:00:02,137
Apply domestication to humans,

4645
03:00:02,137 --> 03:00:04,180
explore the idea of self-domestication.

4646
03:00:04,180 --> 03:00:05,477
(Lex and Nathan laughing)

4647
03:00:05,477 --> 03:00:07,019
- Not good, not good.

4648
03:00:07,019 --> 03:00:09,259
- Where is this going?

4649
03:00:09,259 --> 03:00:10,817
Refine, articulate the insight.

4650
03:00:10,817 --> 03:00:14,208
Greater facial expressiveness and communication ability.

4651
03:00:14,208 --> 03:00:15,619
Yes. (chuckles)

4652
03:00:15,619 --> 03:00:17,340
Plasticity and adaptability. Yes.

4653
03:00:17,340 --> 03:00:19,162
Dependence on social groups. Yes.

4654
03:00:19,162 --> 03:00:20,437
All right.

4655
03:00:20,437 --> 03:00:23,159
And self-critique and refined further.

4656
03:00:23,159 --> 03:00:25,242
Wow. Is this truly novel?

4657
03:00:26,503 --> 03:00:28,683
Is it well-supported? (Nathan chuckles)

4658
03:00:28,683 --> 03:00:30,900
So on and so forth.

4659
03:00:30,900 --> 03:00:32,999
And the insight is getting at is humans

4660
03:00:32,999 --> 03:00:34,633
are not just social animals,

4661
03:00:34,633 --> 03:00:37,823
but profoundly self-domesticated apes.

4662
03:00:37,823 --> 03:00:39,312
And this self-domestication is the key

4663
03:00:39,312 --> 03:00:43,180
to understanding our unique cognitive and social abilities.

4664
03:00:43,180 --> 03:00:45,818
Self-domesticated apes. (Nathan laughing)

4665
03:00:45,818 --> 03:00:46,651
Self- - I prefer

4666
03:00:46,651 --> 03:00:48,652
the DeepSeek response. (chuckles)

4667
03:00:48,652 --> 03:00:49,485
- Self do...

4668
03:00:49,485 --> 03:00:51,068
I mean, it's novel.

4669
03:00:51,961 --> 03:00:54,350
The insight is novel. (Nathan laughing)

4670
03:00:54,350 --> 03:00:58,112
That's like a good book title, self-domesticated apes.

4671
03:00:58,112 --> 03:00:59,524
There could be a case made for that.

4672
03:00:59,524 --> 03:01:02,250
Yeah, it's cool and it's revealing the reasoning.

4673
03:01:02,250 --> 03:01:04,500
It's magical. It's magical.

4674
03:01:05,405 --> 03:01:07,405
This is really powerful.

4675
03:01:08,250 --> 03:01:12,421
Hello, everyone, this is Lex with a quick intermission,

4676
03:01:12,421 --> 03:01:14,641
recorded after the podcast.

4677
03:01:14,641 --> 03:01:17,830
Since we've reviewed responses from DeepSeek-R1

4678
03:01:17,830 --> 03:01:21,468
and Gemini Flash 2.0 Thinking during this conversation,

4679
03:01:21,468 --> 03:01:23,267
I thought at this moment, it would be nice

4680
03:01:23,267 --> 03:01:28,267
to insert myself quickly doing the same for OpenAI o1 Pro

4681
03:01:28,740 --> 03:01:31,652
and o3-mini with the same prompt,

4682
03:01:31,652 --> 03:01:36,652
the prompt being, give one truly novel insight about humans.

4683
03:01:38,061 --> 03:01:39,471
And I thought I would in general,

4684
03:01:39,471 --> 03:01:43,638
give my vibe check and vibe-based anecdotal report

4685
03:01:46,693 --> 03:01:50,531
on my own experiences with the new o-3 mini model,

4686
03:01:50,531 --> 03:01:53,139
now that I got a chance to spend many hours with it

4687
03:01:53,139 --> 03:01:55,789
in different kinds of contexts and applications.

4688
03:01:55,789 --> 03:01:59,351
So, I would probably categorize this question as a,

4689
03:01:59,351 --> 03:02:01,463
let's say, open-ended philosophical question.

4690
03:02:01,463 --> 03:02:04,842
And in particular, the emphasis on novelty

4691
03:02:04,842 --> 03:02:06,241
I think is a nice way

4692
03:02:06,241 --> 03:02:09,466
to test one of the capabilities of the model,

4693
03:02:09,466 --> 03:02:13,910
which is come up with something that makes you pause

4694
03:02:13,910 --> 03:02:16,826
and almost surprise you with its brilliance.

4695
03:02:16,826 --> 03:02:19,323
So, that said, my general review

4696
03:02:19,323 --> 03:02:21,253
after running each of the models

4697
03:02:21,253 --> 03:02:23,035
on this question a bunch of times

4698
03:02:23,035 --> 03:02:27,285
is that o1 Pro consistently gave brilliant answers.

4699
03:02:28,742 --> 03:02:31,591
Once, they gave me pause and made me think,

4700
03:02:31,591 --> 03:02:34,169
both cutting in its insight

4701
03:02:34,169 --> 03:02:38,919
and just really nicely phrased with clarity, with nuance,

4702
03:02:40,180 --> 03:02:43,566
over and over consistently generating the best answers.

4703
03:02:43,566 --> 03:02:46,620
After that is R1, which was less consistent,

4704
03:02:46,620 --> 03:02:49,389
but again, deliver brilliance.

4705
03:02:49,389 --> 03:02:52,383
Gemini Flash 2.0 Thinking was third.

4706
03:02:52,383 --> 03:02:54,883
And last was o3-mini actually.

4707
03:02:56,474 --> 03:02:59,403
It often gave quite a generic answer,

4708
03:02:59,403 --> 03:03:01,676
at least to my particular sensibilities.

4709
03:03:01,676 --> 03:03:03,844
That said, in a bunch of other applications

4710
03:03:03,844 --> 03:03:07,785
that I tested for brainstorming purposes,

4711
03:03:07,785 --> 03:03:12,785
it actually worked extremely well and often outperformed R1.

4712
03:03:13,518 --> 03:03:15,480
But on this open-ended philosophical question,

4713
03:03:15,480 --> 03:03:17,735
it did consistently worse.

4714
03:03:17,735 --> 03:03:19,224
Now, another important element

4715
03:03:19,224 --> 03:03:22,823
for each of these models is how the reasoning is presented.

4716
03:03:22,823 --> 03:03:26,665
DeepSeek-R1 shows the full chain of thought tokens,

4717
03:03:26,665 --> 03:03:29,211
which I personally just love.

4718
03:03:29,211 --> 03:03:31,185
For these open-ended philosophical questions,

4719
03:03:31,185 --> 03:03:32,581
it's really, really interesting

4720
03:03:32,581 --> 03:03:34,424
to see the model think through it,

4721
03:03:34,424 --> 03:03:36,628
but really also just stepping back,

4722
03:03:36,628 --> 03:03:40,107
me as a person who appreciates intelligence

4723
03:03:40,107 --> 03:03:42,067
and reasoning and reflection,

4724
03:03:42,067 --> 03:03:45,596
reading these kind of chain of thought, raw tokens of R1,

4725
03:03:45,596 --> 03:03:48,995
there's something genuinely beautiful

4726
03:03:48,995 --> 03:03:51,604
about observing the path of deliberation

4727
03:03:51,604 --> 03:03:54,257
in an intelligent system.

4728
03:03:54,257 --> 03:03:55,723
I think we don't always

4729
03:03:55,723 --> 03:03:59,200
have that explicitly laid out for us humans.

4730
03:03:59,200 --> 03:04:02,443
So, to see it in another intelligence system,

4731
03:04:02,443 --> 03:04:04,834
the non-linearity of it

4732
03:04:04,834 --> 03:04:08,432
akin to "Ulysses" or "Finnegans Wake" by James Joyce.

4733
03:04:08,432 --> 03:04:09,994
It's just beautiful to watch.

4734
03:04:09,994 --> 03:04:11,994
Anyways, we discussed in the episode,

4735
03:04:11,994 --> 03:04:15,193
DeepSeek-R1 talked about humans being able

4736
03:04:15,193 --> 03:04:17,999
to convert selfish desires into cooperative systems

4737
03:04:17,999 --> 03:04:19,883
by collectively pretending abstract rules

4738
03:04:19,883 --> 03:04:22,801
like money laws and rights are real.

4739
03:04:22,801 --> 03:04:26,063
And these shared hallucinations act as games

4740
03:04:26,063 --> 03:04:28,490
where competition is secretly redirected

4741
03:04:28,490 --> 03:04:32,643
to benefit the group, turning conflict into society's fuel.

4742
03:04:32,643 --> 03:04:35,133
Gemini 2.0 Flash Thinking said,

4743
03:04:35,133 --> 03:04:37,250
"Humans are not just social animals,

4744
03:04:37,250 --> 03:04:39,153
but self-domesticated apes,

4745
03:04:39,153 --> 03:04:41,298
and this self-domestication is the key

4746
03:04:41,298 --> 03:04:44,902
to understanding our unique cognitive and social abilities."

4747
03:04:44,902 --> 03:04:46,064
Now, it's important to say

4748
03:04:46,064 --> 03:04:49,011
that the chain of thought there was really interesting.

4749
03:04:49,011 --> 03:04:51,430
It was looking through the entire evolution

4750
03:04:51,430 --> 03:04:55,180
of life on earth, considering apex predators,

4751
03:04:56,240 --> 03:05:00,595
and considering how from that, we ended up to where we are.

4752
03:05:00,595 --> 03:05:02,236
I think that domestication

4753
03:05:02,236 --> 03:05:05,243
by choice is a really interesting angle.

4754
03:05:05,243 --> 03:05:06,274
Again, it's one of those things

4755
03:05:06,274 --> 03:05:08,840
when somebody presents a different angle

4756
03:05:08,840 --> 03:05:12,050
on a seemingly obvious thing, it just makes me smile.

4757
03:05:12,050 --> 03:05:13,785
And the same with DeepSeek-R1,

4758
03:05:13,785 --> 03:05:18,586
that these hallucinations of money laws and rights

4759
03:05:18,586 --> 03:05:22,026
and us collectively pretending like it's real,

4760
03:05:22,026 --> 03:05:24,673
and we play games with them that look like competition,

4761
03:05:24,673 --> 03:05:27,524
when secretly, we're just cooperating with each other.

4762
03:05:27,524 --> 03:05:31,317
And that is the fuel of progress, beautifully put.

4763
03:05:31,317 --> 03:05:32,866
Now, OpenAI o1 Pro

4764
03:05:32,866 --> 03:05:36,187
consistently over and over, delivered bangers.

4765
03:05:36,187 --> 03:05:37,325
I can go through many of them,

4766
03:05:37,325 --> 03:05:39,798
but the first one was, "Humans are the only species

4767
03:05:39,798 --> 03:05:43,017
that turns raw materials into symbolic resources,

4768
03:05:43,017 --> 03:05:44,796
then uses those symbols

4769
03:05:44,796 --> 03:05:47,507
to reorganize the very materials they came from,

4770
03:05:47,507 --> 03:05:52,507
creating a closed feedback loop between meaning and matter."

4771
03:05:52,946 --> 03:05:55,664
Here, I just ran it again, (laughs)

4772
03:05:55,664 --> 03:05:57,558
banger after banger, I'm telling you.

4773
03:05:57,558 --> 03:05:59,206
"Humans are unique among known species

4774
03:05:59,206 --> 03:06:02,355
in that they simultaneously rewrite two layers of reality,

4775
03:06:02,355 --> 03:06:06,468
the external world and their own private mental landscapes,

4776
03:06:06,468 --> 03:06:09,753
and then merge these two rewritten layers

4777
03:06:09,753 --> 03:06:12,267
into a continuous personal narrative

4778
03:06:12,267 --> 03:06:15,684
that feels objectively true, feels true."

4779
03:06:17,626 --> 03:06:19,866
This is poetry. Okay.

4780
03:06:19,866 --> 03:06:22,433
And then, o3-mini-high for me

4781
03:06:22,433 --> 03:06:26,266
was smart, fast actually, and kind of generic.

4782
03:06:29,555 --> 03:06:31,267
Never quite got there for me.

4783
03:06:31,267 --> 03:06:34,297
So, here's the first one I got from o3-mini.

4784
03:06:34,297 --> 03:06:36,405
"Humans are not fixed beings

4785
03:06:36,405 --> 03:06:39,273
but rather ongoing narratives, dynamic stories

4786
03:06:39,273 --> 03:06:42,704
that we continuously write, edit, and reinterpret.

4787
03:06:42,704 --> 03:06:44,346
This narrative plasticity

4788
03:06:44,346 --> 03:06:47,405
is more than just memory or self-reflection.

4789
03:06:47,405 --> 03:06:49,417
It's an intrinsic cognitive process

4790
03:06:49,417 --> 03:06:52,737
that acts like an internal error correction system.

4791
03:06:52,737 --> 03:06:54,954
It allows us to adapt our identities

4792
03:06:54,954 --> 03:06:56,404
and values over time in response

4793
03:06:56,404 --> 03:07:00,131
to new experiences, challenges, and social context.

4794
03:07:00,131 --> 03:07:01,267
Now, it almost sneaks up

4795
03:07:01,267 --> 03:07:03,746
to something approximating cutting insight

4796
03:07:03,746 --> 03:07:07,406
with narrative plasticity in quotes.

4797
03:07:07,406 --> 03:07:10,781
But then, it goes back to the generic, I don't know.

4798
03:07:10,781 --> 03:07:13,347
All of these models are incredible for different reasons.

4799
03:07:13,347 --> 03:07:16,427
There's a lot of concerns as we discussed in this episode,

4800
03:07:16,427 --> 03:07:21,425
but there's a lot of reasons to be excited as well.

4801
03:07:21,425 --> 03:07:24,925
And I've probably spoken for too long.

4802
03:07:24,925 --> 03:07:29,331
I am severely sleep-deprived, borderline delirious.

4803
03:07:29,331 --> 03:07:32,004
So, hopefully, some of this made sense.

4804
03:07:32,004 --> 03:07:35,587
And now, dear friends, back to the episode.

4805
03:07:36,903 --> 03:07:40,070
- I think when you, to Nathan's point,

4806
03:07:40,070 --> 03:07:43,690
when you look at the reasoning models,

4807
03:07:43,690 --> 03:07:47,155
to me, even when I used R1 versus o1,

4808
03:07:47,155 --> 03:07:48,810
there was that sort of rough

4809
03:07:48,810 --> 03:07:51,750
or edges around the corner feeling.

4810
03:07:51,750 --> 03:07:54,739
And Flash Thinking, earlier, I didn't use this version,

4811
03:07:54,739 --> 03:07:55,805
but the one from December,

4812
03:07:55,805 --> 03:07:57,272
and it definitely had that rough edges

4813
03:07:57,272 --> 03:07:58,262
around the corner feeling,

4814
03:07:58,262 --> 03:08:02,288
where it's just not fleshed out in as many ways.

4815
03:08:02,288 --> 03:08:03,797
Sure, they added math

4816
03:08:03,797 --> 03:08:06,729
and coding capabilities via these verifiers in RL,

4817
03:08:06,729 --> 03:08:10,382
but it feels like they lost something in certain areas.

4818
03:08:10,382 --> 03:08:12,254
And o1 is worse performing than chat

4819
03:08:12,254 --> 03:08:14,811
in many areas as well, to be clear.

4820
03:08:14,811 --> 03:08:16,900
- Not by a lot. - Not by a lot though.

4821
03:08:16,900 --> 03:08:19,882
And R1 definitely felt to me

4822
03:08:19,882 --> 03:08:22,033
like it was worse than V3 in certain areas,

4823
03:08:22,033 --> 03:08:25,252
like doing this RL expressed and learned a lot,

4824
03:08:25,252 --> 03:08:27,426
but then it weakened in other areas.

4825
03:08:27,426 --> 03:08:29,968
And so, I think that's one of the big differences

4826
03:08:29,968 --> 03:08:33,932
between these models, and what o1 offers.

4827
03:08:33,932 --> 03:08:36,336
And then, OpenAI has o1 Pro.

4828
03:08:36,336 --> 03:08:39,093
And what they did with o3, which is like also very unique,

4829
03:08:39,093 --> 03:08:43,546
is that they stacked search on top of chain of thought.

4830
03:08:43,546 --> 03:08:45,444
And so, chain of thought is one thing, where it's able,

4831
03:08:45,444 --> 03:08:48,152
it's one chain, it backtracks, goes back and forth,

4832
03:08:48,152 --> 03:08:50,773
but how they solved the ARC-AGI challenge

4833
03:08:50,773 --> 03:08:52,698
was not just the chain of thought.

4834
03:08:52,698 --> 03:08:55,307
It was also sampling many times,

4835
03:08:55,307 --> 03:08:58,373
i.e, running them in parallel, and then selecting.

4836
03:08:58,373 --> 03:09:00,450
- Is running in parallel actually search?

4837
03:09:00,450 --> 03:09:01,283
Because I don't know

4838
03:09:01,283 --> 03:09:03,573
if we have the full information on how o1 Pro works.

4839
03:09:03,573 --> 03:09:04,452
So, like I'm not, I don't - That's right.

4840
03:09:04,452 --> 03:09:05,285
- have enough information - Agree.

4841
03:09:05,285 --> 03:09:07,124
- to confidently say that it is searched.

4842
03:09:07,124 --> 03:09:09,016
- It is parallel samples. - Yeah.

4843
03:09:09,016 --> 03:09:10,491
And then, what? - And it selects something.

4844
03:09:10,491 --> 03:09:12,564
- And we don't know what the selection function is.

4845
03:09:12,564 --> 03:09:13,771
The reason why we're debating

4846
03:09:13,771 --> 03:09:16,232
is because since o1 was announced,

4847
03:09:16,232 --> 03:09:17,823
there's been a lot of interest in techniques

4848
03:09:17,823 --> 03:09:19,009
called Monte Carlo Tree Search,

4849
03:09:19,009 --> 03:09:19,842
- Mm-hmm. - which is where

4850
03:09:19,842 --> 03:09:21,155
you will break down the chain of thought

4851
03:09:21,155 --> 03:09:22,822
into intermediate steps.

4852
03:09:22,822 --> 03:09:24,291
We haven't defined chain of thought.

4853
03:09:24,291 --> 03:09:26,887
Chain of thought is from a paper from years ago

4854
03:09:26,887 --> 03:09:29,813
where you introduced the idea to ask a language model

4855
03:09:29,813 --> 03:09:32,895
that at the time, was much less easy to use.

4856
03:09:32,895 --> 03:09:34,672
You would say, let's verify step by step,

4857
03:09:34,672 --> 03:09:36,090
and it would induce the model

4858
03:09:36,090 --> 03:09:38,213
to do this bulleted list of steps.

4859
03:09:38,213 --> 03:09:40,959
Chain of thought is now almost a default in models

4860
03:09:40,959 --> 03:09:42,379
where if you ask in a math question,

4861
03:09:42,379 --> 03:09:44,081
you don't need to tell it to think step by step.

4862
03:09:44,081 --> 03:09:46,857
And the idea with Monte Carlo Tree Search

4863
03:09:46,857 --> 03:09:49,750
is that you would take an intermediate point in that train,

4864
03:09:49,750 --> 03:09:52,298
do some sort of expansion, spend more compute,

4865
03:09:52,298 --> 03:09:53,732
and then select the right one.

4866
03:09:53,732 --> 03:09:55,442
That's like a very complex form of search

4867
03:09:55,442 --> 03:09:56,522
that has been used in things

4868
03:09:56,522 --> 03:09:59,316
like MuZero and AlphaZero potentially.

4869
03:09:59,316 --> 03:10:00,722
I know MuZero does this.

4870
03:10:00,722 --> 03:10:01,978
- Another form of search

4871
03:10:01,978 --> 03:10:04,411
is just asking five different people, and then taking

4872
03:10:04,411 --> 03:10:05,482
the majority answer. - Yes.

4873
03:10:05,482 --> 03:10:07,400
- There's a variety of like, (Nathan chuckles)

4874
03:10:07,400 --> 03:10:09,633
it could be complicated, it could be simple.

4875
03:10:09,633 --> 03:10:12,416
We don't know what it is, just that they are not

4876
03:10:12,416 --> 03:10:14,918
just issuing one chain of thought in sequence.

4877
03:10:14,918 --> 03:10:16,490
- Yeah. - They're launching many

4878
03:10:16,490 --> 03:10:18,421
in parallel, and in the ARC-AGI,

4879
03:10:18,421 --> 03:10:20,963
they launched a thousand in parallel for their,

4880
03:10:20,963 --> 03:10:22,960
the one that really shocked everyone,

4881
03:10:22,960 --> 03:10:23,950
that beat the benchmark

4882
03:10:23,950 --> 03:10:26,023
was they would launch a thousand in parallel,

4883
03:10:26,023 --> 03:10:27,622
and then they would get the right answer,

4884
03:10:27,622 --> 03:10:30,591
like 80% of the time or 70% of the time, 90 maybe even.

4885
03:10:30,591 --> 03:10:33,540
Whereas if they just launched one, it was like 30%.

4886
03:10:33,540 --> 03:10:35,769
- There are many extensions to this.

4887
03:10:35,769 --> 03:10:38,582
I would say the simplest one is that our language models

4888
03:10:38,582 --> 03:10:41,731
to date have been designed to give the right answer

4889
03:10:41,731 --> 03:10:44,770
the highest percentage of the time in one response.

4890
03:10:44,770 --> 03:10:47,568
And we are now opening the door to different ways

4891
03:10:47,568 --> 03:10:50,188
of running inference on our models in which we need

4892
03:10:50,188 --> 03:10:53,041
to reevaluate many parts of the training process,

4893
03:10:53,041 --> 03:10:56,036
which normally opens the door to more progress,

4894
03:10:56,036 --> 03:10:58,319
but we don't know if OpenAI changed a lot

4895
03:10:58,319 --> 03:11:01,000
or if just sampling more in multiple choices

4896
03:11:01,000 --> 03:11:01,833
what they're doing,

4897
03:11:01,833 --> 03:11:02,887
or if it's something more complex

4898
03:11:02,887 --> 03:11:03,926
where they changed the training

4899
03:11:03,926 --> 03:11:06,358
and they know that the inference mode

4900
03:11:06,358 --> 03:11:07,548
is going to be different.

4901
03:11:07,548 --> 03:11:10,878
- So, we're talking about o1 Pro $200 a month

4902
03:11:10,878 --> 03:11:12,706
and they're losing money.

4903
03:11:12,706 --> 03:11:15,873
So, the thing that we're referring to,

4904
03:11:16,787 --> 03:11:21,787
this fascinating exploration of the test-time compute space,

4905
03:11:22,955 --> 03:11:24,508
is that actually possible?

4906
03:11:24,508 --> 03:11:26,090
Do we have enough compute for that?

4907
03:11:26,090 --> 03:11:27,976
Does the financials make sense?

4908
03:11:27,976 --> 03:11:29,793
- So, the fantastic thing is,

4909
03:11:29,793 --> 03:11:32,919
and it's in the thing that I pulled up earlier,

4910
03:11:32,919 --> 03:11:36,002
but the cost for GPT-3 has plummeted.

4911
03:11:37,897 --> 03:11:40,677
If you scroll up just a few images, I think.

4912
03:11:40,677 --> 03:11:41,589
The important thing about like,

4913
03:11:41,589 --> 03:11:44,329
hey, is cost limiting factor here?

4914
03:11:44,329 --> 03:11:48,253
My view is that we'll have really awesome intelligence

4915
03:11:48,253 --> 03:11:49,112
before we have,

4916
03:11:49,112 --> 03:11:52,334
like AGI before we have it permeate throughout the economy.

4917
03:11:52,334 --> 03:11:54,425
And this is why that reason is.

4918
03:11:54,425 --> 03:11:58,032
GPT-3 was trained in what, 2020, 2021?

4919
03:11:58,032 --> 03:12:00,221
And the cost for running inference on it

4920
03:12:00,221 --> 03:12:02,804
was 60, $70 per million tokens,

4921
03:12:04,255 --> 03:12:07,002
which was the cost per intelligence was ridiculous.

4922
03:12:07,002 --> 03:12:09,088
Now, as we scaled forward two years,

4923
03:12:09,088 --> 03:12:11,939
we've had a 1,200x reduction in cost

4924
03:12:11,939 --> 03:12:15,300
to achieve the same level of intelligence as GPT-3.

4925
03:12:15,300 --> 03:12:17,361
- So, here on the x-axis is time

4926
03:12:17,361 --> 03:12:19,259
over just a couple of years.

4927
03:12:19,259 --> 03:12:23,842
And on the y-axis is log scale dollars to run inference

4928
03:12:25,859 --> 03:12:27,441
on a millions- - Yes, dollars.

4929
03:12:27,441 --> 03:12:28,391
Yeah, a million.

4930
03:12:28,391 --> 03:12:31,049
- And so, you have just a down,

4931
03:12:31,049 --> 03:12:35,799
like a linear decline on log scale from GPT-3 through 3.5

4932
03:12:36,770 --> 03:12:38,192
to Llama. - It's like 5 cents

4933
03:12:38,192 --> 03:12:40,385
or something like that now,

4934
03:12:40,385 --> 03:12:42,952
which is versus $60, 1,200x. - Yeah.

4935
03:12:42,952 --> 03:12:44,035
Yeah. - That's not

4936
03:12:44,035 --> 03:12:45,076
the exact numbers, but it's 1,200x.

4937
03:12:45,076 --> 03:12:47,217
I remember that number.

4938
03:12:47,217 --> 03:12:50,347
Is humongous cost per intelligence.

4939
03:12:50,347 --> 03:12:52,132
Now, the freak out over DeepSeek is,

4940
03:12:52,132 --> 03:12:53,552
oh my god, they made it so cheap.

4941
03:12:53,552 --> 03:12:55,644
It's like actually, if you look at this trend line,

4942
03:12:55,644 --> 03:12:57,743
they're not below the trend line, first of all,

4943
03:12:57,743 --> 03:12:59,641
and at least for GPT-3. (Nathan laughing)

4944
03:12:59,641 --> 03:13:02,265
They are the first to hit it, which is a big deal.

4945
03:13:02,265 --> 03:13:04,762
But they're not below the trend line as far as GPT-3.

4946
03:13:04,762 --> 03:13:06,182
Now, we have GPT-4,

4947
03:13:06,182 --> 03:13:07,912
what's gonna happen with these reasoning capabilities.

4948
03:13:07,912 --> 03:13:10,493
It's a mix of architectural innovations,

4949
03:13:10,493 --> 03:13:12,423
it's a mix of better data,

4950
03:13:12,423 --> 03:13:14,083
and it's gonna be better training techniques,

4951
03:13:14,083 --> 03:13:17,202
and all of these better inference systems, better hardware.

4952
03:13:17,202 --> 03:13:19,584
Going from each generation

4953
03:13:19,584 --> 03:13:22,263
of GPU to new generations, or ASICs,

4954
03:13:22,263 --> 03:13:24,219
everything is gonna take

4955
03:13:24,219 --> 03:13:26,239
this cost curve down and down and down and down.

4956
03:13:26,239 --> 03:13:30,729
And then, can I just spawn a thousand different LLMs

4957
03:13:30,729 --> 03:13:33,170
to create a task, and then pick from one of them?

4958
03:13:33,170 --> 03:13:35,019
Or whatever search technique,

4959
03:13:35,019 --> 03:13:36,920
I want a Tree, Monte Carlo Tree Research.

4960
03:13:36,920 --> 03:13:39,741
Maybe it gets that complicated, maybe it doesn't,

4961
03:13:39,741 --> 03:13:42,530
'cause it's too complicated to actually scale, who knows?

4962
03:13:42,530 --> 03:13:43,363
Bitter lesson, right?

4963
03:13:43,363 --> 03:13:46,363
The question is I think when not if,

4964
03:13:48,179 --> 03:13:51,906
because the rate of progress is so fast.

4965
03:13:51,906 --> 03:13:53,842
Nine months ago, Dario was saying, hey,

4966
03:13:53,842 --> 03:13:55,281
or Dario said nine months ago,

4967
03:13:55,281 --> 03:13:58,309
the cost to train an inference was this.

4968
03:13:58,309 --> 03:14:00,369
And now, we're much better than this.

4969
03:14:00,369 --> 03:14:01,899
And DeepSeek is much better than this.

4970
03:14:01,899 --> 03:14:03,522
And that cost curve for GPT-4,

4971
03:14:03,522 --> 03:14:06,456
which was also roughly $60 per million tokens

4972
03:14:06,456 --> 03:14:11,097
when it launched, has already fallen to $2 or so.

4973
03:14:11,097 --> 03:14:14,090
And we're gonna get it down to cents probably

4974
03:14:14,090 --> 03:14:15,810
for GPT-4 quality and the same...

4975
03:14:15,810 --> 03:14:19,213
And that's the base for the reasoning models

4976
03:14:19,213 --> 03:14:20,670
like o1 that we have today,

4977
03:14:20,670 --> 03:14:23,087
and o1 Pro is spawning multiple.

4978
03:14:23,087 --> 03:14:24,775
And o3 and so on and so forth.

4979
03:14:24,775 --> 03:14:26,713
These search techniques' too expensive today,

4980
03:14:26,713 --> 03:14:27,971
but they will get cheaper.

4981
03:14:27,971 --> 03:14:31,137
And that's what's gonna unlock the intelligence.

4982
03:14:31,137 --> 03:14:34,193
- So, it'll get cheaper and cheaper and cheaper.

4983
03:14:34,193 --> 03:14:38,765
The big DeepSeek-R1 release freaked everybody out,

4984
03:14:38,765 --> 03:14:39,913
because of the cheaper,

4985
03:14:39,913 --> 03:14:44,105
one of the manifestations of that is Nvidia stock plummeted.

4986
03:14:44,105 --> 03:14:46,343
Can you explain what happened?

4987
03:14:46,343 --> 03:14:48,822
And also just explain this moment

4988
03:14:48,822 --> 03:14:52,836
and whether if Nvidia's gonna keep winning.

4989
03:14:52,836 --> 03:14:55,493
- We are both Nvidia bulls here, I would say.

4990
03:14:55,493 --> 03:14:59,465
And in some ways, the market response is reasonable.

4991
03:14:59,465 --> 03:15:00,577
Most of the market,

4992
03:15:00,577 --> 03:15:03,712
like Nvidia's biggest customers in the US

4993
03:15:03,712 --> 03:15:06,855
are major tech companies, and they're spending a ton on AI.

4994
03:15:06,855 --> 03:15:09,180
And if a simple interpretation

4995
03:15:09,180 --> 03:15:11,145
of DeepSeek is you can get really good models

4996
03:15:11,145 --> 03:15:13,365
without spending as much on AI.

4997
03:15:13,365 --> 03:15:15,194
So, in that capacity, it's like, oh,

4998
03:15:15,194 --> 03:15:16,175
maybe these big tech companies

4999
03:15:16,175 --> 03:15:18,395
won't need to spend as much in AI and go down.

5000
03:15:18,395 --> 03:15:19,623
The actual thing that happened,

5001
03:15:19,623 --> 03:15:21,662
it's much more complex, where there's social factors,

5002
03:15:21,662 --> 03:15:23,736
where there's the rising in the App Store,

5003
03:15:23,736 --> 03:15:26,386
the social contagion that is happening.

5004
03:15:26,386 --> 03:15:28,970
And then, I think some of it's just like...

5005
03:15:28,970 --> 03:15:29,803
I don't trade,

5006
03:15:29,803 --> 03:15:31,135
I don't know anything about financial markets,

5007
03:15:31,135 --> 03:15:32,118
but it builds up over the weekend,

5008
03:15:32,118 --> 03:15:33,079
where the social pressure,

5009
03:15:33,079 --> 03:15:34,971
where it's like if it was during the week

5010
03:15:34,971 --> 03:15:36,480
and there was multiple days of trading

5011
03:15:36,480 --> 03:15:37,961
when this was really becoming,

5012
03:15:37,961 --> 03:15:38,918
but it comes on the weekend,

5013
03:15:38,918 --> 03:15:40,868
and then everybody wants to sell.

5014
03:15:40,868 --> 03:15:43,057
And that is a social contagion.

5015
03:15:43,057 --> 03:15:45,869
- I think, and like there were a lot of false narratives,

5016
03:15:45,869 --> 03:15:46,938
which is like, hey,

5017
03:15:46,938 --> 03:15:48,987
these guys are spending billions on models.

5018
03:15:48,987 --> 03:15:50,496
And they're not spending billions on models.

5019
03:15:50,496 --> 03:15:52,560
No one spent more than a billion dollars

5020
03:15:52,560 --> 03:15:54,519
on a model that's released publicly.

5021
03:15:54,519 --> 03:15:56,960
GPT-4 was a couple hundred million,

5022
03:15:56,960 --> 03:16:00,877
and then they've reduced the cost for Turbo 4o.

5023
03:16:02,263 --> 03:16:04,919
But billion dollar model runs are coming.

5024
03:16:04,919 --> 03:16:06,892
And this concludes pre-training and post-training.

5025
03:16:06,892 --> 03:16:08,087
And then, the other number is like,

5026
03:16:08,087 --> 03:16:09,598
hey, DeepSeek didn't include everything.

5027
03:16:09,598 --> 03:16:10,451
They didn't include...

5028
03:16:10,451 --> 03:16:11,284
A lot of the cost

5029
03:16:11,284 --> 03:16:12,776
goes to research and all this sort of stuff.

5030
03:16:12,776 --> 03:16:13,985
A lot of the cost goes to inference.

5031
03:16:13,985 --> 03:16:15,707
A lot of the cost goes to post-training.

5032
03:16:15,707 --> 03:16:16,540
None of these things were factor.

5033
03:16:16,540 --> 03:16:17,685
- Yeah. - It's research salaries.

5034
03:16:17,685 --> 03:16:20,447
All these things are counted in the billions

5035
03:16:20,447 --> 03:16:22,048
of dollars that OpenAI is spending,

5036
03:16:22,048 --> 03:16:23,036
but they weren't counted in the,

5037
03:16:23,036 --> 03:16:26,116
hey, 6 million, $5 million that DeepSeek spent.

5038
03:16:26,116 --> 03:16:27,445
So, there's a bit

5039
03:16:27,445 --> 03:16:29,468
of misunderstanding of what these numbers are.

5040
03:16:29,468 --> 03:16:33,143
And then, there's also an element of

5041
03:16:33,143 --> 03:16:34,334
Nvidia's just been a straight line up.

5042
03:16:34,334 --> 03:16:37,485
And there's been so many different narratives

5043
03:16:37,485 --> 03:16:39,524
that have been trying to push down Nvidia.

5044
03:16:39,524 --> 03:16:40,935
I don't say push down Nvidia stock.

5045
03:16:40,935 --> 03:16:45,701
Everyone is looking for a reason to sell or to be worried.

5046
03:16:45,701 --> 03:16:47,020
It was Blackwell delays.

5047
03:16:47,020 --> 03:16:48,901
Their GPU was, there's a lot of report...

5048
03:16:48,901 --> 03:16:49,734
Every two weeks,

5049
03:16:49,734 --> 03:16:54,028
there's a new report about their GPUs being delayed.

5050
03:16:54,028 --> 03:16:56,279
There's the whole thing about scaling laws ending.

5051
03:16:56,279 --> 03:16:57,920
It's so ironic.

5052
03:16:57,920 --> 03:16:59,240
- [Nathan] It lasted a month. (laughs)

5053
03:16:59,240 --> 03:17:02,145
It was just literally just,

5054
03:17:02,145 --> 03:17:03,446
hey, models aren't getting better.

5055
03:17:03,446 --> 03:17:04,848
They're just not getting better.

5056
03:17:04,848 --> 03:17:06,258
There's no reason to spend more,

5057
03:17:06,258 --> 03:17:10,177
pre-training scaling is dead, and then it's like, o1, o3.

5058
03:17:10,177 --> 03:17:11,585
- R1. (laughs) - R1.

5059
03:17:11,585 --> 03:17:13,884
And now, it's like, wait, models are getting too,

5060
03:17:13,884 --> 03:17:15,404
they're progressing too fast, (Nathan laughing)

5061
03:17:15,404 --> 03:17:17,665
slow down the progress, stop spending on GPUs.

5062
03:17:17,665 --> 03:17:18,498
- Yeah. - And it's...

5063
03:17:18,498 --> 03:17:19,883
But the funniest thing I think

5064
03:17:19,883 --> 03:17:23,865
that comes out of this is Jevons Paradox is true.

5065
03:17:23,865 --> 03:17:25,843
AWS pricing for H100s

5066
03:17:25,843 --> 03:17:29,098
has gone up over the last couple weeks.

5067
03:17:29,098 --> 03:17:30,987
Since a little bit after Christmas,

5068
03:17:30,987 --> 03:17:34,303
since V3 was launched, AWS H100 pricing has gone up.

5069
03:17:34,303 --> 03:17:36,856
H200s are almost out of stock everywhere,

5070
03:17:36,856 --> 03:17:39,155
because H200 has more memory,

5071
03:17:39,155 --> 03:17:42,966
and therefore, R1 wants that chip over H100.

5072
03:17:42,966 --> 03:17:44,025
- We were trying to get GPUs

5073
03:17:44,025 --> 03:17:45,784
on a short notice this week for a demo,

5074
03:17:45,784 --> 03:17:46,617
and it wasn't that easy.

5075
03:17:46,617 --> 03:17:47,755
We were trying to get just 16

5076
03:17:47,755 --> 03:17:50,825
or 32 H100s for demo, and it was not very easy.

5077
03:17:50,825 --> 03:17:51,711
(Nathan chuckles) - So, for people

5078
03:17:51,711 --> 03:17:53,185
who don't Jevons Paradox

5079
03:17:53,185 --> 03:17:56,518
is when the efficiency goes up, somehow,

5080
03:17:58,807 --> 03:18:00,984
magically, counterintuitively,

5081
03:18:00,984 --> 03:18:03,106
the total resource consumption goes up as well.

5082
03:18:03,106 --> 03:18:07,225
- And semiconductors is, we're at 50 years of Moore's Law,

5083
03:18:07,225 --> 03:18:08,647
every two years half the cost,

5084
03:18:08,647 --> 03:18:10,620
double the transistors just like clockwork.

5085
03:18:10,620 --> 03:18:11,605
And it's slowed down obviously,

5086
03:18:11,605 --> 03:18:15,058
but the semiconductor industry is gone up the whole time.

5087
03:18:15,058 --> 03:18:15,897
It's been wavy.

5088
03:18:15,897 --> 03:18:17,495
There's obviously cycles and stuff,

5089
03:18:17,495 --> 03:18:19,337
and I don't expect AI to be any different.

5090
03:18:19,337 --> 03:18:20,794
There's gonna be ebbs and flows,

5091
03:18:20,794 --> 03:18:22,623
but this is in AI,

5092
03:18:22,623 --> 03:18:24,515
it's just playing out at an insane timescale.

5093
03:18:24,515 --> 03:18:26,552
It was 2x every two years.

5094
03:18:26,552 --> 03:18:29,375
This is 1,200x in like three years.

5095
03:18:29,375 --> 03:18:31,217
So, it's like, (Nathan laughing)

5096
03:18:31,217 --> 03:18:32,072
the scale of improvement

5097
03:18:32,072 --> 03:18:34,461
that is hard to get wrap your head around.

5098
03:18:34,461 --> 03:18:35,611
- Yeah, I was confused,

5099
03:18:35,611 --> 03:18:39,549
because to me, Nvidia stock on that should have gone up.

5100
03:18:39,549 --> 03:18:41,221
But maybe it went down

5101
03:18:41,221 --> 03:18:43,865
because there's suspicion of foul play

5102
03:18:43,865 --> 03:18:45,779
on the side of China, something like this.

5103
03:18:45,779 --> 03:18:47,448
But if you just look purely

5104
03:18:47,448 --> 03:18:51,125
at the actual principles at play here, it's obvious.

5105
03:18:51,125 --> 03:18:53,780
Yeah, Jevons Paradox. - More progress that AI makes

5106
03:18:53,780 --> 03:18:57,640
or the higher the derivative of AI progress is,

5107
03:18:57,640 --> 03:19:00,172
especially you should, because Nvidia's in the best place.

5108
03:19:00,172 --> 03:19:01,150
The higher the derivative is,

5109
03:19:01,150 --> 03:19:03,893
the sooner the market's gonna be bigger and expanding.

5110
03:19:03,893 --> 03:19:04,859
And Nvidia's the only one

5111
03:19:04,859 --> 03:19:06,612
that does everything - Yeah.

5112
03:19:06,612 --> 03:19:07,489
- reliably right now.

5113
03:19:07,489 --> 03:19:10,760
- Because it's not like an Nvidia competitor arose.

5114
03:19:10,760 --> 03:19:14,846
It's another company that's using Nvidia, so-

5115
03:19:14,846 --> 03:19:16,970
- Who historically has been a large Nvidia customer.

5116
03:19:16,970 --> 03:19:19,490
- Yeah. (chuckles) - And has press releases

5117
03:19:19,490 --> 03:19:21,531
about them cheering about being China's biggest

5118
03:19:21,531 --> 03:19:23,821
Nvidia customer. (Nathan laughing)

5119
03:19:23,821 --> 03:19:24,909
- [Lex] Yeah, it made it-

5120
03:19:24,909 --> 03:19:26,225
- Obviously they've quieted down,

5121
03:19:26,225 --> 03:19:28,459
but I think that's another element of is,

5122
03:19:28,459 --> 03:19:30,557
that they don't wanna say how many GPUs they have.

5123
03:19:30,557 --> 03:19:32,563
- Yeah. - Because hey, yes,

5124
03:19:32,563 --> 03:19:33,588
they have H800s,

5125
03:19:33,588 --> 03:19:36,426
yes, they have H20s, they also have some H100s,

5126
03:19:36,426 --> 03:19:38,763
which are smuggled in. - So, can you speak to that,

5127
03:19:38,763 --> 03:19:39,628
to the smuggling?

5128
03:19:39,628 --> 03:19:41,603
What's the scale of smuggling

5129
03:19:41,603 --> 03:19:44,943
that's feasible for a nation state to do for companies?

5130
03:19:44,943 --> 03:19:46,384
Is it possible to-

5131
03:19:46,384 --> 03:19:49,810
- I think there's a few angles of smuggling here.

5132
03:19:49,810 --> 03:19:51,824
One is ByteDance arguably

5133
03:19:51,824 --> 03:19:54,230
is the largest smuggler of GPUs for China.

5134
03:19:54,230 --> 03:19:55,632
China's not supposed to have GPUs.

5135
03:19:55,632 --> 03:19:58,641
ByteDance has like over 500,000 GPUs. Why?

5136
03:19:58,641 --> 03:20:01,420
Because they're all rented from companies around the world.

5137
03:20:01,420 --> 03:20:03,411
They rent from Oracle, they rent from Google,

5138
03:20:03,411 --> 03:20:04,929
they rent from all these mass...

5139
03:20:04,929 --> 03:20:06,619
And a bunch of smaller cloud companies too.

5140
03:20:06,619 --> 03:20:08,689
All the Neoclouds of the world.

5141
03:20:08,689 --> 03:20:11,922
They rent so, so many GPUs, they also buy a bunch.

5142
03:20:11,922 --> 03:20:12,755
And they do this

5143
03:20:12,755 --> 03:20:16,100
for mostly like what Meta does, serving TikTok,

5144
03:20:16,100 --> 03:20:18,541
serving next best separate- - Same discussion.

5145
03:20:18,541 --> 03:20:21,266
- Same as kind, to be clear, that's today the view use.

5146
03:20:21,266 --> 03:20:22,710
- Yeah. - And it's a valid use.

5147
03:20:22,710 --> 03:20:24,921
Hack the dopamine circuit.

5148
03:20:24,921 --> 03:20:28,949
Now, that's theoretically now very much restricted

5149
03:20:28,949 --> 03:20:30,412
with the AI diffusion rules,

5150
03:20:30,412 --> 03:20:32,200
which happened in the last week of the Biden admin

5151
03:20:32,200 --> 03:20:35,378
and Trump admin looks like they're gonna keep 'em,

5152
03:20:35,378 --> 03:20:38,961
which limits allies, even like Singapore,

5153
03:20:38,961 --> 03:20:42,968
which Singapore is like 20, 30% of Nvidia's revenue.

5154
03:20:42,968 --> 03:20:45,206
But Singapore's had a moratorium

5155
03:20:45,206 --> 03:20:47,155
on not building data centers for like 15 years,

5156
03:20:47,155 --> 03:20:47,988
'cause they don't have enough power.

5157
03:20:47,988 --> 03:20:49,515
So, where are they going?

5158
03:20:49,515 --> 03:20:51,433
(Dylan laughing) - [Nathan] Oh yeah. (laughs)

5159
03:20:51,433 --> 03:20:52,718
- I'm not claiming they're all going to China,

5160
03:20:52,718 --> 03:20:53,870
but a portion are,

5161
03:20:53,870 --> 03:20:56,389
many are going to Malaysia, including Microsoft

5162
03:20:56,389 --> 03:20:58,166
and Oracle have big data centers in Malaysia.

5163
03:20:58,166 --> 03:21:00,764
They're going all over Southeast Asia probably,

5164
03:21:00,764 --> 03:21:01,815
India as well.

5165
03:21:01,815 --> 03:21:02,888
There's stuff routing,

5166
03:21:02,888 --> 03:21:05,756
but the diffusion rules are very de facto.

5167
03:21:05,756 --> 03:21:08,165
You can only buy this many GPUs from this country.

5168
03:21:08,165 --> 03:21:11,036
And you can only rent a cluster this large

5169
03:21:11,036 --> 03:21:12,319
to companies that are Chinese.

5170
03:21:12,319 --> 03:21:15,903
They're very explicit on trying to stop smuggling.

5171
03:21:15,903 --> 03:21:18,690
And a big chunk of it was, hey,

5172
03:21:18,690 --> 03:21:23,101
random company by 16 servers ship them to China.

5173
03:21:23,101 --> 03:21:26,738
There's actually, I saw a photo from someone

5174
03:21:26,738 --> 03:21:28,552
in the semiconductor industry

5175
03:21:28,552 --> 03:21:32,054
who leads like a team for networking chips

5176
03:21:32,054 --> 03:21:33,794
that competes with Nvidia.

5177
03:21:33,794 --> 03:21:34,713
And he sent a photo

5178
03:21:34,713 --> 03:21:37,814
of a guy checking into a first-class united flight

5179
03:21:37,814 --> 03:21:41,114
from San Francisco to Shanghai or Shenzhen,

5180
03:21:41,114 --> 03:21:43,842
with a super micro box that is this big,

5181
03:21:43,842 --> 03:21:46,733
which can only contain GPUs. (Nathan chuckles)

5182
03:21:46,733 --> 03:21:48,144
And he was booking first-class,

5183
03:21:48,144 --> 03:21:48,977
'cause think about it,

5184
03:21:48,977 --> 03:21:50,828
3 to 5k for your first-class ticket,

5185
03:21:50,828 --> 03:21:54,609
server costs 240,000 in the US, to 50,000.

5186
03:21:54,609 --> 03:21:56,120
You sell it for 300,000 in China,

5187
03:21:56,120 --> 03:21:58,598
wait, you just got a free first-class ticket

5188
03:21:58,598 --> 03:22:00,270
- Yeah. - and a lot more money.

5189
03:22:00,270 --> 03:22:01,103
So, it's like...

5190
03:22:01,103 --> 03:22:02,520
And then, that's like small-scale smuggling.

5191
03:22:02,520 --> 03:22:03,930
Most of the large-scale smuggling

5192
03:22:03,930 --> 03:22:06,597
is like companies in Singapore and Malaysia

5193
03:22:06,597 --> 03:22:08,848
routing 'em around, or renting GPUs

5194
03:22:08,848 --> 03:22:10,698
completely legally. - I wanna jump in.

5195
03:22:10,698 --> 03:22:11,671
How much does this scale?

5196
03:22:11,671 --> 03:22:13,208
I think there's been some number, like some people

5197
03:22:13,208 --> 03:22:16,719
that are higher level economics understanding,

5198
03:22:16,719 --> 03:22:17,880
say that it's like as you go

5199
03:22:17,880 --> 03:22:19,838
from 1 billion of smuggling to 10 billion,

5200
03:22:19,838 --> 03:22:22,750
it's like you're hiding certain levels of economic activity.

5201
03:22:22,750 --> 03:22:24,271
And that's the most reasonable thing to me

5202
03:22:24,271 --> 03:22:27,060
is that there's gonna be some level where it's so obvious

5203
03:22:27,060 --> 03:22:29,359
that it's easier to find this economic activity.

5204
03:22:29,359 --> 03:22:30,259
And- - Yeah.

5205
03:22:30,259 --> 03:22:33,842
So, my belief is that last year, roughly...

5206
03:22:35,149 --> 03:22:37,216
So, Nvidia made a million H20s,

5207
03:22:37,216 --> 03:22:39,134
which are legally allowed to be shipped to China,

5208
03:22:39,134 --> 03:22:41,012
which we talked about is better for reasoning,

5209
03:22:41,012 --> 03:22:42,377
inference at least.

5210
03:22:42,377 --> 03:22:44,925
Maybe not training, but reasoning, inference.

5211
03:22:44,925 --> 03:22:46,112
And inference generally.

5212
03:22:46,112 --> 03:22:49,403
Then, they also had a couple 100,000,

5213
03:22:49,403 --> 03:22:51,873
we think like 200 to 300,000 GPUs

5214
03:22:51,873 --> 03:22:53,685
were routed to China

5215
03:22:53,685 --> 03:22:56,364
from Singapore, Malaysia, US, wherever.

5216
03:22:56,364 --> 03:22:59,102
Companies spun up by 16 GPUs, 64 GPUs,

5217
03:22:59,102 --> 03:22:59,951
whatever it is routed,

5218
03:22:59,951 --> 03:23:02,604
and Huawei's known for having spent up a massive network

5219
03:23:02,604 --> 03:23:04,728
of companies to get the materials they need

5220
03:23:04,728 --> 03:23:06,563
after they were banned in 2018.

5221
03:23:06,563 --> 03:23:08,833
So, it's not like otherworldly, but I agree.

5222
03:23:08,833 --> 03:23:10,685
Nathan's point is like, hey,

5223
03:23:10,685 --> 03:23:12,753
you can't smuggle up $10 billion of GPUs.

5224
03:23:12,753 --> 03:23:15,163
And then, the third source, which is just now banned

5225
03:23:15,163 --> 03:23:16,913
and which wasn't considered smuggling,

5226
03:23:16,913 --> 03:23:21,330
but is China is renting, I believe from our research,

5227
03:23:22,585 --> 03:23:26,973
Oracle's biggest GPU customer is ByteDance.

5228
03:23:26,973 --> 03:23:29,525
And for Google, I think it's their second biggest customer.

5229
03:23:29,525 --> 03:23:31,554
And so, like, and you go down the list of clouds

5230
03:23:31,554 --> 03:23:33,132
and especially these smaller cloud companies

5231
03:23:33,132 --> 03:23:35,922
that aren't like that hyperscalers.

5232
03:23:35,922 --> 03:23:39,013
Think beyond core of even Lambda even, there's a whole sea,

5233
03:23:39,013 --> 03:23:39,904
there's 60 different

5234
03:23:39,904 --> 03:23:41,712
new cloud companies serving Nvidia GPUs.

5235
03:23:41,712 --> 03:23:44,652
I think ByteDance is renting a lot of these all over it.

5236
03:23:44,652 --> 03:23:48,330
And so, these companies are renting GPUs

5237
03:23:48,330 --> 03:23:49,163
to Chinese companies,

5238
03:23:49,163 --> 03:23:52,973
and that was completely legal up until the diffusion rules,

5239
03:23:52,973 --> 03:23:54,401
which happened just a few weeks ago.

5240
03:23:54,401 --> 03:23:56,843
And even now, you can rent GPU clusters

5241
03:23:56,843 --> 03:23:58,512
that are less than 2,000 GPUs.

5242
03:23:58,512 --> 03:24:01,185
Or you can buy GPUs and ship them wherever you want

5243
03:24:01,185 --> 03:24:03,352
if they're less than 1,500 GPUs.

5244
03:24:03,352 --> 03:24:06,363
So, it's like there are still some ways to smuggle,

5245
03:24:06,363 --> 03:24:08,863
but yeah, as the numbers grow,

5246
03:24:09,832 --> 03:24:11,602
100 something billion dollars of revenue

5247
03:24:11,602 --> 03:24:13,979
for Nvidia last year, 200 something billion this year.

5248
03:24:13,979 --> 03:24:16,086
And if next year,

5249
03:24:16,086 --> 03:24:19,038
it could nearly double again or more than double

5250
03:24:19,038 --> 03:24:21,761
based on what we see with data center footprints

5251
03:24:21,761 --> 03:24:22,594
like being built out

5252
03:24:22,594 --> 03:24:24,393
all across the US and the rest of the world,

5253
03:24:24,393 --> 03:24:25,543
it's gonna be really hard

5254
03:24:25,543 --> 03:24:27,982
for China to keep up with these rules.

5255
03:24:27,982 --> 03:24:30,288
Yes, there will always be smuggling

5256
03:24:30,288 --> 03:24:33,739
and DeepSeek level models of GPT-4 level models,

5257
03:24:33,739 --> 03:24:36,629
o1 level models capable to train on what China can get,

5258
03:24:36,629 --> 03:24:38,041
even the next tier above that.

5259
03:24:38,041 --> 03:24:41,291
But if we speed run a couple more jumps

5260
03:24:42,641 --> 03:24:45,099
to billion dollar models, $10 billion models,

5261
03:24:45,099 --> 03:24:46,768
then it becomes, hey,

5262
03:24:46,768 --> 03:24:48,242
there is a compute disadvantage for China

5263
03:24:48,242 --> 03:24:50,170
for training models and serving them.

5264
03:24:50,170 --> 03:24:52,024
And the serving part is really critical.

5265
03:24:52,024 --> 03:24:53,983
DeepSeek cannot serve their model today.

5266
03:24:53,983 --> 03:24:56,621
It's completely out of inventory.

5267
03:24:56,621 --> 03:24:58,169
It's already started falling

5268
03:24:58,169 --> 03:24:59,873
in the App Store actually, downloads,

5269
03:24:59,873 --> 03:25:01,618
because you download it, you try and sign up,

5270
03:25:01,618 --> 03:25:02,633
they say, we're not taking registrations,

5271
03:25:02,633 --> 03:25:03,840
'cause they have no capacity.

5272
03:25:03,840 --> 03:25:06,590
You open it up, you get less than five tokens per second

5273
03:25:06,590 --> 03:25:08,420
if you even get your request approved.

5274
03:25:08,420 --> 03:25:10,290
Because there's just no capacity,

5275
03:25:10,290 --> 03:25:12,580
because they just don't have enough GPUs to serve the model,

5276
03:25:12,580 --> 03:25:14,318
even though it's incredibly efficient.

5277
03:25:14,318 --> 03:25:16,068
- It'd be fascinating to watch the smuggling,

5278
03:25:16,068 --> 03:25:19,246
'cause there's drug smuggling.

5279
03:25:19,246 --> 03:25:21,045
That's a market.

5280
03:25:21,045 --> 03:25:23,198
There's weapons smuggling.

5281
03:25:23,198 --> 03:25:25,481
And GPUs will surpass that

5282
03:25:25,481 --> 03:25:27,121
at some point. - Chips are highest value

5283
03:25:27,121 --> 03:25:31,870
per kilogram, probably by far. (chuckles)

5284
03:25:31,870 --> 03:25:32,938
- Oh man. - I have another question

5285
03:25:32,938 --> 03:25:33,771
for you, Dylan.

5286
03:25:33,771 --> 03:25:36,907
Do you track model API access internationally?

5287
03:25:36,907 --> 03:25:38,919
How easy is it for Chinese companies

5288
03:25:38,919 --> 03:25:42,246
to use hosted model APIs from the US?

5289
03:25:42,246 --> 03:25:43,918
- Yeah, that's incredibly easy.

5290
03:25:43,918 --> 03:25:46,709
OpenAI publicly stated DeepSeek uses their API.

5291
03:25:46,709 --> 03:25:48,362
And they say they have evidence.

5292
03:25:48,362 --> 03:25:50,742
And this is another element of the training regime

5293
03:25:50,742 --> 03:25:52,307
is people at OpenAI have claimed

5294
03:25:52,307 --> 03:25:53,966
that it's a distilled model.

5295
03:25:53,966 --> 03:25:56,140
i.e, you're taking OpenAI's model,

5296
03:25:56,140 --> 03:25:57,577
you're generating a lot of output,

5297
03:25:57,577 --> 03:25:59,960
and then you're training on the output in their model.

5298
03:25:59,960 --> 03:26:01,379
- Yeah. - And even if that's the case,

5299
03:26:01,379 --> 03:26:03,068
what they did is still amazing, by the way,

5300
03:26:03,068 --> 03:26:04,415
what DeepSeek did, efficiency-wise.

5301
03:26:04,415 --> 03:26:06,372
- Distillation is standard practice in industry.

5302
03:26:06,372 --> 03:26:08,249
Whether or not, if you're at a closed lab

5303
03:26:08,249 --> 03:26:10,539
where you care about terms of service and IP closely,

5304
03:26:10,539 --> 03:26:11,790
you distill from your own models.

5305
03:26:11,790 --> 03:26:13,195
If you are a researcher

5306
03:26:13,195 --> 03:26:14,609
and you're not building any products,

5307
03:26:14,609 --> 03:26:16,405
you distill from the OpenAI models.

5308
03:26:16,405 --> 03:26:17,501
- This is a good opportunity.

5309
03:26:17,501 --> 03:26:21,825
Can you explain big picture distillation as a process?

5310
03:26:21,825 --> 03:26:23,105
What is distillation?

5311
03:26:23,105 --> 03:26:23,938
What's the process - We've

5312
03:26:23,938 --> 03:26:24,771
- of distillation? - talked a lot about

5313
03:26:24,771 --> 03:26:26,048
training language models.

5314
03:26:26,048 --> 03:26:27,470
They are trained on text.

5315
03:26:27,470 --> 03:26:28,599
And post-training,

5316
03:26:28,599 --> 03:26:30,838
you're trying to train on very high quality text

5317
03:26:30,838 --> 03:26:33,119
that you want the model to match the features of.

5318
03:26:33,119 --> 03:26:34,111
Or if you're using RL,

5319
03:26:34,111 --> 03:26:36,169
you're letting the model find its own thing.

5320
03:26:36,169 --> 03:26:37,371
But for supervised fine-tuning,

5321
03:26:37,371 --> 03:26:40,140
for preference data, you need to have some completions

5322
03:26:40,140 --> 03:26:42,739
what the model is trying to learn to imitate.

5323
03:26:42,739 --> 03:26:46,607
And what you do there is, instead of a human data

5324
03:26:46,607 --> 03:26:48,821
or instead of the model you're currently training,

5325
03:26:48,821 --> 03:26:51,291
you take completions from a different,

5326
03:26:51,291 --> 03:26:52,980
normally more powerful model.

5327
03:26:52,980 --> 03:26:56,120
I think there's rumors that these big models

5328
03:26:56,120 --> 03:26:59,302
that people are waiting for, these GPT-5s of the world,

5329
03:26:59,302 --> 03:27:01,273
the Claude 3 Opuses of the world,

5330
03:27:01,273 --> 03:27:04,158
are used internally to do this distillation process

5331
03:27:04,158 --> 03:27:05,060
at OpenAI. - There's also

5332
03:27:05,060 --> 03:27:08,300
public examples, like Meta explicitly stated,

5333
03:27:08,300 --> 03:27:09,728
not necessarily distilling,

5334
03:27:09,728 --> 03:27:12,298
but they used 405b as a reward model

5335
03:27:12,298 --> 03:27:14,395
for 70b in their Llama 3.2

5336
03:27:14,395 --> 03:27:15,228
- Yes. - or 3.3.

5337
03:27:15,228 --> 03:27:16,639
This is all the same topic.

5338
03:27:16,639 --> 03:27:20,931
- So, (sighs) is this ethical? Is this legal?

5339
03:27:20,931 --> 03:27:25,510
Why is that "Financial Times" article headline say,

5340
03:27:25,510 --> 03:27:28,129
OpenAI says that there's evidence

5341
03:27:28,129 --> 03:27:31,804
that China's DeepSeek used its model to train competitor.

5342
03:27:31,804 --> 03:27:34,611
- This is a long, at least in the academic side

5343
03:27:34,611 --> 03:27:36,021
and research side, it has a long history,

5344
03:27:36,021 --> 03:27:37,982
'cause you're trying to interpret OpenAI's rule.

5345
03:27:37,982 --> 03:27:40,201
OpenAI's terms of service say

5346
03:27:40,201 --> 03:27:41,850
that you cannot build a competitor

5347
03:27:41,850 --> 03:27:43,310
with outputs from their models.

5348
03:27:43,310 --> 03:27:44,990
Terms of service are different than a license,

5349
03:27:44,990 --> 03:27:47,823
which are essentially a contract between organizations.

5350
03:27:47,823 --> 03:27:50,310
So, if you have a terms of service on OpenAI's account,

5351
03:27:50,310 --> 03:27:53,071
if I violate it, OpenAI can cancel my account.

5352
03:27:53,071 --> 03:27:54,632
This is very different than like a license

5353
03:27:54,632 --> 03:27:56,742
that says how you could use a downstream artifact.

5354
03:27:56,742 --> 03:27:58,173
So, a lot of it hinges on a word

5355
03:27:58,173 --> 03:28:00,078
that is very unclear in the AI space,

5356
03:28:00,078 --> 03:28:01,491
which is what is a competitor?

5357
03:28:01,491 --> 03:28:02,591
And so- - And then,

5358
03:28:02,591 --> 03:28:04,206
the ethical aspect of it is like,

5359
03:28:04,206 --> 03:28:06,155
why is it unethical for me

5360
03:28:06,155 --> 03:28:07,244
to train on your bottle - Yeah.

5361
03:28:07,244 --> 03:28:09,013
- when you can train on the internet's text?

5362
03:28:09,013 --> 03:28:09,873
- Yeah. - Right?

5363
03:28:09,873 --> 03:28:11,954
- So, there's a bit of a hypocrisy,

5364
03:28:11,954 --> 03:28:16,954
because OpenAI and potentially most of the companies

5365
03:28:16,973 --> 03:28:20,081
trained on the internet's text without permission.

5366
03:28:20,081 --> 03:28:21,502
- There's also a clear loophole,

5367
03:28:21,502 --> 03:28:24,955
which is that I generate data from OpenAI,

5368
03:28:24,955 --> 03:28:26,803
and then I upload it somewhere,

5369
03:28:26,803 --> 03:28:28,394
and then somebody else trains on it

5370
03:28:28,394 --> 03:28:30,304
and the link has been broken.

5371
03:28:30,304 --> 03:28:32,704
They're not under the same terms of service contract.

5372
03:28:32,704 --> 03:28:34,913
- This is why- - There's a lot of hippo...

5373
03:28:34,913 --> 03:28:37,322
There's a lot of to be discovered details

5374
03:28:37,322 --> 03:28:38,544
that don't make a lot of sense.

5375
03:28:38,544 --> 03:28:40,193
- This is why a lot of models today,

5376
03:28:40,193 --> 03:28:42,602
even if they train on zero OpenAI data,

5377
03:28:42,602 --> 03:28:45,153
you ask the model who trained you, it'll say,

5378
03:28:45,153 --> 03:28:47,513
I'm ChatGPT trained by OpenAI. - Yeah.

5379
03:28:47,513 --> 03:28:49,353
- Because there's so much copy-paste

5380
03:28:49,353 --> 03:28:52,203
of OpenAI outputs from that on the internet

5381
03:28:52,203 --> 03:28:53,663
that you just weren't able to filter it out.

5382
03:28:53,663 --> 03:28:55,171
And there was nothing in the RL

5383
03:28:55,171 --> 03:28:59,016
where they implemented like, hey, or post-training or SFT,

5384
03:28:59,016 --> 03:29:00,206
whatever, that says, hey,

5385
03:29:00,206 --> 03:29:03,038
I'm actually modeled by Allen Institute instead

5386
03:29:03,038 --> 03:29:04,725
of OpenAI. - We have to do this

5387
03:29:04,725 --> 03:29:05,558
if we serve a demo.

5388
03:29:05,558 --> 03:29:08,941
We do research and we use OpenAI APIs, because it's useful

5389
03:29:08,941 --> 03:29:10,230
and we want to understand post-training,

5390
03:29:10,230 --> 03:29:11,847
and our research models,

5391
03:29:11,847 --> 03:29:13,870
they'll say they're written by OpenAI

5392
03:29:13,870 --> 03:29:16,134
unless we put in the system prop that we talked about that.

5393
03:29:16,134 --> 03:29:17,901
Like, I am Tulu, I am a language model

5394
03:29:17,901 --> 03:29:19,602
trained by the Allen Institute for AI.

5395
03:29:19,602 --> 03:29:22,055
And if you ask more people around industry,

5396
03:29:22,055 --> 03:29:25,181
especially with post-training, it's a very doable task

5397
03:29:25,181 --> 03:29:27,381
to make the model say who it is

5398
03:29:27,381 --> 03:29:29,459
or to suppress the OpenAI thing.

5399
03:29:29,459 --> 03:29:32,069
So, in some levels, it might be that DeepSeek

5400
03:29:32,069 --> 03:29:35,174
didn't care that it was saying that it was by OpenAI.

5401
03:29:35,174 --> 03:29:36,645
If you're gonna upload model weights,

5402
03:29:36,645 --> 03:29:37,478
it doesn't really matter,

5403
03:29:37,478 --> 03:29:39,519
'cause anyone that's serving it in an application

5404
03:29:39,519 --> 03:29:43,269
and cares a lot about serving is going to, when serving it,

5405
03:29:43,269 --> 03:29:45,327
if they're using it for a specific task,

5406
03:29:45,327 --> 03:29:46,459
they're gonna tailor it to that.

5407
03:29:46,459 --> 03:29:48,891
And it doesn't matter that it's saying it's ChatGPT.

5408
03:29:48,891 --> 03:29:50,949
- Oh, I guess the one of the ways to do that

5409
03:29:50,949 --> 03:29:52,649
is like a system prompt or something like that.

5410
03:29:52,649 --> 03:29:55,146
Like if you're serving it to say that you're-

5411
03:29:55,146 --> 03:29:55,979
- That's what we do.

5412
03:29:55,979 --> 03:29:59,517
If we host a demo, you say you are Tulu 3,

5413
03:29:59,517 --> 03:30:01,947
a language model trained by the Allen Institute for AI.

5414
03:30:01,947 --> 03:30:04,887
We also are benefited from OpenAI data,

5415
03:30:04,887 --> 03:30:06,417
'cause it's a great research tool.

5416
03:30:06,417 --> 03:30:08,367
- Do you think there's any truth

5417
03:30:08,367 --> 03:30:10,284
and value to the claim,

5418
03:30:12,187 --> 03:30:14,872
OpenAI's claim that there's evidence that China's DeepSeek

5419
03:30:14,872 --> 03:30:16,401
use this model to train?

5420
03:30:16,401 --> 03:30:19,379
- I think everyone has benefited regardless,

5421
03:30:19,379 --> 03:30:21,890
because the data's on the internet.

5422
03:30:21,890 --> 03:30:23,811
And therefore, it's in your pre-training now.

5423
03:30:23,811 --> 03:30:25,062
There are like subreddits

5424
03:30:25,062 --> 03:30:27,507
where people share the best ChatGPT outputs.

5425
03:30:27,507 --> 03:30:29,181
And those are in your-

5426
03:30:29,181 --> 03:30:31,631
- I think that they're trying to shift the narrative,

5427
03:30:31,631 --> 03:30:33,421
like they're trying to protect themselves.

5428
03:30:33,421 --> 03:30:35,119
And we saw this years ago

5429
03:30:35,119 --> 03:30:36,622
when ByteDance was actually banned

5430
03:30:36,622 --> 03:30:39,090
from some OpenAI APIs for training on outputs.

5431
03:30:39,090 --> 03:30:42,242
There's other AI startups that most people,

5432
03:30:42,242 --> 03:30:44,555
if you're in the AI culture, were like,

5433
03:30:44,555 --> 03:30:47,049
they just told us they trained on OpenAI outputs

5434
03:30:47,049 --> 03:30:48,078
and they never got banned.

5435
03:30:48,078 --> 03:30:50,857
That's how they bootstrapped their early models.

5436
03:30:50,857 --> 03:30:53,350
So, it's much easier to get off the ground using this

5437
03:30:53,350 --> 03:30:56,206
than to set up human pipelines and build a strong model.

5438
03:30:56,206 --> 03:30:57,718
So, it's long history here

5439
03:30:57,718 --> 03:30:59,136
and a lot of the communications

5440
03:30:59,136 --> 03:31:00,423
are seem like narrative control.

5441
03:31:00,423 --> 03:31:02,519
- Actually, over the last couple days,

5442
03:31:02,519 --> 03:31:05,120
we've seen a lot of people distill DeepSeek's model

5443
03:31:05,120 --> 03:31:06,286
into Llama models,

5444
03:31:06,286 --> 03:31:07,955
because the DeepSeek models - Mm-hmm.

5445
03:31:07,955 --> 03:31:09,708
- are complicated to run inference on,

5446
03:31:09,708 --> 03:31:11,305
because they're mixture of experts

5447
03:31:11,305 --> 03:31:14,257
and they're 600 plus billion parameters and all this.

5448
03:31:14,257 --> 03:31:15,328
And people distilled them

5449
03:31:15,328 --> 03:31:16,365
into the Llama models. (Lex laughing)

5450
03:31:16,365 --> 03:31:18,468
Because the Llama models are so easy to serve,

5451
03:31:18,468 --> 03:31:19,700
and everyone's built the pipelines

5452
03:31:19,700 --> 03:31:20,790
and tooling for inference - Yeah.

5453
03:31:20,790 --> 03:31:21,741
- with the Llama models,

5454
03:31:21,741 --> 03:31:24,270
because it's the open standard. (chuckles)

5455
03:31:24,270 --> 03:31:26,539
So, we've seen it, we've seen a roundabout.

5456
03:31:26,539 --> 03:31:28,662
Is it bad? Is it illegal?

5457
03:31:28,662 --> 03:31:30,281
Maybe it's illegal, whatever. I don't know about that.

5458
03:31:30,281 --> 03:31:31,828
But it's- - It could break contracts.

5459
03:31:31,828 --> 03:31:33,982
I don't think it's illegal, like in any illegal,

5460
03:31:33,982 --> 03:31:35,850
like no one's going to jail for this ever.

5461
03:31:35,850 --> 03:31:37,990
- I think fundamentally, I think it's ethical

5462
03:31:37,990 --> 03:31:39,787
or I hope it's ethical,

5463
03:31:39,787 --> 03:31:44,787
because the moment becomes, we ban that kind of thing,

5464
03:31:44,967 --> 03:31:47,826
it's gonna make everybody much worse off.

5465
03:31:47,826 --> 03:31:51,237
And I also actually, this is difficult,

5466
03:31:51,237 --> 03:31:55,218
but I think you should be allowed to train on the internet.

5467
03:31:55,218 --> 03:31:56,058
I know a lot of authors

5468
03:31:56,058 --> 03:31:57,901
and creators are very sensitive about it.

5469
03:31:57,901 --> 03:31:59,916
That's a difficult question.

5470
03:31:59,916 --> 03:32:03,178
But the moment you're not allowed to train on the internet.

5471
03:32:03,178 --> 03:32:04,011
- I agree.

5472
03:32:04,011 --> 03:32:06,139
- I have a schizo take on how you can solve this,

5473
03:32:06,139 --> 03:32:07,357
because it already works. - All right.

5474
03:32:07,357 --> 03:32:08,515
- I have a reasonable take on it.

5475
03:32:08,515 --> 03:32:10,266
- All right, all right. (Nathan laughing)

5476
03:32:10,266 --> 03:32:13,949
- So, Japan has a law which you're allowed

5477
03:32:13,949 --> 03:32:15,659
to train on any training data

5478
03:32:15,659 --> 03:32:18,860
and copyrights don't apply if you wanna train a model, A.

5479
03:32:18,860 --> 03:32:23,117
B, Japan has nine gigawatts of curtailed nuclear power.

5480
03:32:23,117 --> 03:32:26,218
C, Japan is allowed under the AI diffusion rule

5481
03:32:26,218 --> 03:32:28,911
to import as many GPUs as they'd like.

5482
03:32:28,911 --> 03:32:30,958
So, all we have to do, we have a market here to make,

5483
03:32:30,958 --> 03:32:34,258
we build massive data setters, we rent them to the labs,

5484
03:32:34,258 --> 03:32:37,317
and then we train models in a legally permissible way,

5485
03:32:37,317 --> 03:32:38,966
and there's no if, ands, or buts.

5486
03:32:38,966 --> 03:32:42,835
And now, the models have no potential copyright lawsuit

5487
03:32:42,835 --> 03:32:44,335
from "New York Times" or anything like that.

5488
03:32:44,335 --> 03:32:45,887
No, no, it's just completely legal.

5489
03:32:45,887 --> 03:32:46,927
- Yeah. - No, so-

5490
03:32:46,927 --> 03:32:49,258
- Genius. - The early copyright lawsuits

5491
03:32:49,258 --> 03:32:51,531
have fallen in the favor of AI training.

5492
03:32:51,531 --> 03:32:55,266
I would say that the long tail of use

5493
03:32:55,266 --> 03:32:58,055
is gonna go in the side of AI, which is if you do,

5494
03:32:58,055 --> 03:33:02,374
if you scrape the trillions of tokens of data,

5495
03:33:02,374 --> 03:33:04,505
you're not looking and saying,

5496
03:33:04,505 --> 03:33:06,597
this one "New York Times" article is so important to me.

5497
03:33:06,597 --> 03:33:09,296
But if you're doing a audio generation for music

5498
03:33:09,296 --> 03:33:10,925
or image generation and you say,

5499
03:33:10,925 --> 03:33:12,844
make it in the style of X person,

5500
03:33:12,844 --> 03:33:15,137
that's a reasonable case where you could figure out

5501
03:33:15,137 --> 03:33:17,894
what is their profit margin on inference.

5502
03:33:17,894 --> 03:33:19,955
I don't know if it's gonna be the 50/50

5503
03:33:19,955 --> 03:33:22,448
of YouTube creator program or something,

5504
03:33:22,448 --> 03:33:24,895
but I would opt into that program as a writer,

5505
03:33:24,895 --> 03:33:27,062
like, please, like that...

5506
03:33:28,080 --> 03:33:29,519
It's gonna be a rough journey,

5507
03:33:29,519 --> 03:33:32,697
but there will be some solutions like that that make sense.

5508
03:33:32,697 --> 03:33:35,977
But there's a long tail where it's just on the internet.

5509
03:33:35,977 --> 03:33:37,438
- I think one of the other aspects

5510
03:33:37,438 --> 03:33:40,476
of that "Financial Times" article implied.

5511
03:33:40,476 --> 03:33:43,097
And so, that leads to a more general question.

5512
03:33:43,097 --> 03:33:44,813
Do you think there's...

5513
03:33:44,813 --> 03:33:47,959
How difficult is spying, espionage,

5514
03:33:47,959 --> 03:33:50,792
and stealing of actual secret code

5515
03:33:51,709 --> 03:33:54,069
and data from inside companies?

5516
03:33:54,069 --> 03:33:55,276
How much of that is being attempted?

5517
03:33:55,276 --> 03:33:57,949
- Code and data is hard, but ideas is easy.

5518
03:33:57,949 --> 03:33:58,898
Silicon Valley operates (Dylan chuckles)

5519
03:33:58,898 --> 03:34:00,579
- Yeah. - on the way

5520
03:34:00,579 --> 03:34:03,246
that top employees get bought out

5521
03:34:03,246 --> 03:34:04,999
by other companies for a pay raise,

5522
03:34:04,999 --> 03:34:05,888
and a large reason

5523
03:34:05,888 --> 03:34:08,947
why these companies do this is to bring ideas with them.

5524
03:34:08,947 --> 03:34:11,399
And there's no, in California,

5525
03:34:11,399 --> 03:34:13,109
there's rules that certain,

5526
03:34:13,109 --> 03:34:15,887
like non-competes or whatever, are illegal in California

5527
03:34:15,887 --> 03:34:17,831
and whether or not there's NDAs and things,

5528
03:34:17,831 --> 03:34:19,889
that is how a lot of process happens.

5529
03:34:19,889 --> 03:34:23,099
Recently, there was somebody from Gemini

5530
03:34:23,099 --> 03:34:25,560
who helped make this 1 million context length

5531
03:34:25,560 --> 03:34:28,121
and everyone is saying the next Llama who,

5532
03:34:28,121 --> 03:34:29,351
I mean, he went to the Meta team,

5533
03:34:29,351 --> 03:34:31,711
is gonna have 1 million context length.

5534
03:34:31,711 --> 03:34:34,622
And that's how the world works.

5535
03:34:34,622 --> 03:34:36,651
- As far as industrial espionage and things,

5536
03:34:36,651 --> 03:34:40,601
that has been greatly successful in the past.

5537
03:34:40,601 --> 03:34:42,918
The Americans did the Brits,

5538
03:34:42,918 --> 03:34:44,536
the Chinese have done it to the Americans,

5539
03:34:44,536 --> 03:34:46,287
and so on and so forth.

5540
03:34:46,287 --> 03:34:48,645
It is a fact of life.

5541
03:34:48,645 --> 03:34:51,286
And so, to argue industrial espionage

5542
03:34:51,286 --> 03:34:53,933
can be stopped is probably unlikely.

5543
03:34:53,933 --> 03:34:54,795
You can make it difficult.

5544
03:34:54,795 --> 03:34:56,755
But even then, there's all these stories about,

5545
03:34:56,755 --> 03:35:00,696
hey, F35 and F22 have already been given to China

5546
03:35:00,696 --> 03:35:02,557
in terms of design plans and stuff.

5547
03:35:02,557 --> 03:35:06,118
Code and stuff like between, I say companies,

5548
03:35:06,118 --> 03:35:08,565
not nation states is probably very difficult,

5549
03:35:08,565 --> 03:35:10,960
but ideas are discussed a lot.

5550
03:35:10,960 --> 03:35:13,636
Whether it be a house party in San Francisco

5551
03:35:13,636 --> 03:35:15,370
or a company changing employees,

5552
03:35:15,370 --> 03:35:17,953
or always the mythical honeypot

5553
03:35:19,894 --> 03:35:21,182
that always gets talked about,

5554
03:35:21,182 --> 03:35:23,144
like someone gets honeypotted.

5555
03:35:23,144 --> 03:35:24,554
Because everyone working on AI

5556
03:35:24,554 --> 03:35:26,920
is a single dude who's in their 20s and 30s.

5557
03:35:26,920 --> 03:35:30,170
Not everyone, but a insane percentages.

5558
03:35:31,338 --> 03:35:34,406
So, there's always all these you know, and obviously-

5559
03:35:34,406 --> 03:35:36,648
- So, honeypotted is like a spy,

5560
03:35:36,648 --> 03:35:38,777
a female spy approaches you and like-

5561
03:35:38,777 --> 03:35:41,544
- Yeah, yeah. - Or male.

5562
03:35:41,544 --> 03:35:43,967
It's San Francisco, but... (Nathan laughing)

5563
03:35:43,967 --> 03:35:46,175
As a single dude, I will say, in his late 20s,

5564
03:35:46,175 --> 03:35:48,196
it is like, we are very easily corrupted.

5565
03:35:48,196 --> 03:35:49,798
- [Lex] Yeah.

5566
03:35:49,798 --> 03:35:51,208
- Not corrupted myself,

5567
03:35:51,208 --> 03:35:52,819
but we are, we are. - Yeah.

5568
03:35:52,819 --> 03:35:54,006
Everybody else, not me.

5569
03:35:54,006 --> 03:35:54,839
Everybody else. - Yeah. Exactly.

5570
03:35:54,839 --> 03:35:56,630
- I'm too oblivious that I am not single.

5571
03:35:56,630 --> 03:36:00,911
So, I'm safe from one espionage access. (laughs)

5572
03:36:00,911 --> 03:36:02,333
- Yeah, you have to make sure

5573
03:36:02,333 --> 03:36:05,179
to close all security vulnerabilities.

5574
03:36:05,179 --> 03:36:07,781
So, you, Dylan, collect a lot of information

5575
03:36:07,781 --> 03:36:10,853
about each of the mega clusters

5576
03:36:10,853 --> 03:36:13,335
for each of the major AI companies.

5577
03:36:13,335 --> 03:36:16,006
Can you talk about the buildouts

5578
03:36:16,006 --> 03:36:18,378
for each one that stand out?

5579
03:36:18,378 --> 03:36:19,693
- Yeah, so I think the thing

5580
03:36:19,693 --> 03:36:22,640
that's really important about these mega cluster buildouts

5581
03:36:22,640 --> 03:36:26,706
is they're completely unprecedented in scale.

5582
03:36:26,706 --> 03:36:29,305
US, data center power consumption

5583
03:36:29,305 --> 03:36:32,585
has been slowly on the rise and it's gone up to 2, 3%

5584
03:36:32,585 --> 03:36:34,614
even through the cloud computing revolution.

5585
03:36:34,614 --> 03:36:37,238
Data center consumption as a percentage of total US.

5586
03:36:37,238 --> 03:36:40,625
And that's been over decades of data centers, et cetera.

5587
03:36:40,625 --> 03:36:41,837
It's been climbing, climbing slowly.

5588
03:36:41,837 --> 03:36:43,246
But now, 2 to 3%.

5589
03:36:43,246 --> 03:36:47,147
Now, by the end of this decade, even under like,

5590
03:36:47,147 --> 03:36:50,044
when I say 10%, a lot of people that are traditionally

5591
03:36:50,044 --> 03:36:51,211
by 2028, 2030,

5592
03:36:54,630 --> 03:36:56,621
a traditional data center people like, that's nuts.

5593
03:36:56,621 --> 03:36:58,858
But then, people who are in AI

5594
03:36:58,858 --> 03:37:00,072
who have really looked at this

5595
03:37:00,072 --> 03:37:01,663
at the Anthropics and OpenAIs,

5596
03:37:01,663 --> 03:37:02,565
they're like, that's not enough.

5597
03:37:02,565 --> 03:37:04,537
And I'm like, okay. - Mm-hmm. (chuckles)

5598
03:37:04,537 --> 03:37:08,820
- But this is both through globally distributed

5599
03:37:08,820 --> 03:37:10,306
or distributed throughout the US,

5600
03:37:10,306 --> 03:37:12,440
as well as centralized clusters.

5601
03:37:12,440 --> 03:37:14,756
The distributed throughout the US is exciting

5602
03:37:14,756 --> 03:37:15,801
and it's the bulk of it.

5603
03:37:15,801 --> 03:37:20,134
Like, hey, OpenAI or say, Meta is adding a gigawatt.

5604
03:37:22,741 --> 03:37:25,692
But most of it is distributed through the US for inference

5605
03:37:25,692 --> 03:37:26,525
and all these other things.

5606
03:37:26,525 --> 03:37:29,631
- So, maybe we should lay it out what a cluster is.

5607
03:37:29,631 --> 03:37:31,798
So, does this include AWS?

5608
03:37:33,381 --> 03:37:34,214
Maybe it's good

5609
03:37:34,214 --> 03:37:36,202
to talk about the different kinds of clusters

5610
03:37:36,202 --> 03:37:37,879
and what you mean by mega clusters,

5611
03:37:37,879 --> 03:37:39,018
and what's the GPU - Mm-hmm.

5612
03:37:39,018 --> 03:37:40,530
- and what's the computer and what...

5613
03:37:40,530 --> 03:37:41,400
I'm just kidding. - Yeah, yeah, yeah.

5614
03:37:41,400 --> 03:37:42,953
- Not that far back, but yeah.

5615
03:37:42,953 --> 03:37:45,004
So, what do we mean by the clusters,

5616
03:37:45,004 --> 03:37:45,905
- Oh man. - the build outs?

5617
03:37:45,905 --> 03:37:47,231
- I thought I was about to do the Apple ad.

5618
03:37:47,231 --> 03:37:49,350
What's a computer? (group laughing)

5619
03:37:49,350 --> 03:37:53,321
So, traditionally, data centers and data center tasks

5620
03:37:53,321 --> 03:37:56,151
have been a distributed systems problem

5621
03:37:56,151 --> 03:38:00,043
that is capable of being spread very far and widely.

5622
03:38:00,043 --> 03:38:02,734
i.e, I send a request to Google,

5623
03:38:02,734 --> 03:38:05,520
it gets routed to a data center somewhat close to me.

5624
03:38:05,520 --> 03:38:07,729
It does whatever search ranking recommendation,

5625
03:38:07,729 --> 03:38:09,396
sends a result back.

5626
03:38:11,148 --> 03:38:14,639
The nature of the task is changing rapidly in that the task,

5627
03:38:14,639 --> 03:38:16,631
there's two tasks that people are really focused on now.

5628
03:38:16,631 --> 03:38:17,719
It's not database access,

5629
03:38:17,719 --> 03:38:20,328
it's not serve me the right page, serve me the right ad.

5630
03:38:20,328 --> 03:38:22,078
It's now a inference.

5631
03:38:22,978 --> 03:38:24,549
And inference is dramatically different

5632
03:38:24,549 --> 03:38:25,759
from traditional distributed systems,

5633
03:38:25,759 --> 03:38:27,808
but it looks a lot more similar.

5634
03:38:27,808 --> 03:38:29,635
And then, there's training.

5635
03:38:29,635 --> 03:38:31,947
The train inference side is still like, hey,

5636
03:38:31,947 --> 03:38:33,866
I'm gonna put thousands of GPUs

5637
03:38:33,866 --> 03:38:36,989
in blocks all around these data centers.

5638
03:38:36,989 --> 03:38:38,560
I'm gonna run models on them,

5639
03:38:38,560 --> 03:38:41,029
user submits a request, it gets kicked off,

5640
03:38:41,029 --> 03:38:44,226
or hey, my service, they submit a request to my service.

5641
03:38:44,226 --> 03:38:45,059
They're on Word,

5642
03:38:45,059 --> 03:38:46,388
and they're like, oh yeah, help me, Copilot.

5643
03:38:46,388 --> 03:38:47,221
And it starts, kicks it off,

5644
03:38:47,221 --> 03:38:48,796
or I'm on my Windows, Copilot, whatever,

5645
03:38:48,796 --> 03:38:50,229
Apple Intelligence, whatever it is,

5646
03:38:50,229 --> 03:38:52,419
it gets kicked off to a data center.

5647
03:38:52,419 --> 03:38:54,308
And that data center does some work and sends it back.

5648
03:38:54,308 --> 03:38:59,308
That's inference. That is going to be the bulk of compute.

5649
03:38:59,367 --> 03:39:01,655
And that's like, there's thousands of data centers

5650
03:39:01,655 --> 03:39:03,167
that we're tracking with satellites

5651
03:39:03,167 --> 03:39:04,157
and all these other things.

5652
03:39:04,157 --> 03:39:06,481
And those are the bulk of what's being built.

5653
03:39:06,481 --> 03:39:08,229
But the scale of...

5654
03:39:08,229 --> 03:39:10,121
And so, that's like what's really reshaping

5655
03:39:10,121 --> 03:39:11,537
and that's what's getting millions of GPUs.

5656
03:39:11,537 --> 03:39:14,573
But the scale of the largest cluster

5657
03:39:14,573 --> 03:39:17,193
is also really important.

5658
03:39:17,193 --> 03:39:21,776
When we look back at history, or through the age of AI,

5659
03:39:22,677 --> 03:39:24,088
it was a really big deal

5660
03:39:24,088 --> 03:39:27,568
when they did AlexNet on I think two GPUs

5661
03:39:27,568 --> 03:39:28,485
or four GPUs. - Yeah.

5662
03:39:28,485 --> 03:39:29,318
- [Dylan] I don't remember.

5663
03:39:29,318 --> 03:39:30,824
It was a really big deal. - Oh, it's a big deal,

5664
03:39:30,824 --> 03:39:31,679
'cause you use GPUs.

5665
03:39:31,679 --> 03:39:32,512
(Nathan laughing) - It's a big deal

5666
03:39:32,512 --> 03:39:34,562
they use GPUs and they used multiple.

5667
03:39:34,562 --> 03:39:38,315
But then over time, its scale has just been compounding.

5668
03:39:38,315 --> 03:39:42,033
And so, when you skip forward to GPT-3, then GPT 4,

5669
03:39:42,033 --> 03:39:44,686
GPT-4, 20,000 A100 GPUs,

5670
03:39:44,686 --> 03:39:48,355
unprecedented run in terms of the size and the cost.

5671
03:39:48,355 --> 03:39:50,365
A couple hundred million dollars on a YOLO.

5672
03:39:50,365 --> 03:39:51,971
A YOLO run for GPT-4.

5673
03:39:51,971 --> 03:39:54,535
And it yielded this magical improvement

5674
03:39:54,535 --> 03:39:57,277
that was perfectly in line with what was experimented

5675
03:39:57,277 --> 03:39:59,000
and just like a log scale right up.

5676
03:39:59,000 --> 03:40:00,985
- Oh yeah, they had that plot from the paper.

5677
03:40:00,985 --> 03:40:02,300
Scaling the technical were part.

5678
03:40:02,300 --> 03:40:03,762
- The scaling laws were perfect.

5679
03:40:03,762 --> 03:40:05,997
But that's not a crazy number.

5680
03:40:05,997 --> 03:40:10,571
20,000 A100s, roughly each GPU is consuming 400 watts.

5681
03:40:10,571 --> 03:40:12,203
And then, when you add in the whole server,

5682
03:40:12,203 --> 03:40:16,370
everything, it's like 15 to 20 megawatts of power.

5683
03:40:18,535 --> 03:40:20,117
Maybe you could look up what the power

5684
03:40:20,117 --> 03:40:21,457
of consumption of a person is

5685
03:40:21,457 --> 03:40:22,890
because the numbers are gonna get silly.

5686
03:40:22,890 --> 03:40:24,886
But 15 to 20 megawatts

5687
03:40:24,886 --> 03:40:27,207
was standard data center size was just unprecedented.

5688
03:40:27,207 --> 03:40:28,841
That was all GPUs running one task.

5689
03:40:28,841 --> 03:40:30,873
- [Nathan] How many watts is a toaster? (chuckles)

5690
03:40:30,873 --> 03:40:32,803
- A toaster is like also- - That's a good example.

5691
03:40:32,803 --> 03:40:35,146
- A similar power consumption to an A100.

5692
03:40:35,146 --> 03:40:36,580
H100 comes around, (Nathan chuckles)

5693
03:40:36,580 --> 03:40:39,125
they increase the power from like 400 to 700 watts.

5694
03:40:39,125 --> 03:40:40,419
And that's just per GPU.

5695
03:40:40,419 --> 03:40:42,022
And then, there's all the associated stuff around it.

5696
03:40:42,022 --> 03:40:43,435
So, once you count all that,

5697
03:40:43,435 --> 03:40:46,604
it's roughly like 1,200 to 1,400 watts for everything,

5698
03:40:46,604 --> 03:40:48,713
networking, CPUs, memory, blah, blah, blah.

5699
03:40:48,713 --> 03:40:52,763
- So, we should also say, so what's required?

5700
03:40:52,763 --> 03:40:53,655
You said power.

5701
03:40:53,655 --> 03:40:54,854
So, a lot of power is required,

5702
03:40:54,854 --> 03:40:59,074
a lot of heat is generated, so the cooling is required.

5703
03:40:59,074 --> 03:41:01,854
And because there's a lot of GPUs that have to be,

5704
03:41:01,854 --> 03:41:04,995
or CPUs or whatever, they have to be connected.

5705
03:41:04,995 --> 03:41:06,144
So, there's a lot of networking,

5706
03:41:06,144 --> 03:41:07,723
right? - Yeah, yeah, so I think...

5707
03:41:07,723 --> 03:41:10,054
Yeah, sorry for skipping past that.

5708
03:41:10,054 --> 03:41:11,872
And then, the data center itself is complicated.

5709
03:41:11,872 --> 03:41:13,061
But these are still

5710
03:41:13,061 --> 03:41:15,814
standardized data centers for GPT-4 scale.

5711
03:41:15,814 --> 03:41:19,867
Now, we step forward to what is the scale of clusters

5712
03:41:19,867 --> 03:41:22,027
that people have built last year.

5713
03:41:22,027 --> 03:41:24,584
And it ranges widely.

5714
03:41:24,584 --> 03:41:27,033
It ranges from like, hey, these are standard data centers

5715
03:41:27,033 --> 03:41:28,662
and we're just using multiple of them

5716
03:41:28,662 --> 03:41:30,070
and connecting them together really

5717
03:41:30,070 --> 03:41:31,474
with a ton of fiber between them,

5718
03:41:31,474 --> 03:41:32,603
a lot of networking, et cetera.

5719
03:41:32,603 --> 03:41:35,266
That's what OpenAI and Microsoft did in Arizona.

5720
03:41:35,266 --> 03:41:37,073
And so, they have 100,000 GPUs.

5721
03:41:37,073 --> 03:41:38,504
Meta, similar thing.

5722
03:41:38,504 --> 03:41:40,664
They took their standard existing data center design,

5723
03:41:40,664 --> 03:41:42,158
and it looks like an H,

5724
03:41:42,158 --> 03:41:44,353
and they connected multiple on 'em together.

5725
03:41:44,353 --> 03:41:45,705
And they got to,

5726
03:41:45,705 --> 03:41:49,283
they first did 16,000 GPUs, 24,000 GPUs total.

5727
03:41:49,283 --> 03:41:50,355
Only 16 of them,

5728
03:41:50,355 --> 03:41:51,882
1,000 of 'em were running on the training run,

5729
03:41:51,882 --> 03:41:53,374
because GPUs are very unreliable.

5730
03:41:53,374 --> 03:41:55,455
So, they needed to have spares to swap in and out.

5731
03:41:55,455 --> 03:41:57,364
All the way to like now 100,000 GPUs

5732
03:41:57,364 --> 03:41:59,422
that they're training on Llama 4 on currently.

5733
03:41:59,422 --> 03:42:01,005
Like 128,000 or so.

5734
03:42:02,603 --> 03:42:07,603
Think about 100,000 GPUs with roughly 1,400 watts a piece,

5735
03:42:08,133 --> 03:42:12,875
that's 140 megawatts, 150 megawatts for 128, right?

5736
03:42:12,875 --> 03:42:13,714
So, you're talking about

5737
03:42:13,714 --> 03:42:16,904
you've jumped from 15 to 20 megawatts to 10x,

5738
03:42:16,904 --> 03:42:19,273
almost 10x that number, 9x that number

5739
03:42:19,273 --> 03:42:23,356
to 150 megawatts in two years, from 2022 to 2024.

5740
03:42:24,642 --> 03:42:27,434
And some people like Elon that he admittedly,

5741
03:42:27,434 --> 03:42:30,183
and he says himself got into the game a little bit late

5742
03:42:30,183 --> 03:42:32,151
for pre-training large language models.

5743
03:42:32,151 --> 03:42:33,154
xAI was started later.

5744
03:42:33,154 --> 03:42:36,891
But then, he bet heaven and hell to get his data center up

5745
03:42:36,891 --> 03:42:38,471
and get the largest cluster in the world,

5746
03:42:38,471 --> 03:42:40,687
which is 200,000 GPUs.

5747
03:42:40,687 --> 03:42:43,935
And he did that. He bought a factory in Memphis.

5748
03:42:43,935 --> 03:42:47,046
He's upgrading the substation with the same time

5749
03:42:47,046 --> 03:42:48,845
he's got a bunch of mobile power generation,

5750
03:42:48,845 --> 03:42:50,765
a bunch of single cycle combine.

5751
03:42:50,765 --> 03:42:52,196
He tapped the natural gas line

5752
03:42:52,196 --> 03:42:53,638
that's right next to the factory

5753
03:42:53,638 --> 03:42:55,626
and is just pulling a ton of gas, burning gas.

5754
03:42:55,626 --> 03:42:57,246
He's generating all this power.

5755
03:42:57,246 --> 03:42:59,957
He's in a factory and an old appliance factory

5756
03:42:59,957 --> 03:43:02,334
that's shut down and moved to China long ago.

5757
03:43:02,334 --> 03:43:05,744
And he's got 200,000 GPUs in it.

5758
03:43:05,744 --> 03:43:07,092
And now, what's the next scale?

5759
03:43:07,092 --> 03:43:08,687
All the hyperscalers have done this.

5760
03:43:08,687 --> 03:43:11,396
Now, the next scale is something that's even bigger.

5761
03:43:11,396 --> 03:43:13,575
And so, Elon, just to stick on the topic,

5762
03:43:13,575 --> 03:43:16,196
he's building his own natural gas plant,

5763
03:43:16,196 --> 03:43:18,266
like a proper one right next door.

5764
03:43:18,266 --> 03:43:21,795
He's deploying tons of Tesla megapack batteries

5765
03:43:21,795 --> 03:43:24,544
to make the power more smooth and all sorts of other things.

5766
03:43:24,544 --> 03:43:27,757
He's got industrial chillers to cool the water down

5767
03:43:27,757 --> 03:43:29,649
because he's water-cooling the chips.

5768
03:43:29,649 --> 03:43:30,834
So, all these crazy things

5769
03:43:30,834 --> 03:43:33,710
to get the clusters bigger and bigger.

5770
03:43:33,710 --> 03:43:35,176
But when you look at, like, say,

5771
03:43:35,176 --> 03:43:38,164
what OpenAI did with Stargate,

5772
03:43:38,164 --> 03:43:41,081
that in Arizona, in Abilene, Texas,

5773
03:43:42,481 --> 03:43:43,530
what they've announced at least.

5774
03:43:43,530 --> 03:43:45,735
It's not built. Elon says they don't have the money.

5775
03:43:45,735 --> 03:43:48,297
There's some debates about this.

5776
03:43:48,297 --> 03:43:50,527
But at full-scale, at least the first section

5777
03:43:50,527 --> 03:43:52,326
is definitely money's accounted for,

5778
03:43:52,326 --> 03:43:53,217
but there's multiple sections.

5779
03:43:53,217 --> 03:43:56,967
But full scale, that data center is gonna be 2.2 gigawatts.

5780
03:43:56,967 --> 03:43:58,897
2,200 megawatts of power in

5781
03:43:58,897 --> 03:44:02,564
and roughly 1.8 gigawatts or 1,800 megawatt,

5782
03:44:03,972 --> 03:44:07,387
yeah, 1,800 megawatts of power delivered to chips.

5783
03:44:07,387 --> 03:44:09,503
Now, this is an absurd scale.

5784
03:44:09,503 --> 03:44:13,529
2.2 gigawatts is like more than most cities to be clear.

5785
03:44:13,529 --> 03:44:16,022
And delivered to a single cluster

5786
03:44:16,022 --> 03:44:19,003
that's connected to do training.

5787
03:44:19,003 --> 03:44:20,782
To train these models, to do both the pre-training,

5788
03:44:20,782 --> 03:44:22,483
the post-training, all of this stuff.

5789
03:44:22,483 --> 03:44:24,135
- This is insane. - It is.

5790
03:44:24,135 --> 03:44:24,968
- Insane. - What is a nuclear

5791
03:44:24,968 --> 03:44:26,497
power plant again- - everyone is doing this.

5792
03:44:26,497 --> 03:44:27,992
Everyone is doing this. (Lex laughing)

5793
03:44:27,992 --> 03:44:29,789
Meta in Louisiana,

5794
03:44:29,789 --> 03:44:33,417
they're building two natural gas plants, massive ones,

5795
03:44:33,417 --> 03:44:36,430
and then they're building this massive data center.

5796
03:44:36,430 --> 03:44:39,280
Amazon has plans for this scale.

5797
03:44:39,280 --> 03:44:42,197
Google has plans for this scale.

5798
03:44:42,197 --> 03:44:45,006
xAI has plans for this scale.

5799
03:44:45,006 --> 03:44:46,257
The guys that are racing,

5800
03:44:46,257 --> 03:44:48,643
the companies that are racing are racing hard,

5801
03:44:48,643 --> 03:44:51,965
and they're doing multi gigawatt data centers

5802
03:44:51,965 --> 03:44:53,389
to build this out,

5803
03:44:53,389 --> 03:44:57,406
because they think that, yeah, if I now have...

5804
03:44:57,406 --> 03:44:59,348
Obviously, pre-training scaling is gonna continue,

5805
03:44:59,348 --> 03:45:00,181
but to some extent,

5806
03:45:00,181 --> 03:45:01,586
but then also all this post-training stuff,

5807
03:45:01,586 --> 03:45:04,880
where you have a RL sandbox for computer use or whatever.

5808
03:45:04,880 --> 03:45:05,745
This is where they're gonna...

5809
03:45:05,745 --> 03:45:07,388
And all these viable domains

5810
03:45:07,388 --> 03:45:08,683
where they just keep learning and learning

5811
03:45:08,683 --> 03:45:10,679
and learning, self-play, whatever, whatever it is,

5812
03:45:10,679 --> 03:45:12,606
makes the AI so much more capable

5813
03:45:12,606 --> 03:45:14,458
because the line does go up.

5814
03:45:14,458 --> 03:45:16,693
As you throw more compute, you get more performance.

5815
03:45:16,693 --> 03:45:19,192
The shirt is about scaling laws.

5816
03:45:19,192 --> 03:45:21,081
To some extent, it is diminishing returns.

5817
03:45:21,081 --> 03:45:23,932
You 10x the compute, you don't get 10x better model.

5818
03:45:23,932 --> 03:45:24,965
You get a diminishing returns,

5819
03:45:24,965 --> 03:45:27,113
but also you get efficiency improvements.

5820
03:45:27,113 --> 03:45:28,480
So, you bend the curve.

5821
03:45:28,480 --> 03:45:31,098
And these scale of data centers are doing,

5822
03:45:31,098 --> 03:45:34,733
wreaking a lot of havoc on the network.

5823
03:45:34,733 --> 03:45:37,159
Nathan was mentioning there's,

5824
03:45:37,159 --> 03:45:40,773
Amazon has tried to buy this nuclear power plant, Talen.

5825
03:45:40,773 --> 03:45:42,047
And if you look at the Talen's stock,

5826
03:45:42,047 --> 03:45:43,295
it's just skyrocketing.

5827
03:45:43,295 --> 03:45:45,638
And they're building a massive

5828
03:45:45,638 --> 03:45:47,075
multi gigawatt data center there.

5829
03:45:47,075 --> 03:45:48,030
And you just go down the list.

5830
03:45:48,030 --> 03:45:49,753
There's so many ramifications.

5831
03:45:49,753 --> 03:45:51,985
Interesting thing is certain regions of the US

5832
03:45:51,985 --> 03:45:56,386
transmitting power cost more than actually generating it.

5833
03:45:56,386 --> 03:45:58,886
Because the grid is so slow to build,

5834
03:45:58,886 --> 03:46:01,149
and the demand for power and the ability to build power

5835
03:46:01,149 --> 03:46:03,468
and re-ramping on a natural gas plant

5836
03:46:03,468 --> 03:46:05,407
or even a coal plant is like easy enough to do.

5837
03:46:05,407 --> 03:46:07,177
But transmitting the power is really hard.

5838
03:46:07,177 --> 03:46:10,080
So, in some parts of the US, like in Virginia,

5839
03:46:10,080 --> 03:46:12,643
it costs more to transmit power than it cost to generate it.

5840
03:46:12,643 --> 03:46:14,165
Which is like, there's all sorts

5841
03:46:14,165 --> 03:46:16,616
of second order effects that are insane here.

5842
03:46:16,616 --> 03:46:18,787
- And the power grid support this kind of growth?

5843
03:46:18,787 --> 03:46:20,006
- Trump's executive orders,

5844
03:46:20,006 --> 03:46:21,687
there was a Biden executive order

5845
03:46:21,687 --> 03:46:22,827
before the end of the year,

5846
03:46:22,827 --> 03:46:24,377
but then Trump had some more executive orders,

5847
03:46:24,377 --> 03:46:27,564
which hopefully reduce the regulations

5848
03:46:27,564 --> 03:46:30,252
to where, yes, things can be built.

5849
03:46:30,252 --> 03:46:32,243
But yeah, this is a big, big challenge.

5850
03:46:32,243 --> 03:46:34,013
Is building enough power fast enough?

5851
03:46:34,013 --> 03:46:35,973
- Are you gonna basically have a nuclear power plant

5852
03:46:35,973 --> 03:46:38,628
next to a data center for each one of these?

5853
03:46:38,628 --> 03:46:42,648
- So, the fun thing here is this is too slow.

5854
03:46:42,648 --> 03:46:43,561
- [Nathan] To build the power plant.

5855
03:46:43,561 --> 03:46:44,442
- To build a power plant

5856
03:46:44,442 --> 03:46:48,395
or to reconfigure an existing power plant is too slow.

5857
03:46:48,395 --> 03:46:50,967
And so, therefore, you must use natural...

5858
03:46:50,967 --> 03:46:52,846
Data center power consumption is flat.

5859
03:46:52,846 --> 03:46:54,511
It spike- - Which is why nuclear

5860
03:46:54,511 --> 03:46:55,500
is also good for it.

5861
03:46:55,500 --> 03:46:58,081
Like long-term, nuclear is a very natural fit,

5862
03:46:58,081 --> 03:47:00,612
- Yeah, it's- - but you can't do solar

5863
03:47:00,612 --> 03:47:03,068
or anything in the short-term like that.

5864
03:47:03,068 --> 03:47:03,962
- Because data center power is like this.

5865
03:47:03,962 --> 03:47:07,134
You're telling me, I'm gonna buy tens

5866
03:47:07,134 --> 03:47:09,592
of billions of dollars of GPUs and idle them,

5867
03:47:09,592 --> 03:47:10,843
'cause the power's not being generated?

5868
03:47:10,843 --> 03:47:11,676
Power's cheap.

5869
03:47:11,676 --> 03:47:13,142
If you look at the cost of a cluster,

5870
03:47:13,142 --> 03:47:16,152
less than 20% of it is power.

5871
03:47:16,152 --> 03:47:19,513
Most of it is the capital cost and depreciation of the GPUs.

5872
03:47:19,513 --> 03:47:21,512
And so, it's like, well, screw it,

5873
03:47:21,512 --> 03:47:23,324
I'll just build natural gas plants.

5874
03:47:23,324 --> 03:47:24,523
This is what Meta is doing in Louisiana.

5875
03:47:24,523 --> 03:47:26,803
This is what OpenAI's doing in Texas

5876
03:47:26,803 --> 03:47:28,282
and all these different places.

5877
03:47:28,282 --> 03:47:29,820
They may not be doing it directly,

5878
03:47:29,820 --> 03:47:31,764
but they are partnered with someone.

5879
03:47:31,764 --> 03:47:35,454
And so, there is a couple hopes. One is...

5880
03:47:35,454 --> 03:47:37,681
And Elon, what he is doing in Memphis is like,

5881
03:47:37,681 --> 03:47:38,514
to the extreme,

5882
03:47:38,514 --> 03:47:40,185
they're not just using dual combine cycle gas,

5883
03:47:40,185 --> 03:47:41,793
which is like super efficient,

5884
03:47:41,793 --> 03:47:43,392
he's also just using single cycle

5885
03:47:43,392 --> 03:47:46,593
and mobile generators and stuff, which is less efficient.

5886
03:47:46,593 --> 03:47:49,113
But there's also the flip side,

5887
03:47:49,113 --> 03:47:51,562
which is solar power generation is like this,

5888
03:47:51,562 --> 03:47:55,070
and wind is another like this, correlate different.

5889
03:47:55,070 --> 03:47:56,044
So, if you stack both of those,

5890
03:47:56,044 --> 03:47:58,647
plus you get a big chunk of batteries,

5891
03:47:58,647 --> 03:48:00,435
plus you have a little bit of gas,

5892
03:48:00,435 --> 03:48:02,278
it is possible to run it more green.

5893
03:48:02,278 --> 03:48:04,620
It's just the timescales for that is slow.

5894
03:48:04,620 --> 03:48:06,472
So, people are trying, - Mm-hmm.

5895
03:48:06,472 --> 03:48:09,740
- but Meta basically said, whatever,

5896
03:48:09,740 --> 03:48:11,562
don't care about my sustainability pledge.

5897
03:48:11,562 --> 03:48:13,072
Or they'll buy like a power,

5898
03:48:13,072 --> 03:48:14,920
it's called a PPA, power purchasing agreement,

5899
03:48:14,920 --> 03:48:16,460
or they'll be a massive wind farm

5900
03:48:16,460 --> 03:48:18,340
or solar farm, like wherever. - Oh.

5901
03:48:18,340 --> 03:48:19,249
- And then, they'll just pretend

5902
03:48:19,249 --> 03:48:21,215
like those electrons are being consumed by the data center,

5903
03:48:21,215 --> 03:48:23,260
but in reality, they're paying for the power here

5904
03:48:23,260 --> 03:48:25,260
and selling it to the grid, and they're buying power here.

5905
03:48:25,260 --> 03:48:26,093
- [Lex] Yep.

5906
03:48:26,093 --> 03:48:27,791
- And then, another thing is like Microsoft

5907
03:48:27,791 --> 03:48:30,174
quit on some of their sustainability pledges.

5908
03:48:30,174 --> 03:48:32,502
Elon, what he did with Memphis

5909
03:48:32,502 --> 03:48:33,989
is objectively somewhat dirty,

5910
03:48:33,989 --> 03:48:35,454
but he is also doing it in an area

5911
03:48:35,454 --> 03:48:38,373
where there's a bigger natural gas plant right next door

5912
03:48:38,373 --> 03:48:40,402
and like a sewer next, or not a sewer,

5913
03:48:40,402 --> 03:48:42,512
but like a wastewater treatment and a garbage dump nearby.

5914
03:48:42,512 --> 03:48:45,183
And he's obviously made the world

5915
03:48:45,183 --> 03:48:47,580
a lot more clean than that one data center is gonna do.

5916
03:48:47,580 --> 03:48:50,401
So, I think it's fine to some extent.

5917
03:48:50,401 --> 03:48:51,757
And maybe AGI solves

5918
03:48:51,757 --> 03:48:53,172
global warming and stuff, (Lex laughing)

5919
03:48:53,172 --> 03:48:54,763
whatever it is.

5920
03:48:54,763 --> 03:48:57,604
This is sort of the attitude that people at the labs have,

5921
03:48:57,604 --> 03:48:58,838
which is like, yeah, it's great.

5922
03:48:58,838 --> 03:48:59,671
We'll just use gas.

5923
03:48:59,671 --> 03:49:01,506
Because the race is that important.

5924
03:49:01,506 --> 03:49:03,877
And if we lose, that's way worse.

5925
03:49:03,877 --> 03:49:05,662
- I should say that I got a chance

5926
03:49:05,662 --> 03:49:08,001
to visit the Memphis data center.

5927
03:49:08,001 --> 03:49:10,816
- Oh wow. - And it's incredible.

5928
03:49:10,816 --> 03:49:12,483
I visited with Elon.

5929
03:49:14,396 --> 03:49:18,903
Just the teams and the rate of innovation there is insane.

5930
03:49:18,903 --> 03:49:23,385
My sense is that nobody's ever done anything of this scale

5931
03:49:23,385 --> 03:49:25,884
and nobody has certainly ever done anything

5932
03:49:25,884 --> 03:49:29,097
of this scale at the rate that xAI is doing.

5933
03:49:29,097 --> 03:49:31,784
So, they're like figuring out...

5934
03:49:31,784 --> 03:49:33,895
And so, I was sitting in on all these meetings

5935
03:49:33,895 --> 03:49:36,994
where they're brainstorming, it's like, it's insane.

5936
03:49:36,994 --> 03:49:38,675
It's exciting, 'cause they're like,

5937
03:49:38,675 --> 03:49:40,324
they're trying to figure out what the bottlenecks are,

5938
03:49:40,324 --> 03:49:43,035
how to remove the bottlenecks, how to make sure that,

5939
03:49:43,035 --> 03:49:45,574
there's just so many really cool things

5940
03:49:45,574 --> 03:49:48,071
about putting together a data center,

5941
03:49:48,071 --> 03:49:50,571
'cause everything has to work.

5942
03:49:53,103 --> 03:49:55,538
The people that do like the CIS admin,

5943
03:49:55,538 --> 03:49:58,296
the machine learning, all that is the exciting thing, so on,

5944
03:49:58,296 --> 03:50:00,531
but really, the people that run everything (chuckles)

5945
03:50:00,531 --> 03:50:04,920
are the folks that know the low level software

5946
03:50:04,920 --> 03:50:06,771
and hardware that runs everything,

5947
03:50:06,771 --> 03:50:08,504
the networking, all of that.

5948
03:50:08,504 --> 03:50:10,472
And so, you have to make sure

5949
03:50:10,472 --> 03:50:12,263
you have procedures that test everything.

5950
03:50:12,263 --> 03:50:13,751
I think they're using ethernet.

5951
03:50:13,751 --> 03:50:15,341
I don't know how they're doing the networking, but-

5952
03:50:15,341 --> 03:50:17,868
- They're using Nvidia Spectrum-X ethernet.

5953
03:50:17,868 --> 03:50:19,693
There's actually like, I think yeah,

5954
03:50:19,693 --> 03:50:22,232
the unsung heroes are the cooling and electrical systems,

5955
03:50:22,232 --> 03:50:23,065
which are just like (Lex chuckles)

5956
03:50:23,065 --> 03:50:23,925
- Exactly. - glossed over.

5957
03:50:23,925 --> 03:50:24,883
- [Lex] Yeah.

5958
03:50:24,883 --> 03:50:27,298
- But I think one story

5959
03:50:27,298 --> 03:50:30,447
that maybe is like exemplifies how insane this stuff is,

5960
03:50:30,447 --> 03:50:33,863
is when you're training, you're always doing,

5961
03:50:33,863 --> 03:50:35,996
you're running through the model a bunch,

5962
03:50:35,996 --> 03:50:37,164
in the most simplistic terms,

5963
03:50:37,164 --> 03:50:38,426
running through the model a bunch.

5964
03:50:38,426 --> 03:50:41,314
And then, you're gonna exchange everything

5965
03:50:41,314 --> 03:50:42,858
and synchronize the weights.

5966
03:50:42,858 --> 03:50:44,286
So, you'll do a step,

5967
03:50:44,286 --> 03:50:45,778
this is like a step in model training.

5968
03:50:45,778 --> 03:50:47,519
And every step, your loss goes down, hopefully.

5969
03:50:47,519 --> 03:50:50,050
And it doesn't always, but in the simplest terms,

5970
03:50:50,050 --> 03:50:52,630
you'll be computing a lot, and then you'll exchange.

5971
03:50:52,630 --> 03:50:54,699
The interesting thing is GPU power is most of it,

5972
03:50:54,699 --> 03:50:56,299
networking power is some, but it's a lot less.

5973
03:50:56,299 --> 03:50:57,819
So, while you're computing,

5974
03:50:57,819 --> 03:50:59,337
your power for your GPUs is here.

5975
03:50:59,337 --> 03:51:00,838
But then when you're exchanging weights,

5976
03:51:00,838 --> 03:51:03,238
if you're not able to overlap communications

5977
03:51:03,238 --> 03:51:05,250
and compute perfectly, there may be a time period

5978
03:51:05,250 --> 03:51:08,027
where your GPUs are just idle, and you're exchanging weights

5979
03:51:08,027 --> 03:51:09,449
and you're like, hey, the model's updating.

5980
03:51:09,449 --> 03:51:11,496
So, you're exchanging the gradient, you do the model update,

5981
03:51:11,496 --> 03:51:13,317
and then you start training again.

5982
03:51:13,317 --> 03:51:15,111
So, the power goes...

5983
03:51:15,111 --> 03:51:16,868
- Mm-hmm. - And it's super spiky.

5984
03:51:16,868 --> 03:51:19,378
- Yeah. - And so, funnily enough,

5985
03:51:19,378 --> 03:51:21,907
when you talk about the scale of data center power,

5986
03:51:21,907 --> 03:51:22,856
you can blow stuff up

5987
03:51:22,856 --> 03:51:24,710
so easily. - Yeah.

5988
03:51:24,710 --> 03:51:27,128
- And so, Meta actually has,

5989
03:51:27,128 --> 03:51:30,449
accidentally opened upstream something to code in PyTorch

5990
03:51:30,449 --> 03:51:31,888
where they added an operator.

5991
03:51:31,888 --> 03:51:33,319
And I kid you not, whoever made this,

5992
03:51:33,319 --> 03:51:36,959
I wanna hug the guy, because it says PyTorch,

5993
03:51:36,959 --> 03:51:39,269
it's like pytorch.powerplantnoblowup

5994
03:51:39,269 --> 03:51:40,798
(Lex laughing) equals zero or equal one.

5995
03:51:40,798 --> 03:51:42,709
(Nathan laughing) And what it does,

5996
03:51:42,709 --> 03:51:43,849
what it does is amazing.

5997
03:51:43,849 --> 03:51:44,682
- [Lex] Yeah.

5998
03:51:44,682 --> 03:51:46,380
- When you're exchanging the weights,

5999
03:51:46,380 --> 03:51:48,251
the GP will just compute fake numbers,

6000
03:51:48,251 --> 03:51:49,869
so the power doesn't spike too much.

6001
03:51:49,869 --> 03:51:51,550
And so then, the power plants don't blow up

6002
03:51:51,550 --> 03:51:54,269
because the transient spikes screw stuff up.

6003
03:51:54,269 --> 03:51:55,102
- Well, that makes sense.

6004
03:51:55,102 --> 03:51:56,751
You have to do that kind of thing.

6005
03:51:56,751 --> 03:51:58,899
You have to make sure they're not idle.

6006
03:51:58,899 --> 03:52:00,309
Yeah, that's- - And Elon's solution

6007
03:52:00,309 --> 03:52:01,740
was like, "Let me throw a bunch of Tesla mega packs

6008
03:52:01,740 --> 03:52:02,620
and a few other things."

6009
03:52:02,620 --> 03:52:03,709
- Stabilize, yeah. - Everyone

6010
03:52:03,709 --> 03:52:04,542
has different solutions,

6011
03:52:04,542 --> 03:52:05,958
but Meta's at least (Lex laughing)

6012
03:52:05,958 --> 03:52:07,374
was publicly and openly known,

6013
03:52:07,374 --> 03:52:09,400
which is just like, set this operator.

6014
03:52:09,400 --> 03:52:11,628
And what this operator does is it just makes the GPUs

6015
03:52:11,628 --> 03:52:14,269
compute nothing so that the power doesn't spike.

6016
03:52:14,269 --> 03:52:15,259
- But that just tells you

6017
03:52:15,259 --> 03:52:17,237
how much power you're working with.

6018
03:52:17,237 --> 03:52:18,070
It's insane.

6019
03:52:18,070 --> 03:52:20,044
It's insane. - People should just go Google

6020
03:52:20,044 --> 03:52:23,428
like scale, like what does X watts do?

6021
03:52:23,428 --> 03:52:25,210
And go through all the scales from one watt

6022
03:52:25,210 --> 03:52:28,667
to a kilowatt to a megawatt, and you look and stare at that,

6023
03:52:28,667 --> 03:52:31,279
and you're how high on the list a gigawatt is,

6024
03:52:31,279 --> 03:52:33,900
and it's mind-blowing.

6025
03:52:33,900 --> 03:52:35,665
- Can you say something about the cooling?

6026
03:52:35,665 --> 03:52:39,090
So, I know Elon's using liquid cooling,

6027
03:52:39,090 --> 03:52:41,090
I believe, in all cases.

6028
03:52:42,259 --> 03:52:43,797
That's a new thing, right?

6029
03:52:43,797 --> 03:52:45,333
Most of 'em don't use liquid cooling.

6030
03:52:45,333 --> 03:52:46,843
Is there something interesting to say about the cooling?

6031
03:52:46,843 --> 03:52:47,676
- Yeah, yeah.

6032
03:52:47,676 --> 03:52:49,829
So, air cooling has been the de facto standard.

6033
03:52:49,829 --> 03:52:53,219
Throw a bunch of metal heat pipes, et cetera, and fans,

6034
03:52:53,219 --> 03:52:56,415
and that's cooled, that's been enough to cool it.

6035
03:52:56,415 --> 03:52:58,055
People have been dabbling in water cooling.

6036
03:52:58,055 --> 03:53:01,365
Google's TPUs are water-cooled.

6037
03:53:01,365 --> 03:53:03,530
So, they've been doing that for a few years.

6038
03:53:03,530 --> 03:53:06,025
But with GPUs, no one's ever done...

6039
03:53:06,025 --> 03:53:06,957
And no one's ever done the scale

6040
03:53:06,957 --> 03:53:09,331
of water cooling that Elon just did.

6041
03:53:09,331 --> 03:53:10,945
Now, next generation Nvidia

6042
03:53:10,945 --> 03:53:15,239
is for the highest end GPU, it is mandatory water cooling.

6043
03:53:15,239 --> 03:53:16,255
You have to water cool it.

6044
03:53:16,255 --> 03:53:18,587
But Elon did it on this current generation,

6045
03:53:18,587 --> 03:53:20,406
and that required a lot of stuff.

6046
03:53:20,406 --> 03:53:21,936
If you look at some of the satellite photos

6047
03:53:21,936 --> 03:53:25,369
and stuff of the Memphis facility,

6048
03:53:25,369 --> 03:53:27,793
there's all these external water chillers that are sitting,

6049
03:53:27,793 --> 03:53:30,884
basically, it looks like a semi-truck pod thing.

6050
03:53:30,884 --> 03:53:31,935
What's it called? The container.

6051
03:53:31,935 --> 03:53:33,453
But really, those are water chillers.

6052
03:53:33,453 --> 03:53:35,102
And he has like 90 of those water chillers

6053
03:53:35,102 --> 03:53:38,019
just sitting outside, 90 different containers with water,

6054
03:53:38,019 --> 03:53:40,872
like chill the water, bring it back to the data center,

6055
03:53:40,872 --> 03:53:42,494
and then you distribute it to all the chips,

6056
03:53:42,494 --> 03:53:44,564
pull all the heat out, and then send it back.

6057
03:53:44,564 --> 03:53:47,855
And this is both a way to cool the chips,

6058
03:53:47,855 --> 03:53:49,734
but it's also an efficiency thing.

6059
03:53:49,734 --> 03:53:50,567
And going back

6060
03:53:50,567 --> 03:53:53,788
to that sort of three vector thing right there is,

6061
03:53:53,788 --> 03:53:56,630
there is memory bandwidth FLOPS and interconnect.

6062
03:53:56,630 --> 03:53:58,151
The closer the chips are together,

6063
03:53:58,151 --> 03:54:02,748
the easier it is to do high speed interconnects.

6064
03:54:02,748 --> 03:54:04,780
And so, this is also like a reason

6065
03:54:04,780 --> 03:54:05,862
why you wanna go water cooling

6066
03:54:05,862 --> 03:54:07,650
is because you can just put the chips

6067
03:54:07,650 --> 03:54:08,680
right next to each other,

6068
03:54:08,680 --> 03:54:12,347
and therefore get higher speed connectivity.

6069
03:54:13,511 --> 03:54:17,678
- I gotta ask you, so in one of your recent posts,

6070
03:54:18,851 --> 03:54:23,100
there's a section called cluster measuring contest, so...

6071
03:54:23,100 --> 03:54:24,920
- There's another word there, but I won't say it, you know.

6072
03:54:24,920 --> 03:54:27,881
(Dylan and Nathan laughing)

6073
03:54:27,881 --> 03:54:31,001
- Who's got the biggest now and who's gonna have

6074
03:54:31,001 --> 03:54:32,211
the biggest? - Today,

6075
03:54:32,211 --> 03:54:33,678
individual largest is Elon.

6076
03:54:33,678 --> 03:54:35,678
- Right. Elon's cluster.

6077
03:54:36,908 --> 03:54:38,841
- Elon's cluster in Memphis, 200,000 GPUs.

6078
03:54:38,841 --> 03:54:40,413
- Okay.

6079
03:54:40,413 --> 03:54:43,444
- Meta has like 128,000, OpenAI has 100,000 now.

6080
03:54:43,444 --> 03:54:45,881
Now, to be clear, other companies have more GPUs than Elon.

6081
03:54:45,881 --> 03:54:47,905
They just don't have 'em in one place.

6082
03:54:47,905 --> 03:54:50,744
And for training, you want them tightly connected.

6083
03:54:50,744 --> 03:54:52,298
There's some techniques that people are researching

6084
03:54:52,298 --> 03:54:56,334
and working on that lets you train across multiple regions.

6085
03:54:56,334 --> 03:54:59,791
But for the most part, you want them all in one area.

6086
03:54:59,791 --> 03:55:02,476
So, you can connect them with high speed networking.

6087
03:55:02,476 --> 03:55:06,323
And so, Elon today has 200,000 H100s.

6088
03:55:06,323 --> 03:55:09,546
And 100,000 H100s, 100,000 H200s.

6089
03:55:09,546 --> 03:55:12,296
Meta, OpenAI, and Amazon all have

6090
03:55:14,564 --> 03:55:16,907
on the scale of 100,000, a little bit less.

6091
03:55:16,907 --> 03:55:20,268
But this year, this year, people are building much more.

6092
03:55:20,268 --> 03:55:22,255
Anthropic and Amazon are building a cluster

6093
03:55:22,255 --> 03:55:25,492
of 400,000 Trainium 2, which is Amazon-specific chip

6094
03:55:25,492 --> 03:55:28,075
trying to get away from Nvidia.

6095
03:55:30,502 --> 03:55:33,471
Meta and OpenAI have scales for hundreds of thousands,

6096
03:55:33,471 --> 03:55:34,423
but by next year,

6097
03:55:34,423 --> 03:55:38,011
you'll have like 500,000 to 700,000 GPU clusters.

6098
03:55:38,011 --> 03:55:40,823
And note, those GPUs are much higher power consumption

6099
03:55:40,823 --> 03:55:41,732
than existing ones.

6100
03:55:41,732 --> 03:55:45,670
Hopper, 700 watts, Blackwell goes to 1,200 watts.

6101
03:55:45,670 --> 03:55:46,544
So, the power

6102
03:55:46,544 --> 03:55:47,694
per chip is growing (Lex laughing)

6103
03:55:47,694 --> 03:55:49,995
and the number of chips is growing.

6104
03:55:49,995 --> 03:55:53,412
- Nuts. Elon said he'll get to a million.

6105
03:55:54,321 --> 03:55:57,103
You think that's actually feasible?

6106
03:55:57,103 --> 03:55:59,026
- I don't doubt Elon.

6107
03:55:59,026 --> 03:56:01,769
The filings that he has for the power plan

6108
03:56:01,769 --> 03:56:03,264
and the Tesla battery packs,

6109
03:56:03,264 --> 03:56:05,513
it's clear he has some crazy plans for Memphis.

6110
03:56:05,513 --> 03:56:08,721
Like permits and stuff is open record.

6111
03:56:08,721 --> 03:56:13,721
But it's not quite clear what and what the timescales are.

6112
03:56:13,809 --> 03:56:16,641
I just never doubt Elon. He's gonna surprise us.

6113
03:56:16,641 --> 03:56:18,064
- So, what's the idea with these clusters?

6114
03:56:18,064 --> 03:56:19,283
If you have a million GPUs,

6115
03:56:19,283 --> 03:56:21,700
what percentage in let's say,

6116
03:56:23,033 --> 03:56:26,722
two, three years is used for training

6117
03:56:26,722 --> 03:56:27,735
and what percent of pre-training

6118
03:56:27,735 --> 03:56:29,203
and what percent is used

6119
03:56:29,203 --> 03:56:31,912
for the actual computation? - So, these mega clusters

6120
03:56:31,912 --> 03:56:33,820
make no sense for inference.

6121
03:56:33,820 --> 03:56:36,931
You could route inference there and just not train.

6122
03:56:36,931 --> 03:56:39,652
But most of the inference capacity is being,

6123
03:56:39,652 --> 03:56:41,733
hey, I've got a 30-megawatt data center here.

6124
03:56:41,733 --> 03:56:43,823
I've got 50 megawatts here, I've got 100 here, whatever.

6125
03:56:43,823 --> 03:56:45,638
I'll just throw inference in all of those,

6126
03:56:45,638 --> 03:56:47,904
because the mega clusters,

6127
03:56:47,904 --> 03:56:50,879
multi gigawatt data centers, I want to train there.

6128
03:56:50,879 --> 03:56:52,683
Because that's where all of my GPUs are co-located,

6129
03:56:52,683 --> 03:56:53,533
where I can put them

6130
03:56:53,533 --> 03:56:56,632
at a super high networking speed connected together.

6131
03:56:56,632 --> 03:56:58,304
Because that's what you need for training.

6132
03:56:58,304 --> 03:57:00,523
Now, with pre-training, this is the old scale.

6133
03:57:00,523 --> 03:57:02,294
You would increase parameters,

6134
03:57:02,294 --> 03:57:04,754
you'd increase data, model gets better.

6135
03:57:04,754 --> 03:57:06,834
That doesn't apply anymore,

6136
03:57:06,834 --> 03:57:09,745
because there's not much more data in the pre-training side.

6137
03:57:09,745 --> 03:57:11,873
Yes, there's video and audio and image

6138
03:57:11,873 --> 03:57:13,934
that has not been fully taken advantage of.

6139
03:57:13,934 --> 03:57:15,054
So, there's a lot more scaling,

6140
03:57:15,054 --> 03:57:17,458
but a lot of people have transcript,

6141
03:57:17,458 --> 03:57:18,822
taken transcripts of YouTube videos,

6142
03:57:18,822 --> 03:57:20,272
and that gets you a lot of the data.

6143
03:57:20,272 --> 03:57:21,631
Doesn't get you all of the learning value

6144
03:57:21,631 --> 03:57:23,004
out of the video and image data,

6145
03:57:23,004 --> 03:57:26,191
but there's still scaling to be done on pre-training.

6146
03:57:26,191 --> 03:57:27,538
But this post-training world

6147
03:57:27,538 --> 03:57:29,831
is where all the FLOPS are gonna be spent.

6148
03:57:29,831 --> 03:57:31,154
The model's gonna play with itself.

6149
03:57:31,154 --> 03:57:33,566
It's gonna self-play, it's gonna do verifiable tasks,

6150
03:57:33,566 --> 03:57:36,219
it's gonna do computer use in sandboxes.

6151
03:57:36,219 --> 03:57:38,793
It might even do simulated robotics things.

6152
03:57:38,793 --> 03:57:41,344
All of these things are gonna be environments

6153
03:57:41,344 --> 03:57:44,685
where compute is spent in, quote, unquote, "post-training".

6154
03:57:44,685 --> 03:57:46,526
But I think it's gonna be good.

6155
03:57:46,526 --> 03:57:48,633
We're gonna drop the post from post-training.

6156
03:57:48,633 --> 03:57:49,812
- Yeah. Wow. - It's gonna be pre-training

6157
03:57:49,812 --> 03:57:51,245
and it's gonna be training, I think.

6158
03:57:51,245 --> 03:57:52,229
At point at some, (Dylan drowns out Nathan)

6159
03:57:52,229 --> 03:57:53,935
at some point. (Nathan laughing)

6160
03:57:53,935 --> 03:57:57,073
Because for the bulk of the last few years,

6161
03:57:57,073 --> 03:57:59,730
pre-training has dwarfed post-training.

6162
03:57:59,730 --> 03:58:00,563
- [Lex] Mm-hmm.

6163
03:58:00,563 --> 03:58:01,723
- But with these verifiable methods,

6164
03:58:01,723 --> 03:58:04,129
especially ones that scale really,

6165
03:58:04,129 --> 03:58:05,522
potentially infinitely,

6166
03:58:05,522 --> 03:58:07,792
like computer use and robotics, not just math and coding,

6167
03:58:07,792 --> 03:58:09,180
where you can verify what's happening,

6168
03:58:09,180 --> 03:58:11,104
those infinitely verifiable tasks,

6169
03:58:11,104 --> 03:58:13,556
it seems you can spend as much compute as you want on them.

6170
03:58:13,556 --> 03:58:15,042
- Especially at the context length increase.

6171
03:58:15,042 --> 03:58:16,493
'Cause at the end of pre-training

6172
03:58:16,493 --> 03:58:19,131
is when you increase the context length for these models.

6173
03:58:19,131 --> 03:58:21,492
And we've talked earlier in the conversation

6174
03:58:21,492 --> 03:58:24,133
about how the context length, when you have a long input,

6175
03:58:24,133 --> 03:58:25,813
is much easier to manage than output.

6176
03:58:25,813 --> 03:58:27,253
And a lot of these post-training

6177
03:58:27,253 --> 03:58:30,533
and reasoning techniques rely on a ton of sampling

6178
03:58:30,533 --> 03:58:32,562
and it's becoming increasingly long context.

6179
03:58:32,562 --> 03:58:34,286
So, there's just your,

6180
03:58:34,286 --> 03:58:37,879
effectively your compute efficiency goes down.

6181
03:58:37,879 --> 03:58:40,863
I think FLOPS is the standard for how you measure it.

6182
03:58:40,863 --> 03:58:43,452
But with RL and you have to do all these things,

6183
03:58:43,452 --> 03:58:45,353
where you move your weights around

6184
03:58:45,353 --> 03:58:49,533
in a different way than at pre-training and just generation.

6185
03:58:49,533 --> 03:58:51,484
It's going to be become less efficient

6186
03:58:51,484 --> 03:58:53,953
and FLOPS is gonna be less of a useful term.

6187
03:58:53,953 --> 03:58:55,332
And then, as the infrastructure gets better,

6188
03:58:55,332 --> 03:58:56,876
it's probably gonna go back to FLOPS.

6189
03:58:56,876 --> 03:58:59,024
- So, all of the things we've been talking about

6190
03:58:59,024 --> 03:59:02,273
is most likely going to be Nvidia.

6191
03:59:02,273 --> 03:59:03,174
Is there any competitors?

6192
03:59:03,174 --> 03:59:06,195
- Google, I ignored them. - TPU. Yeah. (chuckles)

6193
03:59:06,195 --> 03:59:08,278
- I was like, huh? - Yeah,

6194
03:59:08,278 --> 03:59:09,468
what's the story with TPU?

6195
03:59:09,468 --> 03:59:10,826
Like what's the...

6196
03:59:10,826 --> 03:59:12,997
- TPU is awesome. It's great.

6197
03:59:12,997 --> 03:59:16,025
Google is, they're a bit more tepid

6198
03:59:16,025 --> 03:59:17,609
on building data centers for some reason.

6199
03:59:17,609 --> 03:59:19,220
They're building big data centers, don't get me wrong.

6200
03:59:19,220 --> 03:59:21,969
And they actually have the biggest cluster.

6201
03:59:21,969 --> 03:59:23,507
I was talking about Nvidia clusters,

6202
03:59:23,507 --> 03:59:26,012
they actually have the biggest cluster, period.

6203
03:59:26,012 --> 03:59:28,729
But the way they do it is very interesting.

6204
03:59:28,729 --> 03:59:32,677
They have two data center super regions

6205
03:59:32,677 --> 03:59:34,460
in that, the data center isn't physically

6206
03:59:34,460 --> 03:59:36,498
like all of the GPUs aren't physically on one site,

6207
03:59:36,498 --> 03:59:37,764
but they're like 30 miles from each other.

6208
03:59:37,764 --> 03:59:39,178
Or not GPUs, TPUs.

6209
03:59:39,178 --> 03:59:41,466
They have like in Iowa and Nebraska,

6210
03:59:41,466 --> 03:59:42,595
they have four data centers

6211
03:59:42,595 --> 03:59:44,499
that are just right next to each other.

6212
03:59:44,499 --> 03:59:48,357
- Why doesn't Google flex it's cluster size?

6213
03:59:48,357 --> 03:59:49,727
- Go to multi-datacenter training.

6214
03:59:49,727 --> 03:59:51,936
There's good images in there. So, I'll show you what I mean.

6215
03:59:51,936 --> 03:59:55,088
It's just SemiAnalysis multi-datacenter.

6216
03:59:55,088 --> 03:59:56,716
So, this is an image

6217
03:59:56,716 --> 03:59:58,886
of what a standard Google data center looks like.

6218
03:59:58,886 --> 04:00:00,536
By the way, their data centers look very different

6219
04:00:00,536 --> 04:00:01,978
than anyone else's data centers.

6220
04:00:01,978 --> 04:00:02,948
- [Lex] What are we looking at here?

6221
04:00:02,948 --> 04:00:03,964
- So, these are, yeah,

6222
04:00:03,964 --> 04:00:05,597
so if you see this image right,

6223
04:00:05,597 --> 04:00:08,097
in this center, there are these big rectangular boxes.

6224
04:00:08,097 --> 04:00:10,564
Those are where the actual chips are kept.

6225
04:00:10,564 --> 04:00:13,417
And then, if you scroll down a little bit further,

6226
04:00:13,417 --> 04:00:16,131
you can see there's like these water pipes,

6227
04:00:16,131 --> 04:00:18,281
there's these chiller cooling towers in the top,

6228
04:00:18,281 --> 04:00:20,010
and a bunch of diesel generators.

6229
04:00:20,010 --> 04:00:22,319
The diesel generators are backup power.

6230
04:00:22,319 --> 04:00:24,061
The data center itself

6231
04:00:24,061 --> 04:00:26,911
look physically smaller than the water chillers.

6232
04:00:26,911 --> 04:00:29,801
So, the chips are actually easier to keep together,

6233
04:00:29,801 --> 04:00:31,160
but then cooling all the water

6234
04:00:31,160 --> 04:00:33,441
for the water cooling is very difficult.

6235
04:00:33,441 --> 04:00:35,391
So, Google has a very advanced infrastructure

6236
04:00:35,391 --> 04:00:38,192
that no one else has for the TPU.

6237
04:00:38,192 --> 04:00:40,587
And what they do is they've stamped these data center,

6238
04:00:40,587 --> 04:00:42,227
they've stamped a bunch of these data centers out

6239
04:00:42,227 --> 04:00:43,327
in a few regions.

6240
04:00:43,327 --> 04:00:46,462
So, if you go a little bit further down,

6241
04:00:46,462 --> 04:00:48,549
this is a Microsoft, this is in Arizona.

6242
04:00:48,549 --> 04:00:50,685
This is where GPT-5, quote, unquote, "will be trained".

6243
04:00:50,685 --> 04:00:52,974
(Lex laughing)

6244
04:00:52,974 --> 04:00:54,397
- [Nathan] If it doesn't exist already.

6245
04:00:54,397 --> 04:00:56,481
- Yeah, if it doesn't exist already.

6246
04:00:56,481 --> 04:00:57,448
But each of these data centers,

6247
04:00:57,448 --> 04:00:58,986
I've shown a couple images of them,

6248
04:00:58,986 --> 04:01:00,916
they're really closely co-located

6249
04:01:00,916 --> 04:01:03,196
in the same region, Nebraska, Iowa.

6250
04:01:03,196 --> 04:01:06,565
And then, they also have a similar one in Ohio complex.

6251
04:01:06,565 --> 04:01:09,282
And so, these data centers are really close to each other.

6252
04:01:09,282 --> 04:01:10,152
And what they've done

6253
04:01:10,152 --> 04:01:13,408
is they've connected them super high bandwidth with fiber.

6254
04:01:13,408 --> 04:01:14,936
And so, these are just a bunch of data centers.

6255
04:01:14,936 --> 04:01:17,115
And the point here is that Google

6256
04:01:17,115 --> 04:01:19,690
has a very advanced infrastructure,

6257
04:01:19,690 --> 04:01:21,749
very tightly connected in a small region.

6258
04:01:21,749 --> 04:01:22,723
So, Elon will always

6259
04:01:22,723 --> 04:01:24,743
have the biggest cluster fully connected,

6260
04:01:24,743 --> 04:01:26,451
because it's all in one building.

6261
04:01:26,451 --> 04:01:27,700
- Yeah. - And he's completely right

6262
04:01:27,700 --> 04:01:28,533
on that.

6263
04:01:28,533 --> 04:01:30,482
Google has the biggest cluster,

6264
04:01:30,482 --> 04:01:32,129
but you have to spread over three sites,

6265
04:01:32,129 --> 04:01:33,946
and by a significant margin,

6266
04:01:33,946 --> 04:01:35,717
but you have to go across multiple sites.

6267
04:01:35,717 --> 04:01:39,296
- Why doesn't Google compete with Nvidia?

6268
04:01:39,296 --> 04:01:42,027
Why don't they sell TPUs?

6269
04:01:42,027 --> 04:01:44,044
- I think there's a couple problems with it.

6270
04:01:44,044 --> 04:01:46,877
It's like one, TPU has been a form

6271
04:01:48,067 --> 04:01:51,846
of allowing search to be really freaking cheap

6272
04:01:51,846 --> 04:01:53,042
and build models for that.

6273
04:01:53,042 --> 04:01:55,065
And so, a big chunk of the search,

6274
04:01:55,065 --> 04:01:57,279
GPU purchases or TPU purchases,

6275
04:01:57,279 --> 04:02:00,774
or big chunk of Google's purchases and usage,

6276
04:02:00,774 --> 04:02:02,295
all of it is for internal workloads.

6277
04:02:02,295 --> 04:02:06,666
Whether it be Search, now, Gemini, YouTube,

6278
04:02:06,666 --> 04:02:10,573
all these different applications that they have ads,

6279
04:02:10,573 --> 04:02:12,236
these are where all their TPUs are being spent,

6280
04:02:12,236 --> 04:02:14,297
and that's what they're hyperfocused on.

6281
04:02:14,297 --> 04:02:16,944
And so, there's certain aspects of the architecture

6282
04:02:16,944 --> 04:02:18,763
that are optimized for their use case

6283
04:02:18,763 --> 04:02:21,219
that are not optimized elsewhere.

6284
04:02:21,219 --> 04:02:23,654
One simple one is they've open sourced the Gemma model

6285
04:02:23,654 --> 04:02:25,991
and they called it Gemma 7B.

6286
04:02:25,991 --> 04:02:27,773
But then, it's actually 8 billion parameters

6287
04:02:27,773 --> 04:02:29,454
because the vocabulary is so large.

6288
04:02:29,454 --> 04:02:30,287
(Lex laughing)

6289
04:02:30,287 --> 04:02:32,092
And the reason they made the vocabulary so large

6290
04:02:32,092 --> 04:02:35,601
is because TPU's matrix-multiply unit is massive.

6291
04:02:35,601 --> 04:02:38,019
Because that's what they've optimized for.

6292
04:02:38,019 --> 04:02:38,953
And so, they decided, oh, well,

6293
04:02:38,953 --> 04:02:40,344
I'll just make the vocabulary large too,

6294
04:02:40,344 --> 04:02:41,423
even though it makes no sense

6295
04:02:41,423 --> 04:02:42,834
to do so in such a small model,

6296
04:02:42,834 --> 04:02:44,201
because that fits on their hardware.

6297
04:02:44,201 --> 04:02:45,092
So, Gemma doesn't run

6298
04:02:45,092 --> 04:02:47,507
as efficiently on a GPU as a Llama does.

6299
04:02:47,507 --> 04:02:49,762
But vice versa, Llama doesn't run

6300
04:02:49,762 --> 04:02:52,151
as efficiently on a TPU as a Gemma does.

6301
04:02:52,151 --> 04:02:53,121
And it's so like,

6302
04:02:53,121 --> 04:02:55,692
there's certain aspects of hardware, software, co-design.

6303
04:02:55,692 --> 04:02:57,071
So, all their search models,

6304
04:02:57,071 --> 04:02:58,841
or their ranking and recommendation models,

6305
04:02:58,841 --> 04:03:00,574
all these different models that are AI,

6306
04:03:00,574 --> 04:03:02,379
but not like GenAI,

6307
04:03:02,379 --> 04:03:05,331
have been hyper optimized with TPUs forever.

6308
04:03:05,331 --> 04:03:06,971
The software stack is super optimized,

6309
04:03:06,971 --> 04:03:08,681
but all of this software stack

6310
04:03:08,681 --> 04:03:11,644
has not been released publicly at all.

6311
04:03:11,644 --> 04:03:13,934
Very small portions of it, Jax and XLA have been.

6312
04:03:13,934 --> 04:03:16,350
But the experience when you're inside of Google

6313
04:03:16,350 --> 04:03:18,564
and you're training on TPUs as a researcher,

6314
04:03:18,564 --> 04:03:19,621
you don't need to know anything

6315
04:03:19,621 --> 04:03:20,868
about the hardware in many cases.

6316
04:03:20,868 --> 04:03:22,404
It's pretty beautiful.

6317
04:03:22,404 --> 04:03:23,701
- They all love it. - But soon as you

6318
04:03:23,701 --> 04:03:25,240
step outside... (Nathan chuckles)

6319
04:03:25,240 --> 04:03:27,046
- A lot of 'em go back. (chuckles)

6320
04:03:27,046 --> 04:03:28,457
They leave Google and then they go back.

6321
04:03:28,457 --> 04:03:29,397
- Yeah. - Yeah.

6322
04:03:29,397 --> 04:03:31,194
They leave and they start a company,

6323
04:03:31,194 --> 04:03:32,558
'cause they have all these amazing research ideas

6324
04:03:32,558 --> 04:03:34,857
and they're like, wait, infrastructure's hard,

6325
04:03:34,857 --> 04:03:36,587
software is hard, and this is on GPUs.

6326
04:03:36,587 --> 04:03:38,015
Or if they try to use TPUs, same thing,

6327
04:03:38,015 --> 04:03:39,654
'cause they don't have access to all this code.

6328
04:03:39,654 --> 04:03:41,747
And so, it's like, how do you convince a company

6329
04:03:41,747 --> 04:03:43,367
whose Golden Goose is Search,

6330
04:03:43,367 --> 04:03:45,680
where they're making hundreds of billions of dollars from,

6331
04:03:45,680 --> 04:03:48,469
to start selling GPU, or TPUs,

6332
04:03:48,469 --> 04:03:51,215
which they used to only buy a couple billion of...

6333
04:03:51,215 --> 04:03:55,819
I think in 2023, they bought like a couple billion,

6334
04:03:55,819 --> 04:03:58,169
and now they're buying 10 billion to $15 billion worth.

6335
04:03:58,169 --> 04:03:59,065
But how do you convince them

6336
04:03:59,065 --> 04:04:01,135
that they should just buy like twice as many

6337
04:04:01,135 --> 04:04:03,455
and figure out how to sell them, and make $30 billion?

6338
04:04:03,455 --> 04:04:04,889
Like, who cares about making $30 billion?

6339
04:04:04,889 --> 04:04:07,375
- Won't that 30 billion

6340
04:04:07,375 --> 04:04:11,643
exceed actually the search profit eventually?

6341
04:04:11,643 --> 04:04:13,486
- You're always gonna make more money on services

6342
04:04:13,486 --> 04:04:15,608
than hardware. - Always.

6343
04:04:15,608 --> 04:04:18,063
- Like yeah, to be clear, today,

6344
04:04:18,063 --> 04:04:19,059
people are spending a lot more

6345
04:04:19,059 --> 04:04:21,583
on hardware than they are the services,

6346
04:04:21,583 --> 04:04:25,105
because the hardware runs the service spend.

6347
04:04:25,105 --> 04:04:26,065
- Yeah. - But like-

6348
04:04:26,065 --> 04:04:27,157
- [Lex] You're investing, yeah.

6349
04:04:27,157 --> 04:04:28,977
- If there's no revenue for AI stuff

6350
04:04:28,977 --> 04:04:31,606
or not enough revenue, then obviously, it's gonna blow up.

6351
04:04:31,606 --> 04:04:34,146
People won't continue to spend on GPUs forever.

6352
04:04:34,146 --> 04:04:36,778
And then, Nvidia's trying to move up the stack with software

6353
04:04:36,778 --> 04:04:38,508
that they're trying to sell and license and stuff.

6354
04:04:38,508 --> 04:04:41,930
But Google has never had that DNA

6355
04:04:41,930 --> 04:04:43,967
of this is a product we should sell.

6356
04:04:43,967 --> 04:04:46,000
The Google Cloud does,

6357
04:04:46,000 --> 04:04:47,968
which is a separate organization from the TPU team,

6358
04:04:47,968 --> 04:04:49,927
which is a separate organization from the DeepMind team,

6359
04:04:49,927 --> 04:04:51,700
which is a separate organization from the Search team,

6360
04:04:51,700 --> 04:04:52,689
there's a lot of bureaucracy here.

6361
04:04:52,689 --> 04:04:55,598
- Wait, Google Cloud is a separate team than the TPU team?

6362
04:04:55,598 --> 04:04:58,419
- Technically, TPU sits under infrastructure,

6363
04:04:58,419 --> 04:04:59,627
which sits under Google Cloud,

6364
04:04:59,627 --> 04:05:02,544
but Google Cloud, for renting stuff

6365
04:05:03,728 --> 04:05:07,419
and TPU architecture are very different goals.

6366
04:05:07,419 --> 04:05:10,038
And hardware and software, like all of this.

6367
04:05:10,038 --> 04:05:11,408
The JAX, XLA teams

6368
04:05:11,408 --> 04:05:14,107
do not serve Google's customers externally.

6369
04:05:14,107 --> 04:05:15,649
Whereas Nvidia's various CUDA teams

6370
04:05:15,649 --> 04:05:18,136
for things like NCCL serve external customers.

6371
04:05:18,136 --> 04:05:19,484
- [Lex] Mm-hmm.

6372
04:05:19,484 --> 04:05:22,135
- The internal teams like JAX and XLA and stuff,

6373
04:05:22,135 --> 04:05:23,204
they more so serve DeepMind

6374
04:05:23,204 --> 04:05:24,429
and Search. - Yeah.

6375
04:05:24,429 --> 04:05:25,793
- And so, their customers' different.

6376
04:05:25,793 --> 04:05:27,266
They're not building a product for them.

6377
04:05:27,266 --> 04:05:30,347
- Do you understand why AWS keeps winning

6378
04:05:30,347 --> 04:05:33,930
versus Azure for cloud versus Google Cloud?

6379
04:05:34,935 --> 04:05:36,698
- Yeah, there's- - Google Cloud is tiny,

6380
04:05:36,698 --> 04:05:38,527
isn't it, relative to AWS? - Google Cloud is third.

6381
04:05:38,527 --> 04:05:39,702
- [Nathan] Yeah, yeah.

6382
04:05:39,702 --> 04:05:41,446
- Microsoft is the second biggest,

6383
04:05:41,446 --> 04:05:43,008
but Amazon is the biggest. - Yeah.

6384
04:05:43,008 --> 04:05:44,955
- And Microsoft deceptively

6385
04:05:44,955 --> 04:05:48,206
sort of includes Microsoft Office 365, and things like that.

6386
04:05:48,206 --> 04:05:49,646
Like some of these enterprise-wide licenses.

6387
04:05:49,646 --> 04:05:52,045
So, in reality, the Gulf is even larger,

6388
04:05:52,045 --> 04:05:53,723
Microsoft is still second though.

6389
04:05:53,723 --> 04:05:55,360
Amazon is way bigger. Why?

6390
04:05:55,360 --> 04:05:57,740
Because using AWS is better and easier.

6391
04:05:57,740 --> 04:05:58,589
And in many cases,

6392
04:05:58,589 --> 04:05:59,462
it's cheaper. - It was first.

6393
04:05:59,462 --> 04:06:00,402
- And it's first. - It was first.

6394
04:06:00,402 --> 04:06:01,577
- [Lex] Yeah, but there's a lot of things

6395
04:06:01,577 --> 04:06:03,891
that are first that- - Well, it's easier...

6396
04:06:03,891 --> 04:06:04,898
It's harder to switch

6397
04:06:04,898 --> 04:06:06,161
than it is to- - Yeah, okay.

6398
04:06:06,161 --> 04:06:07,650
- AWS is- - Because it's large-

6399
04:06:07,650 --> 04:06:09,118
- There's big fees for switching too.

6400
04:06:09,118 --> 04:06:10,915
- AWS generates over 80% (Nathan chuckles)

6401
04:06:10,915 --> 04:06:12,987
of Amazon's profit, I think over 90%.

6402
04:06:12,987 --> 04:06:14,729
- That's insane. - The distribution centers

6403
04:06:14,729 --> 04:06:15,858
are just like, one day,

6404
04:06:15,858 --> 04:06:18,839
we'll decide to make money from this, but they haven't yet.

6405
04:06:18,839 --> 04:06:20,400
They make tiny little profit from-

6406
04:06:20,400 --> 04:06:22,371
- Yeah, one day, Amazon Prime will triple in price.

6407
04:06:22,371 --> 04:06:26,931
- You would think they would improve AWS interface,

6408
04:06:26,931 --> 04:06:28,707
'cause it's horrible.

6409
04:06:28,707 --> 04:06:30,771
It's clunky, but everybody is...

6410
04:06:30,771 --> 04:06:32,402
(Lex laughing) - [Nathan] I don't, yeah,

6411
04:06:32,402 --> 04:06:33,998
one would think.

6412
04:06:33,998 --> 04:06:36,088
- I think actually, Google's interface is sometimes nice,

6413
04:06:36,088 --> 04:06:37,231
but it's also they don't care

6414
04:06:37,231 --> 04:06:38,866
about anyone besides their top customers.

6415
04:06:38,866 --> 04:06:39,699
- Yeah, exactly. - And like

6416
04:06:39,699 --> 04:06:40,697
their customer service sucks

6417
04:06:40,697 --> 04:06:42,128
and they have a lot less, like...

6418
04:06:42,128 --> 04:06:43,008
- All these companies,

6419
04:06:43,008 --> 04:06:45,219
they optimize for the big customers, yeah.

6420
04:06:45,219 --> 04:06:46,238
It's supposed to be

6421
04:06:46,238 --> 04:06:47,579
for business. - Well, and Amazon

6422
04:06:47,579 --> 04:06:49,617
has always optimized for the small customer too though.

6423
04:06:49,617 --> 04:06:51,286
Obviously, they optimize a lot for the big customer,

6424
04:06:51,286 --> 04:06:53,660
but when they started, they just would go

6425
04:06:53,660 --> 04:06:56,350
to like random Bay Area things and give out credits.

6426
04:06:56,350 --> 04:06:57,360
And then, they like,

6427
04:06:57,360 --> 04:06:58,968
or just put in your credit card and use us.

6428
04:06:58,968 --> 04:07:00,190
It went back in the early days.

6429
04:07:00,190 --> 04:07:01,023
So, they've always,

6430
04:07:01,023 --> 04:07:03,128
the business has grown with them in burgeon.

6431
04:07:03,128 --> 04:07:06,257
So, why does Amazon, why is Snowflake all over Amazon?

6432
04:07:06,257 --> 04:07:07,339
Because Snowflake, in the beginning,

6433
04:07:07,339 --> 04:07:10,218
when Amazon didn't care about them, was still using Amazon.

6434
04:07:10,218 --> 04:07:11,051
And then, of course, one day,

6435
04:07:11,051 --> 04:07:13,133
Snowflake and Amazon has a super huge partnership,

6436
04:07:13,133 --> 04:07:14,279
but this is the case,

6437
04:07:14,279 --> 04:07:17,063
like Amazon's user experience and quality is better.

6438
04:07:17,063 --> 04:07:19,750
Also, a lot of the silicon they've engineered makes them

6439
04:07:19,750 --> 04:07:21,701
have a lower cost structure in traditional cloud,

6440
04:07:21,701 --> 04:07:23,068
storage, CPU, networking,

6441
04:07:23,068 --> 04:07:26,151
that kind of stuff than in databases.

6442
04:07:27,597 --> 04:07:32,158
I think like four of Amazon's top five revenue products,

6443
04:07:32,158 --> 04:07:34,158
margin products are gross profit products,

6444
04:07:34,158 --> 04:07:35,628
are all database-related products

6445
04:07:35,628 --> 04:07:38,414
like Redshift and all these things.

6446
04:07:38,414 --> 04:07:43,414
So, Amazon has a very good silicon to user experience,

6447
04:07:43,476 --> 04:07:45,310
entire pipeline with AWS.

6448
04:07:45,310 --> 04:07:47,786
I think Google, their silicon teams,

6449
04:07:47,786 --> 04:07:49,424
yeah, they have awesome silicon internally,

6450
04:07:49,424 --> 04:07:51,257
TPU, the YouTube chip,

6451
04:07:52,091 --> 04:07:53,705
some of these other chips that they've made.

6452
04:07:53,705 --> 04:07:56,918
And the problem is they're not serving external customers,

6453
04:07:56,918 --> 04:07:58,535
they're serving internal customers.

6454
04:07:58,535 --> 04:07:59,797
- Nvidia's entire culture

6455
04:07:59,797 --> 04:08:01,823
is designed from the bottom-up to do this.

6456
04:08:01,823 --> 04:08:04,190
There's this recent book, "The Nvidia Way" by Tae Kim,

6457
04:08:04,190 --> 04:08:05,335
that details this

6458
04:08:05,335 --> 04:08:08,156
and they're how they look for future opportunities

6459
04:08:08,156 --> 04:08:11,726
and ready their CUDA software libraries

6460
04:08:11,726 --> 04:08:13,844
to make it so that new applications

6461
04:08:13,844 --> 04:08:15,224
of high performance computing

6462
04:08:15,224 --> 04:08:19,649
can very rapidly be evolved on CUDA and Nvidia chips.

6463
04:08:19,649 --> 04:08:22,062
And that is entirely different

6464
04:08:22,062 --> 04:08:24,294
than Google as a services business.

6465
04:08:24,294 --> 04:08:27,124
- Yeah, Nvidia, it should be said,

6466
04:08:27,124 --> 04:08:29,149
is a truly special company.

6467
04:08:29,149 --> 04:08:31,182
There's the whole, the culture and everything,

6468
04:08:31,182 --> 04:08:33,164
they're really optimized for that kind of thing.

6469
04:08:33,164 --> 04:08:34,534
Speaking of which, is there somebody

6470
04:08:34,534 --> 04:08:37,695
that can even challenge Nvidia hardware-wise?

6471
04:08:37,695 --> 04:08:38,612
Intel, AMD?

6472
04:08:39,674 --> 04:08:41,284
- I really don't think so.

6473
04:08:41,284 --> 04:08:45,093
We went through a very long process of working

6474
04:08:45,093 --> 04:08:48,170
with AMD on training on their GPU's inference and stuff.

6475
04:08:48,170 --> 04:08:49,363
And they're decent.

6476
04:08:49,363 --> 04:08:52,785
Their hardware is better in many ways than in Nvidia's.

6477
04:08:52,785 --> 04:08:54,625
The problem is their software is really bad,

6478
04:08:54,625 --> 04:08:56,074
and I think they're getting better,

6479
04:08:56,074 --> 04:08:57,876
they're getting better faster, but they're just,

6480
04:08:57,876 --> 04:09:00,244
the Gulf is so large

6481
04:09:00,244 --> 04:09:02,534
and they don't spend enough resources on,

6482
04:09:02,534 --> 04:09:03,849
or haven't historically.

6483
04:09:03,849 --> 04:09:05,295
Maybe they're changing their tune now,

6484
04:09:05,295 --> 04:09:08,873
but for multiple months, we were submitting the most bugs.

6485
04:09:08,873 --> 04:09:11,114
Like us, SemiAnalysis. - Mm-hmm.

6486
04:09:11,114 --> 04:09:11,947
- Like what the fuck?

6487
04:09:11,947 --> 04:09:14,223
Why are we submitting the most bugs?

6488
04:09:14,223 --> 04:09:15,056
Because they only,

6489
04:09:15,056 --> 04:09:17,253
and they only cared about their biggest customers.

6490
04:09:17,253 --> 04:09:19,543
And so, they'd ship them a private image, blah, blah, blah.

6491
04:09:19,543 --> 04:09:23,322
And it's like, okay, but I am just using PyTorch

6492
04:09:23,322 --> 04:09:24,274
and I wanna use the publicly

6493
04:09:24,274 --> 04:09:25,266
available libraries - Mm-hmm.

6494
04:09:25,266 --> 04:09:26,099
Yeah. - and you don't care

6495
04:09:26,099 --> 04:09:26,932
about that.

6496
04:09:26,932 --> 04:09:28,477
So, they're getting better.

6497
04:09:28,477 --> 04:09:30,552
But I think AMD's not possible.

6498
04:09:30,552 --> 04:09:32,896
Intel's obviously in dire straits right now

6499
04:09:32,896 --> 04:09:35,231
and needs to be saved somehow.

6500
04:09:35,231 --> 04:09:36,444
Very important for national security,

6501
04:09:36,444 --> 04:09:39,144
for American technology dominance.

6502
04:09:39,144 --> 04:09:40,241
- Can you explain the, obviously,

6503
04:09:40,241 --> 04:09:41,663
so why are they in dire straits?

6504
04:09:41,663 --> 04:09:44,975
- Going back to earlier, only three companies can R&D.

6505
04:09:44,975 --> 04:09:47,019
- Yeah. - Taiwan, Hsinchu,

6506
04:09:47,019 --> 04:09:50,921
Samsung, Pyongyang, and then Intel, Hillsboro.

6507
04:09:50,921 --> 04:09:53,479
Samsung's doing horribly. Intel's doing horribly.

6508
04:09:53,479 --> 04:09:54,312
We could be in a world

6509
04:09:54,312 --> 04:09:56,101
where there's only one company that can do R&D,

6510
04:09:56,101 --> 04:09:58,008
and that one company already manufactures most of chips.

6511
04:09:58,008 --> 04:09:59,243
They've been gaining market share anyways.

6512
04:09:59,243 --> 04:10:01,113
But that's a critical thing.

6513
04:10:01,113 --> 04:10:02,544
So, what happens to Taiwan

6514
04:10:02,544 --> 04:10:04,175
means the rest of the world's semiconductor industry,

6515
04:10:04,175 --> 04:10:07,034
and therefore tech, relies on Taiwan.

6516
04:10:07,034 --> 04:10:09,115
And that's obviously precarious.

6517
04:10:09,115 --> 04:10:12,963
As far as Intel, they've been slowly, steadily declining.

6518
04:10:12,963 --> 04:10:15,895
They were on top of servers and PCs,

6519
04:10:15,895 --> 04:10:17,812
but now, Apple's done the M1

6520
04:10:17,812 --> 04:10:19,436
and Nvidia's releasing a PC chip,

6521
04:10:19,436 --> 04:10:21,154
and Qualcomm's releasing a PC chip,

6522
04:10:21,154 --> 04:10:22,791
and in servers, hyperscalers are all

6523
04:10:22,791 --> 04:10:25,264
making their own ARM-based server chips.

6524
04:10:25,264 --> 04:10:28,014
And Intel has no AI silicon wins.

6525
04:10:28,996 --> 04:10:30,935
They have very small wins.

6526
04:10:30,935 --> 04:10:32,735
And they never got into mobile,

6527
04:10:32,735 --> 04:10:33,914
because they said no to the iPhone.

6528
04:10:33,914 --> 04:10:35,757
And all these things have compounded

6529
04:10:35,757 --> 04:10:37,849
and they've lost their process technology leadership.

6530
04:10:37,849 --> 04:10:38,873
They were ahead for 20 years

6531
04:10:38,873 --> 04:10:41,028
and now they're behind by at least a couple years.

6532
04:10:41,028 --> 04:10:42,624
And they're trying to catch back up

6533
04:10:42,624 --> 04:10:46,165
and we'll see if their 18A, 14A strategy works out,

6534
04:10:46,165 --> 04:10:48,368
where they try and leapfrog TSMC.

6535
04:10:48,368 --> 04:10:49,601
But like...

6536
04:10:49,601 --> 04:10:51,344
And Intel is just losing tons of money anyways.

6537
04:10:51,344 --> 04:10:52,966
And they just fired their CEO,

6538
04:10:52,966 --> 04:10:54,527
even though their CEO was the only person

6539
04:10:54,527 --> 04:10:56,277
who understood the company well.

6540
04:10:56,277 --> 04:10:57,110
We'll see.

6541
04:10:57,110 --> 04:11:00,544
He was not the best, but he was pretty good, relatively.

6542
04:11:00,544 --> 04:11:01,446
Technical guy.

6543
04:11:01,446 --> 04:11:03,244
- [Lex] Where does Intel make most of its money?

6544
04:11:03,244 --> 04:11:04,571
The CPU still, right? - PCs

6545
04:11:04,571 --> 04:11:05,693
and data center CPUs, yeah.

6546
04:11:05,693 --> 04:11:07,316
But data center CPUs are all going Cloud

6547
04:11:07,316 --> 04:11:11,173
and Amazon, Microsoft, Google are making ARM-based CPUs.

6548
04:11:11,173 --> 04:11:14,729
And then, PC side, AMDs gained market share.

6549
04:11:14,729 --> 04:11:17,114
Nvidia's launching a chip that's not gonna be a success.

6550
04:11:17,114 --> 04:11:19,062
Mediatek, Qualcomm ever launch chips.

6551
04:11:19,062 --> 04:11:20,694
Apple's doing well.

6552
04:11:20,694 --> 04:11:23,233
They could get squeezed a little bit in PC,

6553
04:11:23,233 --> 04:11:24,760
although PC generally I imagine,

6554
04:11:24,760 --> 04:11:26,858
will just stick Intel mostly for Windows side.

6555
04:11:26,858 --> 04:11:31,276
- Let's talk about the broad AI race. Who do you think wins?

6556
04:11:31,276 --> 04:11:32,875
We talked about Google, Meta,

6557
04:11:32,875 --> 04:11:34,071
xAI. - The default leader

6558
04:11:34,071 --> 04:11:37,771
has been Google because of their infrastructure advantage.

6559
04:11:37,771 --> 04:11:40,961
- [Lex] Well, like in the news, OpenAI is the leader.

6560
04:11:40,961 --> 04:11:42,384
- They're leading in the-

6561
04:11:42,384 --> 04:11:43,217
- They have the best model.

6562
04:11:43,217 --> 04:11:45,161
- They have the best model that people can use.

6563
04:11:45,161 --> 04:11:46,897
And they're experts- - And they have

6564
04:11:46,897 --> 04:11:48,261
the most AI revenue.

6565
04:11:48,261 --> 04:11:51,062
- Yeah. OpenAI is winning.

6566
04:11:51,062 --> 04:11:53,837
- So, who's making money on AI right now?

6567
04:11:53,837 --> 04:11:55,262
Is anyone making money?

6568
04:11:55,262 --> 04:11:57,831
- So, accounting profit-wise, Microsoft is making money,

6569
04:11:57,831 --> 04:11:59,918
but they're spending a lot of CapEx,

6570
04:11:59,918 --> 04:12:02,420
and that gets depreciated over years.

6571
04:12:02,420 --> 04:12:03,641
Meta's making tons of money,

6572
04:12:03,641 --> 04:12:05,252
but with recommendation systems,

6573
04:12:05,252 --> 04:12:06,349
which is AI, (Nathan chuckles)

6574
04:12:06,349 --> 04:12:07,481
but not with Llama. - Right.

6575
04:12:07,481 --> 04:12:10,433
- Llama's losing money for sure.

6576
04:12:10,433 --> 04:12:13,016
I think Anthropic and OpenAI are obviously not making money,

6577
04:12:13,016 --> 04:12:14,951
'cause otherwise, they wouldn't be raising money.

6578
04:12:14,951 --> 04:12:17,921
They have to raise money to build more.

6579
04:12:17,921 --> 04:12:20,188
Although theoretically, they are making money.

6580
04:12:20,188 --> 04:12:22,806
You spent a few hundred million dollars on GPT-4,

6581
04:12:22,806 --> 04:12:24,196
and it's doing billions in revenue.

6582
04:12:24,196 --> 04:12:26,359
So, obviously, it's making money.

6583
04:12:26,359 --> 04:12:27,444
Although they had to continue to research

6584
04:12:27,444 --> 04:12:29,157
to get the compute efficiency wins,

6585
04:12:29,157 --> 04:12:31,394
and move down the curve

6586
04:12:31,394 --> 04:12:35,707
to get that 1,200x that has been achieved for GPT-3.

6587
04:12:35,707 --> 04:12:38,454
Maybe we're only at a couple 100x now,

6588
04:12:38,454 --> 04:12:40,737
but with GPT-4 Turbo and 4o,

6589
04:12:40,737 --> 04:12:41,570
and there'll be another one

6590
04:12:41,570 --> 04:12:44,218
probably cheaper than GPT-4o even

6591
04:12:44,218 --> 04:12:45,616
that comes out at some point.

6592
04:12:45,616 --> 04:12:47,811
- And that research costs a lot of money.

6593
04:12:47,811 --> 04:12:49,073
- [Dylan] Yep. Exactly.

6594
04:12:49,073 --> 04:12:50,082
- That's the thing that I guess

6595
04:12:50,082 --> 04:12:52,892
is not talked about with the cost,

6596
04:12:52,892 --> 04:12:55,394
that when you're referring to the cost of the model,

6597
04:12:55,394 --> 04:12:59,043
it's not just the training or the test runs,

6598
04:12:59,043 --> 04:13:02,050
it's the actual research, the manpower that-

6599
04:13:02,050 --> 04:13:03,571
- Yeah, to do things like reasoning right now

6600
04:13:03,571 --> 04:13:04,810
that that exists, they're gonna scale it,

6601
04:13:04,810 --> 04:13:05,974
they're gonna do a lot of research still.

6602
04:13:05,974 --> 04:13:10,244
I think people focus on the payback question,

6603
04:13:10,244 --> 04:13:12,633
but it's really easy to just be like, well,

6604
04:13:12,633 --> 04:13:15,875
GDP is humans and industrial capital.

6605
04:13:15,875 --> 04:13:20,337
And if you can make intelligence cheap, you can grow a lot.

6606
04:13:20,337 --> 04:13:22,132
That's the sort of dumb way to explain it.

6607
04:13:22,132 --> 04:13:25,224
But that's what basically the investment thesis is.

6608
04:13:25,224 --> 04:13:27,973
I think only Nvidia is actually making tons of money

6609
04:13:27,973 --> 04:13:29,932
and other hardware vendors.

6610
04:13:29,932 --> 04:13:32,268
The hyperscalers are all on paper making money,

6611
04:13:32,268 --> 04:13:34,644
but in reality, they're spending a lot more

6612
04:13:34,644 --> 04:13:36,054
on purchasing the GPUs,

6613
04:13:36,054 --> 04:13:37,463
which you don't know if they're still

6614
04:13:37,463 --> 04:13:40,328
gonna make this much money on each GPU in two years.

6615
04:13:40,328 --> 04:13:45,328
You don't know if all of a sudden, OpenAI goes kapoof,

6616
04:13:45,414 --> 04:13:47,907
and now Microsoft has hundreds of thousands

6617
04:13:47,907 --> 04:13:50,281
of GPUs they were renting to OpenAI that are,

6618
04:13:50,281 --> 04:13:54,133
that they paid for themselves with their investment in them

6619
04:13:54,133 --> 04:13:55,749
that no longer have a customer.

6620
04:13:55,749 --> 04:13:59,043
this is always a possibility. I don't believe that.

6621
04:13:59,043 --> 04:14:00,924
I think OpenAI will keep raising money.

6622
04:14:00,924 --> 04:14:02,370
I think others will keep raising money,

6623
04:14:02,370 --> 04:14:05,069
because the investments, the returns from it

6624
04:14:05,069 --> 04:14:08,062
are gonna be eventually huge once we have AGI.

6625
04:14:08,062 --> 04:14:10,874
- So, do you think multiple companies will get...

6626
04:14:10,874 --> 04:14:11,707
Let's assume that- - I don't think

6627
04:14:11,707 --> 04:14:13,058
it's winner take all.

6628
04:14:13,058 --> 04:14:16,809
- Okay. So, let's not call it AGI, whatever.

6629
04:14:16,809 --> 04:14:17,878
It's like a single day.

6630
04:14:17,878 --> 04:14:19,112
It's a gradual thing. - Powerful AI,

6631
04:14:19,112 --> 04:14:20,502
super powerful AI.

6632
04:14:20,502 --> 04:14:23,669
- But it's a gradually increasing set of features

6633
04:14:23,669 --> 04:14:25,753
that are useful and make-

6634
04:14:25,753 --> 04:14:26,832
- [Nathan] Rapidly increasing

6635
04:14:26,832 --> 04:14:27,665
set of features. - Rapidly,

6636
04:14:27,665 --> 04:14:30,611
rapidly increasing set of features.

6637
04:14:30,611 --> 04:14:33,621
So, you're saying a lot of companies will be...

6638
04:14:33,621 --> 04:14:37,621
It just seems absurd that all of these companies

6639
04:14:38,785 --> 04:14:40,839
are building gigantic data centers.

6640
04:14:40,839 --> 04:14:42,909
- There were companies that will benefit from AI,

6641
04:14:42,909 --> 04:14:44,955
but not because they train the best model.

6642
04:14:44,955 --> 04:14:46,581
Like Meta has so many avenues

6643
04:14:46,581 --> 04:14:48,804
to benefit from AI and all of their services.

6644
04:14:48,804 --> 04:14:51,681
People are there, people spend time on Meta's platforms

6645
04:14:51,681 --> 04:14:54,209
and it's a way to make more money per user per hour.

6646
04:14:54,209 --> 04:14:59,209
- Yeah it seems like Google/xAI/Tesla, important to say,

6647
04:15:01,987 --> 04:15:04,745
and then Meta will benefit not directly from the AI

6648
04:15:04,745 --> 04:15:08,162
like the LLMs, but from the intelligence,

6649
04:15:10,086 --> 04:15:11,615
like the additional boost of intelligence

6650
04:15:11,615 --> 04:15:13,449
to the products they already sell.

6651
04:15:13,449 --> 04:15:15,048
So, whether that's the recommendation system

6652
04:15:15,048 --> 04:15:19,576
or for Elon who's been talking about Optimus, the robot,

6653
04:15:19,576 --> 04:15:22,675
potentially the intelligence of the robot.

6654
04:15:22,675 --> 04:15:24,751
And then, you have personalized robots in the home,

6655
04:15:24,751 --> 04:15:25,927
that kind of thing.

6656
04:15:25,927 --> 04:15:30,844
He thinks it's a 10 plus trillion dollar business, which...

6657
04:15:31,860 --> 04:15:32,693
(Lex laughing)

6658
04:15:32,693 --> 04:15:33,651
- Yeah, sure. - At some point maybe.

6659
04:15:33,651 --> 04:15:36,059
Not soon, but who knows what robotics will-

6660
04:15:36,059 --> 04:15:37,540
- Let's do a TAM analysis.

6661
04:15:37,540 --> 04:15:38,492
8 billion humans

6662
04:15:38,492 --> 04:15:40,183
and let's get (Nathan laughing)

6663
04:15:40,183 --> 04:15:41,141
8 billion robots.

6664
04:15:41,141 --> 04:15:43,449
And let's pay 'em the average salary,

6665
04:15:43,449 --> 04:15:46,328
and yeah, there we go, 10 trillion, more than 10 trillion.

6666
04:15:46,328 --> 04:15:47,576
- Yeah.

6667
04:15:47,576 --> 04:15:50,432
If there's robots everywhere, why does it have to be

6668
04:15:50,432 --> 04:15:53,143
just 8 billion robots? - Yeah, yeah,

6669
04:15:53,143 --> 04:15:54,104
of course, of course.

6670
04:15:54,104 --> 04:15:54,937
- [Lex] It could be-

6671
04:15:54,937 --> 04:15:57,262
- I'm gonna have one robot, you're gonna have like 20.

6672
04:15:57,262 --> 04:16:00,005
- Yeah, I see a use case for that.

6673
04:16:00,005 --> 04:16:01,764
So, yeah, I guess the benefit

6674
04:16:01,764 --> 04:16:03,197
would be in the products they sell,

6675
04:16:03,197 --> 04:16:06,306
which is why OpenAI's in a trickier position, 'cause they-

6676
04:16:06,306 --> 04:16:08,436
- All of the value of OpenAI right now

6677
04:16:08,436 --> 04:16:10,227
as a brand is in ChatGPT.

6678
04:16:10,227 --> 04:16:13,077
And there is actually not that, for most users,

6679
04:16:13,077 --> 04:16:14,685
there's not that much of a reason

6680
04:16:14,685 --> 04:16:17,057
that they need OpenAI to be spending billions

6681
04:16:17,057 --> 04:16:18,998
and billions of dollars on the next best model,

6682
04:16:18,998 --> 04:16:20,134
- Mm-hmm. - when they could

6683
04:16:20,134 --> 04:16:23,214
just license Llama 5, and for be way cheaper.

6684
04:16:23,214 --> 04:16:25,085
So, that's like ChatGPT

6685
04:16:25,085 --> 04:16:29,337
is an extremely valuable entity to them, (chuckles)

6686
04:16:29,337 --> 04:16:31,305
but they could make more money just off that

6687
04:16:31,305 --> 04:16:32,456
than trying- - The chat application

6688
04:16:32,456 --> 04:16:35,325
is clearly like, does not have tons of room to continue.

6689
04:16:35,325 --> 04:16:36,175
Like the standard chat,

6690
04:16:36,175 --> 04:16:38,918
where you're just using it for a random question and stuff.

6691
04:16:38,918 --> 04:16:40,358
The cost continues to collapse.

6692
04:16:40,358 --> 04:16:41,986
V3 is the latest- - It'll go down to ads.

6693
04:16:41,986 --> 04:16:46,704
- Biggest, but it's gonna get supported by ads.

6694
04:16:46,704 --> 04:16:48,297
Meta already serves 405b

6695
04:16:48,297 --> 04:16:50,261
and probably loses the money, but at some point,

6696
04:16:50,261 --> 04:16:53,300
they're going to get, the models are gonna get so cheap

6697
04:16:53,300 --> 04:16:56,214
that they can just serve them for free with ads supported.

6698
04:16:56,214 --> 04:16:57,581
And that's what Google's gonna be able to do.

6699
04:16:57,581 --> 04:16:59,446
And that's obviously they've got a bigger reach.

6700
04:16:59,446 --> 04:17:01,781
So, chat is not gonna be the only use case.

6701
04:17:01,781 --> 04:17:05,812
It's like these reasoning, code, agents, computer use,

6702
04:17:05,812 --> 04:17:07,180
all this stuff is where OpenAI

6703
04:17:07,180 --> 04:17:09,191
has to actually go to make money in the future.

6704
04:17:09,191 --> 04:17:10,399
Otherwise, they're kaputs.

6705
04:17:10,399 --> 04:17:14,978
- But X, Google, and Meta have these other products.

6706
04:17:14,978 --> 04:17:17,561
So, isn't it likely that OpenAI

6707
04:17:18,612 --> 04:17:21,529
and Anthropic disappear eventually?

6708
04:17:22,440 --> 04:17:23,312
Because it's- - Unless they're so good

6709
04:17:23,312 --> 04:17:24,429
at models, 'cause they are.

6710
04:17:24,429 --> 04:17:25,500
- [Lex] But it's such a cutting edge,

6711
04:17:25,500 --> 04:17:26,835
I mean, you have to get- - It depends on where

6712
04:17:26,835 --> 04:17:28,562
you think AI capabilities are going.

6713
04:17:28,562 --> 04:17:30,269
- You have to keep winning. - Yes.

6714
04:17:30,269 --> 04:17:32,048
- You have to keep winning.

6715
04:17:32,048 --> 04:17:33,256
As you climb,

6716
04:17:33,256 --> 04:17:36,831
even if the AI capabilities are going super rapidly awesome

6717
04:17:36,831 --> 04:17:40,825
into the direction of AGI, there's still a boost

6718
04:17:40,825 --> 04:17:44,597
for X in terms of data, Google in terms of data,

6719
04:17:44,597 --> 04:17:46,009
Meta in terms of data,

6720
04:17:46,009 --> 04:17:48,486
in terms of other products and the money.

6721
04:17:48,486 --> 04:17:49,379
There's just

6722
04:17:49,379 --> 04:17:50,563
huge amount of money. - But if the whole idea

6723
04:17:50,563 --> 04:17:52,955
is human data is tapped out, we don't care,

6724
04:17:52,955 --> 04:17:55,602
we all care about self-play verifiable tasks-

6725
04:17:55,602 --> 04:17:57,120
- Yes, the self-play, - Think about AWS-

6726
04:17:57,120 --> 04:17:58,223
- [Lex] which is an R&D problem.

6727
04:17:58,223 --> 04:17:59,599
- AWS does not make a lot of money

6728
04:17:59,599 --> 04:18:01,857
on each individual machine.

6729
04:18:01,857 --> 04:18:05,639
And the same can be said for the most powerful AI platform,

6730
04:18:05,639 --> 04:18:08,308
which is even though the calls to the API are so cheap,

6731
04:18:08,308 --> 04:18:09,428
there's still a lot of money

6732
04:18:09,428 --> 04:18:11,801
to be made by owning that platform.

6733
04:18:11,801 --> 04:18:13,186
And there's a lot of discussions

6734
04:18:13,186 --> 04:18:15,492
as it's the next compute layer.

6735
04:18:15,492 --> 04:18:17,028
- You have to believe that...

6736
04:18:17,028 --> 04:18:19,186
And there's a lot of discussions that tokens

6737
04:18:19,186 --> 04:18:22,444
and tokenomics and LLM APIs are the next compute layer,

6738
04:18:22,444 --> 04:18:24,277
or the next paradigm for the economy,

6739
04:18:24,277 --> 04:18:26,008
like energy and oil was.

6740
04:18:26,008 --> 04:18:29,407
But there's also, you have to believe that APIs

6741
04:18:29,407 --> 04:18:32,800
and chat are not where AI is stuck.

6742
04:18:32,800 --> 04:18:33,978
It is actually just tasks

6743
04:18:33,978 --> 04:18:36,486
and agents and robotics and computer use.

6744
04:18:36,486 --> 04:18:37,319
And those are the areas

6745
04:18:37,319 --> 04:18:40,048
where all the value will be delivered,

6746
04:18:40,048 --> 04:18:42,487
not API, not chat application.

6747
04:18:42,487 --> 04:18:44,172
- So, is it possible you have,

6748
04:18:44,172 --> 04:18:46,414
I mean, it all just becomes a commodity,

6749
04:18:46,414 --> 04:18:50,664
and you have the very thin wrapper like Perplexity.

6750
04:18:53,187 --> 04:18:55,019
Just joking.

6751
04:18:55,019 --> 04:18:56,024
- [Nathan] There are a lot of wrappers

6752
04:18:56,024 --> 04:18:57,268
making a lot of money.

6753
04:18:57,268 --> 04:18:58,842
- Yeah, but do you think it's possible

6754
04:18:58,842 --> 04:19:01,811
that people would just even forget what OpenAI

6755
04:19:01,811 --> 04:19:03,408
and Anthropic is, and just,

6756
04:19:03,408 --> 04:19:05,611
'cause there'll be wrappers around the API,

6757
04:19:05,611 --> 04:19:06,487
and it just dynamically-

6758
04:19:06,487 --> 04:19:08,423
- If model progress is not rapid, yeah.

6759
04:19:08,423 --> 04:19:10,124
It's becoming a commodity.

6760
04:19:10,124 --> 04:19:11,752
DeepSeek-V3 shows this,

6761
04:19:11,752 --> 04:19:14,915
but also the GPT-3 chart earlier, chart showed this.

6762
04:19:14,915 --> 04:19:17,911
Llama 3B is 1,200x cheaper than GPT-3.

6763
04:19:17,911 --> 04:19:20,410
Any GPT-3, like anyone whose business model

6764
04:19:20,410 --> 04:19:22,471
was GPT-3 level capabilities is dead.

6765
04:19:22,471 --> 04:19:23,702
- Yeah. - Anyone whose business model

6766
04:19:23,702 --> 04:19:26,489
is GPT-4 level capabilities is dead.

6767
04:19:26,489 --> 04:19:27,530
- It is a common saying

6768
04:19:27,530 --> 04:19:29,980
that the best businesses being made now are ones

6769
04:19:29,980 --> 04:19:32,162
that are predicated on models getting better.

6770
04:19:32,162 --> 04:19:34,272
- Right, which would be wrappers,

6771
04:19:34,272 --> 04:19:37,602
thing that is riding the wave of the models.

6772
04:19:37,602 --> 04:19:40,087
- The short-term that company that could make the most money

6773
04:19:40,087 --> 04:19:41,021
is the one that figures out

6774
04:19:41,021 --> 04:19:44,189
what advertising targeting method works

6775
04:19:44,189 --> 04:19:45,691
for language model generations.

6776
04:19:45,691 --> 04:19:48,843
We have the Meta ads which are hyper targeted in feed,

6777
04:19:48,843 --> 04:19:50,396
not within specific pieces - Mm-hmm.

6778
04:19:50,396 --> 04:19:51,229
- of content.

6779
04:19:51,229 --> 04:19:53,024
And we have search ads that are used by Google

6780
04:19:53,024 --> 04:19:54,897
and Amazon has been rising a lot on Search.

6781
04:19:54,897 --> 04:19:57,942
But within a piece, within a return from ChatGPT,

6782
04:19:57,942 --> 04:19:58,775
it is not clear

6783
04:19:58,775 --> 04:20:02,858
how you get a high quality placed ad within the output.

6784
04:20:02,858 --> 04:20:05,924
And if you can do that with model costs coming down,

6785
04:20:05,924 --> 04:20:09,610
you can just get super high revenue per...

6786
04:20:09,610 --> 04:20:10,971
That revenue is totally untapped

6787
04:20:10,971 --> 04:20:12,651
and it's not clear technically how it is done.

6788
04:20:12,651 --> 04:20:14,169
- Yeah, that is...

6789
04:20:14,169 --> 04:20:17,502
the ad sense innovation that Google did,

6790
04:20:18,651 --> 04:20:22,770
the one day you'll have in GPT output an ad

6791
04:20:22,770 --> 04:20:23,999
and that's gonna make

6792
04:20:23,999 --> 04:20:25,733
billions, not- - And it could be very subtle.

6793
04:20:25,733 --> 04:20:27,086
It could be in conversation.

6794
04:20:27,086 --> 04:20:28,316
We have voice mode now.

6795
04:20:28,316 --> 04:20:30,179
It could be some way of making it,

6796
04:20:30,179 --> 04:20:32,783
so the voice introduces certain things.

6797
04:20:32,783 --> 04:20:33,686
It's much harder to measure

6798
04:20:33,686 --> 04:20:35,903
and it takes imagination, but yeah.

6799
04:20:35,903 --> 04:20:39,389
- And it wouldn't come off shady,

6800
04:20:39,389 --> 04:20:42,393
so you would receive public blow back, that kind of thing.

6801
04:20:42,393 --> 04:20:43,623
So, you have to do it loud enough

6802
04:20:43,623 --> 04:20:46,483
to where it's clear it's an ad that and balance all of that.

6803
04:20:46,483 --> 04:20:48,597
So, that's the open question they're trying to solve.

6804
04:20:48,597 --> 04:20:51,374
Anthropic and OpenAI, they need to-

6805
04:20:51,374 --> 04:20:52,207
- They might not say

6806
04:20:52,207 --> 04:20:53,040
that they- - I don't think

6807
04:20:53,040 --> 04:20:53,873
they care about that at all.

6808
04:20:53,873 --> 04:20:54,808
- They don't care about it right now.

6809
04:20:54,808 --> 04:20:56,708
I think it's places - I think they're surely-

6810
04:20:56,708 --> 04:20:58,883
- like Perplexity are experimenting on that more.

6811
04:20:58,883 --> 04:21:01,345
- [Lex] Oh, interesting. Yeah, for sure.

6812
04:21:01,345 --> 04:21:04,080
- Like Perplexity, Google, Meta care about this.

6813
04:21:04,080 --> 04:21:08,069
I think OpenAI and Anthropic are purely laser-focused on-

6814
04:21:08,069 --> 04:21:10,669
- AGI. - Yeah, agents and AGI.

6815
04:21:10,669 --> 04:21:14,259
And if I build AGI, I can make tons of money.

6816
04:21:14,259 --> 04:21:16,030
Or I can spend, pay for everything.

6817
04:21:16,030 --> 04:21:20,590
And it is just predicated back on the export control thing.

6818
04:21:20,590 --> 04:21:23,568
If you think AGI is 5, 10 years away or less,

6819
04:21:23,568 --> 04:21:26,311
these labs think it's 2, 3 years away.

6820
04:21:26,311 --> 04:21:28,644
Obviously, your actions are,

6821
04:21:29,880 --> 04:21:31,031
if you assume they're rational actors,

6822
04:21:31,031 --> 04:21:33,489
which they are mostly,

6823
04:21:33,489 --> 04:21:36,361
what you do in a two-year AGI versus five-year

6824
04:21:36,361 --> 04:21:40,172
versus 10 years is very, very, very different.

6825
04:21:40,172 --> 04:21:42,612
- Do you think agents are promising?

6826
04:21:42,612 --> 04:21:46,010
We have to talk about this. (chuckles)

6827
04:21:46,010 --> 04:21:48,366
This is like the excitement of the year

6828
04:21:48,366 --> 04:21:49,292
that agents are gonna rev...

6829
04:21:49,292 --> 04:21:51,709
This is the generic hype term

6830
04:21:53,065 --> 04:21:55,132
that a lot of business folks are using.

6831
04:21:55,132 --> 04:21:57,216
AI agents are gonna revolutionize everything.

6832
04:21:57,216 --> 04:22:01,152
- Okay. So, mostly, the term agent is obviously overblown.

6833
04:22:01,152 --> 04:22:02,864
We've talked a lot about reinforcement learning

6834
04:22:02,864 --> 04:22:06,054
as a way to train for verifiable outcomes.

6835
04:22:06,054 --> 04:22:08,011
Agents should mean something that is open-ended

6836
04:22:08,011 --> 04:22:10,724
and is solving a task independently on its own,

6837
04:22:10,724 --> 04:22:12,823
and able to adapt to uncertainty.

6838
04:22:12,823 --> 04:22:14,541
There's a lot of the term agent applied

6839
04:22:14,541 --> 04:22:16,014
to things like Apple Intelligence,

6840
04:22:16,014 --> 04:22:19,984
which we still don't have after the last WWDC,

6841
04:22:19,984 --> 04:22:22,105
which is orchestrating between apps.

6842
04:22:22,105 --> 04:22:24,209
And that type of tool use thing is something

6843
04:22:24,209 --> 04:22:26,559
that language models can do really well.

6844
04:22:26,559 --> 04:22:28,263
Apple Intelligence I suspect will work,

6845
04:22:28,263 --> 04:22:29,465
so will come eventually.

6846
04:22:29,465 --> 04:22:30,623
It's a closed domain.

6847
04:22:30,623 --> 04:22:33,070
It's your messages app integrating with your photos,

6848
04:22:33,070 --> 04:22:34,539
with AI in the background.

6849
04:22:34,539 --> 04:22:35,563
That will work.

6850
04:22:35,563 --> 04:22:37,700
That has been described as an agent

6851
04:22:37,700 --> 04:22:40,640
by a lot of software companies to get into the narrative.

6852
04:22:40,640 --> 04:22:44,207
- Yeah. - The question is what ways

6853
04:22:44,207 --> 04:22:48,206
can we get language models to generalize to new domains

6854
04:22:48,206 --> 04:22:51,426
and solve their own problems in real time.

6855
04:22:51,426 --> 04:22:52,859
Maybe some tiny amount of training

6856
04:22:52,859 --> 04:22:54,870
when they're doing this with fine-tuning themselves

6857
04:22:54,870 --> 04:22:56,214
or in context learning,

6858
04:22:56,214 --> 04:22:59,161
which is the idea of storing information in a prompt.

6859
04:22:59,161 --> 04:23:01,840
And you can use learning algorithms to update that.

6860
04:23:01,840 --> 04:23:03,950
And whether or not you believe

6861
04:23:03,950 --> 04:23:06,206
that that is gonna actually generalize to things

6862
04:23:06,206 --> 04:23:10,956
like me saying, book my trip to go to Austin in two days.

6863
04:23:12,309 --> 04:23:15,946
I have X, Y, Z constraints, and actually trusting it.

6864
04:23:15,946 --> 04:23:19,580
I think there's a HCI problem coming back for information.

6865
04:23:19,580 --> 04:23:21,167
- Well, what's your prediction there?

6866
04:23:21,167 --> 04:23:24,508
Because my gut says we're very far away from that.

6867
04:23:24,508 --> 04:23:27,390
- I think OpenAI's statement,

6868
04:23:27,390 --> 04:23:29,338
I don't know if you've seen the five levels.

6869
04:23:29,338 --> 04:23:31,164
Or it's chat is level one,

6870
04:23:31,164 --> 04:23:34,748
reasoning is level two, and then agents is level three.

6871
04:23:34,748 --> 04:23:35,929
And I think there's a couple more levels,

6872
04:23:35,929 --> 04:23:38,229
but it's important to note, we were in chat

6873
04:23:38,229 --> 04:23:39,859
for a couple years. - Mm-hmm.

6874
04:23:39,859 --> 04:23:42,898
- We just theoretically got to reasoning.

6875
04:23:42,898 --> 04:23:44,385
We'll be here for a year or two.

6876
04:23:44,385 --> 04:23:46,686
And then, agents, but at the same time,

6877
04:23:46,686 --> 04:23:49,725
people can train like approximate capabilities

6878
04:23:49,725 --> 04:23:50,558
of the next level,

6879
04:23:50,558 --> 04:23:53,417
but the agents are doing things autonomously,

6880
04:23:53,417 --> 04:23:54,952
doing things for minutes at a time,

6881
04:23:54,952 --> 04:23:57,491
hours at a time, et cetera.

6882
04:23:57,491 --> 04:24:02,025
Reasoning is doing things for tens of seconds at a time.

6883
04:24:02,025 --> 04:24:02,858
And then, coming back with an output

6884
04:24:02,858 --> 04:24:06,541
that I still need to verify and use and try check out.

6885
04:24:06,541 --> 04:24:09,572
And the biggest problem is of course,

6886
04:24:09,572 --> 04:24:10,641
it's the same thing with manufacturing.

6887
04:24:10,641 --> 04:24:12,881
There's the whole six sigma thing.

6888
04:24:12,881 --> 04:24:14,010
How many nines do you get,

6889
04:24:14,010 --> 04:24:16,172
and then you compound the nines onto each other,

6890
04:24:16,172 --> 04:24:17,973
and it's like if you multiply

6891
04:24:17,973 --> 04:24:20,074
by the number of steps that are six sigma,

6892
04:24:20,074 --> 04:24:22,741
you get to a yield or something.

6893
04:24:23,939 --> 04:24:25,347
So, like in semiconductor manufacturing,

6894
04:24:25,347 --> 04:24:26,967
tens of thousands of steps.

6895
04:24:26,967 --> 04:24:28,800
9999999 is not enough,

6896
04:24:30,179 --> 04:24:31,547
because you multiply that many times,

6897
04:24:31,547 --> 04:24:33,645
you actually end up with like 60% yield.

6898
04:24:33,645 --> 04:24:35,291
- Or zero. - Really low yield. Yeah.

6899
04:24:35,291 --> 04:24:36,124
Or zero.

6900
04:24:36,124 --> 04:24:37,958
And this is the same thing with agents.

6901
04:24:37,958 --> 04:24:41,470
Chaining tasks together each time, LLMs,

6902
04:24:41,470 --> 04:24:45,826
even the best LLMs in particularly pretty good benchmarks

6903
04:24:45,826 --> 04:24:47,977
don't get 100%. - Yeah.

6904
04:24:47,977 --> 04:24:49,188
- They get a little bit below that,

6905
04:24:49,188 --> 04:24:50,937
because there is a lot of noise.

6906
04:24:50,937 --> 04:24:54,609
And so, how do you get to enough nines?

6907
04:24:54,609 --> 04:24:55,828
This is the same thing with self-driving.

6908
04:24:55,828 --> 04:24:57,309
We can't have self-driving

6909
04:24:57,309 --> 04:25:00,815
because without it being super geofenced like Google's.

6910
04:25:00,815 --> 04:25:02,957
And even then, they have a bunch of tele operators

6911
04:25:02,957 --> 04:25:04,016
to make sure it doesn't get stuck.

6912
04:25:04,016 --> 04:25:07,084
But you can't do that because it doesn't have enough nines.

6913
04:25:07,084 --> 04:25:10,825
- And self-driving has quite a lot of structure

6914
04:25:10,825 --> 04:25:12,747
because roads have rules.

6915
04:25:12,747 --> 04:25:15,803
It's well-defined. There's regulation.

6916
04:25:15,803 --> 04:25:19,359
When you're talking about computer use for the open web,

6917
04:25:19,359 --> 04:25:23,942
for example, or the open operating system, it's a mess.

6918
04:25:25,169 --> 04:25:28,269
So, the possibility, I'm always skeptical

6919
04:25:28,269 --> 04:25:32,019
of any system that is tasked with interacting

6920
04:25:33,088 --> 04:25:34,999
with the human world,

6921
04:25:34,999 --> 04:25:36,794
the open message human world. - That's the thing.

6922
04:25:36,794 --> 04:25:37,975
If we can't get intelligence,

6923
04:25:37,975 --> 04:25:41,146
that's enough to solve the human world on its own.

6924
04:25:41,146 --> 04:25:43,119
We can create infrastructure

6925
04:25:43,119 --> 04:25:44,708
like the human operators for Waymo

6926
04:25:44,708 --> 04:25:46,298
- Yeah. - over many years

6927
04:25:46,298 --> 04:25:47,716
that enable certain workflows.

6928
04:25:47,716 --> 04:25:49,487
- There is a company, I don't remember it,

6929
04:25:49,487 --> 04:25:51,137
but that's literally their pitches.

6930
04:25:51,137 --> 04:25:52,868
Yeah, we're just gonna be the human operator

6931
04:25:52,868 --> 04:25:55,410
when agents fail, and you just call us and we fix it.

6932
04:25:55,410 --> 04:25:56,778
- Yeah. - It's like an API call

6933
04:25:56,778 --> 04:25:57,611
and it's hilarious.

6934
04:25:57,611 --> 04:25:58,879
- There's gonna be teleoperation markets

6935
04:25:58,879 --> 04:25:59,947
when we get human robots,

6936
04:25:59,947 --> 04:26:01,700
which is there's gonna be somebody

6937
04:26:01,700 --> 04:26:04,251
around the world that's happy to fix the fact

6938
04:26:04,251 --> 04:26:06,002
that it can't finish loading my dishwasher

6939
04:26:06,002 --> 04:26:07,412
- Yeah. - when I'm unhappy with it,

6940
04:26:07,412 --> 04:26:10,373
but that's just gonna be part of the Tesla service package.

6941
04:26:10,373 --> 04:26:13,871
- I'm just imagining an AI agent

6942
04:26:13,871 --> 04:26:15,780
talking to another AI agent.

6943
04:26:15,780 --> 04:26:17,261
One company has an AI agent

6944
04:26:17,261 --> 04:26:20,976
that specializes in helping other AI agents.

6945
04:26:20,976 --> 04:26:23,010
- But if you can make things that are good at one step,

6946
04:26:23,010 --> 04:26:25,060
- Yeah. - you can stack them together.

6947
04:26:25,060 --> 04:26:28,110
So, that's why I'm like, if it takes a long time,

6948
04:26:28,110 --> 04:26:30,444
we're gonna build infrastructure that enables it.

6949
04:26:30,444 --> 04:26:31,577
You see the operator launch,

6950
04:26:31,577 --> 04:26:34,240
they have partnerships with certain websites, with DoorDash,

6951
04:26:34,240 --> 04:26:35,639
with OpenTable, - Mm-hmm.

6952
04:26:35,639 --> 04:26:37,041
- with things like this.

6953
04:26:37,041 --> 04:26:39,929
Those partnerships are gonna let them climb really fast.

6954
04:26:39,929 --> 04:26:42,112
Their model's gonna get really good at those things.

6955
04:26:42,112 --> 04:26:44,602
It's gonna proof of concept that might be a network effect

6956
04:26:44,602 --> 04:26:47,538
where more companies wanna make it easier for AI.

6957
04:26:47,538 --> 04:26:48,590
Some companies will be like,

6958
04:26:48,590 --> 04:26:51,018
no, let's at put blockers in place.

6959
04:26:51,018 --> 04:26:52,381
- Yep. - And this is the story

6960
04:26:52,381 --> 04:26:53,751
of the internet we've seen.

6961
04:26:53,751 --> 04:26:55,489
We see it now with training data for language models,

6962
04:26:55,489 --> 04:26:57,454
where companies are like, no, you have to pay,

6963
04:26:57,454 --> 04:26:59,437
- [Lex] Mm-hmm. (Nathan chuckles)

6964
04:26:59,437 --> 04:27:00,728
- business working it out.

6965
04:27:00,728 --> 04:27:03,696
- That said, I think airlines have a very,

6966
04:27:03,696 --> 04:27:05,406
and hotels have high incentive

6967
04:27:05,406 --> 04:27:09,721
to make their site work really well, and they usually don't.

6968
04:27:09,721 --> 04:27:11,548
If you look at how many clicks it takes

6969
04:27:11,548 --> 04:27:14,347
to order a airplane ticket, it's insane.

6970
04:27:14,347 --> 04:27:15,180
I don't- - You actually

6971
04:27:15,180 --> 04:27:17,986
can't call an American Airlines agent anymore.

6972
04:27:17,986 --> 04:27:19,864
They don't have a phone number.

6973
04:27:19,864 --> 04:27:24,155
- It's horrible on many, on the interface front and all...

6974
04:27:24,155 --> 04:27:26,087
To imagine that agents will be able

6975
04:27:26,087 --> 04:27:29,968
to deal with that website, when I, as a human, struggle,

6976
04:27:29,968 --> 04:27:31,216
like I have an existential crisis

6977
04:27:31,216 --> 04:27:33,221
every time I try to book an airplane ticket,

6978
04:27:33,221 --> 04:27:37,746
that I think it's gonna be very extremely difficult

6979
04:27:37,746 --> 04:27:40,225
to build a AI agent that's robust

6980
04:27:40,225 --> 04:27:41,058
in that way. - But think about it,

6981
04:27:41,058 --> 04:27:43,386
United has accepted the Starlink term,

6982
04:27:43,386 --> 04:27:45,544
which is they have to provide Starlink for free

6983
04:27:45,544 --> 04:27:47,316
and the users are going to love it.

6984
04:27:47,316 --> 04:27:50,195
What if one Airline is like, we're gonna take a year

6985
04:27:50,195 --> 04:27:51,385
and we're gonna make our website

6986
04:27:51,385 --> 04:27:54,411
have white text that works perfectly for the AIs.

6987
04:27:54,411 --> 04:27:58,003
Every time anyone asks about an AI flight, they buy

6988
04:27:58,003 --> 04:27:59,663
whatever airline it is. (Dylan laughing)

6989
04:27:59,663 --> 04:28:02,258
- Or they're just like, here's an API in,

6990
04:28:02,258 --> 04:28:03,590
it's only exposed to AI agents

6991
04:28:03,590 --> 04:28:05,988
and if anyone queries it, the price is 10% higher.

6992
04:28:05,988 --> 04:28:06,821
- [Lex] Yeah.

6993
04:28:06,821 --> 04:28:07,810
- And for any flight,

6994
04:28:07,810 --> 04:28:09,270
but we'll let you see any of our flights

6995
04:28:09,270 --> 04:28:10,310
and you can just book any of them.

6996
04:28:10,310 --> 04:28:11,970
Here you go, agent- - And that's-

6997
04:28:11,970 --> 04:28:13,630
- Then, it's, oh, and I made 10% higher price.

6998
04:28:13,630 --> 04:28:14,559
Awesome. - Yeah.

6999
04:28:14,559 --> 04:28:16,350
- And am I willing to say that for like,

7000
04:28:16,350 --> 04:28:17,737
hey, book me a flight to see Lex.

7001
04:28:17,737 --> 04:28:18,798
And it's like, yeah, whatever.

7002
04:28:18,798 --> 04:28:19,631
- Yeah, yeah.

7003
04:28:19,631 --> 04:28:23,240
- I think computers and real world

7004
04:28:23,240 --> 04:28:26,427
and the open world are really, really messy.

7005
04:28:26,427 --> 04:28:30,392
But if you start defining the problem in narrow regions,

7006
04:28:30,392 --> 04:28:31,225
people are gonna be able

7007
04:28:31,225 --> 04:28:34,319
to create very, very productive things,

7008
04:28:34,319 --> 04:28:36,986
and ratchet down cost massively.

7009
04:28:37,898 --> 04:28:41,486
Now, crazy things like robotics in the home,

7010
04:28:41,486 --> 04:28:43,870
those are gonna be a lot harder to do

7011
04:28:43,870 --> 04:28:45,032
just like self-driving.

7012
04:28:45,032 --> 04:28:47,942
Because there's just a billion different failure modes.

7013
04:28:47,942 --> 04:28:52,608
But agents that can navigate a certain set of websites

7014
04:28:52,608 --> 04:28:53,777
and do certain sets of tasks,

7015
04:28:53,777 --> 04:28:56,360
or take a photo of your fridge,

7016
04:28:58,389 --> 04:28:59,918
or like upload your recipes,

7017
04:28:59,918 --> 04:29:01,304
and then it figures out what to order

7018
04:29:01,304 --> 04:29:04,833
from Amazon/Whole Foods food delivery.

7019
04:29:04,833 --> 04:29:07,634
And that's gonna be pretty quick and easy to do, I think.

7020
04:29:07,634 --> 04:29:10,008
So, it's gonna be be a whole range of business outcomes

7021
04:29:10,008 --> 04:29:12,750
and it's gonna be tons of optimism

7022
04:29:12,750 --> 04:29:14,594
around people can just figure out ways to make money.

7023
04:29:14,594 --> 04:29:17,018
- To be clear, these sandboxes already exist in research.

7024
04:29:17,018 --> 04:29:18,927
There are people who have built clones

7025
04:29:18,927 --> 04:29:21,980
of all the most popular websites of Google, Amazon,

7026
04:29:21,980 --> 04:29:24,701
blah, blah, blah, to make it so that there's...

7027
04:29:24,701 --> 04:29:26,626
And I mean, OpenAI probably has them internally

7028
04:29:26,626 --> 04:29:27,665
to train these things.

7029
04:29:27,665 --> 04:29:29,396
It's the same as DeepMind's robotics team

7030
04:29:29,396 --> 04:29:31,887
for years has had clusters for robotics,

7031
04:29:31,887 --> 04:29:34,763
where you interact with robots fully, remotely.

7032
04:29:34,763 --> 04:29:37,358
They just have a lab in London and you send tasks to it,

7033
04:29:37,358 --> 04:29:39,599
it arrange the blocks and you do this research.

7034
04:29:39,599 --> 04:29:42,196
Obviously, there's texts there that fix stuff.

7035
04:29:42,196 --> 04:29:46,663
But we've turned these cranks of automation before.

7036
04:29:46,663 --> 04:29:48,375
You go from sandbox to progress,

7037
04:29:48,375 --> 04:29:50,718
and then you add one more domain at a time,

7038
04:29:50,718 --> 04:29:52,274
and generalize, I think.

7039
04:29:52,274 --> 04:29:55,455
In the history of NLP and language processing,

7040
04:29:55,455 --> 04:29:57,844
instruction tuning and tasks per language model

7041
04:29:57,844 --> 04:30:00,015
used to be one language model did one task.

7042
04:30:00,015 --> 04:30:01,823
And then, in the instruction tuning literature,

7043
04:30:01,823 --> 04:30:03,704
there's this point where you start adding more

7044
04:30:03,704 --> 04:30:05,072
and more tasks together,

7045
04:30:05,072 --> 04:30:07,494
where it just starts to generalize to every task.

7046
04:30:07,494 --> 04:30:08,946
And we don't know where on this curve we are.

7047
04:30:08,946 --> 04:30:10,454
I think for reasoning with this RL

7048
04:30:10,454 --> 04:30:12,513
and verifiable domains, we're early,

7049
04:30:12,513 --> 04:30:14,142
but we don't know where the point is,

7050
04:30:14,142 --> 04:30:17,659
where you just start training on enough domains and poof,

7051
04:30:17,659 --> 04:30:19,401
more domains just start working,

7052
04:30:19,401 --> 04:30:21,891
and you've crossed the generalization barrier.

7053
04:30:21,891 --> 04:30:25,774
- Well, what do you think about the programming context?

7054
04:30:25,774 --> 04:30:27,786
So, software engineering.

7055
04:30:27,786 --> 04:30:29,976
That's where I personally,

7056
04:30:29,976 --> 04:30:34,789
and I know a lot of people interact with AI the most.

7057
04:30:34,789 --> 04:30:35,622
- There's a lot of fear

7058
04:30:35,622 --> 04:30:37,414
and angst too from current CS students,

7059
04:30:37,414 --> 04:30:39,645
but there's also, that is the area

7060
04:30:39,645 --> 04:30:41,465
where probably the most AI revenue

7061
04:30:41,465 --> 04:30:43,373
and productivity gains have come.

7062
04:30:43,373 --> 04:30:44,707
- [Lex] Yeah.

7063
04:30:44,707 --> 04:30:48,459
- Whether it be Copilots or Cursor, or what have you,

7064
04:30:48,459 --> 04:30:50,393
or just standard ChatGPT.

7065
04:30:50,393 --> 04:30:51,741
Like a lot of...

7066
04:30:51,741 --> 04:30:54,332
I know very few programmers who don't have ChatGPT

7067
04:30:54,332 --> 04:30:56,535
and actually many of them have the $200 tier

7068
04:30:56,535 --> 04:30:59,110
because that's what it's so good for.

7069
04:30:59,110 --> 04:31:01,899
I think that in that world,

7070
04:31:01,899 --> 04:31:04,108
we already see it like SWE-bench,

7071
04:31:04,108 --> 04:31:05,789
I don't know if you've looked at the benchmark

7072
04:31:05,789 --> 04:31:07,445
made by some Stanford students.

7073
04:31:07,445 --> 04:31:09,313
I wouldn't say it's really hard,

7074
04:31:09,313 --> 04:31:10,447
but I wouldn't say it's easy either.

7075
04:31:10,447 --> 04:31:11,416
I think it takes someone

7076
04:31:11,416 --> 04:31:14,044
who's been through at least a few years of CS,

7077
04:31:14,044 --> 04:31:16,793
or a couple years of programming to do SWE-bench well.

7078
04:31:16,793 --> 04:31:21,793
And the models went from 4% to 60% in like a year.

7079
04:31:21,873 --> 04:31:23,839
And where are they gonna go to next year?

7080
04:31:23,839 --> 04:31:25,078
It's gonna be higher.

7081
04:31:25,078 --> 04:31:26,228
It probably won't be 100%,

7082
04:31:26,228 --> 04:31:28,559
'cause again, that nines is really hard to do,

7083
04:31:28,559 --> 04:31:30,796
but we're gonna get to some point, where that's,

7084
04:31:30,796 --> 04:31:31,629
and then we're gonna need

7085
04:31:31,629 --> 04:31:32,775
harder software engineering benchmarks,

7086
04:31:32,775 --> 04:31:33,745
and so on and so forth.

7087
04:31:33,745 --> 04:31:37,437
But the way that people think of it now

7088
04:31:37,437 --> 04:31:38,957
is it's can do code completion easy.

7089
04:31:38,957 --> 04:31:41,256
It can do some function generation and I have to review it.

7090
04:31:41,256 --> 04:31:42,089
Great.

7091
04:31:42,089 --> 04:31:44,996
But really, the software engineering agents,

7092
04:31:44,996 --> 04:31:47,565
I think can be done faster sooner than any other agent,

7093
04:31:47,565 --> 04:31:49,848
because it is a verifiable domain.

7094
04:31:49,848 --> 04:31:52,823
You can always unit test or compile,

7095
04:31:52,823 --> 04:31:55,054
and there's many different regions

7096
04:31:55,054 --> 04:31:58,742
of it can inspect the whole code base at once,

7097
04:31:58,742 --> 04:32:00,625
which no engineer really can.

7098
04:32:00,625 --> 04:32:01,922
Only the architects can really think

7099
04:32:01,922 --> 04:32:02,755
about this stuff, - Mm-hmm.

7100
04:32:02,755 --> 04:32:05,057
- the really senior guys, and they can define stuff,

7101
04:32:05,057 --> 04:32:07,025
and then the agent can execute on it.

7102
04:32:07,025 --> 04:32:08,691
So, I think software engineering costs

7103
04:32:08,691 --> 04:32:09,943
are gonna plummet like crazy.

7104
04:32:09,943 --> 04:32:12,254
And one interesting aspect of that

7105
04:32:12,254 --> 04:32:14,435
is when software engineering costs are really low,

7106
04:32:14,435 --> 04:32:16,286
you get very different markets.

7107
04:32:16,286 --> 04:32:18,916
So, in the US, you have all these platform SaaS companies,

7108
04:32:18,916 --> 04:32:21,726
Salesforce, and so on and so forth.

7109
04:32:21,726 --> 04:32:24,726
In China, no one uses platform SaaS.

7110
04:32:25,717 --> 04:32:27,438
Everyone just builds their own stack,

7111
04:32:27,438 --> 04:32:30,822
because software engineering is much cheaper in China,

7112
04:32:30,822 --> 04:32:32,564
and partially because people,

7113
04:32:32,564 --> 04:32:34,720
number of STEM graduates, et cetera.

7114
04:32:34,720 --> 04:32:38,313
So, STEM. So, it's generally just cheaper to do.

7115
04:32:38,313 --> 04:32:39,505
And so, at the same time,

7116
04:32:39,505 --> 04:32:42,719
code LLMs have been adopted much less in China,

7117
04:32:42,719 --> 04:32:45,299
because the cost of an engineer there is much lower.

7118
04:32:45,299 --> 04:32:47,244
But what happens when every company can just invent

7119
04:32:47,244 --> 04:32:50,005
their own business logic really cheaply and quickly?

7120
04:32:50,005 --> 04:32:51,685
You stop using platform SaaS,

7121
04:32:51,685 --> 04:32:53,183
you start building custom-tailored solutions,

7122
04:32:53,183 --> 04:32:54,797
you change them really quickly.

7123
04:32:54,797 --> 04:32:55,630
Now, all of a sudden,

7124
04:32:55,630 --> 04:32:57,571
your business is a little bit more efficient too potentially

7125
04:32:57,571 --> 04:32:58,962
because you're not dealing with the hell

7126
04:32:58,962 --> 04:33:01,771
that is some random platform SaaS company stuff

7127
04:33:01,771 --> 04:33:04,058
not working perfectly and having to adjust workflows,

7128
04:33:04,058 --> 04:33:05,986
or random business automation cases

7129
04:33:05,986 --> 04:33:08,080
that aren't necessarily AI-required.

7130
04:33:08,080 --> 04:33:08,914
It's just logic

7131
04:33:08,914 --> 04:33:10,249
that needs to be built that no one has built.

7132
04:33:10,249 --> 04:33:12,170
All of these zings can go happen faster.

7133
04:33:12,170 --> 04:33:13,258
And so, I think software...

7134
04:33:13,258 --> 04:33:15,175
And then, the other domain is like industrial,

7135
04:33:15,175 --> 04:33:16,544
chemical, mechanical engineers

7136
04:33:16,544 --> 04:33:19,059
suck at coding just generally.

7137
04:33:19,059 --> 04:33:21,096
And their tools, like semiconductor engineers,

7138
04:33:21,096 --> 04:33:22,326
their tools are 20 years old.

7139
04:33:22,326 --> 04:33:23,687
All the tools run on XP,

7140
04:33:23,687 --> 04:33:27,478
including ASML lithography tools, run on Windows XP.

7141
04:33:27,478 --> 04:33:31,383
And a lot of the analysis happens in Excel.

7142
04:33:31,383 --> 04:33:32,755
It's just like, guys,

7143
04:33:32,755 --> 04:33:34,122
you guys can move 20 years forward

7144
04:33:34,122 --> 04:33:35,091
with all the data you have

7145
04:33:35,091 --> 04:33:36,690
and gathered and do a lot better.

7146
04:33:36,690 --> 04:33:38,703
It's just you need the engineering skills

7147
04:33:38,703 --> 04:33:39,713
for software engineering

7148
04:33:39,713 --> 04:33:42,229
to be delivered to the actual domain expert engineer.

7149
04:33:42,229 --> 04:33:43,721
So, I think that's the area,

7150
04:33:43,721 --> 04:33:45,112
where I'm super, duper bullish

7151
04:33:45,112 --> 04:33:47,677
of generally AI creating value.

7152
04:33:47,677 --> 04:33:48,693
- The big picture

7153
04:33:48,693 --> 04:33:50,741
is that I don't think it's gonna be a cliff.

7154
04:33:50,741 --> 04:33:52,115
- Yeah. - We talked to,

7155
04:33:52,115 --> 04:33:53,823
I think a really good example

7156
04:33:53,823 --> 04:33:57,905
of how growth changes is when Meta added stories.

7157
04:33:58,761 --> 04:34:00,544
So, Snapchat was on an exponential,

7158
04:34:00,544 --> 04:34:02,532
they added stories, it flatlined.

7159
04:34:02,532 --> 04:34:05,022
Software engineers, then up until the right,

7160
04:34:05,022 --> 04:34:07,663
AI's gonna come in, it's probably just gonna be flat.

7161
04:34:07,663 --> 04:34:10,002
It is not like everyone's gonna lose their job.

7162
04:34:10,002 --> 04:34:13,753
It's hard because the supply corrects more slowly.

7163
04:34:13,753 --> 04:34:15,732
So, the amount of students is still growing

7164
04:34:15,732 --> 04:34:19,274
and that'll correct on a multi-year, like a year delay,

7165
04:34:19,274 --> 04:34:21,503
but the amount of jobs will just turn,

7166
04:34:21,503 --> 04:34:26,189
and then maybe in 20, 40 years, it'll be well down.

7167
04:34:26,189 --> 04:34:27,080
But in the few years,

7168
04:34:27,080 --> 04:34:28,553
there'll never gonna be the snap moment,

7169
04:34:28,553 --> 04:34:30,290
where it's like software engineers aren't useful.

7170
04:34:30,290 --> 04:34:31,279
- I think also the nature

7171
04:34:31,279 --> 04:34:32,878
of what it means to be a programmer

7172
04:34:32,878 --> 04:34:35,522
and what kind of jobs programmers do changes,

7173
04:34:35,522 --> 04:34:38,514
'cause I think there needs

7174
04:34:38,514 --> 04:34:41,249
to be a human in the loop of everything you've talked about.

7175
04:34:41,249 --> 04:34:43,205
There's a really important human

7176
04:34:43,205 --> 04:34:46,455
in that picture of correcting the code,

7177
04:34:47,674 --> 04:34:49,880
like fix- - Think more

7178
04:34:49,880 --> 04:34:51,633
than the context length. - Yep.

7179
04:34:51,633 --> 04:34:53,604
And debugging also,

7180
04:34:53,604 --> 04:34:56,214
like debugging by sort of reading the code,

7181
04:34:56,214 --> 04:34:59,784
understanding the steering the system, like no, no, no.

7182
04:34:59,784 --> 04:35:02,079
You missed the point. Adding more to the prompt.

7183
04:35:02,079 --> 04:35:05,393
Kind of like, yes, adding the human-

7184
04:35:05,393 --> 04:35:07,234
- Designing the perfect Google button.

7185
04:35:07,234 --> 04:35:09,322
Google's famous for having people design buttons

7186
04:35:09,322 --> 04:35:13,446
that are so perfect, and it's like how is AI gonna do that?

7187
04:35:13,446 --> 04:35:15,383
Is like they could give you all the ideas.

7188
04:35:15,383 --> 04:35:16,907
Perfect. Fine. - Yeah.

7189
04:35:16,907 --> 04:35:20,407
- That's the thing, you can call it taste.

7190
04:35:21,338 --> 04:35:23,258
One thing humans can do is figure out

7191
04:35:23,258 --> 04:35:25,761
what other humans enjoy better than AI systems.

7192
04:35:25,761 --> 04:35:28,349
That's where the preference, you loading that in.

7193
04:35:28,349 --> 04:35:29,183
But ultimately,

7194
04:35:29,183 --> 04:35:31,256
humans are the greatest preference generat...

7195
04:35:31,256 --> 04:35:32,988
That's where the preference comes from.

7196
04:35:32,988 --> 04:35:35,005
- And humans are actually very good at reading

7197
04:35:35,005 --> 04:35:37,394
or judging between two things versus,

7198
04:35:37,394 --> 04:35:38,366
this goes back to the core

7199
04:35:38,366 --> 04:35:40,454
of what RLHF and preference tuning is,

7200
04:35:40,454 --> 04:35:41,287
is that it's hard

7201
04:35:41,287 --> 04:35:43,097
to generate a good answer for a lot of problems,

7202
04:35:43,097 --> 04:35:45,101
but it's easy to see which one is better.

7203
04:35:45,101 --> 04:35:47,175
And that's how we're using humans for AI now

7204
04:35:47,175 --> 04:35:48,577
is judging which one is better.

7205
04:35:48,577 --> 04:35:50,675
And that's what software engineering could look like,

7206
04:35:50,675 --> 04:35:54,704
is the PR review, here's a few options, what are the,

7207
04:35:54,704 --> 04:35:57,122
like here are some potential pros and cons,

7208
04:35:57,122 --> 04:35:59,679
and they're gonna be judges.

7209
04:35:59,679 --> 04:36:02,064
- I think the thing I would very much recommend

7210
04:36:02,064 --> 04:36:06,326
is people start, programmers start using AI,

7211
04:36:06,326 --> 04:36:09,556
and embracing that role of the supervisor of the AI system,

7212
04:36:09,556 --> 04:36:13,097
and partner the AI system versus, writing from scratch

7213
04:36:13,097 --> 04:36:16,876
or not learning coding at all and just generating stuff.

7214
04:36:16,876 --> 04:36:18,926
Because I think there actually has to be a pretty high level

7215
04:36:18,926 --> 04:36:20,675
of expertise as a programmer

7216
04:36:20,675 --> 04:36:23,950
to be able to manage increasingly intelligent systems.

7217
04:36:23,950 --> 04:36:25,255
- I think it's that,

7218
04:36:25,255 --> 04:36:26,963
and then becoming a domain expert in something.

7219
04:36:26,963 --> 04:36:28,214
- Sure. Yeah. - Right?

7220
04:36:28,214 --> 04:36:30,463
Because seriously, if you go look at aerospace

7221
04:36:30,463 --> 04:36:32,704
or semiconductors or chemical engineering,

7222
04:36:32,704 --> 04:36:35,133
everyone is using really crappy platforms,

7223
04:36:35,133 --> 04:36:36,664
really old software.

7224
04:36:36,664 --> 04:36:41,446
The job of a data sciences is like a joke in many cases.

7225
04:36:41,446 --> 04:36:42,462
In many cases, it's very real,

7226
04:36:42,462 --> 04:36:44,755
but it's like bring what the forefront

7227
04:36:44,755 --> 04:36:46,799
of human capabilities are to your domain.

7228
04:36:46,799 --> 04:36:48,779
And even if the forefront is from the AI,

7229
04:36:48,779 --> 04:36:50,955
your domain, you're like at the forefront.

7230
04:36:50,955 --> 04:36:53,276
So, it's like you have to be at the forefront of something,

7231
04:36:53,276 --> 04:36:56,225
and then leverage the like rising tide

7232
04:36:56,225 --> 04:36:57,653
that is AI for everything else.

7233
04:36:57,653 --> 04:36:58,487
- Oh, yeah.

7234
04:36:58,487 --> 04:37:02,019
There's so many low-hanging fruit everywhere

7235
04:37:02,019 --> 04:37:05,448
in terms of where software can help automate a thing

7236
04:37:05,448 --> 04:37:07,115
or digitize a thing.

7237
04:37:07,996 --> 04:37:09,033
In the legal system...

7238
04:37:09,033 --> 04:37:11,366
That's why Doge is exciting.

7239
04:37:12,421 --> 04:37:13,839
I got to hang out

7240
04:37:15,032 --> 04:37:16,504
with a bunch of the Doge folks, and they...

7241
04:37:16,504 --> 04:37:17,348
(Lex chuckles)

7242
04:37:17,348 --> 04:37:20,636
I mean, government is so old school,

7243
04:37:20,636 --> 04:37:25,636
it's like begging for the modernization of software,

7244
04:37:25,654 --> 04:37:28,175
of organizing the data, all this kind of stuff.

7245
04:37:28,175 --> 04:37:30,085
In that case, it's by design,

7246
04:37:30,085 --> 04:37:34,835
because bureaucracy protects centers of power, and so on.

7247
04:37:35,913 --> 04:37:39,628
But software breaks down those barriers,

7248
04:37:39,628 --> 04:37:43,232
so it hurts those that are holding onto power,

7249
04:37:43,232 --> 04:37:45,613
but ultimately, benefits humanity.

7250
04:37:45,613 --> 04:37:49,676
So, there's a bunch of domains of that kind.

7251
04:37:49,676 --> 04:37:52,723
One thing we didn't fully finish

7252
04:37:52,723 --> 04:37:55,182
talking about is open source.

7253
04:37:55,182 --> 04:37:58,364
So, first of all, congrats, you released a new model.

7254
04:37:58,364 --> 04:38:00,523
- Yeah. This is the- - Tulu. (chuckles)

7255
04:38:00,523 --> 04:38:01,463
- I'll explain what a Tulu is.

7256
04:38:01,463 --> 04:38:02,914
- Yeah. - A tulu is a hybrid camel

7257
04:38:02,914 --> 04:38:06,731
when you breed a Dromedary with a Bactrian camel.

7258
04:38:06,731 --> 04:38:09,253
Back in the early days, after ChatGPT, there was a big wave

7259
04:38:09,253 --> 04:38:12,335
of models coming out like Alpaca, Vicuna, et cetera,

7260
04:38:12,335 --> 04:38:15,981
that were all named after various mammalian species.

7261
04:38:15,981 --> 04:38:18,482
So, Tulu is the brand is multiple years old,

7262
04:38:18,482 --> 04:38:19,845
which comes from that. - Mm-hmm.

7263
04:38:19,845 --> 04:38:23,141
- And we've been playing at the frontiers

7264
04:38:23,141 --> 04:38:25,342
of post-training with open source code.

7265
04:38:25,342 --> 04:38:28,873
And this first part of this release was in the fall,

7266
04:38:28,873 --> 04:38:33,042
where we've built on Llama's open models,

7267
04:38:33,042 --> 04:38:33,875
open-weight models,

7268
04:38:33,875 --> 04:38:37,831
and then we add in our fully open code, our fully open data.

7269
04:38:37,831 --> 04:38:40,415
There's a popular benchmark that is chat bot arena.

7270
04:38:40,415 --> 04:38:42,304
And that's generally the metric

7271
04:38:42,304 --> 04:38:44,693
by which how these chat models are evaluated.

7272
04:38:44,693 --> 04:38:47,282
And it's humans compare random models

7273
04:38:47,282 --> 04:38:48,515
from different organizations.

7274
04:38:48,515 --> 04:38:49,955
And if you looked at the leaderboard

7275
04:38:49,955 --> 04:38:51,212
in November or December,

7276
04:38:51,212 --> 04:38:55,569
among the top 60 models from 10s to 20s of organizations,

7277
04:38:55,569 --> 04:38:58,860
none of them had open code or data for just post-training.

7278
04:38:58,860 --> 04:38:59,831
Among that, even fewer

7279
04:38:59,831 --> 04:39:02,163
or none have pre-training data and code available.

7280
04:39:02,163 --> 04:39:03,134
But it's like post-training

7281
04:39:03,134 --> 04:39:04,482
is much more accessible at this time.

7282
04:39:04,482 --> 04:39:06,148
It's still pretty cheap and you can do it.

7283
04:39:06,148 --> 04:39:08,522
And the thing is how high can we push this number,

7284
04:39:08,522 --> 04:39:11,004
where people have access to all the code and data.

7285
04:39:11,004 --> 04:39:12,930
So, that's the motivation of the project.

7286
04:39:12,930 --> 04:39:14,570
We draw in lessons from Llama.

7287
04:39:14,570 --> 04:39:16,929
Nvidia had a Nemotron model,

7288
04:39:16,929 --> 04:39:18,399
where the recipe for their post-training

7289
04:39:18,399 --> 04:39:21,740
was fairly open with some data and a paper.

7290
04:39:21,740 --> 04:39:23,428
And it's putting all these together to try

7291
04:39:23,428 --> 04:39:24,491
to create a recipe that people

7292
04:39:24,491 --> 04:39:27,899
can fine-tune models like GPT-4 to their domain.

7293
04:39:27,899 --> 04:39:30,799
- So, to be clear, in the case of Tulu,

7294
04:39:30,799 --> 04:39:32,171
maybe you can talk about OLMO 2,

7295
04:39:32,171 --> 04:39:36,671
but in the case of Tulu, you're taking Llama 3, 4, 5B.

7296
04:39:39,178 --> 04:39:41,506
- Tulu has been a series of recipes for post-training.

7297
04:39:41,506 --> 04:39:42,339
So, we've done - Okay.

7298
04:39:42,339 --> 04:39:43,906
- multiple models over years. - Okay.

7299
04:39:43,906 --> 04:39:46,661
And so, you're open sourcing everything.

7300
04:39:46,661 --> 04:39:49,067
- Yeah, if you start with an open-weight-based model,

7301
04:39:49,067 --> 04:39:51,700
the whole model technically isn't open source,

7302
04:39:51,700 --> 04:39:53,470
because you don't know what Llama put into it,

7303
04:39:53,470 --> 04:39:55,960
which is why we have a separate thing that we'll get to.

7304
04:39:55,960 --> 04:39:58,288
But it's just getting parts of the pipeline,

7305
04:39:58,288 --> 04:40:00,271
where people can zoom in and customize.

7306
04:40:00,271 --> 04:40:01,951
I know I hear from startups and businesses,

7307
04:40:01,951 --> 04:40:03,949
they're like, okay, I can take this post-training

7308
04:40:03,949 --> 04:40:05,602
and try to apply it to my domain.

7309
04:40:05,602 --> 04:40:07,170
We talk about verifiers a lot.

7310
04:40:07,170 --> 04:40:09,829
We use this idea which is reinforcement learning

7311
04:40:09,829 --> 04:40:13,746
with verifiable rewards, RLVR, similar to RLHF.

7312
04:40:15,497 --> 04:40:17,810
And we applied it to map.

7313
04:40:17,810 --> 04:40:20,681
And the model today, which is we applied it

7314
04:40:20,681 --> 04:40:24,321
to the Llama 405b base model from last year.

7315
04:40:24,321 --> 04:40:25,691
And we have our other stuff,

7316
04:40:25,691 --> 04:40:28,930
we have our instruction tuning and our preference tuning.

7317
04:40:28,930 --> 04:40:31,501
But the math thing is interesting,

7318
04:40:31,501 --> 04:40:34,497
which is it's easier to improve this math benchmark.

7319
04:40:34,497 --> 04:40:37,051
There's a benchmark, M-A-T-H, MATH, all capitals,

7320
04:40:37,051 --> 04:40:39,109
tough name when the benchmark,

7321
04:40:39,109 --> 04:40:41,158
its name is the area that you're evaluating.

7322
04:40:41,158 --> 04:40:44,600
We're researchers, we're not brands, brand strategists.

7323
04:40:44,600 --> 04:40:46,330
And this is something

7324
04:40:46,330 --> 04:40:48,231
that the DeepSeek paper talked about as well

7325
04:40:48,231 --> 04:40:49,613
is at this bigger model,

7326
04:40:49,613 --> 04:40:51,941
it's easier to elicit powerful capabilities

7327
04:40:51,941 --> 04:40:53,189
with this RL training.

7328
04:40:53,189 --> 04:40:54,669
And then, they distill it down

7329
04:40:54,669 --> 04:40:56,507
from that big model to the small model.

7330
04:40:56,507 --> 04:40:58,680
And this model we released today, we saw the same thing,

7331
04:40:58,680 --> 04:41:02,219
is we're AI2, we don't have a ton of compute,

7332
04:41:02,219 --> 04:41:04,318
we can't train 405b models all the time.

7333
04:41:04,318 --> 04:41:06,659
So, we just did a few runs and they tend to work,

7334
04:41:06,659 --> 04:41:09,930
and it's like, it just shows that there's a lot of room

7335
04:41:09,930 --> 04:41:12,453
for people to play in these things.

7336
04:41:12,453 --> 04:41:13,651
And that's- - And they crushed Llama's

7337
04:41:13,651 --> 04:41:15,584
actual release.

7338
04:41:15,584 --> 04:41:17,205
They're way better than it. - Yeah.

7339
04:41:17,205 --> 04:41:19,706
So, our eval numbers, we have extra months in this,

7340
04:41:19,706 --> 04:41:21,078
but our eval numbers are much better

7341
04:41:21,078 --> 04:41:23,420
than the Llama instruct model - Mm-hmm.

7342
04:41:23,420 --> 04:41:24,253
- that they released.

7343
04:41:24,253 --> 04:41:26,387
- And then, you also said better than DeepSeek-V3.

7344
04:41:26,387 --> 04:41:28,857
- Yeah. On our eval benchmark.

7345
04:41:28,857 --> 04:41:31,117
The most DeepSeek-V3 is really similar.

7346
04:41:31,117 --> 04:41:33,162
We have a safety benchmark to understand

7347
04:41:33,162 --> 04:41:35,498
if it will say harmful things and things like that.

7348
04:41:35,498 --> 04:41:37,037
And that's what draws down most of the way.

7349
04:41:37,037 --> 04:41:37,887
It's still like-

7350
04:41:37,887 --> 04:41:38,720
- It's like an amalgamation

7351
04:41:38,720 --> 04:41:40,224
of multiple benchmarks or what do you mean?

7352
04:41:40,224 --> 04:41:41,594
- Yeah, so we have a 10 evaluat...

7353
04:41:41,594 --> 04:41:43,544
This is standard practice in post-training

7354
04:41:43,544 --> 04:41:45,314
is you choose your evaluations you care about.

7355
04:41:45,314 --> 04:41:47,332
In academics, in smaller labs,

7356
04:41:47,332 --> 04:41:48,714
you'll have fewer evaluations.

7357
04:41:48,714 --> 04:41:50,252
In companies, you'll have a really

7358
04:41:50,252 --> 04:41:51,625
one domain that you really care about.

7359
04:41:51,625 --> 04:41:54,205
In Frontier Labs, you'll have 10s to 20s,

7360
04:41:54,205 --> 04:41:56,664
to maybe even 100 valuations of specific things.

7361
04:41:56,664 --> 04:41:58,626
So, we'd choose a representative suite of things

7362
04:41:58,626 --> 04:42:01,534
that look like chat, precise instruction following,

7363
04:42:01,534 --> 04:42:03,664
which is like respond only in emojis,

7364
04:42:03,664 --> 04:42:05,435
just model follow weird things like that.

7365
04:42:05,435 --> 04:42:06,651
- Yeah. - Math, code.

7366
04:42:06,651 --> 04:42:08,015
And you create a suite like this.

7367
04:42:08,015 --> 04:42:11,617
So, safety would be 1 of 10 in that type of suite,

7368
04:42:11,617 --> 04:42:12,450
where you have like,

7369
04:42:12,450 --> 04:42:14,444
what is the broader community of AI care about?

7370
04:42:14,444 --> 04:42:17,233
And for example, in comparison to DeepSeek,

7371
04:42:17,233 --> 04:42:18,066
it would be something

7372
04:42:18,066 --> 04:42:20,934
like our average eval for our model would be 80,

7373
04:42:20,934 --> 04:42:23,339
including safety and similar without.

7374
04:42:23,339 --> 04:42:28,339
And DeepSeek would be like 79% average score without safety

7375
04:42:29,979 --> 04:42:31,502
and their safety score would bring it down

7376
04:42:31,502 --> 04:42:32,872
to like 76- - Oh, so you beat them

7377
04:42:32,872 --> 04:42:34,123
even ignoring safety? - Yeah.

7378
04:42:34,123 --> 04:42:35,472
So, this is something that internally,

7379
04:42:35,472 --> 04:42:37,440
it's like I don't want to win

7380
04:42:37,440 --> 04:42:39,559
only by how you shape the eval benchmark.

7381
04:42:39,559 --> 04:42:41,248
So, if there's something that's like people may

7382
04:42:41,248 --> 04:42:43,086
or may not care about safety in their model,

7383
04:42:43,086 --> 04:42:44,531
safety can come downstream,

7384
04:42:44,531 --> 04:42:46,499
safety can be when you host the model for an API,

7385
04:42:46,499 --> 04:42:48,461
safety is addressed

7386
04:42:48,461 --> 04:42:50,594
in a spectrum of locations in AI applications.

7387
04:42:50,594 --> 04:42:51,427
So, it's like,

7388
04:42:51,427 --> 04:42:52,720
if you wanna say that you have the best recipe,

7389
04:42:52,720 --> 04:42:54,389
you can't just gauge it on these things

7390
04:42:54,389 --> 04:42:55,760
that some people might not want.

7391
04:42:55,760 --> 04:42:56,593
- [Lex] Mm-hmm.

7392
04:42:56,593 --> 04:43:00,638
- And this is just, it's like the time of progress.

7393
04:43:00,638 --> 04:43:03,128
We benefit if we can release model later,

7394
04:43:03,128 --> 04:43:05,078
we have more time to learn new techniques.

7395
04:43:05,078 --> 04:43:07,629
Like this RL technique, we had started this in the fall.

7396
04:43:07,629 --> 04:43:09,700
It's now really popular reasoning models.

7397
04:43:09,700 --> 04:43:12,009
The next thing to do for open source post-training

7398
04:43:12,009 --> 04:43:13,901
is to scale up verifiers,

7399
04:43:13,901 --> 04:43:17,010
to scale up data to replicate some of DeepSeek's results.

7400
04:43:17,010 --> 04:43:18,771
And it's awesome that we have a paper to draw

7401
04:43:18,771 --> 04:43:20,603
and that it makes it a lot easier.

7402
04:43:20,603 --> 04:43:22,219
And that's the type of things

7403
04:43:22,219 --> 04:43:24,802
that is going on among academic

7404
04:43:25,732 --> 04:43:28,519
and closed frontier research in AI.

7405
04:43:28,519 --> 04:43:29,940
- Since you're pushing open source,

7406
04:43:29,940 --> 04:43:31,310
what do you think is the future of it?

7407
04:43:31,310 --> 04:43:33,522
Do you think DeepSeek actually changes things

7408
04:43:33,522 --> 04:43:35,911
since it's open source or open-weight

7409
04:43:35,911 --> 04:43:37,710
or is pushing the open source movement

7410
04:43:37,710 --> 04:43:39,077
into the open direction?

7411
04:43:39,077 --> 04:43:41,030
- This goes very back to the license discussion.

7412
04:43:41,030 --> 04:43:44,131
So, DeepSeek-R1 with a friendly license is a major reset.

7413
04:43:44,131 --> 04:43:45,050
So, it's like the first time

7414
04:43:45,050 --> 04:43:47,820
that we've had a really clear frontier model

7415
04:43:47,820 --> 04:43:48,882
that is open-weights

7416
04:43:48,882 --> 04:43:50,809
and with a commercially friendly license

7417
04:43:50,809 --> 04:43:53,102
with no restrictions on downstream use cases,

7418
04:43:53,102 --> 04:43:54,720
synthetic data, distillation, whatever.

7419
04:43:54,720 --> 04:43:56,690
This has never been the case at all

7420
04:43:56,690 --> 04:43:59,801
in the history of AI in the last few years since ChatGPT.

7421
04:43:59,801 --> 04:44:01,363
There have been models that are off the frontier

7422
04:44:01,363 --> 04:44:03,001
or models with weird licenses

7423
04:44:03,001 --> 04:44:04,361
that you can't really use them.

7424
04:44:04,361 --> 04:44:06,217
- So, isn't Meta's license

7425
04:44:06,217 --> 04:44:09,551
pretty much permissible except for five companies?

7426
04:44:09,551 --> 04:44:10,384
And there's also...

7427
04:44:10,384 --> 04:44:13,071
So, this goes to what open source AI is, which is,

7428
04:44:13,071 --> 04:44:15,971
there's also use case restrictions in the Llama license,

7429
04:44:15,971 --> 04:44:17,698
which says you can't use it for specific things.

7430
04:44:17,698 --> 04:44:19,820
So, if you come from an open source software background,

7431
04:44:19,820 --> 04:44:22,301
you would say that that is not an open source license.

7432
04:44:22,301 --> 04:44:23,811
- What kind of things are those though?

7433
04:44:23,811 --> 04:44:25,624
Are they like...

7434
04:44:25,624 --> 04:44:27,582
- At this point, I can't pull them off the top of my head.

7435
04:44:27,582 --> 04:44:29,029
But it'll be like- - Like stuff like competitor,

7436
04:44:29,029 --> 04:44:29,862
probably. - It used to be

7437
04:44:29,862 --> 04:44:31,203
military use was one, - Oh-

7438
04:44:31,203 --> 04:44:32,289
- and they removed that for scale.

7439
04:44:32,289 --> 04:44:36,717
It'll be like CSAM, like child abuse material

7440
04:44:36,717 --> 04:44:39,896
or that's the type of thing that is forbidden there.

7441
04:44:39,896 --> 04:44:42,218
But that's enough from an open source background

7442
04:44:42,218 --> 04:44:43,794
to say it's not open source license.

7443
04:44:43,794 --> 04:44:45,935
And also, the Llama license has this horrible thing,

7444
04:44:45,935 --> 04:44:49,309
where you have to name your model Llama if you touch it

7445
04:44:49,309 --> 04:44:50,987
to the Llama model. - Mm-hmm.

7446
04:44:50,987 --> 04:44:52,037
- So, it's like the branding thing.

7447
04:44:52,037 --> 04:44:54,356
So, if a company uses Llama, technically,

7448
04:44:54,356 --> 04:44:56,336
the license says that they should say built with Llama

7449
04:44:56,336 --> 04:44:57,506
at the bottom of their application.

7450
04:44:57,506 --> 04:45:00,495
And from a marketing perspective, that just hurts.

7451
04:45:00,495 --> 04:45:03,067
I can suck it up as a researcher, I'm like, oh, it's fine.

7452
04:45:03,067 --> 04:45:07,046
It says Llama dash on all of our materials for this release.

7453
04:45:07,046 --> 04:45:08,886
But this is why we need truly open models,

7454
04:45:08,886 --> 04:45:12,082
which is we don't know DeepSeek-R1's data, but-

7455
04:45:12,082 --> 04:45:15,036
- Wait, so you're saying I can't make a cheap copy of Llama

7456
04:45:15,036 --> 04:45:15,869
and pretend it's mine,

7457
04:45:15,869 --> 04:45:17,341
but I can do this with the Chinese model?

7458
04:45:17,341 --> 04:45:18,174
- [Lex] Yeah.

7459
04:45:18,174 --> 04:45:19,007
- Yeah. - Hell yeah.

7460
04:45:19,007 --> 04:45:20,085
(Nathan and Dylan laughing)

7461
04:45:20,085 --> 04:45:22,183
- That's what I'm saying. - Yeah.

7462
04:45:22,183 --> 04:45:24,346
- And that's why it's like we want to,

7463
04:45:24,346 --> 04:45:26,827
this whole open language models thing, the OLMO thing,

7464
04:45:26,827 --> 04:45:29,765
is to try to keep the model where everything is open

7465
04:45:29,765 --> 04:45:31,988
with the data as close to the frontier as possible.

7466
04:45:31,988 --> 04:45:34,597
So, we're compute-constrained, we're personnel-constrained.

7467
04:45:34,597 --> 04:45:37,166
We rely on getting insights from people

7468
04:45:37,166 --> 04:45:39,987
like John Schulman tells us to do RL on outputs.

7469
04:45:39,987 --> 04:45:41,566
We can make these big jumps,

7470
04:45:41,566 --> 04:45:43,108
but it just takes a long time

7471
04:45:43,108 --> 04:45:44,805
to push the frontier of open source.

7472
04:45:44,805 --> 04:45:47,096
And fundamentally, I would say

7473
04:45:47,096 --> 04:45:48,997
that that's because open source AI

7474
04:45:48,997 --> 04:45:50,759
does not have the same feedback loops

7475
04:45:50,759 --> 04:45:51,949
as open source software.

7476
04:45:51,949 --> 04:45:54,079
We talked about open source software for security

7477
04:45:54,079 --> 04:45:56,810
also is just because you build something once

7478
04:45:56,810 --> 04:45:57,643
and you can reuse it.

7479
04:45:57,643 --> 04:46:00,053
If you go into a new company, there's so many benefits.

7480
04:46:00,053 --> 04:46:02,659
But if you open source a language model,

7481
04:46:02,659 --> 04:46:04,299
you have this data sitting around,

7482
04:46:04,299 --> 04:46:07,329
you have this training code, it's not that easy

7483
04:46:07,329 --> 04:46:09,169
for someone to come and build on and improve,

7484
04:46:09,169 --> 04:46:10,567
'cause you need to spend a lot on compute.

7485
04:46:10,567 --> 04:46:11,758
You need to have expertise.

7486
04:46:11,758 --> 04:46:15,099
So, until there are feedback loops of open source AI,

7487
04:46:15,099 --> 04:46:18,161
it seems mostly an ideological mission.

7488
04:46:18,161 --> 04:46:19,717
Like people like Mark Zuckerberg, which is like,

7489
04:46:19,717 --> 04:46:22,460
America needs this, and I agree with him,

7490
04:46:22,460 --> 04:46:26,889
but in the time where the motivation ideologically is high,

7491
04:46:26,889 --> 04:46:29,080
we need to capitalize and build this ecosystem

7492
04:46:29,080 --> 04:46:30,801
around what benefits

7493
04:46:30,801 --> 04:46:33,248
do you get from seeing the language model data?

7494
04:46:33,248 --> 04:46:35,196
And there's not a lot about that.

7495
04:46:35,196 --> 04:46:36,819
We're gonna try to launch a demo soon

7496
04:46:36,819 --> 04:46:39,340
where you can look at a OLMO model and a query,

7497
04:46:39,340 --> 04:46:41,741
and see what pre-training data is similar to it,

7498
04:46:41,741 --> 04:46:44,460
which is legally risky and complicated,

7499
04:46:44,460 --> 04:46:46,571
but it's like, what does it mean

7500
04:46:46,571 --> 04:46:48,861
to see the data that the AI was trained on?

7501
04:46:48,861 --> 04:46:51,307
It's hard to parse. It's terabytes of files.

7502
04:46:51,307 --> 04:46:54,224
It's like, I don't know what I'm gonna find in there.

7503
04:46:54,224 --> 04:46:56,419
But that's what we need to do as an ecosystem

7504
04:46:56,419 --> 04:47:01,266
if people want open source AI to be financially useful.

7505
04:47:01,266 --> 04:47:02,610
- We didn't really talk about Stargate.

7506
04:47:02,610 --> 04:47:03,980
I would love to get your opinion

7507
04:47:03,980 --> 04:47:07,357
on what the new administration, the Trump administration,

7508
04:47:07,357 --> 04:47:08,847
everything that's doing

7509
04:47:08,847 --> 04:47:12,037
that's being done from the America side

7510
04:47:12,037 --> 04:47:14,278
and supporting AI infrastructure

7511
04:47:14,278 --> 04:47:16,516
and the efforts of the different AI companies.

7512
04:47:16,516 --> 04:47:17,407
What do you think about Stargate?

7513
04:47:17,407 --> 04:47:20,167
What are we supposed to think about Stargate?

7514
04:47:20,167 --> 04:47:22,802
And does Sam have the money?

7515
04:47:22,802 --> 04:47:23,765
(Nathan chuckles) - Yeah,

7516
04:47:23,765 --> 04:47:26,817
so I think Stargate is a opaque thing.

7517
04:47:26,817 --> 04:47:28,906
It definitely doesn't have $500 billion,

7518
04:47:28,906 --> 04:47:30,427
doesn't even have $100 billion.

7519
04:47:30,427 --> 04:47:33,363
So, what they announced is this $500 billion number,

7520
04:47:33,363 --> 04:47:37,326
Larry Ellison, Sam Altman, and Trump said it.

7521
04:47:37,326 --> 04:47:38,159
They thanked Trump

7522
04:47:38,159 --> 04:47:41,854
and Trump did do some executive actions

7523
04:47:41,854 --> 04:47:44,451
that do significantly improve the ability

7524
04:47:44,451 --> 04:47:46,784
for this to be built faster.

7525
04:47:47,686 --> 04:47:49,860
One of the executive actions you did is on federal land,

7526
04:47:49,860 --> 04:47:53,247
you can just basically build data centers in power,

7527
04:47:53,247 --> 04:47:54,292
pretty much like that. - Mm-hmm.

7528
04:47:54,292 --> 04:47:56,675
- And then, the permitting process is basically gone

7529
04:47:56,675 --> 04:47:58,068
or you file after the fact.

7530
04:47:58,068 --> 04:47:59,426
So, one of the, again,

7531
04:47:59,426 --> 04:48:01,507
like I had a schizo take earlier, another schizo take,

7532
04:48:01,507 --> 04:48:04,041
if you've ever been to the Presidio in San Francisco,

7533
04:48:04,041 --> 04:48:06,668
beautiful area, you could build a power plant

7534
04:48:06,668 --> 04:48:08,218
and a data center there if you wanted to.

7535
04:48:08,218 --> 04:48:09,640
Because it is federal land. (Nathan laughing)

7536
04:48:09,640 --> 04:48:11,135
It used to be a military base, so.

7537
04:48:11,135 --> 04:48:12,934
- It did. - But obviously,

7538
04:48:12,934 --> 04:48:14,307
this would piss people off.

7539
04:48:14,307 --> 04:48:16,188
It's a good bit. Anyways.

7540
04:48:16,188 --> 04:48:17,718
(Nathan and Lex laughing)

7541
04:48:17,718 --> 04:48:20,850
Trump has made it much easier to do this generally.

7542
04:48:20,850 --> 04:48:24,050
Texas has the only unregulated grid in the nation as well.

7543
04:48:24,050 --> 04:48:25,090
- [Lex] Let's go, Texas.

7544
04:48:25,090 --> 04:48:26,742
- And so, therefore,

7545
04:48:26,742 --> 04:48:29,749
like ERCOT enables people to build faster as well.

7546
04:48:29,749 --> 04:48:32,564
In addition, the federal regulations are coming down.

7547
04:48:32,564 --> 04:48:35,636
And so, Stargate is predicated,

7548
04:48:35,636 --> 04:48:37,479
and this is why that whole show happened.

7549
04:48:37,479 --> 04:48:38,405
Now, how they came up

7550
04:48:38,405 --> 04:48:41,077
with a $500 billion number is beyond me.

7551
04:48:41,077 --> 04:48:42,590
How they came up with $100 billion number

7552
04:48:42,590 --> 04:48:44,894
makes sense to some extent.

7553
04:48:44,894 --> 04:48:47,453
And there's actually a good table in here

7554
04:48:47,453 --> 04:48:52,453
that I would like to show in that Stargate piece that I had.

7555
04:48:54,779 --> 04:48:56,524
It's the most recent one. Yeah.

7556
04:48:56,524 --> 04:48:59,691
So, anyways, Stargate, it's basically,

7557
04:49:02,239 --> 04:49:04,239
it's a table about cost.

7558
04:49:05,522 --> 04:49:09,670
There, you passed it already. It's that one.

7559
04:49:09,670 --> 04:49:12,000
So, this table is explaining what happens.

7560
04:49:12,000 --> 04:49:14,282
So, Stargate is in Abilene, Texas,

7561
04:49:14,282 --> 04:49:16,222
the first $100 billion of it.

7562
04:49:16,222 --> 04:49:18,852
That site is 2.2 gigawatts of power in,

7563
04:49:18,852 --> 04:49:22,019
about 1.8 gigawatts of power consumed.

7564
04:49:22,937 --> 04:49:25,770
Per GPU, they have like roughly...

7565
04:49:26,719 --> 04:49:29,691
Oracle is already building the first part of this

7566
04:49:29,691 --> 04:49:31,421
before Stargate came about.

7567
04:49:31,421 --> 04:49:32,764
To be clear, they've been building it for a year.

7568
04:49:32,764 --> 04:49:35,554
They tried to rent it to Elon, in fact.

7569
04:49:35,554 --> 04:49:37,388
But Elon was like, it's too slow, I need it faster.

7570
04:49:37,388 --> 04:49:38,221
So, then he went

7571
04:49:38,221 --> 04:49:39,936
and did his Memphis thing. - Mm-hmm.

7572
04:49:39,936 --> 04:49:41,512
- And so, OpenAI was able to get it

7573
04:49:41,512 --> 04:49:44,163
with this weird joint venture called Stargate.

7574
04:49:44,163 --> 04:49:46,185
They initially signed a deal with just Oracle

7575
04:49:46,185 --> 04:49:47,657
for the first section of this cluster.

7576
04:49:47,657 --> 04:49:50,057
This first section of this cluster

7577
04:49:50,057 --> 04:49:54,390
is roughly $5 billion to $6 billion of server spend.

7578
04:49:56,968 --> 04:49:58,367
And then, there's another billion or so

7579
04:49:58,367 --> 04:50:00,347
of data center spend.

7580
04:50:00,347 --> 04:50:01,576
And then, likewise,

7581
04:50:01,576 --> 04:50:03,835
if you fill out that entire 1.8 gigawatts

7582
04:50:03,835 --> 04:50:06,027
with the next two generations of Nvidia's chips,

7583
04:50:06,027 --> 04:50:10,288
GB200, GB300, VR200, and you fill it out completely,

7584
04:50:10,288 --> 04:50:15,205
that ends up being roughly $50 billion of server cost.

7585
04:50:15,205 --> 04:50:18,057
Plus there's data center cost, plus maintenance cost,

7586
04:50:18,057 --> 04:50:21,273
plus operation costs, plus all these things.

7587
04:50:21,273 --> 04:50:22,349
And that's where OpenAI

7588
04:50:22,349 --> 04:50:25,700
gets to their $100 billion announcement that they had.

7589
04:50:25,700 --> 04:50:27,749
Because they talked about 100 billion is phase one.

7590
04:50:27,749 --> 04:50:30,558
That's this Abilene, Texas data center.

7591
04:50:30,558 --> 04:50:33,450
$100 billion of "total cost of ownership", quote, unquote.

7592
04:50:33,450 --> 04:50:35,180
So, it's not CapEx, it's not investment,

7593
04:50:35,180 --> 04:50:38,433
it's $100 billion of total cost of ownership.

7594
04:50:38,433 --> 04:50:40,703
And then, there will be future phases.

7595
04:50:40,703 --> 04:50:41,710
They're looking at other sites

7596
04:50:41,710 --> 04:50:44,053
that are even bigger than this 2.2 gigawatts, by the way,

7597
04:50:44,053 --> 04:50:46,128
in Texas and elsewhere.

7598
04:50:46,128 --> 04:50:49,268
And so, they're not completely ignoring that.

7599
04:50:49,268 --> 04:50:52,620
But there is, the number of $100 billion

7600
04:50:52,620 --> 04:50:54,068
that they say is for phase one,

7601
04:50:54,068 --> 04:50:55,429
which I do think will happen.

7602
04:50:55,429 --> 04:50:57,171
They don't even have the money for that.

7603
04:50:57,171 --> 04:50:58,575
Furthermore, it's not $100 billion,

7604
04:50:58,575 --> 04:51:00,172
it's $50 billion of spend.

7605
04:51:00,172 --> 04:51:04,337
And then, $50 billion of operational cost, power, et cetera,

7606
04:51:04,337 --> 04:51:06,825
rental pricing, et cetera.

7607
04:51:06,825 --> 04:51:09,535
'Cause OpenAI is renting the GPUs

7608
04:51:09,535 --> 04:51:11,576
from the Stargate joint venture.

7609
04:51:11,576 --> 04:51:14,266
What money do they actually have? SoftBank.

7610
04:51:14,266 --> 04:51:15,227
SoftBank is gonna invest,

7611
04:51:15,227 --> 04:51:16,691
Oracle's gonna invest, OpenAI is gonna invest.

7612
04:51:16,691 --> 04:51:17,671
OpenAI is on the line - Mm-hmm.

7613
04:51:17,671 --> 04:51:18,948
- for $19 billion.

7614
04:51:18,948 --> 04:51:20,835
Everyone knows that they've only got 6 billion

7615
04:51:20,835 --> 04:51:23,267
in their last round and 4 billion in debt.

7616
04:51:23,267 --> 04:51:26,201
But there's news of SoftBank

7617
04:51:26,201 --> 04:51:28,994
maybe investing 25 billion into OpenAI.

7618
04:51:28,994 --> 04:51:30,703
So, that's part of it.

7619
04:51:30,703 --> 04:51:32,442
So, 19 billion can come from there.

7620
04:51:32,442 --> 04:51:35,770
So, OpenAI does not have the money at all, to be clear.

7621
04:51:35,770 --> 04:51:37,139
Ink is not dried on anything.

7622
04:51:37,139 --> 04:51:38,316
OpenAI has $0

7623
04:51:38,316 --> 04:51:39,750
- Yeah. - for this 50 billion.

7624
04:51:39,750 --> 04:51:41,139
And which they're legally obligated

7625
04:51:41,139 --> 04:51:43,709
to put 19 billion of CapEx or into the joint venture.

7626
04:51:43,709 --> 04:51:45,090
And then, the rest, they're gonna pay

7627
04:51:45,090 --> 04:51:47,120
via renting the GPUs from the joint venture.

7628
04:51:47,120 --> 04:51:49,203
And then, there's Oracle.

7629
04:51:50,141 --> 04:51:51,810
Oracle has a lot of money.

7630
04:51:51,810 --> 04:51:53,444
They're building the first section completely.

7631
04:51:53,444 --> 04:51:54,774
They were spending for it themselves.

7632
04:51:54,774 --> 04:51:58,524
This $6 billion of CapEx, $10 billion of TCO.

7633
04:51:59,581 --> 04:52:00,810
And they were gonna do that first section.

7634
04:52:00,810 --> 04:52:02,859
They're paying for that.

7635
04:52:02,859 --> 04:52:03,887
As far as the rest of the section,

7636
04:52:03,887 --> 04:52:06,488
I don't know how much Larry wants to spend.

7637
04:52:06,488 --> 04:52:07,321
At any point, he could pull out.

7638
04:52:07,321 --> 04:52:09,428
This is, again, this is completely voluntary.

7639
04:52:09,428 --> 04:52:10,261
So, at any point,

7640
04:52:10,261 --> 04:52:11,639
there's no signed ink on this. - Mm-hmm.

7641
04:52:11,639 --> 04:52:13,006
- But he potentially

7642
04:52:13,006 --> 04:52:14,858
could contribute tens of billions of dollars, to be clear.

7643
04:52:14,858 --> 04:52:17,381
He's got the money, Oracle's got the money.

7644
04:52:17,381 --> 04:52:18,753
And then, there's like MGX,

7645
04:52:18,753 --> 04:52:20,332
which is the south, the UAE fund,

7646
04:52:20,332 --> 04:52:23,989
which technically has $1.5 trillion for investing in AI.

7647
04:52:23,989 --> 04:52:27,022
But again, I don't know how real that money is.

7648
04:52:27,022 --> 04:52:29,724
And whereas there's no ink signed for this,

7649
04:52:29,724 --> 04:52:33,031
SoftBank does not have $25 billion of cash.

7650
04:52:33,031 --> 04:52:35,832
They have to sell down their stake in Arm,

7651
04:52:35,832 --> 04:52:38,461
which is the leader in CPUs, and they IPO'ed it.

7652
04:52:38,461 --> 04:52:39,829
This is obviously what they've always wanted to do.

7653
04:52:39,829 --> 04:52:41,570
They just didn't know where they'd redeploy the capital.

7654
04:52:41,570 --> 04:52:44,569
Selling down the stake in Arm makes a ton of sense.

7655
04:52:44,569 --> 04:52:45,402
So, they can sell that down

7656
04:52:45,402 --> 04:52:47,438
and invest in this if they want to

7657
04:52:47,438 --> 04:52:49,595
and invest in OpenAI if they want to.

7658
04:52:49,595 --> 04:52:50,962
As far as money secured,

7659
04:52:50,962 --> 04:52:54,795
the first 100,000 GB200 cluster can be funded.

7660
04:52:55,798 --> 04:52:57,678
Everything else after that

7661
04:52:57,678 --> 04:52:58,863
- Up in the air. - is up in the air.

7662
04:52:58,863 --> 04:52:59,722
Money's coming.

7663
04:52:59,722 --> 04:53:01,202
I believe the money will come.

7664
04:53:01,202 --> 04:53:03,237
I personally do. (Lex laughing)

7665
04:53:03,237 --> 04:53:04,665
- It's a belief. Okay. - It's a belief

7666
04:53:04,665 --> 04:53:06,034
that they are gonna release better models

7667
04:53:06,034 --> 04:53:06,867
and be able to raise more money,

7668
04:53:06,867 --> 04:53:07,866
right? - Yeah, yeah.

7669
04:53:07,866 --> 04:53:10,328
But the actual reality is, is that Elon's right,

7670
04:53:10,328 --> 04:53:12,136
the money does not exist.

7671
04:53:12,136 --> 04:53:14,773
- What is the US government have to do with anything?

7672
04:53:14,773 --> 04:53:16,493
What does Trump have to do with everything?

7673
04:53:16,493 --> 04:53:18,725
He's just a hype man? - So, Trump is,

7674
04:53:18,725 --> 04:53:22,176
he's reducing the regulation so they can build it faster.

7675
04:53:22,176 --> 04:53:23,514
And he is allowing them to do it.

7676
04:53:23,514 --> 04:53:25,206
Because any investment of this side

7677
04:53:25,206 --> 04:53:27,397
is gonna involve antitrust stuff.

7678
04:53:27,397 --> 04:53:29,172
So, obviously, he's gonna allow them to do it.

7679
04:53:29,172 --> 04:53:30,825
He's gonna enable the regulations

7680
04:53:30,825 --> 04:53:33,118
to actually allow to be built.

7681
04:53:33,118 --> 04:53:35,486
I don't believe there's any US government dollars

7682
04:53:35,486 --> 04:53:37,318
being spent on this though. - Yeah.

7683
04:53:37,318 --> 04:53:40,601
So, I think he's also just creating a general vibe

7684
04:53:40,601 --> 04:53:42,148
that this regulation will go down

7685
04:53:42,148 --> 04:53:44,815
and this is the era of building.

7686
04:53:45,886 --> 04:53:46,951
So, if you're a builder, - Yeah.

7687
04:53:46,951 --> 04:53:47,990
- you want to create stuff,

7688
04:53:47,990 --> 04:53:49,995
you wanna launch stuff, this is the time to do it.

7689
04:53:49,995 --> 04:53:52,579
- And so, we've had this 1.8 gigawatt data center

7690
04:53:52,579 --> 04:53:54,750
in our data for over a year now,

7691
04:53:54,750 --> 04:53:56,271
and we've been sending it to all of our clients,

7692
04:53:56,271 --> 04:53:57,380
including many of these companies

7693
04:53:57,380 --> 04:53:58,909
that are building the multi gigawatts.

7694
04:53:58,909 --> 04:54:01,420
But that is at a level that's not quite,

7695
04:54:01,420 --> 04:54:04,547
maybe executives seeing $500 billion, $100 billion,

7696
04:54:04,547 --> 04:54:06,270
and then everyone's asking them like...

7697
04:54:06,270 --> 04:54:09,689
So, it could spur another, an even faster arms race.

7698
04:54:09,689 --> 04:54:11,161
Because there's already an arms race,

7699
04:54:11,161 --> 04:54:13,018
but this like 100 billion,

7700
04:54:13,018 --> 04:54:16,086
$500 billion number, Trump talking about it on TV.

7701
04:54:16,086 --> 04:54:18,371
It could spur the arm race to be even faster

7702
04:54:18,371 --> 04:54:21,183
and more investors to flood in and et cetera, et cetera.

7703
04:54:21,183 --> 04:54:25,046
So, I think you're right in that sense that OpenAI,

7704
04:54:25,046 --> 04:54:28,259
or Trump is championing people are gonna build more

7705
04:54:28,259 --> 04:54:30,352
and his actions are gonna let people build more.

7706
04:54:30,352 --> 04:54:32,390
- What are you excited

7707
04:54:32,390 --> 04:54:35,973
about these several years that are upcoming

7708
04:54:37,771 --> 04:54:40,601
in terms of cluster build outs,

7709
04:54:40,601 --> 04:54:43,447
in terms of breakthroughs in AI?

7710
04:54:43,447 --> 04:54:45,901
Like the best possible future you can imagine

7711
04:54:45,901 --> 04:54:48,632
in the next couple years, two, three, four years?

7712
04:54:48,632 --> 04:54:50,002
What does that look like?

7713
04:54:50,002 --> 04:54:52,246
Just it could be very specific technical things

7714
04:54:52,246 --> 04:54:54,942
like breakthroughs on post post-training,

7715
04:54:54,942 --> 04:54:57,359
or it could be just size big.

7716
04:54:58,848 --> 04:54:59,773
- [Dylan] Yeah, I mean it's-

7717
04:54:59,773 --> 04:55:01,664
- Impressive clusters.

7718
04:55:01,664 --> 04:55:04,174
- I really enjoy tracking supply chain

7719
04:55:04,174 --> 04:55:05,585
and who's involved in what. (Lex laughing)

7720
04:55:05,585 --> 04:55:06,418
- Yeah. - I really do.

7721
04:55:06,418 --> 04:55:08,504
It's really fun to see the numbers, the cost,

7722
04:55:08,504 --> 04:55:10,165
who's building what capacity,

7723
04:55:10,165 --> 04:55:12,203
helping them figure out how much capacity they should build,

7724
04:55:12,203 --> 04:55:14,412
winning deals, strategic stuff, that's really cool.

7725
04:55:14,412 --> 04:55:16,804
I think technologically, there's a lot

7726
04:55:16,804 --> 04:55:19,622
around the networking side that really excites me

7727
04:55:19,622 --> 04:55:23,704
with optics and electronics getting closer and closer,

7728
04:55:23,704 --> 04:55:24,952
whether it be co-packaged optics,

7729
04:55:24,952 --> 04:55:28,233
or some sort of forms of new forms of switching.

7730
04:55:28,233 --> 04:55:29,230
- This is internal

7731
04:55:29,230 --> 04:55:31,202
to a cluster. - Cluster. Yeah.

7732
04:55:31,202 --> 04:55:34,343
Also multi-data center training,

7733
04:55:34,343 --> 04:55:36,854
people are putting so much fiber between these data centers

7734
04:55:36,854 --> 04:55:40,680
and lighting it up with so much bandwidth that there's a lot

7735
04:55:40,680 --> 04:55:42,470
of interesting stuff happening on that end.

7736
04:55:42,470 --> 04:55:44,583
Telecom has been really boring since 5G,

7737
04:55:44,583 --> 04:55:47,984
and now it's like really exciting again on the fiber side.

7738
04:55:47,984 --> 04:55:50,370
- Can you educate me a little bit about the speed of things?

7739
04:55:50,370 --> 04:55:53,370
So, the speed of memory versus the speed of interconnect,

7740
04:55:53,370 --> 04:55:55,749
versus the speed of fiber between data centers?

7741
04:55:55,749 --> 04:55:58,897
Are these orders of magnitude different?

7742
04:55:58,897 --> 04:56:01,446
Can we, at some point, converge towards a place,

7743
04:56:01,446 --> 04:56:04,314
where it all just feels like one computer?

7744
04:56:04,314 --> 04:56:05,147
- No, I don't think

7745
04:56:05,147 --> 04:56:06,995
that's possible. - Okay. All right. (chuckles)

7746
04:56:06,995 --> 04:56:08,226
- It's only gonna get harder to program,

7747
04:56:08,226 --> 04:56:09,187
not easier. - Okay.

7748
04:56:09,187 --> 04:56:10,734
- It's only gonna get more difficult

7749
04:56:10,734 --> 04:56:12,934
and complicated in more layers.

7750
04:56:12,934 --> 04:56:14,597
The general image that people

7751
04:56:14,597 --> 04:56:16,744
like to have is this hierarchy of memory.

7752
04:56:16,744 --> 04:56:18,465
So, on chip is really close.

7753
04:56:18,465 --> 04:56:21,426
Localized within the chip, you have registers.

7754
04:56:21,426 --> 04:56:23,290
And those are shared between some compute elements,

7755
04:56:23,290 --> 04:56:24,483
and then you'll have caches,

7756
04:56:24,483 --> 04:56:26,070
which are shared between more compute elements.

7757
04:56:26,070 --> 04:56:28,584
Then, you have memory, like HBM or DRAM,

7758
04:56:28,584 --> 04:56:30,340
like DDR memory or whatever it is.

7759
04:56:30,340 --> 04:56:32,288
And that's shared between the whole chip.

7760
04:56:32,288 --> 04:56:34,739
And then, you can have pools of memory

7761
04:56:34,739 --> 04:56:36,440
that are shared between many chips.

7762
04:56:36,440 --> 04:56:39,460
And then, storage and you keep zoning out.

7763
04:56:39,460 --> 04:56:41,597
The access latency across data centers,

7764
04:56:41,597 --> 04:56:44,315
across within the data center within a chip differs.

7765
04:56:44,315 --> 04:56:45,325
So, you're obviously always,

7766
04:56:45,325 --> 04:56:48,489
you're always gonna have different

7767
04:56:48,489 --> 04:56:49,871
programming paradigms for this.

7768
04:56:49,871 --> 04:56:50,879
It's not gonna be easy.

7769
04:56:50,879 --> 04:56:51,926
Programming this stuff is gonna be hard.

7770
04:56:51,926 --> 04:56:55,472
Maybe AI can help with programming this.

7771
04:56:55,472 --> 04:56:59,055
But the way to think about it is that like,

7772
04:57:01,960 --> 04:57:04,023
(Dylan sighs)

7773
04:57:04,023 --> 04:57:08,081
there's the more elements you add to a task,

7774
04:57:08,081 --> 04:57:10,706
you don't gain, you don't get strong scaling.

7775
04:57:10,706 --> 04:57:11,555
If I double the number of chips,

7776
04:57:11,555 --> 04:57:12,902
I don't get 2x the performance.

7777
04:57:12,902 --> 04:57:15,032
This is just like a reality of computing,

7778
04:57:15,032 --> 04:57:16,965
'cause there's inefficiencies.

7779
04:57:16,965 --> 04:57:18,414
And there's a lot of interesting work

7780
04:57:18,414 --> 04:57:21,697
being done to make it more linear.

7781
04:57:21,697 --> 04:57:23,075
Whether it's making the chips

7782
04:57:23,075 --> 04:57:24,805
more networked together more tightly

7783
04:57:24,805 --> 04:57:28,686
or cool programming models, or cool algorithmic things

7784
04:57:28,686 --> 04:57:30,593
that you can do on the model side.

7785
04:57:30,593 --> 04:57:32,474
DeepSeek did some of these really cool innovations

7786
04:57:32,474 --> 04:57:33,844
because they were limited on interconnect,

7787
04:57:33,844 --> 04:57:35,641
but they still needed to parallelize.

7788
04:57:35,641 --> 04:57:36,474
All sorts of...

7789
04:57:36,474 --> 04:57:37,432
Everyone's always doing stuff.

7790
04:57:37,432 --> 04:57:38,312
Google's got a bunch of work

7791
04:57:38,312 --> 04:57:40,313
and everyone's got a bunch of work about this.

7792
04:57:40,313 --> 04:57:42,055
That stuff is super exciting

7793
04:57:42,055 --> 04:57:44,635
on the model and workload and innovation side.

7794
04:57:44,635 --> 04:57:47,170
Hardware, solid-state transformers

7795
04:57:47,170 --> 04:57:48,864
are interesting for the power side.

7796
04:57:48,864 --> 04:57:50,632
There's all sorts of stuff on batteries

7797
04:57:50,632 --> 04:57:52,814
and there's all sorts of stuff on...

7798
04:57:52,814 --> 04:57:54,057
I think when you look at,

7799
04:57:54,057 --> 04:57:55,885
if you look at every layer of the compute stack,

7800
04:57:55,885 --> 04:57:58,024
whether it goes from lithography and etch,

7801
04:57:58,024 --> 04:58:00,301
all the way to like fabrication, to optics,

7802
04:58:00,301 --> 04:58:02,715
to networking, to power, to transformers,

7803
04:58:02,715 --> 04:58:05,135
to cooling, to a networking

7804
04:58:05,135 --> 04:58:07,184
and you just go on up and up and up and up the stack.

7805
04:58:07,184 --> 04:58:09,829
Even air conditioners for data centers are innovating.

7806
04:58:09,829 --> 04:58:12,612
There's like copper cables are innovating.

7807
04:58:12,612 --> 04:58:14,775
You wouldn't think it, but copper cables are,

7808
04:58:14,775 --> 04:58:16,572
there's some innovations happening there

7809
04:58:16,572 --> 04:58:18,375
with the density of how you can pack them.

7810
04:58:18,375 --> 04:58:20,662
And it's like all of these layers of the stack,

7811
04:58:20,662 --> 04:58:21,654
all the way up to the models.

7812
04:58:21,654 --> 04:58:24,829
Human progress is at a pace that's never been seen before.

7813
04:58:24,829 --> 04:58:27,084
- I'm just imagining you sitting back in a layer somewhere

7814
04:58:27,084 --> 04:58:29,664
with screens everywhere, just monitoring the supply chain

7815
04:58:29,664 --> 04:58:30,834
where all these clusters,

7816
04:58:30,834 --> 04:58:33,624
all the information you're gathering.

7817
04:58:33,624 --> 04:58:34,694
You do incredible- - There's a big team.

7818
04:58:34,694 --> 04:58:35,527
There's a big team.

7819
04:58:35,527 --> 04:58:37,794
- Yeah. (laughs)

7820
04:58:37,794 --> 04:58:41,195
You do quite incredible work with SemiAnalysis.

7821
04:58:41,195 --> 04:58:44,278
Just keeping your finger on the pulse

7822
04:58:46,281 --> 04:58:49,351
of human civilization in the digital world.

7823
04:58:49,351 --> 04:58:51,667
It's pretty cool just to watch, feel that.

7824
04:58:51,667 --> 04:58:53,124
- [Dylan] Yeah. Thank you, I guess-

7825
04:58:53,124 --> 04:58:56,374
- Feel all of us doing shit, epic shit.

7826
04:58:57,605 --> 04:59:00,434
- [Dylan] Feel the AGI. (Lex laughing)

7827
04:59:00,434 --> 04:59:02,806
- From meme to reality.

7828
04:59:02,806 --> 04:59:04,336
What Nathan, is there breakthroughs

7829
04:59:04,336 --> 04:59:07,299
that you're looking forward to potentially?

7830
04:59:07,299 --> 04:59:08,937
- I had a while to think about this while listening

7831
04:59:08,937 --> 04:59:10,686
to Dylan's beautiful response. (Dylan laughing)

7832
04:59:10,686 --> 04:59:12,097
- He did listen to me. He was so-

7833
04:59:12,097 --> 04:59:13,586
- No, I knew this was coming.

7834
04:59:13,586 --> 04:59:17,843
And it's like, realistically, training models is very fun

7835
04:59:17,843 --> 04:59:19,409
because there's so much low-hanging fruit.

7836
04:59:19,409 --> 04:59:21,704
And the thing that makes my job entertaining,

7837
04:59:21,704 --> 04:59:24,543
I train models, I write analysis

7838
04:59:24,543 --> 04:59:27,552
about what's happening with models, and it's fun

7839
04:59:27,552 --> 04:59:31,333
because there is obviously so much more progress to be had.

7840
04:59:31,333 --> 04:59:32,830
And the real motivation why I do this

7841
04:59:32,830 --> 04:59:35,714
somewhere where I can share things is that there's just,

7842
04:59:35,714 --> 04:59:38,123
I don't trust people that are like,

7843
04:59:38,123 --> 04:59:39,755
"Trust me, bro, we're gonna make AI good."

7844
04:59:39,755 --> 04:59:41,743
It's like we're the ones that it's like, we're gonna do it

7845
04:59:41,743 --> 04:59:44,422
and you can trust us and we're just gonna have all the AI,

7846
04:59:44,422 --> 04:59:47,205
and it's just I would like a future,

7847
04:59:47,205 --> 04:59:48,334
where more people have a say

7848
04:59:48,334 --> 04:59:51,072
in what AI is and can understand it.

7849
04:59:51,072 --> 04:59:53,702
And it's a little bit less fun,

7850
04:59:53,702 --> 04:59:55,498
that it's not a like positive thing of like,

7851
04:59:55,498 --> 04:59:56,543
this is just all really fun.

7852
04:59:56,543 --> 04:59:59,582
Training models is fun and bring people in is fun,

7853
04:59:59,582 --> 05:00:01,285
but it's really like AI, if it is going

7854
05:00:01,285 --> 05:00:03,584
to be the most powerful technology of my lifetime,

7855
05:00:03,584 --> 05:00:06,780
it's like, we need to have a lot of people involved

7856
05:00:06,780 --> 05:00:08,054
in making that and...

7857
05:00:08,054 --> 05:00:10,895
- Making it open (Nathan chuckles)

7858
05:00:10,895 --> 05:00:11,972
helps with that,

7859
05:00:11,972 --> 05:00:14,550
as successful as possible, as open as possible, yeah.

7860
05:00:14,550 --> 05:00:16,253
- In my read of the last few years

7861
05:00:16,253 --> 05:00:19,211
is that more openness would help the AI ecosystem

7862
05:00:19,211 --> 05:00:21,926
in terms of having more people understand what's going on,

7863
05:00:21,926 --> 05:00:23,893
whether that's researchers from non-AI fields

7864
05:00:23,893 --> 05:00:25,811
to governments, to everything.

7865
05:00:25,811 --> 05:00:27,740
It doesn't mean that openness will always be the answer.

7866
05:00:27,740 --> 05:00:30,081
I think then it'll reassess of like,

7867
05:00:30,081 --> 05:00:31,619
what is the biggest problem facing AI

7868
05:00:31,619 --> 05:00:33,520
and tack on a different angle

7869
05:00:33,520 --> 05:00:35,996
to the wild ride that we're on.

7870
05:00:35,996 --> 05:00:40,135
- And for me, just from even the user experience,

7871
05:00:40,135 --> 05:00:42,591
anytime you have the, like Karpathy said,

7872
05:00:42,591 --> 05:00:45,258
the aha moments, like the magic,

7873
05:00:46,208 --> 05:00:50,530
like seeing the reasoning, the chain of thought,

7874
05:00:50,530 --> 05:00:52,571
it's like there's something really

7875
05:00:52,571 --> 05:00:54,613
just fundamentally beautiful about that.

7876
05:00:54,613 --> 05:00:58,081
It's putting a mirror to ourselves and seeing like, oh shit.

7877
05:00:58,081 --> 05:00:59,541
It is solving intelligence

7878
05:00:59,541 --> 05:01:02,482
as the cliche goal of these companies is.

7879
05:01:02,482 --> 05:01:07,006
And you get to understand why we humans are special.

7880
05:01:07,006 --> 05:01:09,433
The intelligence within us is special.

7881
05:01:09,433 --> 05:01:11,832
And for now also, why we're special in terms of,

7882
05:01:11,832 --> 05:01:16,573
we seem to be conscious in the AI systems for now aren't,

7883
05:01:16,573 --> 05:01:20,329
and we get to solve, we get to explore that mystery.

7884
05:01:20,329 --> 05:01:21,710
So, it's just really cool

7885
05:01:21,710 --> 05:01:25,827
to get to explore these questions that I don't think,

7886
05:01:25,827 --> 05:01:30,354
I would've never imagined would be even possible

7887
05:01:30,354 --> 05:01:34,541
back when sort of just watching with excitement

7888
05:01:34,541 --> 05:01:37,497
the Deep Blue beat Kasparov.

7889
05:01:37,497 --> 05:01:38,677
I wouldn't have ever thought

7890
05:01:38,677 --> 05:01:41,756
this kind of AI would be possible in my lifetime.

7891
05:01:41,756 --> 05:01:43,864
It's like this is really feels like AI.

7892
05:01:43,864 --> 05:01:44,968
- Yeah. (chuckles) - It's incredible.

7893
05:01:44,968 --> 05:01:48,806
- I started with AI of learning to fly a silly quadrotor.

7894
05:01:48,806 --> 05:01:49,665
It's like learning to fly

7895
05:01:49,665 --> 05:01:51,904
and it just like, it learned to fly up,

7896
05:01:51,904 --> 05:01:53,526
it would hit the ceiling and stop and catch it.

7897
05:01:53,526 --> 05:01:54,359
It's like, okay,

7898
05:01:54,359 --> 05:01:56,817
that is really stupid compared to what's going on now.

7899
05:01:56,817 --> 05:01:59,610
- And now, you could probably, with natural language,

7900
05:01:59,610 --> 05:02:01,361
tell it to learn to fly

7901
05:02:01,361 --> 05:02:03,090
and it's going to generate the control algorithm

7902
05:02:03,090 --> 05:02:05,356
required to do that. (laughs) - Probably.

7903
05:02:05,356 --> 05:02:06,454
There's low level blockers.

7904
05:02:06,454 --> 05:02:08,201
We have to do some weird stuff for that,

7905
05:02:08,201 --> 05:02:09,034
- Yeah, for sure. - but you can,

7906
05:02:09,034 --> 05:02:10,002
you definitely can. - It's back

7907
05:02:10,002 --> 05:02:11,590
to our robotics conversation, yeah.

7908
05:02:11,590 --> 05:02:12,423
When you have to interact

7909
05:02:12,423 --> 05:02:14,353
in the actual physical world, that's hard.

7910
05:02:14,353 --> 05:02:18,188
What gives you hope about the future of human civilization?

7911
05:02:18,188 --> 05:02:22,771
Looking into the next 10 years, 100 years, 1,000 years,

7912
05:02:23,755 --> 05:02:25,810
how long you think we'll make it?

7913
05:02:25,810 --> 05:02:26,643
You think we've got

7914
05:02:26,643 --> 05:02:28,477
1,000 years? - I think humans

7915
05:02:28,477 --> 05:02:30,669
will definitely be around in 1,000 years.

7916
05:02:30,669 --> 05:02:33,547
I think there's ways that very bad things could happen.

7917
05:02:33,547 --> 05:02:35,057
There'll be way fewer humans,

7918
05:02:35,057 --> 05:02:37,269
but humans are very good at surviving.

7919
05:02:37,269 --> 05:02:40,898
There's been a lot of things that that is true.

7920
05:02:40,898 --> 05:02:42,176
I don't think they're necessarily,

7921
05:02:42,176 --> 05:02:45,606
we're good at long-term credit assignment of risk,

7922
05:02:45,606 --> 05:02:47,323
but when the risk becomes immediate,

7923
05:02:47,323 --> 05:02:49,013
we tend to figure things out,

7924
05:02:49,013 --> 05:02:50,677
- Oh yeah. - and for that reason,

7925
05:02:50,677 --> 05:02:53,083
I am like, there's physical constraints

7926
05:02:53,083 --> 05:02:55,166
to things like AGI hyper,

7927
05:02:56,456 --> 05:02:59,544
like recursive improvement to kill us all type stuff

7928
05:02:59,544 --> 05:03:01,053
for physical reasons,

7929
05:03:01,053 --> 05:03:03,104
and for how humans have figured things out before,

7930
05:03:03,104 --> 05:03:04,211
I'm not too worried about it.

7931
05:03:04,211 --> 05:03:05,758
AI takeover.

7932
05:03:05,758 --> 05:03:07,692
There are other international things that are worrying,

7933
05:03:07,692 --> 05:03:11,275
but there's just fundamental human goodness

7934
05:03:12,205 --> 05:03:17,104
and trying to amplify that, and we're on a tenuous time.

7935
05:03:17,104 --> 05:03:20,555
And if you look at humanity as a whole,

7936
05:03:20,555 --> 05:03:23,045
there's been times where things go backwards,

7937
05:03:23,045 --> 05:03:25,078
there's times when things don't happen at all.

7938
05:03:25,078 --> 05:03:26,264
And we're on a,

7939
05:03:26,264 --> 05:03:28,934
what should be very positive trajectory right now.

7940
05:03:28,934 --> 05:03:33,285
- Yeah, there seems to be progress, but just with power,

7941
05:03:33,285 --> 05:03:35,914
there's spikes of human suffering,

7942
05:03:35,914 --> 05:03:38,763
and we wanna try to minimize the amount of spikes.

7943
05:03:38,763 --> 05:03:41,703
- Generally, humanity is gonna suffer a lot less.

7944
05:03:41,703 --> 05:03:44,439
I'm very optimistic about that.

7945
05:03:44,439 --> 05:03:47,844
I do worry of techno fascism type stuff arising

7946
05:03:47,844 --> 05:03:51,554
as AI becomes more and more prevalent and powerful,

7947
05:03:51,554 --> 05:03:54,181
and those who control it can do more and more.

7948
05:03:54,181 --> 05:03:55,324
Maybe it doesn't kill us all,

7949
05:03:55,324 --> 05:03:59,164
but at some point, every very powerful human

7950
05:03:59,164 --> 05:04:01,004
is gonna wanna brain computer interface,

7951
05:04:01,004 --> 05:04:02,882
so that they can interact with the AGI,

7952
05:04:02,882 --> 05:04:05,030
and all of its advantages in many more way,

7953
05:04:05,030 --> 05:04:06,850
and merge its mind with sort of like,

7954
05:04:06,850 --> 05:04:09,561
and its capabilities or that person's capabilities

7955
05:04:09,561 --> 05:04:12,031
can leverage those much better than anyone else,

7956
05:04:12,031 --> 05:04:14,736
and therefore be, it won't be one person rule them all,

7957
05:04:14,736 --> 05:04:16,625
but it will be...

7958
05:04:16,625 --> 05:04:19,229
The thing I worry about is it'll be like few people,

7959
05:04:19,229 --> 05:04:21,298
hundreds, thousands, tens of thousands,

7960
05:04:21,298 --> 05:04:25,647
maybe millions of people rule whoever's left,

7961
05:04:25,647 --> 05:04:27,385
and the economy around it.

7962
05:04:27,385 --> 05:04:30,238
And that's the thing that's probably more worrisome

7963
05:04:30,238 --> 05:04:33,576
is human machine amalgamations.

7964
05:04:33,576 --> 05:04:35,518
This enables an individual human

7965
05:04:35,518 --> 05:04:37,079
to have more impact on the world.

7966
05:04:37,079 --> 05:04:40,252
And that impact can be both positive and negative.

7967
05:04:40,252 --> 05:04:42,658
Generally, humans have positive impacts on the world,

7968
05:04:42,658 --> 05:04:43,790
at least societally,

7969
05:04:43,790 --> 05:04:45,790
but it's possible for individual humans

7970
05:04:45,790 --> 05:04:47,247
to have such negative impacts.

7971
05:04:47,247 --> 05:04:51,276
And AGI, at least as I think the labs define it,

7972
05:04:51,276 --> 05:04:53,539
which is not a runaway sentient thing,

7973
05:04:53,539 --> 05:04:55,449
but rather just something that can do a lot

7974
05:04:55,449 --> 05:04:59,237
of tasks really efficiently, amplifies the capabilities

7975
05:04:59,237 --> 05:05:01,953
of someone causing extreme damage.

7976
05:05:01,953 --> 05:05:03,097
But for the most part,

7977
05:05:03,097 --> 05:05:06,124
I think it'll be used for profit-seeking motives,

7978
05:05:06,124 --> 05:05:09,025
which will increase the abundance and supply of things.

7979
05:05:09,025 --> 05:05:11,054
and therefore reduce suffering.

7980
05:05:11,054 --> 05:05:11,887
(Lex laughing)

7981
05:05:11,887 --> 05:05:12,720
- Yeah. - That's the goal.

7982
05:05:12,720 --> 05:05:15,176
- Scrolling on a timeline, (Nathan laughing)

7983
05:05:15,176 --> 05:05:17,296
just drowning in dopamine- - Crawling stasis.

7984
05:05:17,296 --> 05:05:19,232
- That is- - Scrolling holds

7985
05:05:19,232 --> 05:05:20,602
the status quo of the world.

7986
05:05:20,602 --> 05:05:22,008
- That is a positive outcome (Nathan and Lex laughing)

7987
05:05:22,008 --> 05:05:24,253
is like, if I have food tubes - Yeah.

7988
05:05:24,253 --> 05:05:25,928
- and I'm scrolling and I'm happy,

7989
05:05:25,928 --> 05:05:26,935
that's a positive outcome.

7990
05:05:26,935 --> 05:05:29,106
(group laughing)

7991
05:05:29,106 --> 05:05:31,886
- While expanding out into the cosmos.

7992
05:05:31,886 --> 05:05:35,342
Well, this is a fun time to be alive.

7993
05:05:35,342 --> 05:05:37,856
And thank you for pushing the forefront

7994
05:05:37,856 --> 05:05:40,036
of what is possible in humans.

7995
05:05:40,036 --> 05:05:41,909
And thank you for talking today. This was fun.

7996
05:05:41,909 --> 05:05:44,074
- Thanks for having us. - Thanks for having us.

7997
05:05:44,074 --> 05:05:44,907
- Thanks for listening

7998
05:05:44,907 --> 05:05:48,497
to this conversation with Dylan Patel and Nathan Lambert.

7999
05:05:48,497 --> 05:05:49,513
To support this podcast,

8000
05:05:49,513 --> 05:05:52,636
please check out our sponsors in the description.

8001
05:05:52,636 --> 05:05:57,536
And now, let me leave you some words from Richard Feynman.

8002
05:05:57,536 --> 05:06:01,456
"For a successful technology, reality must take precedence

8003
05:06:01,456 --> 05:06:06,456
over public relations, for nature cannot be fooled."

8004
05:06:06,547 --> 05:06:11,047
Thank you for listening and hope to see you next time.

