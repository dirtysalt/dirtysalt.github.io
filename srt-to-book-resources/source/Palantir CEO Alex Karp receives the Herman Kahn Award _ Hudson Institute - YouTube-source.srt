1
00:00:00,120 --> 00:00:09,920
 Well, I'd- I'd like to introduce you to Alex Karp

2
00:00:09,920 --> 00:00:11,560
and I know all of you are thinking

3
00:00:11,560 --> 00:00:15,320
"Well, this man needs no introduction," and you're right about that.

4
00:00:15,320 --> 00:00:18,710
But, you know, you know Alex Karp

5
00:00:18,710 --> 00:00:21,320
the tech mogul, the defender of the West

6
00:00:21,320 --> 00:00:25,270
the man who dreams of spraying fentanyl-laced ur- urine on

7
00:00:25,270 --> 00:00:29,800
uh, short sellers, the author of The Technological Republic

8
00:00:29,800 --> 00:00:32,720
the subject of The Philosopher in the Valley

9
00:00:32,720 --> 00:00:37,610
the only person who has an optimistic of view- view of AI and American greatness.

10
00:00:37,610 --> 00:00:40,300
But in other words, you know Alex Karp

11
00:00:40,300 --> 00:00:43,820
my boss, and I want you to know Alex Karp

12
00:00:43,820 --> 00:00:44,720
my friend.

13
00:00:44,720 --> 00:00:50,740
Next to my late father, no person has a greater impact on my life than Alex.

14
00:00:50,740 --> 00:00:53,940
And that's true in obvious ways, the success of Palantir

15
00:00:53,940 --> 00:00:55,900
the face that he- the faith that he's placed in me

16
00:00:55,900 --> 00:00:58,460
but it's also true in other ways.

17
00:00:58,460 --> 00:00:59,580
You know, he's been my rabbi.

18
00:00:59,580 --> 00:01:02,180
He's been my confidant, my guru, and at times

19
00:01:02,180 --> 00:01:04,640
when I've given him cause, and I've given him cause

20
00:01:04,640 --> 00:01:07,000
he's been my parole officer.

21
00:01:07,000 --> 00:01:14,800
 One of the first things Alex told me 20 years ago when we started working together is that he thought I had the constitution of a goat.

22
00:01:14,800 --> 00:01:19,360
 And he did not mean the Gen Z expression of greatest of all time.

23
00:01:19,360 --> 00:01:22,580
 He meant the animal.

24
00:01:22,580 --> 00:01:26,830
That it seemed like I could survive eating trash and plastic in a barren wasteland.

25
00:01:26,830 --> 00:01:29,530
 And such a skill would come in handy in Washington

26
00:01:29,530 --> 00:01:29,930
DC.

27
00:01:29,930 --> 00:01:33,860
 That's not a standard rubric for getting a job

28
00:01:33,860 --> 00:01:37,320
but there's nothing conventional ab- about Alex.

29
00:01:37,320 --> 00:01:42,140
Some months later, he asked me if I knew why French restaurants were so good.

30
00:01:42,140 --> 00:01:44,020
I, in fact, did not.

31
00:01:44,020 --> 00:01:47,400
At the age of 24, the closest I'd gotten to a French restaurant is a french fry.

32
00:01:47,400 --> 00:01:53,140
 He educated me that it's in part because the waitstaff are actually part of the kitchen staff.

33
00:01:53,140 --> 00:01:54,260
They understand the technique.

34
00:01:54,260 --> 00:01:55,640
They understand how the food is made.

35
00:01:55,640 --> 00:01:56,860
They understand the methodology.

36
00:01:56,860 --> 00:01:59,800
In other words, they understand the food from first principles.

37
00:01:59,800 --> 00:02:05,340
They were a part of a complex system of delivering the experience

38
00:02:05,340 --> 00:02:12,280
and this was his inspiration for creating the conception of Forward Deployed Engineering.

39
00:02:12,280 --> 00:02:19,213
Now, I didn't have the heart to tell my father that my Cornell and Stanford education was about to qualify me to be a waiter at Chez Karp-  .

40
00:02:19,213 --> 00:02:19,220
..

41
00:02:19,220 --> 00:02:21,200
but that's what happened.

42
00:02:21,200 --> 00:02:25,440
My- my father and Alex did agree on one thing for certain

43
00:02:25,440 --> 00:02:30,400
which is that the Bhagavad Gita is amongst the most important books ever written.

44
00:02:30,400 --> 00:02:33,620
Now, my father believed this despite never having read it.

45
00:02:33,620 --> 00:02:38,360
 To Alex, it was the most important business book ever written.

46
00:02:38,360 --> 00:02:42,260
Well, maybe co-equal with Menachem Begin's biography.

47
00:02:42,260 --> 00:02:44,630
And frequently, and as my literal guru in this case

48
00:02:44,630 --> 00:02:52,060
he'd remind me of how Krishna selectively revealed the truth to Arjuna as he was driving his chariot into battle

49
00:02:52,060 --> 00:02:57,260
purposefully matching what Arjuna was ready to hear and how he was ready to hear it.

50
00:02:57,260 --> 00:02:58,510
And that begs the question, of course

51
00:02:58,510 --> 00:03:00,130
that I'm sure you have, which is

52
00:03:00,130 --> 00:03:04,260
what movie does Alex hold as the most important business movie ever made?

53
00:03:04,260 --> 00:03:06,080
Well, that would be the kung fu classic

54
00:03:06,080 --> 00:03:08,500
The 36 Chambers of Shaolin.

55
00:03:08,500 --> 00:03:11,520
 If you know the plot, then maybe you can see why.

56
00:03:11,520 --> 00:03:15,300
But for everyone else, you're just gonna have to ask Alex.

57
00:03:15,300 --> 00:03:17,940
You know, talking to Alex has always been like talking to an oracle.

58
00:03:17,940 --> 00:03:20,320
Um, even when you understand what he's saying

59
00:03:20,320 --> 00:03:22,880
the revelations are inscrutable.

60
00:03:22,880 --> 00:03:27,000
 When I do understand him, the predictions beggar belief

61
00:03:27,000 --> 00:03:29,980
but then somehow, he's almost always right.

62
00:03:29,980 --> 00:03:33,190
Al, as his friends at Haverford knew him

63
00:03:33,190 --> 00:03:35,680
is a mensch.

64
00:03:35,680 --> 00:03:37,440
He wants to see you succeed.

65
00:03:37,440 --> 00:03:38,040
You.

66
00:03:38,040 --> 00:03:41,900
He has a wildly positive-sum view of the world

67
00:03:41,900 --> 00:03:44,200
rooted in a deep belief in the individual

68
00:03:44,200 --> 00:03:46,600
especially the individual as an artist.

69
00:03:46,600 --> 00:03:48,490
And he modeled Palantir as an artist colony

70
00:03:48,490 --> 00:03:49,680
not a factory.

71
00:03:49,680 --> 00:03:54,760
In place of conformity to process, he created a place that celebrated creative rebellion

72
00:03:54,760 --> 00:03:58,090
a place of risk-taking, a place that seeks to tease out

73
00:03:58,090 --> 00:04:01,620
develop, and back the artist.

74
00:04:01,620 --> 00:04:03,460
And he says that makes us feel like a cult.

75
00:04:03,460 --> 00:04:03,940
I agree.

76
00:04:03,940 --> 00:04:05,840
We're a cult, not a company.

77
00:04:05,840 --> 00:04:09,840
But he'd quip now and again he was the leader of a cult

78
00:04:09,840 --> 00:04:12,080
but had none of the benefits.

79
00:04:12,080 --> 00:04:15,900
 I'll leave you with one final story.

80
00:04:15,900 --> 00:04:19,240
I remember in 2008, our first Department of War deployment.

81
00:04:19,240 --> 00:04:21,399
I was leading a ta- team of 10 people.

82
00:04:21,399 --> 00:04:24,160
We were working for literally 14 hou- 14 days straight

83
00:04:24,160 --> 00:04:25,100
19-hour days.

84
00:04:25,100 --> 00:04:27,340
We consumed a heroic amount of Red Bull.

85
00:04:27,340 --> 00:04:29,700
At the end of this, I called him incredibly grumpy

86
00:04:29,700 --> 00:04:32,580
humorless from sleep deprivation, and I told him

87
00:04:32,580 --> 00:04:33,500
"This is not gonna work.

88
00:04:33,500 --> 00:04:34,750
It's unsustainable.

89
00:04:34,750 --> 00:04:37,425
You know, like, we're gonna fail here.

90
00:04:37,425 --> 00:04:43,640
" Without skipping he- a beat, he broke out in deep and self-satisfied laughter what was probably 30 seconds

91
00:04:43,640 --> 00:04:46,420
but in my humorless mood felt like 10 minutes.

92
00:04:46,420 --> 00:04:47,580
And I was shocked.

93
00:04:47,580 --> 00:04:51,540
And of course, he understood that this was the only way that it was going to work

94
00:04:51,540 --> 00:04:52,550
even if no one else did.

95
00:04:52,550 --> 00:04:57,120
Not our customers, not our investors, and least of all not me.

96
00:04:57,120 --> 00:05:00,040
The Hudson Institute is presenting Alex with the Herman Kahn Award

97
00:05:00,040 --> 00:05:03,300
which is fitting because both men share many similar qualities

98
00:05:03,300 --> 00:05:10,240
brilliant, unconventional, and people who make others nervous at dinner parties.

99
00:05:10,240 --> 00:05:16,560
 Herman thought about the unthinkable, and Alex built a company to make sense of it all.

100
00:05:16,560 --> 00:05:24,280
And beneath that philosopher's hair and the ski gear is someone who believes that serious technology should serve serious purposes.

101
00:05:24,280 --> 00:05:27,380
That the West needs to defend itself with both wisdom and strength

102
00:05:27,380 --> 00:05:33,080
and that you can't protect democracy without occasionally making everyone a little uncomfortable.

103
00:05:33,080 --> 00:05:42,860
So please join me in welting- welcoming a man who proves daily that you could run a half trillion dollar tech company while looking like you got lost on the way to Burning Man

104
00:05:42,860 --> 00:05:45,040
Alex Karp.

105
00:05:45,040 --> 00:06:09,936
 Thank you.

106
00:06:09,936 --> 00:06:10,816
Thank you so much.

107
00:06:10,816 --> 00:06:15,710
I, um, um, you know, I.

108
00:06:15,710 --> 00:06:15,825
..

109
00:06:15,825 --> 00:06:18,956
This award, award means a lot to me

110
00:06:18,956 --> 00:06:22,916
um, and I obviously, um, you know

111
00:06:22,916 --> 00:06:26,476
the alignment, the way in which the founder and participants

112
00:06:26,476 --> 00:06:32,916
uh, here have shaped the world on the back of technology is super impressive.

113
00:06:32,916 --> 00:06:35,836
I have a lot of remarks that I wanted to make

114
00:06:35,836 --> 00:06:38,956
but I would say, um, for all

115
00:06:38,956 --> 00:06:44,076
all of you who are, uh, younger and on your way to build important things

116
00:06:44,076 --> 00:06:49,076
there is no greater award than getting a speech like that from Cham

117
00:06:49,076 --> 00:06:52,066
um, especially since Cham can barely write.

118
00:06:52,066 --> 00:06:55,096
He must have spent a lot of time on that one.

119
00:06:55,096 --> 00:06:58,645
 So it's, uh, it's, uh

120
00:06:58,645 --> 00:06:59,431
it's.

121
00:06:59,431 --> 00:06:59,746
..

122
00:06:59,746 --> 00:07:03,206
Um, and we all know that, uh

123
00:07:03,206 --> 00:07:05,596
the ups and downs, uh, we went through.

124
00:07:05,596 --> 00:07:12,895
Cham's father, he mentioned, uh, was one of the great figures in building Palantir and told Cham

125
00:07:12,895 --> 00:07:14,557
"You may never leave Palantir.

126
00:07:14,557 --> 00:07:16,256
" That's probably why he didn't leave.

127
00:07:16,256 --> 00:07:19,736
 Also, with a great deal of love towards me

128
00:07:19,736 --> 00:07:21,336
which I really appreciate.

129
00:07:21,336 --> 00:07:25,286
And I'll tell you a story where maybe I've saved Cham and others many

130
00:07:25,286 --> 00:07:28,700
many times, but Cham saved me when I.

131
00:07:28,700 --> 00:07:28,885
..

132
00:07:28,885 --> 00:07:31,676
After, in COVID, I'm, I'm basically an introvert

133
00:07:31,676 --> 00:07:35,395
and every day, I enjoyed being alone more than the day before.

134
00:07:35,395 --> 00:07:37,276
 Every day.

135
00:07:37,276 --> 00:07:41,996
It's like the silence, it was just the best thing that I.

136
00:07:41,996 --> 00:07:42,166
..

137
00:07:42,166 --> 00:07:45,296
I mean, obviously, once we realized we were gonna survive

138
00:07:45,296 --> 00:07:46,269
uh-  .

139
00:07:46,269 --> 00:07:46,476
..

140
00:07:46,476 --> 00:07:47,518
I was like, this is.

141
00:07:47,518 --> 00:07:47,706
..

142
00:07:47,706 --> 00:07:49,895
I, I have this place in New Hampshire.

143
00:07:49,895 --> 00:07:51,576
I have no neighbors.

144
00:07:51,576 --> 00:07:54,366
I have bears and deer and, uh

145
00:07:54,366 --> 00:07:58,816
and actually, all the neighbors I do have vote the same way everyone in this room does.

146
00:07:58,816 --> 00:08:03,756
So, uh-  Um, and then Cham showed up one day

147
00:08:03,756 --> 00:08:05,496
and he's like, "This isn't cutting it.

148
00:08:05,496 --> 00:08:06,656
You gotta get back.

149
00:08:06,656 --> 00:08:07,436
You gotta come back.

150
00:08:07,436 --> 00:08:08,731
This isn't gonna work.

151
00:08:08,731 --> 00:08:10,936
" And honestly, if anyone else had showed up

152
00:08:10,936 --> 00:08:12,996
I'd been like, "That's so nice you said that.

153
00:08:12,996 --> 00:08:14,456
Really enjoying  myself.

154
00:08:14,456 --> 00:08:16,644
Can't spend a lot of money in New Hampshire where I.

155
00:08:16,644 --> 00:08:16,832
..

156
00:08:16,832 --> 00:08:20,056
" So like, so, um, and thank you

157
00:08:20,056 --> 00:08:22,466
uh, and thank you all the Palantirians who

158
00:08:22,466 --> 00:08:25,106
uh, burned the midnight oil and still do

159
00:08:25,106 --> 00:08:29,796
uh, for, you know, sticking up for the West and listening to my crazy ideas.

160
00:08:29,796 --> 00:08:31,350
And, um, I wanted.

161
00:08:31,350 --> 00:08:31,446
..

162
00:08:31,446 --> 00:08:33,135
I have a bunch of remarks that, to me

163
00:08:33,135 --> 00:08:36,476
honestly, are less important than, than that introduction

164
00:08:36,476 --> 00:08:38,414
which really means a lot to me.

165
00:08:38,414 --> 00:08:42,395
Um, but just slightly remarks on, like

166
00:08:42,395 --> 00:08:46,776
what have we learned about AI and business that's relevant?

167
00:08:46,776 --> 00:08:51,935
And, and what I, what I, what am I'm pretty certain is going to affect our culture?

168
00:08:51,935 --> 00:08:59,425
In, in philosophy, which I was once gonna be my metier and I realized should not be

169
00:08:59,425 --> 00:09:03,296
um, uh, um, and, uh, for

170
00:09:03,296 --> 00:09:07,336
for lots of reasons, but, um, in- including

171
00:09:07,336 --> 00:09:09,896
most importantly, I thought it was more important to actually do things.

172
00:09:09,896 --> 00:09:13,296
But it, it, one of the things people forget about philosophy is

173
00:09:13,296 --> 00:09:16,915
the core concepts are actually downstream from things that were built in the world.

174
00:09:16,915 --> 00:09:19,336
Modern architecture led to modern philosophy.

175
00:09:19,336 --> 00:09:23,016
Post-modern architecture led to post-modern philosophy.

176
00:09:23,016 --> 00:09:30,156
Um, and there's a weird way in which I think a lot of the trends that look inscrutable to us are actually downstream of

177
00:09:30,156 --> 00:09:34,076
of tech and culture and are particularly pronounced in this culture

178
00:09:34,076 --> 00:09:38,116
which is the only culture that really, really matters in AI.

179
00:09:38,116 --> 00:09:42,256
It's from chips and what we call ontology to large language models

180
00:09:42,256 --> 00:09:44,435
uh, to the talent, to the venture backing

181
00:09:44,435 --> 00:09:46,636
to the understanding of how to organize the talent

182
00:09:46,636 --> 00:09:49,271
they're in one country, and this.

183
00:09:49,271 --> 00:09:49,446
..

184
00:09:49,446 --> 00:09:50,415
our country.

185
00:09:50,415 --> 00:09:53,476
And this does not make sense because the

186
00:09:53,476 --> 00:09:54,448
the, the.

187
00:09:54,448 --> 00:09:54,565
..

188
00:09:54,565 --> 00:09:57,036
It would've, could've made sense in other trends

189
00:09:57,036 --> 00:10:01,336
but actually, the mathematical basis of large language models started in

190
00:10:01,336 --> 00:10:02,536
in, uh, in Germany.

191
00:10:02,536 --> 00:10:05,296
It was Leibniz's idea of how to do probability

192
00:10:05,296 --> 00:10:11,576
and then moved through France, and the best mathematicians in the world are mostly in France and in Russia.

193
00:10:11,576 --> 00:10:14,306
And so what was it and what is it about

194
00:10:14,306 --> 00:10:19,536
uh, what's happening that is affecting things that look very odd?

195
00:10:19,536 --> 00:10:22,796
Like, um, why do, um, uh

196
00:10:22,796 --> 00:10:28,476
you know, Ivy League-trained humanities, uh, uh

197
00:10:28,476 --> 00:10:36,996
highly privileged people vote against their interest seemingly to elect political figures with no experience

198
00:10:36,996 --> 00:10:38,720
uh, and who clearly.

199
00:10:38,720 --> 00:10:38,866
..

200
00:10:38,866 --> 00:10:42,935
Why are they misaligned with their own interests would be a question.

201
00:10:42,935 --> 00:10:45,915
Um, and then why is it in America?

202
00:10:45,915 --> 00:10:49,006
And I want to just start with the most obvious

203
00:10:49,006 --> 00:10:52,276
um, obvious point that is so obvious we

204
00:10:52,276 --> 00:11:00,016
we forget that the rights we have in this country are inalienable and they are given to us by God.

205
00:11:00,016 --> 00:11:01,986
Because if you compare this to, say

206
00:11:01,986 --> 00:11:04,986
the German constitution, which I, you know

207
00:11:04,986 --> 00:11:10,195
wrote down the same as where we talk about endowed by their creator with certain unalienable rights

208
00:11:10,195 --> 00:11:13,636
the German con- the German version, which is

209
00:11:13,636 --> 00:11:18,956
um, translate roughly as, "A cons- conscience of his responsibility before God

210
00:11:18,956 --> 00:11:24,137
" so not from God, "and mankind, inspired by the will of an equal member it- to be part of

211
00:11:24,137 --> 00:11:28,827
" this is rough translation, "United Europe and to contribute to world peace.

212
00:11:28,827 --> 00:11:35,126
" Okay, so we have these rights that are incorporated in our First and Second and

213
00:11:35,126 --> 00:11:39,996
and now in our Fourth Amendment that aren't given to us by humans

214
00:11:39,996 --> 00:11:42,296
they're given to us by a higher power.

215
00:11:42,296 --> 00:11:46,276
That's very, very important in understanding.

216
00:11:46,276 --> 00:11:49,076
When, when, when you look at businesses as an example

217
00:11:49,076 --> 00:11:53,116
some businesses that look, that are five years appeal to be 20 years old.

218
00:11:53,116 --> 00:11:57,956
Some businesses, uh, you know, we have Rupert Murdoch here who seems to be getting younger every year.

219
00:11:57,956 --> 00:12:00,876
 Uh, every year.

220
00:12:00,876 --> 00:12:03,596
Uh, I've known you for a long time.

221
00:12:03,596 --> 00:12:06,736
Uh, um, why is that?

222
00:12:06,736 --> 00:12:08,914
Why, wh- and, and-.

223
00:12:08,914 --> 00:12:09,242
..

224
00:12:09,242 --> 00:12:12,732
the, the, the, the, the principle and then w- you have this

225
00:12:12,732 --> 00:12:17,551
so you have this revolution, it's happening i- in a country which at least constitutionally

226
00:12:17,551 --> 00:12:21,151
obviously not historically, is much, much older than Europe 'cause Europe

227
00:12:21,151 --> 00:12:26,441
United Europe is only, it's under 30 years old and it's still not clear what it is really.

228
00:12:26,441 --> 00:12:34,151
Um, you have a country which should not have been the leader of what matters in the world but is the leader in the world.

229
00:12:34,151 --> 00:12:38,452
And, um, and then we have a very big moral problem in

230
00:12:38,452 --> 00:12:40,671
in, i- the basis of most humanity

231
00:12:40,671 --> 00:12:45,492
human rights historically, philosophically, legally, except for in America

232
00:12:45,492 --> 00:12:50,171
have been based on our sec- basically superior cognitive capacities.

233
00:12:50,171 --> 00:12:52,541
Um, and, you know, as Huntington

234
00:12:52,541 --> 00:12:57,415
uh, once famously said, "We believe in the West that our values are superior.

235
00:12:57,415 --> 00:13:01,012
" The non-West understands it's our org- it's our ability to organize violence

236
00:13:01,012 --> 00:13:04,651
which is an in- intellectual and technological endeavor.

237
00:13:04,651 --> 00:13:06,771
And that's the thing that has made us

238
00:13:06,771 --> 00:13:08,382
at the end of the day, uh

239
00:13:08,382 --> 00:13:12,061
be able to be superior over, uh

240
00:13:12,061 --> 00:13:14,612
non, non-human life forms.

241
00:13:14,612 --> 00:13:18,421
But with large language, which is, which-  .

242
00:13:18,421 --> 00:13:18,421
..

243
00:13:18,421 --> 00:13:23,492
but with large language models in AI, you actually have the first time where we

244
00:13:23,492 --> 00:13:28,431
how do we, how do we justify our obvious superiority not just as a culture

245
00:13:28,431 --> 00:13:33,732
but as a species vis-a-vis something that may be able to do cognitive things that we can't do?

246
00:13:33,732 --> 00:13:37,891
And we are the lone country that has the basis for this.

247
00:13:37,891 --> 00:13:41,112
Our rights are not given to us by our intellectual superiority.

248
00:13:41,112 --> 00:13:45,352
They're not given to us by consensus built in Germany or in the United Kingdom.

249
00:13:45,352 --> 00:13:47,832
They're not built to dedicate to world peace

250
00:13:47,832 --> 00:13:49,171
although that would be nice.

251
00:13:49,171 --> 00:13:54,872
They're given to us by our right that we were born here as Americans and those rights are indelible.

252
00:13:54,872 --> 00:13:57,051
This is a very big distinction.

253
00:13:57,051 --> 00:14:05,612
 And it's one that we neglect and do not talk about and for lots of reasons many in the Valley may not be interested in talking about.

254
00:14:05,612 --> 00:14:09,632
Um, and then there's, and then w- why are these so important?

255
00:14:09,632 --> 00:14:13,832
Well, first of all, so obviously we have our rights vis-a-vis the machine.

256
00:14:13,832 --> 00:14:16,512
Um, you know, and, and, and I think

257
00:14:16,512 --> 00:14:26,432
and then you also have what does it mean to be in a world where you have very high-end labor being exceedingly valuable

258
00:14:26,432 --> 00:14:34,621
formerly high-end labor, meaning you studied humanities and you would be working at a 501) dedicated to the United Nations-  .

259
00:14:34,621 --> 00:14:34,621
..

260
00:14:34,621 --> 00:14:36,632
not being objectively valuable.

261
00:14:36,632 --> 00:14:38,411
I don't mean morally.

262
00:14:38,411 --> 00:14:42,811
Most of us here would look askance at that as being misaligned with what they're saying.

263
00:14:42,811 --> 00:14:44,531
But even if you don't view it that way

264
00:14:44,531 --> 00:14:49,012
the reality is, a large language model can do all that work.

265
00:14:49,012 --> 00:14:51,121
And if it's an ontology, we can do it much

266
00:14:51,121 --> 00:14:52,112
much better.

267
00:14:52,112 --> 00:14:55,061
So it's objectively not valuable.

268
00:14:55,061 --> 00:15:01,512
What is valuable is working class, vocationally trained people with specific knowledge.

269
00:15:01,512 --> 00:15:09,141
And here, I would see a, I see a tethering between the specificity of America that we have a Constitution given to us by God

270
00:15:09,141 --> 00:15:14,051
that we believe in higher principles, that we defend them if we need to with our own guns

271
00:15:14,051 --> 00:15:17,021
with our own free speech and with our own rights of privacy

272
00:15:17,021 --> 00:15:20,911
and the specific things that create value in the world now.

273
00:15:20,911 --> 00:15:23,771
Meaning specific knowledge of how to build something

274
00:15:23,771 --> 00:15:26,071
how to target something, how to protect data

275
00:15:26,071 --> 00:15:28,301
how to be a nurse or a doctor.

276
00:15:28,301 --> 00:15:31,942
These skills can be very levered and are excruciatingly important.

277
00:15:31,942 --> 00:15:40,992
So you have a constitutional and moral tethering of exactly what you would need in one specific culture and that's this culture.

278
00:15:40,992 --> 00:15:44,651
But you're gonna have a lot of turbulence because we're now moving from

279
00:15:44,651 --> 00:15:50,732
you know, w- people who believe they deserved empathy and this is when I think the biggest tensions

280
00:15:50,732 --> 00:15:56,071
one of the very radical things about Palantir that our enemies do not believe

281
00:15:56,071 --> 00:16:00,192
we at Palantir have maximal empathy for Americans

282
00:16:00,192 --> 00:16:02,271
for the right of Americans to be free

283
00:16:02,271 --> 00:16:05,752
and for men, men and women that actually do work.

284
00:16:05,752 --> 00:16:10,051
 And our society, if you, if you go

285
00:16:10,051 --> 00:16:14,071
if you look, if you look at any professions what you'll see

286
00:16:14,071 --> 00:16:21,932
we read the law so somehow the law will not allow us to protect Americans who are being murdered by fentanyl.

287
00:16:21,932 --> 00:16:28,031
But more th- more Americans have died of fentanyl than have died in any war since World War II.

288
00:16:28,031 --> 00:16:31,911
If those were Stanford grads or Cornell grads

289
00:16:31,911 --> 00:16:35,862
you can bet your last dollar they are your d- uh

290
00:16:35,862 --> 00:16:40,331
uh, jurisdictions would read the law so that you could stop that.

291
00:16:40,331 --> 00:16:41,222
But they're not.

292
00:16:41,222 --> 00:16:44,831
They're working class, they're mostly male, they're mostly white

293
00:16:44,831 --> 00:16:46,942
and mostly people don't care.

294
00:16:46,942 --> 00:16:48,362
And we have to change that.

295
00:16:48,362 --> 00:16:51,811
When things go wrong because those people know that no one cares about them

296
00:16:51,811 --> 00:16:53,732
that's also our fault.

297
00:16:53,732 --> 00:16:57,911
Like not anyone in this room, but like we don't show empathy if you look at

298
00:16:57,911 --> 00:17:01,831
if you look at, for example, the way analysts more neutrally do models

299
00:17:01,831 --> 00:17:03,291
they love the people they like.

300
00:17:03,291 --> 00:17:05,290
It shows up downstream in the model.

301
00:17:05,290 --> 00:17:07,232
That's why Palantir was undervalued.

302
00:17:07,232 --> 00:17:13,212
We were undervalued and misunderstood because we were doing radical crazy stuff they didn't like.

303
00:17:13,212 --> 00:17:26,512
If you look all across the world and what you're seeing go on in America now is an enormous shift towards we have to actually look at where the value is created and have empathy with people who really deserve it.

304
00:17:26,512 --> 00:17:34,572
And those people have not got enough empathy and technology is about to change that and it's gonna change everything politically because it is actually real.

305
00:17:34,572 --> 00:17:36,612
The worker, the person on the front

306
00:17:36,612 --> 00:17:39,112
the people that write the scripts that protect us

307
00:17:39,112 --> 00:17:44,942
all those projects you saw are being done by people with vocational training in the military.

308
00:17:44,942 --> 00:17:52,292
They're creating enormous value and they deserve the respect that they are going to ask for and they're going to get.

309
00:17:52,292 --> 00:17:56,292
And Palantir and all of us who are Palantir are behind that.

310
00:17:56,292 --> 00:17:58,012
Uh, and th- and I, this just and

311
00:17:58,012 --> 00:18:07,692
and then conversely, how do you get a situation in the city where you just vote for someone who has no experience in doing what they're going to do?

312
00:18:07,692 --> 00:18:08,532
None.

313
00:18:08,532 --> 00:18:12,411
Even like I grew up, I view myself as classically progressive.

314
00:18:12,411 --> 00:18:13,263
I care.

315
00:18:13,263 --> 00:18:13,492
..

316
00:18:13,492 --> 00:18:15,411
You know what classically progressive is?

317
00:18:15,411 --> 00:18:17,159
The government has to work.

318
00:18:17,159 --> 00:18:18,207
You know what that means?

319
00:18:18,207 --> 00:18:19,418
You need the very best people.

320
00:18:19,418 --> 00:18:20,268
Who are the people?

321
00:18:20,268 --> 00:18:23,158
They have experience running complicated organizations.

322
00:18:23,158 --> 00:18:25,318
They have ideas on how to run software.

323
00:18:25,318 --> 00:18:27,388
They understand the complexity of the law.

324
00:18:27,388 --> 00:18:28,528
They enforce it.

325
00:18:28,528 --> 00:18:32,148
If you're really serious about making th- things work for the worker

326
00:18:32,148 --> 00:18:34,088
those are the people you vote for.

327
00:18:34,088 --> 00:18:36,947
Those are the people that need to be in charge.

328
00:18:36,947 --> 00:18:48,467
And that's, that's why a lot of these crazy things are actually very much explicable based on what is going on in AI and who's creating the value and how they create it.

329
00:18:48,467 --> 00:18:51,928
Th- th- then last, maybe not least

330
00:18:51,928 --> 00:19:04,988
on the AI front, one of the reasons we are in a kind of a doomer AI circle is because you can only explain the promise of AI if you understand and embrace the superiority of America and its culture.

331
00:19:04,988 --> 00:19:08,788
Because there are dangers in AI, but the reality is

332
00:19:08,788 --> 00:19:12,187
there's only two cultures that are going to win in the next year.

333
00:19:12,187 --> 00:19:14,868
It's gonna be us or China.

334
00:19:14,868 --> 00:19:18,408
Th- th- Europe, I spent half my life in Europe.

335
00:19:18,408 --> 00:19:20,928
At this point, we, when we talk about Europe

336
00:19:20,928 --> 00:19:25,018
we do exactly the same thing as when we hear about educational programs th- in

337
00:19:25,018 --> 00:19:29,288
led by mayors or safety programs in the inner city.

338
00:19:29,288 --> 00:19:32,283
We just say that, "I really hope it works out.

339
00:19:32,283 --> 00:19:34,768
"  Please.

340
00:19:34,768 --> 00:19:38,738
And I want Europe to win, but we're on our own.

341
00:19:38,738 --> 00:19:43,248
Uh, and, and that, of course there are dangers in AI.

342
00:19:43,248 --> 00:19:50,128
And, uh, and AI is also and will never be anything other than a dual use technology

343
00:19:50,128 --> 00:19:54,048
but we must, must embrace our ability to build it

344
00:19:54,048 --> 00:19:56,268
our ability to own the chips, to own the software

345
00:19:56,268 --> 00:19:58,978
build the large language models, and run very

346
00:19:58,978 --> 00:20:02,707
very quickly, because again, to quote Huntington again

347
00:20:02,707 --> 00:20:06,768
"If we are not the ones controlling the violence

348
00:20:06,768 --> 00:20:09,176
we will not be dictating the rule of law.

349
00:20:09,176 --> 00:20:11,768
" The things we hold precious in this culture

350
00:20:11,768 --> 00:20:17,187
I would say embodied by our Constitution and especially in our first four amendments

351
00:20:17,187 --> 00:20:22,148
those things will not be the same if we are not the dominant technological culture in the world.

352
00:20:22,148 --> 00:20:25,868
And while we may have tons of disagreements on these things

353
00:20:25,868 --> 00:20:28,508
we must, the, the, uh, th- those are really the alternatives

354
00:20:28,508 --> 00:20:31,388
and most of what we can do, in my view

355
00:20:31,388 --> 00:20:37,148
to counter adversaries is also to build the technology and then

356
00:20:37,148 --> 00:20:40,888
you know, one of the lessons of Palantir is no one's coming to defend you.

357
00:20:40,888 --> 00:20:43,028
You have to defend yourself.

358
00:20:43,028 --> 00:20:48,328
We used to live in a world where you could rely on experts to articulate our vision.

359
00:20:48,328 --> 00:20:52,837
But who showed up to articulate our vision in this last election in my hometown?

360
00:20:52,837 --> 00:20:54,467
Formerly hometown.

361
00:20:54,467 --> 00:20:57,928
Like, we, if we're waiting for people to articulate and fight for us

362
00:20:57,928 --> 00:20:59,528
we are cooked.

363
00:20:59,528 --> 00:21:01,848
 And like, and, and  it's

364
00:21:01,848 --> 00:21:06,068
a- and so, and, you know, the people who disagree with us are

365
00:21:06,068 --> 00:21:10,048
like, often better organized and better motiv- and more motivated.

366
00:21:10,048 --> 00:21:12,328
And that's a huge problem we have to solve.

367
00:21:12,328 --> 00:21:17,108
Palantirians, half of the reason we succeeded is we were just motivated

368
00:21:17,108 --> 00:21:20,687
and very organized, and very meritocratic.

369
00:21:20,687 --> 00:21:24,348
Uh, meritocracy is the most underestimated, powerful

370
00:21:24,348 --> 00:21:28,987
revolutionary tool that exists in any enterprise ever.

371
00:21:28,987 --> 00:21:31,928
And that means, you know, Sham is a great example.

372
00:21:31,928 --> 00:21:34,908
Sham was not everyone's cup of tea.

373
00:21:34,908 --> 00:21:39,447
 And s- he advanced because he, for really two reasons.

374
00:21:39,447 --> 00:21:44,148
He advanced because I thought he was gonna be an incredible human being

375
00:21:44,148 --> 00:21:46,828
and second, and I'll leave you with this

376
00:21:46,828 --> 00:21:50,228
because I saw him interacting with his brother and I was like

377
00:21:50,228 --> 00:21:53,508
"If he could love Palantir half as much  as he loves his brother

378
00:21:53,508 --> 00:21:57,354
we are going to have the most important enterprise in the world.

379
00:21:57,354 --> 00:22:07,328
" And like the capacity to identify and advance people on a meritocratic base that have capacity to care is the single most revolutionary tool.

380
00:22:07,328 --> 00:22:10,947
And my kind of thinker has been infected

381
00:22:10,947 --> 00:22:14,947
meaning the progressive party of the Democrats, by people who do not believe in this.

382
00:22:14,947 --> 00:22:19,208
And by the way, to my Democratic friends here and whoever sees this

383
00:22:19,208 --> 00:22:21,508
that we must stand up.

384
00:22:21,508 --> 00:22:24,388
One of the most important lessons in the West is

385
00:22:24,388 --> 00:22:28,148
if you do not fight and in a- as importantly

386
00:22:28,148 --> 00:22:31,928
if you accept the logic that you can never vote for somebody

387
00:22:31,928 --> 00:22:35,848
uh, because of they have some sub-belief structure you disagree with

388
00:22:35,848 --> 00:22:40,317
you are empowering the radicals in your own party and they will control everything.

389
00:22:40,317 --> 00:22:47,928
Whether it's immigration in Germany, which functionally controlled by people that still believe in chain migration because you can never do a coalition with the right

390
00:22:47,928 --> 00:22:54,888
or whether it's the Democratic Party that is functionally being torn apart by people who believe in anti-progressive

391
00:22:54,888 --> 00:22:58,878
anti-meritocratic values as embodied by not having a border.

392
00:22:58,878 --> 00:23:02,248
No progressive ever has believed in an open border.

393
00:23:02,248 --> 00:23:08,268
None, never, has never existed because that com- undermines the value of workers and their labor.

394
00:23:08,268 --> 00:23:11,908
And if you wanna see what happens to a culture when you don't fight

395
00:23:11,908 --> 00:23:15,068
look at, there's, who's standing up in my former party to say

396
00:23:15,068 --> 00:23:16,148
"This is ridiculous"?

397
00:23:16,148 --> 00:23:19,187
Who stood up in Germany to talk about migration?

398
00:23:19,187 --> 00:23:22,028
We cannot allow ourselves to become Europe.

399
00:23:22,028 --> 00:23:23,528
We really have to fight.

400
00:23:23,528 --> 00:23:25,248
And I'm, I'm sure many of the

401
00:23:25,248 --> 00:23:27,308
in here are gonna be involved in that.

402
00:23:27,308 --> 00:23:36,608
And Palantir, I think, has been at the forefront of this and I'm most proud of the fact that we've been able to shift the culture to being

403
00:23:36,608 --> 00:23:40,848
in Silicon Valley, away from being skeptical of America to being pro-America.

404
00:23:40,848 --> 00:23:49,748
The idea that you would give tools to give Americans an unfair advantage in th- on the battlefield was viewed as heretical until recently.

405
00:23:49,748 --> 00:23:51,708
Now it's viewed as common sense.

406
00:23:51,708 --> 00:24:01,848
The idea that we have, uh, we enforced meritocracy when we were getting sued by the DOJ for j- basically just hiring the most qualified people.

407
00:24:01,848 --> 00:24:04,768
Um, and, and, and if you do that and you succeed

408
00:24:04,768 --> 00:24:08,248
you can actually change the way people think.

409
00:24:08,248 --> 00:24:09,911
And .

410
00:24:09,911 --> 00:24:09,918
..

411
00:24:09,918 --> 00:24:15,928
 Thank you.

