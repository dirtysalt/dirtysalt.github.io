1
00:00:00,000 --> 00:00:02,720
Speaker 1: On the recruitment wars, I mean, there's

2
00:00:02,720 --> 00:00:10,160
got a lot of attention clearly, and it looked like  Meta was quite aggressive. What exactly does this

3
00:00:10,160 --> 00:00:13,120
tit for tat look like? What stage are we Speaker 2:

4
00:00:13,120 --> 00:00:23,040
At? Yeah, I mean, there is a pool of talent and  everyone kind of knows who they are, and I think

5
00:00:23,040 --> 00:00:29,200
many companies have realized that one of the key  ingredients, not the only important ingredient,

6
00:00:29,200 --> 00:00:33,040
but one of the key ingredients [00:00:30] to  building a great AI lab is to get the best talent.

7
00:00:33,040 --> 00:00:41,840
And I think not a surprise that Meta has been  aggressively employing the strategy. We haven't

8
00:00:41,840 --> 00:00:47,760
sat back idly, and I actually want to tell the  story from Open AI's point of view. I think that

9
00:00:47,760 --> 00:00:53,520
a lot has been made in the media of, oh, there's  this unidirectional flow of talent over to Meta,

10
00:00:53,520 --> 00:01:00,160
but the way that I've seen it is no meta. They've  gone after a lot of people quite unsuccessfully.

11
00:01:00,160 --> 00:01:05,680
So [00:01:00] just to give you context, within my  staff, within my direct reports, before they hired

12
00:01:05,680 --> 00:01:12,960
anyone from opening, I think they went after half  of my direct reports and they all declined. And of

13
00:01:12,960 --> 00:01:17,920
course, if they have something like 10 billion  of capital per year to deploy towards talent,

14
00:01:18,720 --> 00:01:25,120
they're going to get someone. So I actually feel  like we've been fairly good about protecting our

15
00:01:25,120 --> 00:01:30,960
top talent and it's been interesting and fun  to see it escalate [00:01:30] over time.

16
00:01:33,440 --> 00:01:38,960
Some interesting stories here are Zuck  actually went and hand delivered soup

17
00:01:38,960 --> 00:01:46,160
to people that he was trying to recruit  from us just to show how far he would,

18
00:01:46,160 --> 00:01:50,880
yeah, I think he hand cooked the soup  and it was shocking to me at the time,

19
00:01:50,880 --> 00:01:56,880
but over time I've kind of updated towards these  things can be effective in their own way. And

20
00:01:57,440 --> 00:02:01,829
I've also delivered soup to people that we've been  [00:02:00] recruiting from Meta. You're doing a

21
00:02:01,829 --> 00:02:03,200
Speaker 1: Soup, soup counting.

22
00:02:03,200 --> 00:02:03,760
Speaker 2: I've thought

23
00:02:03,760 --> 00:02:07,840
of if I had an offsite, the next offsite  for my staff, I'm going to take them to a

24
00:02:07,840 --> 00:02:08,479
cooking class. Speaker 1:

25
00:02:08,479 --> 00:02:09,440
Okay. Speaker 2:

26
00:02:09,440 --> 00:02:15,920
Yeah. I mean it's just been, but I do think  there's something I've learned about recruiting.

27
00:02:15,920 --> 00:02:21,560
Did you cook your soup? It's better if you get Speaker 1:

28
00:02:21,560 --> 00:02:23,920
Michelin star soup. You know what I mean? Speaker 2:

29
00:02:23,920 --> 00:02:29,120
Yeah. No, no, no. I think Dejo is very, very good  and probably better than any soup I could cook.

30
00:02:29,840 --> 00:02:35,920
But [00:02:30] yeah, I do think there is something  I've learned about just how to go aggressively

31
00:02:35,920 --> 00:02:44,960
after top talent. And I think the thing I've  been actually very inspired by is that at OpenAI,

32
00:02:45,680 --> 00:02:52,480
even among people who have offered meta, I haven't  heard anyone say a GI is going to be developed at

33
00:02:52,480 --> 00:03:00,240
Meta first. Everyone is very confident in the  research program at OpenAI. And [00:03:00] one

34
00:03:00,240 --> 00:03:06,880
thing that I've made very clear to my staff to the  whole research org is we don't counter dollar for

35
00:03:06,880 --> 00:03:14,240
dollar with meta and the multiples that below what  meta is offering that people are very happy to

36
00:03:14,240 --> 00:03:20,000
stay at OpenAI gives me so much conviction that  people really believe in the upside and believe

37
00:03:20,000 --> 00:03:21,840
that we're going to do it. Speaker 1:

38
00:03:21,840 --> 00:03:25,120
And Alex Wang, he used to  be one of the math, the,

39
00:03:26,080 --> 00:03:26,949
Speaker 2: I'm

40
00:03:26,949 --> 00:03:27,909
Speaker 1: Sure you guys hung

41
00:03:27,909 --> 00:03:29,760
Speaker 2: Out. Well, yeah, I mean I have hung

42
00:03:29,760 --> 00:03:35,749
out with [00:03:30] Alex a handful of times, but  we don't do much anymore. Yeah, I mean, yeah,

43
00:03:35,749 --> 00:03:38,000
Speaker 1: Why did soup become the thing? It just,

44
00:03:38,000 --> 00:03:38,960
Speaker 2: I don't know.

45
00:03:39,520 --> 00:03:43,440
There's been soup, there's been flowers, there's  been anything you can think of under the sun,

46
00:03:43,440 --> 00:03:47,600
but I dunno, I think life's an  adventure. I play into the meme.

47
00:03:47,600 --> 00:03:51,760
Speaker 1: Is there any poker strategy

48
00:03:51,760 --> 00:03:54,960
to employ is your thinking? Speaker 2:

49
00:03:54,960 --> 00:03:58,720
Well again, I think it really goes  back to what I've said about the

50
00:03:58,720 --> 00:04:03,600
media narrative. [00:04:00] The game is not  to retain every single person in the org,

51
00:04:03,600 --> 00:04:09,760
it's to trust in this pipeline that we have for  developing talent and to understand who the key

52
00:04:09,760 --> 00:04:15,520
people we need to keep are and to keep those. And  I think we've done a phenomenal job at that.

53
00:04:15,520 --> 00:04:30,240
Speaker 1: [00:04:30]

54
00:04:30,240 --> 00:04:37,840
We have a special treat today. I'm excited. Mark  Chen is here from OpenAI. He's the chief research

55
00:04:37,840 --> 00:04:42,789
officer. He is somebody I've gotten to know over  the last couple of years. Thank you so much for

56
00:04:42,789 --> 00:04:43,360
Speaker 2: Getting that. Yeah, no,

57
00:04:43,360 --> 00:04:45,680
it's been great to know you for so long. Speaker 1:

58
00:04:45,680 --> 00:04:56,080
I feel like there's a handful of people in this  world working on this very important project and

59
00:04:56,080 --> 00:05:01,349
I mean, you're right at the top of it, so it's so  cool to have a chance [00:05:00] to chat. Yeah,

60
00:05:01,349 --> 00:05:02,469
Speaker 2: Thanks for having me on.

61
00:05:02,469 --> 00:05:04,880
Speaker 1: It is my pleasure. And I mean there's

62
00:05:04,880 --> 00:05:10,160
a bunch of things that I want to talk to you about  because I've gotten to know you, like we said over

63
00:05:10,160 --> 00:05:16,400
those last couple of years. I want people to know  a bit more about your biography, but I also know

64
00:05:16,400 --> 00:05:22,600
there's going to be AI enthusiasts who want us  to go deep on a couple of things there. So we

65
00:05:22,600 --> 00:05:30,400
will try to do everything. I wanted to start just  by giving people a feel for your job, which in my

66
00:05:30,400 --> 00:05:40,560
[00:05:30] head, I mean just correct me if I get  any of this wrong, but Sam has been, he's really

67
00:05:40,560 --> 00:05:46,880
into research. He's the boss, he's at the top of  the food chain, but then you and Jakob are working

68
00:05:46,880 --> 00:05:55,840
together to shape open AI's research direction.  And then you're in this additional part of this

69
00:05:55,840 --> 00:06:02,240
role is actually deciding which compute goes where  [00:06:00] onto these projects. So you kind of

70
00:06:02,240 --> 00:06:07,200
have to chart where OpenAI is heading and then the  mechanics of how you're going to get there. And

71
00:06:07,200 --> 00:06:14,560
this always strikes me as a horrible job because I  picture people doing everything in their power to

72
00:06:14,560 --> 00:06:17,040
get GPUs from Houston. It's true. People are Speaker 2:

73
00:06:17,040 --> 00:06:22,560
Very creative in the ways that they try to make  backroom deals to get the GPUs they need. But

74
00:06:22,560 --> 00:06:28,000
yeah, I mean it is a big part of the job figuring  out the priorities for the research org and also

75
00:06:28,000 --> 00:06:33,200
being accountable for execution. So [00:06:30]  really to that first point, there's this exercise

76
00:06:33,200 --> 00:06:40,800
that Jakob and I do every one to two months where  we take stock of all the projects at OpenAI and

77
00:06:40,800 --> 00:06:46,160
it's this big spreadsheet, about 300 projects,  and we go and try to deeply understand each one

78
00:06:46,160 --> 00:06:54,160
as best as we can and really rank them. And  I think for a company of 500 people, it's

79
00:06:54,160 --> 00:06:58,480
important for people to understand what the core  priorities are and for those to be communicated

80
00:06:58,480 --> 00:07:03,280
clearly both explicitly, verbally [00:07:00] and  also through the way that we allocate compute.

81
00:07:03,280 --> 00:07:04,960
Speaker 1: Alright, what do we do at Core Memory?

82
00:07:04,960 --> 00:07:12,400
We cover innovative, fast moving forward thinking  companies, which is why Core Memory is sponsored

83
00:07:12,400 --> 00:07:20,000
by Brex because Brex is the intelligent finance  platform for many of these companies. 30,000

84
00:07:20,000 --> 00:07:26,400
companies from startups to the world's largest  corporations rely on Brex is technology for

85
00:07:26,400 --> 00:07:32,800
their finances. They've got smart corporate cards,  high [00:07:30] yield business banking and expense

86
00:07:32,800 --> 00:07:40,560
automation tools that are fantastic. I hate doing  my expenses. And Brex is ais software run right

87
00:07:40,560 --> 00:07:45,120
through those expenses, figure out where we're  spending money and take care of so much stuff

88
00:07:45,120 --> 00:07:53,040
for you so you don't have to waste your time on it  yourself. Go to brex.com/core memory to learn more

89
00:07:53,040 --> 00:07:59,120
and just get with the program. Let's get going.  Let's get out of this archaic finance software

90
00:07:59,120 --> 00:08:05,040
and move toward [00:08:00] the future core memory  and Brex. So you've got, when you're talking about

91
00:08:05,040 --> 00:08:10,400
the 500, these are the 500. This is the heart of  the research team in an organization now that's

92
00:08:10,400 --> 00:08:16,960
thousands of people. Yeah. Okay. And then in  that, when you're talking about this 300 projects

93
00:08:16,960 --> 00:08:23,280
I imagine mean obviously some of those are the  giant frontier models and then some are probably

94
00:08:23,280 --> 00:08:30,320
experiments the people are working on. And so how  do you possibly keep track of all [00:08:30] that

95
00:08:30,320 --> 00:08:36,149
and then come to some sort of conclusion  about what merits GPUs and what doesn't?

96
00:08:36,149 --> 00:08:39,520
Speaker 2: Absolutely. So I think it is very important when

97
00:08:39,520 --> 00:08:45,360
doing this exercise to keep the core roadmap in  focus. And one thing that differentiates, I think

98
00:08:45,360 --> 00:08:53,040
OpenAI with other big labs out there is OpenAI has  always had core exploratory research. At its core.

99
00:08:53,680 --> 00:08:59,440
We are not in the business of replicating the  results of other labs, of catching up to other

100
00:08:59,440 --> 00:09:03,840
labs in terms of [00:09:00] benchmarks that isn't  really our bread and butter. We're always trying

101
00:09:03,840 --> 00:09:09,360
to figure out what that next paradigm is and we're  willing to invest the resources to make sure that

102
00:09:09,360 --> 00:09:15,520
we find that right. And I think most people  might be surprised at this, but more compute

103
00:09:15,520 --> 00:09:21,360
goes into that endeavor of doing exploration  than it is to training the actual artifact.

104
00:09:21,360 --> 00:09:23,360
Speaker 1: It's still got to be

105
00:09:23,360 --> 00:09:27,600
how do you stop yourself from being persuaded  by someone because everybody's going to put,

106
00:09:28,400 --> 00:09:32,240
when I think about this, sometimes I picture  [00:09:30] when I was at the New York Times, you

107
00:09:32,240 --> 00:09:38,400
would have this page one meeting where everybody  wants to be on page one. Everybody thinks their

108
00:09:38,400 --> 00:09:43,200
story is the most important story. They're all  doing their very best job to tell you why this

109
00:09:43,200 --> 00:09:50,080
thing is so important. Everybody in that room has  worked weeks, months on whatever they're pitching.

110
00:09:50,080 --> 00:09:56,309
And so it feels like life and death and that.  Yeah, I mean it seems so difficult for me.

111
00:09:56,309 --> 00:09:58,400
Speaker 2: Yeah, no, it is also a difficult

112
00:09:58,400 --> 00:10:03,680
process and I think [00:10:00] the hardest cause  you have to make are this is a project that we

113
00:10:03,680 --> 00:10:09,840
just can't fund right now. But I also think that's  good leadership. You need to clearly communicate

114
00:10:09,840 --> 00:10:14,160
that, hey, these are the priorities, this is what  we're going to talk about. These are the types of

115
00:10:14,160 --> 00:10:20,000
results that we think move the research program.  And there can be other things, but those have to

116
00:10:20,000 --> 00:10:21,680
be clearly number two. Speaker 1:

117
00:10:21,680 --> 00:10:27,200
When you were talking about not being reactive  to your competitors. When I was looking through

118
00:10:27,200 --> 00:10:32,240
my notes, I don't know if I could go to the line  [00:10:30] quick enough, but I mean this was a

119
00:10:32,240 --> 00:10:40,880
point of pride that I saw that you feel like some  of the other companies are, well, you guys were

120
00:10:40,880 --> 00:10:48,800
in this position where you were ahead and setting  the bar for others, so they were reactive right to

121
00:10:48,800 --> 00:10:54,320
what you had coming out. We happened to be doing  this interview a few days after Gemini three came

122
00:10:54,320 --> 00:11:01,440
out, and there is a degree to which your rivals  at times. [00:11:00] I mean, there's this back

123
00:11:01,440 --> 00:11:06,480
and forth going on and I know the benchmarks are  sort of controversial, how valuable they are,

124
00:11:06,480 --> 00:11:15,600
but people go ahead on these things. So how do  you also as time has gone on maintain that luxury

125
00:11:15,600 --> 00:11:19,581
or that intellectual position where you feel like  we're just going to do what we're going to do?

126
00:11:19,581 --> 00:11:23,200
Speaker 2: Yeah, I think AI research today,

127
00:11:23,200 --> 00:11:29,920
the landscape is much more competitive than it's  ever been. And the important thing is to not

128
00:11:29,920 --> 00:11:34,000
get [00:11:30] caught up in that competitive  dynamic because you can always say, Hey,

129
00:11:34,000 --> 00:11:39,840
I'm going to ship an incremental update that puts  me in front of my competitor for a couple weeks

130
00:11:39,840 --> 00:11:45,440
or a couple months. And I don't think that's the  long-term sustainable way to do research because

131
00:11:45,440 --> 00:11:50,080
if you crack that next paradigm, that's just going  to matter so much more, you're going to shape the

132
00:11:50,080 --> 00:11:55,600
evolution of it. You're going to understand all  the side research directions around that sphere of

133
00:11:55,600 --> 00:12:01,680
ideas. And so when you think about our RO program  as an example [00:12:00] of this, right? We bet

134
00:12:01,680 --> 00:12:06,720
more than two years ago that we're really going  to crack RO on language models. And this was a

135
00:12:06,720 --> 00:12:12,320
very unpopular bet at the time. Right now it seems  obvious, but back then the environment was, Hey,

136
00:12:12,320 --> 00:12:16,000
there's this pre-training machine that's working  great, there's this post-training machine that's

137
00:12:16,000 --> 00:12:21,920
working great. Why invest in something else? And  I think today everyone would tell you thinking

138
00:12:21,920 --> 00:12:29,840
and language models, it's just a primitive you  can't have, can't live without. And so we're

139
00:12:29,840 --> 00:12:33,760
really [00:12:30] there to make these bold  bets and to figure out how we can scale and

140
00:12:33,760 --> 00:12:38,827
build the algorithms to really scale to orders  of magnitude more compute than we have today.

141
00:12:38,827 --> 00:12:42,640
Speaker 1: And I get that intellectually in my,

142
00:12:44,560 --> 00:12:50,720
it gets harder as you guys started, as basically  a pure research company. When you look at OpenAI

143
00:12:50,720 --> 00:12:55,360
today, I mean you have product line, there's  parts of OpenAI that look much more familiar

144
00:12:55,360 --> 00:13:00,080
to a mature Microsoft or a Google where you  have product lines, you've got all these

145
00:13:00,080 --> 00:13:04,160
different [00:13:00] things that you have to serve  typically. I feel like you guys are still young

146
00:13:04,160 --> 00:13:09,600
enough, so maybe you don't have these exact  pressures yet, but as those companies go on,

147
00:13:09,600 --> 00:13:13,920
it always becomes, well, we're more focused  on the things that are serving the bottom

148
00:13:13,920 --> 00:13:20,960
line than spending a ton of money on research  always seems to get dwindled down over time.

149
00:13:20,960 --> 00:13:22,080
Speaker 2: And I think that's really

150
00:13:22,080 --> 00:13:27,760
one of the most special things about OpenAI  at its core. We're pure AI research company,

151
00:13:27,760 --> 00:13:32,000
and I don't think you can say that of  [00:13:30] many other companies out there.

152
00:13:32,000 --> 00:13:40,160
And we were founded as a nonprofit and I joined  during that era. And I think the spirit is build

153
00:13:40,160 --> 00:13:48,160
a GI advance a GI research at all costs and do it  in a safe way, of course. But yeah, I actually do

154
00:13:48,160 --> 00:13:54,240
think that's the best head fake to really creating  value. If you focus and you win at the research,

155
00:13:54,240 --> 00:14:00,080
the value is easy to create. So I think there's a  trap of getting too lost into like, oh, [00:14:00]

156
00:14:00,720 --> 00:14:06,240
let's strive up the bottom line. When in reality  if you do the best research, that part of the

157
00:14:06,240 --> 00:14:07,280
picture is very easy. Speaker 1:

158
00:14:07,280 --> 00:14:12,000
And you started in 2018. In 2018, and  so you feel like that soul, that

159
00:14:14,080 --> 00:14:16,240
Speaker 2: That core culture and that core nucleus,

160
00:14:16,240 --> 00:14:18,000
it's really persistent. It's still there. Speaker 1:

161
00:14:18,000 --> 00:14:24,240
What does Elon says? What is he? He says, we  shouldn't call any of you guys researchers. It's

162
00:14:24,240 --> 00:14:25,520
just engineering, right? Speaker 2:

163
00:14:25,520 --> 00:14:31,760
Yeah, no, I think, yeah, no, it's true because I  feel like once you have this hierarchy [00:14:30]

164
00:14:31,760 --> 00:14:38,240
and you elevate, let's say research science as  a thing beyond engineering, you've completely

165
00:14:38,240 --> 00:14:45,920
already lost the game. Because when you're  building a big model, so much is in the practice

166
00:14:45,920 --> 00:14:53,600
of optimizing all of those little percentages of  how you make your kernels that much faster. How

167
00:14:53,600 --> 00:14:59,680
do you make sure the numerics all work? And that's  a deep engineering practice. And if you don't have

168
00:14:59,680 --> 00:15:03,840
that part of [00:15:00] the picture, you can't  scale to the number of GPUs we use today

169
00:15:03,840 --> 00:15:06,560
Speaker 1: Because I think there, well, okay, but there

170
00:15:06,560 --> 00:15:13,600
is a mystique that surrounds a researcher versus  an engineer. You know what I mean? So do you feel

171
00:15:13,600 --> 00:15:22,720
like it is better to stay levelheaded on that?  Is that kind of what you're saying, or? Well,

172
00:15:22,720 --> 00:15:24,160
Speaker 2: I just feel like researchers,

173
00:15:24,160 --> 00:15:28,960
they come in so many different shapes.  Some of our best researchers, they're

174
00:15:29,520 --> 00:15:35,600
the type that [00:15:30] they come up with a  billion ideas and many of them are not good,

175
00:15:35,600 --> 00:15:41,360
but just when you're about to be like, ah, is this  person really worth it? They come up with some

176
00:15:41,360 --> 00:15:48,320
phenomenal idea. Some of them are just so good at  executing on the clear path ahead. And so there's

177
00:15:48,320 --> 00:15:53,120
just so many different shapes of researchers  and I think it's hard to just lump it into one

178
00:15:53,120 --> 00:15:55,275
stereotypical type that works Speaker 1:

179
00:15:55,275 --> 00:16:02,400
Box. That makes sense. Okay. I won't belabor  you with too many competitive [00:16:00] rival

180
00:16:02,400 --> 00:16:08,560
questions. It's just since Gemini three did come  out, I did wonder what happens with you personally

181
00:16:08,560 --> 00:16:16,320
or the team when one of your rivals puts it, does  everybody go and look and see what it can do? Is

182
00:16:16,320 --> 00:16:25,840
there a prompt or a question that you often throw  at these new models to see what they can do?

183
00:16:25,840 --> 00:16:28,080
Speaker 2: Yeah, so to speak to Gemini

184
00:16:28,080 --> 00:16:34,960
three specifically, it's a pretty good [00:16:30]  model. And I think one thing we do is try to

185
00:16:34,960 --> 00:16:42,240
book consensus. The benchmarks only tell you so  much, and just looking purely at the benchmarks,

186
00:16:42,240 --> 00:16:49,200
we actually felt quite confident. We have models  internally that perform at the level of Gemini

187
00:16:49,200 --> 00:16:53,920
three, and we're pretty confident that we will  release them soon and we can release successor

188
00:16:53,920 --> 00:16:59,360
models that are even better. But yeah, again,  kind of the benchmarks only tell you so much,

189
00:16:59,360 --> 00:17:06,079
and [00:17:00] I think everyone probes the models  in their own way. There is this math problem,

190
00:17:06,079 --> 00:17:12,800
I like to give the models. I think so far, none  of them has quite cracked it, even the thinking

191
00:17:12,800 --> 00:17:16,240
models. So yeah, I'll wait for that. Speaker 1:

192
00:17:16,240 --> 00:17:18,079
Is this like a secret math problem? Speaker 2:

193
00:17:18,079 --> 00:17:22,480
Oh, no, no, no. Well, if I now announce it  here, maybe it gets trained on, but yeah,

194
00:17:22,480 --> 00:17:28,160
I do think it's one of the nice puzzles of last  year. It's this, the 42 problem. So you want to

195
00:17:28,160 --> 00:17:32,800
create this random number generator mod [00:17:30]  42, and you have access to a bunch of primitives,

196
00:17:32,800 --> 00:17:37,120
which are random number generators, modular  primes less than 42, and you want to make as

197
00:17:37,120 --> 00:17:43,840
few calls on expectation to these sub generators  as possible. So it's a very cute puzzle, but

198
00:17:44,720 --> 00:17:49,280
the language models, they get pretty close to  the optimal solution. But I haven't seen one

199
00:17:49,280 --> 00:17:50,720
quite crack it. Speaker 1:

200
00:17:50,720 --> 00:17:53,509
Okay. We're heading down a  direction I want to ask you about

201
00:17:53,509 --> 00:17:53,680
Speaker 2: Absolutely.

202
00:17:53,680 --> 00:17:56,480
Speaker 1: Just before we get there. So I know I've seen you,

203
00:17:56,480 --> 00:17:58,942
you're very competitive. You've also told me Speaker 2:

204
00:17:58,942 --> 00:17:58,960
Yes. Speaker 1:

205
00:17:58,960 --> 00:18:00,000
I think I found I Speaker 2:

206
00:18:00,000 --> 00:18:00,560
Love [00:18:00] competition. Speaker 1:

207
00:18:00,560 --> 00:18:01,760
I hate to fucking lose Speaker 2:

208
00:18:01,760 --> 00:18:03,600
Somewhere. I really hate losing. I Speaker 1:

209
00:18:03,600 --> 00:18:09,360
Hate losing. Yeah. So I'm picturing, I'm  just curious if this is at all right. I mean,

210
00:18:09,360 --> 00:18:16,000
if we know Gemini three or whatever  is coming out on a Thursday, I mean,

211
00:18:16,000 --> 00:18:21,760
are you up at midnight throwing that problem  at it? Or is it not quite that drastic?

212
00:18:21,760 --> 00:18:24,560
Speaker 2: No, I mean, I think it's in long arcs

213
00:18:25,440 --> 00:18:31,840
and any endeavor, I am kind of a person who has  obsessions. [00:18:30] I think any endeavor you

214
00:18:31,840 --> 00:18:38,880
have to play the long game. And we've actually  been focusing on pre-training, specifically

215
00:18:38,880 --> 00:18:44,160
supercharging our pre-training efforts for the  last half year. And I think it's a result of some

216
00:18:44,160 --> 00:18:50,480
of those efforts together with Jacob focusing and  building that muscle of pre-training at OpenAI,

217
00:18:51,200 --> 00:18:57,440
crafting a really superstar team around it,  making sure that all of the important areas

218
00:18:57,440 --> 00:19:02,800
and aspects of pre-training are emphasized.  [00:19:00] That's what creates the artifacts

219
00:19:02,800 --> 00:19:07,269
today that feels like we can go head to head  with Gemini through easily on pre-training.

220
00:19:07,269 --> 00:19:09,920
Speaker 1: Okay. I want to ask about the pre-training,

221
00:19:09,920 --> 00:19:16,400
because I've been talking to all you guys about  this a lot. Okay. But you're saying that you're

222
00:19:16,400 --> 00:19:23,440
less obsessed about lobbying problems at these  new models just when they appear and more at

223
00:19:23,440 --> 00:19:25,520
this long journey? Speaker 2:

224
00:19:25,520 --> 00:19:27,360
Absolutely. Yeah. Okay. Speaker 1:

225
00:19:27,360 --> 00:19:31,360
Okay. The reason I wanted to talk about the  puzzle [00:19:30] that you were at. I mean,

226
00:19:31,360 --> 00:19:39,680
I first met Jaka before OpenAI ever started when  he was doing a coding competition. And I got super

227
00:19:39,680 --> 00:19:44,960
into coding competitions for a while. There's this  guy, Kennedy, I don't know if he's still famous,

228
00:19:44,960 --> 00:19:50,000
but he was like the Michael Jordan of these  coding competitions. And so I went to watch

229
00:19:50,000 --> 00:19:54,389
one at Facebook, used to, I don't know if  they still do, but they had an annual

230
00:19:54,389 --> 00:19:55,109
Speaker 2: Hacker cup.

231
00:19:55,109 --> 00:19:56,800
Speaker 1: Yeah. Hacker cup. And that's

232
00:19:56,800 --> 00:20:01,280
where I saw Jacob for the first time. And then I  know you, I think did [00:20:00] math competitions

233
00:20:01,280 --> 00:20:07,822
in high school, probably grade school through  high school. And then did you also do i I

234
00:20:07,822 --> 00:20:09,280
Speaker 2: Stuff. So I got into coding

235
00:20:09,280 --> 00:20:14,720
really late in life. It was a roommate in college  that convinced me to take my first coding class,

236
00:20:14,720 --> 00:20:20,720
and I had all the hubris of a mathematician  at that time. Whereas math is the purest

237
00:20:20,720 --> 00:20:25,600
and hard of science, and that's where you really  prove your worth. I mean, I think I was probably

238
00:20:25,600 --> 00:20:30,880
too into the competition back then, but yeah, I  mean, it became this super [00:20:30] rewarding

239
00:20:30,880 --> 00:20:38,480
endeavor and it started out as purely a way to  keep in touch with my friends from college.

240
00:20:38,480 --> 00:20:40,229
Speaker 1: You went to MIT?

241
00:20:40,229 --> 00:20:44,080
Speaker 2: Yeah, I went to MITI graduated, and every

242
00:20:44,080 --> 00:20:49,360
weekend we would just log on and do these contests  just to keep in with each other. And over time,

243
00:20:49,360 --> 00:20:55,360
I've found myself having a talent for it. I  started competing fairly well and then writing

244
00:20:55,360 --> 00:21:00,720
problems for contests like the USA coding Olympiad  eventually started coaching that [00:21:00] team.

245
00:21:00,720 --> 00:21:04,880
And yeah, it's been a great community where  I've met people like Scott that you know.

246
00:21:04,880 --> 00:21:09,520
Speaker 1: Yeah. Okay. So I think lots of people might be

247
00:21:09,520 --> 00:21:15,440
familiar with math competitions. They probably see  kids going through that. The i I and these coding

248
00:21:15,440 --> 00:21:21,280
competitions are a little bit different. I mean,  it's so much better, but when I saw them, I mean,

249
00:21:21,280 --> 00:21:27,440
it looks like it's almost like a word problem  that's a puzzle, and you are trying to find the

250
00:21:27,440 --> 00:21:33,440
most efficient and correct way to solve [00:21:30]  that. And you're in this race against everybody

251
00:21:33,440 --> 00:21:39,760
and everybody's writing code on their computer.  And then some people try to get there really fast,

252
00:21:39,760 --> 00:21:44,223
but then their thing kind of doesn't solve the  problem and there's this trade off, right?

253
00:21:44,223 --> 00:21:44,629
Speaker 2: Yeah, absolutely.

254
00:21:44,629 --> 00:21:47,120
Speaker 1: Right. And so you actually

255
00:21:47,120 --> 00:21:49,280
were on the MIT team? No, Speaker 2:

256
00:21:49,280 --> 00:21:51,360
No. It's something I did after college. After Speaker 1:

257
00:21:51,360 --> 00:21:54,789
College, okay. But today you are  the coach of the US National?

258
00:21:54,789 --> 00:21:55,669
Speaker 2: Yeah, one of the coaches.

259
00:21:55,669 --> 00:21:56,480
Speaker 1: One of the coaches,

260
00:21:56,480 --> 00:22:02,640
okay. And was it last year or the year before  [00:22:00] the us? We hadn't won one in a

261
00:22:02,640 --> 00:22:03,600
long time, Speaker 2:

262
00:22:03,600 --> 00:22:04,640
Right? Yeah, yeah. Speaker 1:

263
00:22:04,640 --> 00:22:05,680
Didn't we? Speaker 2:

264
00:22:05,680 --> 00:22:12,800
Yeah, yeah, yeah. So the team, you can  never predict what the makeup of top

265
00:22:12,800 --> 00:22:16,400
talent looks like every year, but we had a  very spiky team, I think two years ago.

266
00:22:16,400 --> 00:22:17,040
Speaker 1: And

267
00:22:17,040 --> 00:22:19,189
Speaker 2: Yeah, I believe they won the olympiad

268
00:22:19,189 --> 00:22:20,960
Speaker 1: Because I feel like usually it's like China

269
00:22:20,960 --> 00:22:31,040
or Russia or Belarus and Poland, I mean, right?  Yeah. So the big competition [00:22:30] takes

270
00:22:31,040 --> 00:22:36,080
place in a different country every year. What  does it look like? How many people show up?

271
00:22:36,080 --> 00:22:37,440
Speaker 2: So they take the

272
00:22:37,440 --> 00:22:44,400
top four students from every single country. It  is as much of a competition as it is a social

273
00:22:44,400 --> 00:22:50,800
event. This is a tight knit community. They  all go on to do phenomenal things. And yeah,

274
00:22:50,800 --> 00:22:56,960
it's this intense two day contest where each day  you get just three problems, five hours to solve

275
00:22:56,960 --> 00:23:02,560
them, and you can really feel the adrenaline and  [00:23:00] all the pressure in the room. But it's

276
00:23:02,560 --> 00:23:08,147
also great fun. I think people settle down and  they make lifetime friends through it. And as

277
00:23:08,147 --> 00:23:09,040
Speaker 1: A coach, I mean,

278
00:23:09,040 --> 00:23:14,789
you're so freaking busy, man. How much time do  you spend on this? What does that look like?

279
00:23:14,789 --> 00:23:18,720
Speaker 2: Honestly, the kids are, so sometimes it's

280
00:23:18,720 --> 00:23:25,760
really about just managing their performance and  strategy. I think you're going to have good days,

281
00:23:25,760 --> 00:23:29,040
you're going to have bad days, you're going to  have good hours within the contest, bad hours,

282
00:23:29,040 --> 00:23:33,920
and you can't let [00:23:30] that get into your  head. There's a lot of similarities between

283
00:23:33,920 --> 00:23:40,880
managing contestants and managing researchers.  It's on a much longer timescale, but researchers

284
00:23:40,880 --> 00:23:47,760
have good months, bad months. You can't really  let those strings of failures get into your head

285
00:23:47,760 --> 00:23:54,880
because that's just the nature of research. And I  think a lot of it's morale management at a certain

286
00:23:54,880 --> 00:24:01,680
point. Yeah. I think one other interesting thing  that contests [00:24:00] have helped me realize

287
00:24:01,680 --> 00:24:06,720
lately is when you put the models and deploy them  towards solving these contest problems, which

288
00:24:06,720 --> 00:24:08,320
they're quite good at these days. Absolutely. Speaker 3:

289
00:24:08,320 --> 00:24:08,800
Yeah, I was going to Speaker 2:

290
00:24:08,800 --> 00:24:14,800
Ask you about that. They work in a very  different way from humans. We typically think

291
00:24:14,800 --> 00:24:21,680
of these machines as they're very good at pattern  recognition. You can take any problem if it maps

292
00:24:21,680 --> 00:24:25,920
to a previous problem, which is probably going  to be able to solve it. But what I've noticed is

293
00:24:25,920 --> 00:24:30,560
in some of the previous iis, there's this problem  message is very ad hoc. [00:24:30] I didn't think

294
00:24:30,560 --> 00:24:36,160
the models would solve it at all, but actually one  of the easier problems for the ai. So yeah, I mean

295
00:24:36,160 --> 00:24:41,680
this has given me the sense that AI is plus humans  in Frontier research, it's going to do something

296
00:24:41,680 --> 00:24:46,160
amazing just because the AI has a different  intuition for what's easy and what's not.

297
00:24:46,160 --> 00:24:50,480
Speaker 1: Okay. Is it vaguely, when D minded,

298
00:24:50,480 --> 00:24:56,640
the whole Alpha go thing, there was that moment  where it was doing things human, it was playing

299
00:24:56,640 --> 00:25:01,040
in ways humans hadn't played before. So kind  of vaguely similar [00:25:00] to that or

300
00:25:01,040 --> 00:25:02,240
Speaker 2: I think so. I think

301
00:25:05,600 --> 00:25:13,760
really with g PT five Pro, there's been  an inflection point in frontier research,

302
00:25:14,320 --> 00:25:20,800
and one of the best anecdotes I have for  this is I think three days after the launch,

303
00:25:20,800 --> 00:25:27,280
I met up with a friend who was a physicist and  he had been playing around with the models,

304
00:25:27,280 --> 00:25:32,400
felt like they were cute but not super useful.  [00:25:30] And I challenged him with the ProModel,

305
00:25:32,400 --> 00:25:39,200
just try something ambitious. And he put in his  latest paper, it thought for 30 minutes and just

306
00:25:39,200 --> 00:25:47,200
got it. And I would say that that reaction in that  moment, it was kind of seeing Lisa at all during

307
00:25:47,200 --> 00:25:54,400
that move 37, move 38. And I just think that is  just going to keep happening more and more for

308
00:25:54,400 --> 00:26:00,320
frontier mathematics, for science, for biology,  material science. The models [00:26:00] have

309
00:26:00,320 --> 00:26:02,240
really gotten to that point. Speaker 1:

310
00:26:02,240 --> 00:26:06,480
I was going to ask you this question, which is  not very original because I think we've been doing

311
00:26:06,480 --> 00:26:11,760
this ever since Big Blue and all the chess stuff.  But yeah, just as somebody who had followed all

312
00:26:11,760 --> 00:26:17,200
these competitions, if, I don't know, there's  a sadness when you start seeing these models,

313
00:26:17,200 --> 00:26:22,080
solving these things that were like this  height of these achievement for these very

314
00:26:22,080 --> 00:26:23,760
unique human minds. Speaker 2:

315
00:26:23,760 --> 00:26:29,120
Well, yes and no. I mean, I was good at  competitive programming. I was never at

316
00:26:29,120 --> 00:26:36,400
the absolute top, [00:26:30] and maybe this is a  way to get revenge, but I do think, no, there's

317
00:26:36,400 --> 00:26:42,560
certainly a moment for myself. We tracked coding  conscious performance while we were developing

318
00:26:42,560 --> 00:26:51,360
reasoning models for a while. And at the start  of the program, they were not super great at

319
00:26:51,360 --> 00:26:59,760
the level of any average competitor going to the  contest. And over time they just started creeping

320
00:26:59,760 --> 00:27:03,920
up and [00:27:00] up in terms of capability. And  you still remember that moment when you walk into

321
00:27:03,920 --> 00:27:11,120
the meeting and they have where your performance  is, and then the models exceeded that. Man, that

322
00:27:11,120 --> 00:27:16,320
was also a shock to me. It's just like, wow, we've  automated to this level of capability so fast. And

323
00:27:16,320 --> 00:27:21,920
of course, Yako was there still a bit smug, but  within one or two months it was also surpassing

324
00:27:21,920 --> 00:27:29,200
him. So yeah, no, the models are at the frontier  today. It's so clear by even through the results

325
00:27:29,200 --> 00:27:34,480
we've done this summer [00:27:30] at Coder, right  top optimization competitive programmers in the

326
00:27:34,480 --> 00:27:42,160
world, I think it achieved second place there.  And so really it's jumped from hundredth place

327
00:27:42,160 --> 00:27:47,269
last year to top five this year. Do you think  we'll still be doing these competitions

328
00:27:47,269 --> 00:27:48,469
Speaker 1: In 10 years?

329
00:27:48,469 --> 00:27:49,440
Speaker 2: I think so. I mean,

330
00:27:49,440 --> 00:27:56,080
they're just fun. I mean, certainly a bunch  of people who use it to had their resume or

331
00:27:56,080 --> 00:28:00,640
going to drop off from doing it, but I think the  people who've always excelled [00:28:00] at it

332
00:28:00,640 --> 00:28:04,389
the most are people who just do it for the fun  of it. And I don't think that'll go away.

333
00:28:04,389 --> 00:28:05,680
Speaker 1: When I was doing this story, I mean,

334
00:28:05,680 --> 00:28:10,880
they were telling me that if you're from Russia  or I don't, which countries that you basically

335
00:28:10,880 --> 00:28:16,400
get an automatic free ride to any university that  you want. I mean, I see the guys on the US team

336
00:28:16,400 --> 00:28:21,829
go to Harvard and MIT, so they seem to be doing  okay, but it doesn't seem like the US has a

337
00:28:21,829 --> 00:28:24,640
Speaker 2: Yeah, don't you think? Yeah. I mean interviews,

338
00:28:24,640 --> 00:28:29,200
right? They're going to be kind of broken going  forward, and everyone's seeing this a little bit,

339
00:28:29,200 --> 00:28:35,200
and even [00:28:30] college exams or college  homework, it's all broken at this point. And I

340
00:28:35,200 --> 00:28:40,480
do think we're going to need new ways of assessing  and gauging who's performing what, who's learned

341
00:28:40,480 --> 00:28:41,920
the material where somebody's actually Speaker 1:

342
00:28:41,920 --> 00:28:42,560
At. Speaker 2:

343
00:28:42,560 --> 00:28:48,960
Yeah. Yeah. So I mean, I've had this idea  here where maybe for our interviews we

344
00:28:48,960 --> 00:28:56,000
should just have candidates talk to chat  GPT, and it's a special kind of chat GPT,

345
00:28:56,000 --> 00:29:00,960
where the model's trying to gauge whether you know  the material [00:29:00] or whether you're at the

346
00:29:00,960 --> 00:29:07,280
capability level to work at OpenAI. And you have  to have this conversation with it that convinces

347
00:29:07,280 --> 00:29:12,000
it deeply you along at OpenAI. And of course you  can't be allowed to jailbreak it, and we look at

348
00:29:12,000 --> 00:29:16,880
the transcript after, but maybe tests like this  will more accurately reflect in the future.

349
00:29:16,880 --> 00:29:18,640
Speaker 1: So you don't do that yet,

350
00:29:18,640 --> 00:29:20,400
but you're thinking about Speaker 2:

351
00:29:20,400 --> 00:29:23,040
Just creative ways to revamp the interviews. Speaker 1:

352
00:29:23,040 --> 00:29:28,229
Yeah. Well, I mean, Silicon Valley is  famous for doing the brain teasers during

353
00:29:28,229 --> 00:29:28,709
Speaker 3: The interviews

354
00:29:28,709 --> 00:29:30,400
Speaker 1: And everything. Yeah. [00:29:30]

355
00:29:30,400 --> 00:29:41,520
So you were very good at math growing up, and  I think, were you born on the East Coast?

356
00:29:41,520 --> 00:29:42,789
Speaker 2: Yeah, born on the east coast.

357
00:29:42,789 --> 00:29:44,469
Speaker 1: And then you lived on the West Coast too,

358
00:29:44,469 --> 00:29:45,029
Speaker 2: On the west coast,

359
00:29:45,029 --> 00:29:46,480
Speaker 1: And then you lived in Taiwan for

360
00:29:48,000 --> 00:29:49,440
grade school to high school Speaker 2:

361
00:29:49,440 --> 00:29:50,480
Four years, yeah. Speaker 1:

362
00:29:50,480 --> 00:29:52,560
Okay. And your parents worked at Bell Labs? Speaker 2:

363
00:29:52,560 --> 00:29:52,960
Yep. Speaker 1:

364
00:29:52,960 --> 00:29:55,040
So you come from engineering Speaker 2:

365
00:29:55,040 --> 00:29:57,440
Stock.

