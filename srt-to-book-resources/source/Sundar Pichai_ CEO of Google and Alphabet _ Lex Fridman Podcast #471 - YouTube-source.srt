1
00:00:00,099 --> 00:00:05,399
There was a five-year waiting list and we got a rotary telephone

2
00:00:05,399 --> 00:00:08,339
but it dramatically changed our lives.

3
00:00:08,339 --> 00:00:11,619
You know, people would come to our house to make calls to their loved ones.

4
00:00:11,619 --> 00:00:18,440
You know, I would, I would have to go all the way to the hospital to get blood test records and it would take two hours to go and they would say

5
00:00:18,440 --> 00:00:19,239
"Sorry, it's not ready.

6
00:00:19,239 --> 00:00:20,208
Come back the next day.

7
00:00:20,208 --> 00:00:24,119
" Two hours to come back, and that became a five-minute thing.

8
00:00:24,119 --> 00:00:27,840
So as a kid, like even this light bulb went in my head

9
00:00:27,840 --> 00:00:30,959
you know, this power of technology to kind of change people's lives.

10
00:00:30,959 --> 00:00:33,680
We had no running water, you know

11
00:00:33,680 --> 00:00:38,619
it was a massive drought, so they would get water in these trucks

12
00:00:38,619 --> 00:00:41,279
maybe eight buckets per household.

13
00:00:41,279 --> 00:00:43,639
So me and my brother, sometimes my mom

14
00:00:43,639 --> 00:00:47,619
we would wait in line, get that and bring it back home.

15
00:00:47,619 --> 00:00:54,000
Many years later, like, we had running water and we had a water heater

16
00:00:54,000 --> 00:00:56,659
and you could get hot water to take a shower.

17
00:00:56,659 --> 00:00:59,139
I mean, like, so, you know

18
00:00:59,139 --> 00:01:01,359
for me, everything was discrete like that.

19
00:01:01,359 --> 00:01:05,599
Uh, and so I've always had this thing

20
00:01:05,599 --> 00:01:09,899
you know, firsthand feeling of, like, how technology can dramatically change

21
00:01:09,899 --> 00:01:13,639
like, your life and, like, the opportunity it brings.

22
00:01:13,639 --> 00:01:18,499
I think if PDOOM is actually high, at some point all of humanity is

23
00:01:18,499 --> 00:01:21,199
like, aligned in making sure that's not the case

24
00:01:21,199 --> 00:01:21,539
right?

25
00:01:21,539 --> 00:01:24,539
And so we'll actually make more progress against it

26
00:01:24,539 --> 00:01:24,939
I think.

27
00:01:24,939 --> 00:01:30,939
So the irony is, so there is a self-modulating aspect there

28
00:01:30,939 --> 00:01:35,259
like, I think if humanity collectively puts their mind to solving a problem

29
00:01:35,259 --> 00:01:37,199
whatever it is, I think we can get there.

30
00:01:37,199 --> 00:01:42,979
So because of that, I think I'm optimistic on the PDOOM scenarios

31
00:01:42,979 --> 00:01:45,459
but that doesn't mean.

32
00:01:45,459 --> 00:01:45,619
..

33
00:01:45,619 --> 00:01:48,019
I think the underlying risk is actually pretty high

34
00:01:48,019 --> 00:01:54,179
but I'm, uh, you know, I have a lot of faith in humanity kind of rising up to the

35
00:01:54,179 --> 00:01:55,459
to meet that moment.

36
00:01:55,459 --> 00:01:59,019
Take me through that experience when there's all these articles saying

37
00:01:59,019 --> 00:02:02,739
"You're the wrong guy to lead Google through this.

38
00:02:02,739 --> 00:02:04,099
Google is lost.

39
00:02:04,099 --> 00:02:04,719
It's done.

40
00:02:04,719 --> 00:02:05,882
It's over.

41
00:02:05,882 --> 00:02:11,419
" The following is a conversation with Sundar Pichai

42
00:02:11,419 --> 00:02:15,499
the CEO of Google and Alphabet, on this

43
00:02:15,499 --> 00:02:17,499
The Lex Fridman Podcast.

44
00:02:17,499 --> 00:02:20,859
Your life story is inspiring to a lot of people.

45
00:02:20,859 --> 00:02:21,899
It's inspiring to me.

46
00:02:21,899 --> 00:02:28,379
You grew up in India, whole family living in a humble two-room apartment

47
00:02:28,379 --> 00:02:31,639
very little, almost no access to technology

48
00:02:31,639 --> 00:02:41,019
and from those humble beginnings, you rose to lead a $2 trillion technology company.

49
00:02:41,019 --> 00:02:44,139
So if you could travel back in time and told that

50
00:02:44,139 --> 00:02:48,939
let's say, 12-year-old Sundar that you're now leading one of the largest companies in human history

51
00:02:48,939 --> 00:02:50,859
what do you think that young kid would say?

52
00:02:50,859 --> 00:02:53,679
I would have probably laughed it off, um

53
00:02:53,679 --> 00:03:00,059
you know, uh, probably too far-fetched to imagine or believe at that time.

54
00:03:00,059 --> 00:03:02,459
You would have to explain the internet first.

55
00:03:02,459 --> 00:03:03,399
For sure.

56
00:03:03,399 --> 00:03:06,139
I mean, computers to me at that time

57
00:03:06,139 --> 00:03:08,879
you know, I was 12 in 1984

58
00:03:08,879 --> 00:03:16,619
so probably, uh, you know, by then I had started reading about them but I hadn't seen one.

59
00:03:16,619 --> 00:03:17,939
What was that place like?

60
00:03:17,939 --> 00:03:19,179
Take me to your childhood.

61
00:03:19,179 --> 00:03:20,679
I grew up in Chennai.

62
00:03:20,679 --> 00:03:22,219
Uh, it's in south of India.

63
00:03:22,219 --> 00:03:25,599
It's a beautiful bustling city, lots of people

64
00:03:25,599 --> 00:03:28,259
lots of energy, you know, simple life

65
00:03:28,259 --> 00:03:33,379
definitely, like fond memories of playing cricket outside the home.

66
00:03:33,379 --> 00:03:35,139
We just used to play on the streets.

67
00:03:35,139 --> 00:03:40,779
All the neighborhood kids would come out and we would play till it got dark and we couldn't play anymore

68
00:03:40,779 --> 00:03:41,759
barefoot.

69
00:03:41,759 --> 00:03:45,559
Um, traffic would come, we would just stop the game

70
00:03:45,559 --> 00:03:48,399
everything would drive through and you would just continue playing

71
00:03:48,399 --> 00:03:51,299
right, just to kind of get the visual in your head.

72
00:03:51,299 --> 00:03:54,379
You know, pre-computers, there was a lot of free time now I

73
00:03:54,379 --> 00:03:55,679
now that I think about it.

74
00:03:55,679 --> 00:03:59,799
Now you have to go and seek that quiet solitude or something.

75
00:03:59,799 --> 00:04:06,319
Newspapers, books is how I gained access to the world's information at the time very well.

76
00:04:06,319 --> 00:04:09,539
Uh, my grandfather was a big influence.

77
00:04:09,539 --> 00:04:11,119
He worked in the post office.

78
00:04:11,119 --> 00:04:15,279
He was so good with language, his English

79
00:04:15,279 --> 00:04:19,959
you know, his handwriting till today is the most beautiful handwriting I've ever seen.

80
00:04:19,959 --> 00:04:23,659
He would write so clearly, he was so articulate

81
00:04:23,659 --> 00:04:27,239
and so he kind of got me introduced into books.

82
00:04:27,239 --> 00:04:31,799
He loved politics, so we, we could talk about anything

83
00:04:31,799 --> 00:04:35,119
and, you know, that was there in my family throughout.

84
00:04:35,119 --> 00:04:37,859
So, uh, lots of books, trashy books

85
00:04:37,859 --> 00:04:44,639
good books, everything from Ayn Rand to books on philosophy to stupid crime novels.

86
00:04:44,639 --> 00:04:46,959
So books was a big part of my life

87
00:04:46,959 --> 00:04:49,489
but that kind of, this whole.

88
00:04:49,489 --> 00:04:49,559
..

89
00:04:49,559 --> 00:04:54,499
It's not surprising I ended up at Google because Google's mission kind of always resonated deeply with me

90
00:04:54,499 --> 00:04:57,799
this access to knowledge, I was hungry for it

91
00:04:57,799 --> 00:05:01,399
but definitely have, you know, fond memories of my childhood.

92
00:05:01,399 --> 00:05:05,719
Access to knowledge was there, so that's the wealth we had.

93
00:05:05,719 --> 00:05:09,819
Uh, you know, every aspect of technology I had to wait for a while

94
00:05:09,819 --> 00:05:13,219
I've obviously spoken before about how long it took for us to get a phone

95
00:05:13,219 --> 00:05:15,599
about five years, but it's not the only thing.

96
00:05:15,599 --> 00:05:16,479
A telephone.

97
00:05:16,479 --> 00:05:19,059
There was a five-year waiting list, uh

98
00:05:19,059 --> 00:05:22,039
and we got a rotary, uh, telephone

99
00:05:22,039 --> 00:05:24,959
but it dramatically changed our lives.

100
00:05:24,959 --> 00:05:28,299
You know, people would come to our house to make calls to their loved ones.

101
00:05:28,299 --> 00:05:35,119
You know, I, I would have to go all the way to the hospital to get blood test records and it would take two hours to go and they would say

102
00:05:35,119 --> 00:05:35,899
"Sorry, it's not ready.

103
00:05:35,899 --> 00:05:36,883
Come back the next day.

104
00:05:36,883 --> 00:05:40,779
" Two hours to come back, and that became a five-minute thing.

105
00:05:40,779 --> 00:05:44,499
So as a kid, like even this light bulb went in my head

106
00:05:44,499 --> 00:05:47,639
you know, this power of technology to kind of change people's lives.

107
00:05:47,639 --> 00:05:50,339
We had no running water, you know

108
00:05:50,339 --> 00:05:55,339
it was a massive drought, so they would get water in these trucks

109
00:05:55,339 --> 00:05:57,939
maybe eight buckets per household.

110
00:05:57,939 --> 00:06:00,339
So me and my brother, sometimes my mom

111
00:06:00,339 --> 00:06:05,023
we would wait in line, get that and bring it back home.

112
00:06:05,023 --> 00:06:10,659
Many years later, like, we had running water and we had a water heater

113
00:06:10,659 --> 00:06:13,339
and you could get hot water to take a shower.

114
00:06:13,339 --> 00:06:15,840
I mean, like, so, you know

115
00:06:15,840 --> 00:06:17,980
for me, everything was discrete like that.

116
00:06:17,980 --> 00:06:22,280
Uh, and so I've always had this thing

117
00:06:22,280 --> 00:06:26,559
you know, firsthand feeling of, like, how technology can dramatically change

118
00:06:26,559 --> 00:06:30,339
like, your life and, like, the opportunity it brings.

119
00:06:30,339 --> 00:06:36,399
So, you know, that was kind of a subliminal takeaway for me throughout growing up.

120
00:06:36,399 --> 00:06:39,780
And, you know, I, I kinda actually observed it and felt it

121
00:06:39,780 --> 00:06:40,139
you know?

122
00:06:40,139 --> 00:06:44,959
So we had to convince my dad for a long time to get a

123
00:06:44,959 --> 00:06:46,260
a VCR.

124
00:06:46,260 --> 00:06:47,319
Do you know what a VCR is?

125
00:06:47,319 --> 00:06:47,440
Yeah?

126
00:06:47,440 --> 00:06:49,059
  Yes.

127
00:06:49,059 --> 00:06:50,159
I'm trying to date you now.

128
00:06:50,159 --> 00:06:50,479
Yeah.

129
00:06:50,479 --> 00:06:54,759
 You know, because before that you only had

130
00:06:54,759 --> 00:06:56,439
like, kind of one TV channel.

131
00:06:56,439 --> 00:06:57,339
Mm-hmm.

132
00:06:57,339 --> 00:06:57,799
Right?

133
00:06:57,799 --> 00:06:58,339
That's it.

134
00:06:58,339 --> 00:07:03,279
Um, and so, you know, you can watch movies or something like that

135
00:07:03,279 --> 00:07:05,979
but this was by the time I was in 12th grade

136
00:07:05,979 --> 00:07:07,679
we got a VCR, you know?

137
00:07:07,679 --> 00:07:11,739
It was a, uh, like a Panasonic which we had to go to some

138
00:07:11,739 --> 00:07:14,079
like, shop which had kinda smuggled it in

139
00:07:14,079 --> 00:07:16,019
I guess, and that's where we bought a VCR.

140
00:07:16,019 --> 00:07:19,619
But then being able to record, like

141
00:07:19,619 --> 00:07:22,050
a World Cup football game and then.

142
00:07:22,050 --> 00:07:22,159
..

143
00:07:22,159 --> 00:07:25,539
Or, like, get, put, like, videotapes and watch movies.

144
00:07:25,539 --> 00:07:26,919
Like, all that.

145
00:07:26,919 --> 00:07:30,039
So, like, you know, I had these discrete memories growing up.

146
00:07:30,039 --> 00:07:38,179
And so, it always left me with the feeling of like how getting access to technology drives that step change in your life.

147
00:07:38,179 --> 00:07:42,019
I don't think you'll ever be able to equal the first time you get hot water.

148
00:07:42,019 --> 00:07:45,039
To have that convenience of going and- Right.

149
00:07:45,039 --> 00:07:45,039
.

150
00:07:45,039 --> 00:07:45,039
.

151
00:07:45,039 --> 00:07:46,959
opening a tap and have hot water come out.

152
00:07:46,959 --> 00:07:47,299
Yeah.

153
00:07:47,299 --> 00:07:52,059
It's interesting, we take for granted the progress we've made.

154
00:07:52,059 --> 00:07:57,587
If you look at human history, just those plots that look at GDP across 2

155
00:07:57,587 --> 00:08:03,219
000 years and you see that exponential growth to where most of the progress happened since the Industrial Revolution.

156
00:08:03,219 --> 00:08:06,139
And we just take for granted, we forget how

157
00:08:06,139 --> 00:08:07,559
how far we've gone.

158
00:08:07,559 --> 00:08:16,899
So our ability to understand how great we have it and also how quickly technology can improve is quite poor.

159
00:08:16,899 --> 00:08:18,799
Oh, I mean, it's, it's extraordinary.

160
00:08:18,799 --> 00:08:20,339
You know, I go back to India now

161
00:08:20,339 --> 00:08:22,432
the power of mobile.

162
00:08:22,432 --> 00:08:22,619
..

163
00:08:22,619 --> 00:08:25,779
You know, it's mind-blowing to see the progress through the arc of time.

164
00:08:25,779 --> 00:08:26,779
It's phenomenal.

165
00:08:26,779 --> 00:08:34,578
What advice would you give to young folks listening to this all over the world who look up to you and

166
00:08:34,578 --> 00:08:36,279
uh, find your story inspiring?

167
00:08:36,279 --> 00:08:39,099
Who wanna be maybe the next Sundar Pichai?

168
00:08:39,099 --> 00:08:44,119
Who wanna start, create companies, uh, build something that has a lot of impact in the world?

169
00:08:44,119 --> 00:08:44,950
Look, it's.

170
00:08:44,950 --> 00:08:45,019
..

171
00:08:45,019 --> 00:08:46,739
You have a lot of luck along the way

172
00:08:46,739 --> 00:08:48,979
but you obviously have to make smart choices.

173
00:08:48,979 --> 00:08:52,159
You're thinking about what you wanna do, your brain is telling you something.

174
00:08:52,159 --> 00:08:56,104
But when you do things, I think it's important to kind of get that.

175
00:08:56,104 --> 00:08:56,219
..

176
00:08:56,219 --> 00:08:59,519
Listen to your heart and see whether you actually enjoy doing it

177
00:08:59,519 --> 00:09:00,359
right?

178
00:09:00,359 --> 00:09:03,759
That, that feeling of if you love what you do

179
00:09:03,759 --> 00:09:10,199
it's so much easier and you're going to see the best version of yourself.

180
00:09:10,199 --> 00:09:12,059
And it's easier said than done.

181
00:09:12,059 --> 00:09:13,999
I think it's tough to find things, uh

182
00:09:13,999 --> 00:09:21,959
you love doing, um, but I think kind of listening to your heart a bit more than your mind in terms of figuring out what you wanna do

183
00:09:21,959 --> 00:09:24,179
I think, I think is one of the best things I would

184
00:09:24,179 --> 00:09:25,039
uh, tell people.

185
00:09:25,039 --> 00:09:31,199
The second thing is, I mean, trying to work with people who you feel are.

186
00:09:31,199 --> 00:09:31,319
..

187
00:09:31,319 --> 00:09:34,879
At various points in my life, I worked with people who I felt were better than me

188
00:09:34,879 --> 00:09:36,059
right?

189
00:09:36,059 --> 00:09:40,579
Kind of like, you know, you almost are sitting in a room talking to someone and they're like

190
00:09:40,579 --> 00:09:40,979
"Wow.

191
00:09:40,979 --> 00:09:42,191
" Like, you know, uh, uh.

192
00:09:42,191 --> 00:09:42,279
..

193
00:09:42,279 --> 00:09:44,339
You know, and you want that feeling a few times.

194
00:09:44,339 --> 00:09:51,579
Trying to get yourself in a position where you're working with people who you feel are kind of

195
00:09:51,579 --> 00:09:55,059
like, stretching your abilities is what helps you grow

196
00:09:55,059 --> 00:09:55,639
I think.

197
00:09:55,639 --> 00:09:58,719
Uh, so putting yourself in uncomfortable situations.

198
00:09:58,719 --> 00:10:01,879
And I think often you'll surprise yourself.

199
00:10:01,879 --> 00:10:07,479
So I think being open-minded enough to kind of put yourself in those positions is maybe

200
00:10:07,479 --> 00:10:08,919
uh, maybe another thing I would say.

201
00:10:08,919 --> 00:10:12,139
What lessons can we learn, maybe from an outsider perspective

202
00:10:12,139 --> 00:10:15,739
for me, looking at your story and gotten to know you a bit?

203
00:10:15,739 --> 00:10:17,379
You're humble, you're kind.

204
00:10:17,379 --> 00:10:23,999
Usually, when I think of somebody who has had a journey like yours and climbs to the very top of leadership

205
00:10:23,999 --> 00:10:29,219
they're us- in a cutthroat world, they're usually gonna be a bit of an asshole.

206
00:10:29,219 --> 00:10:34,199
So  what wisdom are we supposed to draw from the fact that

207
00:10:34,199 --> 00:10:36,819
uh, your general approach of, is of balance

208
00:10:36,819 --> 00:10:39,819
of humility, of kindness, listening to everybody?

209
00:10:39,819 --> 00:10:41,679
What's, what's, what's your secret?

210
00:10:41,679 --> 00:10:44,119
 I do get angry, I do get frustrated.

211
00:10:44,119 --> 00:10:45,699
I have, I have the same emotions.

212
00:10:45,699 --> 00:10:47,239
All, all of us do, right?

213
00:10:47,239 --> 00:10:49,219
In the context of work and everything.

214
00:10:49,219 --> 00:10:51,419
Uh, but a few things, right?

215
00:10:51,419 --> 00:10:53,409
I, I think, you know, I.

216
00:10:53,409 --> 00:10:53,719
..

217
00:10:53,719 --> 00:10:59,619
Over time, I figured out the best way to get the most out of people.

218
00:10:59,619 --> 00:11:05,639
Uh, you know, you kinda find mission-oriented people who are on the shared journey

219
00:11:05,639 --> 00:11:09,259
who have this inner drive to excellence to do their best.

220
00:11:09,259 --> 00:11:12,999
And, and, you know, you kinda motivate people and

221
00:11:12,999 --> 00:11:15,879
and, and you can, you can achieve a lot that way

222
00:11:15,879 --> 00:11:16,699
right?

223
00:11:16,699 --> 00:11:19,479
And so it, it often tends to work out that way.

224
00:11:19,479 --> 00:11:21,579
But have there been times like, you know

225
00:11:21,579 --> 00:11:22,899
I loo- lose it?

226
00:11:22,899 --> 00:11:23,279
Yeah.

227
00:11:23,279 --> 00:11:24,052
But, you know, not.

228
00:11:24,052 --> 00:11:24,179
..

229
00:11:24,179 --> 00:11:25,679
Maybe less often than others.

230
00:11:25,679 --> 00:11:29,659
Uh, and maybe over the years, uh

231
00:11:29,659 --> 00:11:31,379
less and less so because, you know

232
00:11:31,379 --> 00:11:35,379
I, I find it's not needed to achieve what you need to do.

233
00:11:35,379 --> 00:11:37,639
So losing your shit has not been productive.

234
00:11:37,639 --> 00:11:39,159
Yeah, less often than not.

235
00:11:39,159 --> 00:11:39,199
 Okay.

236
00:11:39,199 --> 00:11:40,859
I think people respond to that.

237
00:11:40,859 --> 00:11:41,239
Yeah.

238
00:11:41,239 --> 00:11:43,439
They may do stuff to react to that.

239
00:11:43,439 --> 00:11:45,739
Like, but you, you actually want them to do the right thing.

240
00:11:45,739 --> 00:11:48,259
And, and, and so, you know

241
00:11:48,259 --> 00:11:50,259
maybe there's a bit of, like, sports

242
00:11:50,259 --> 00:11:51,899
you know, you know, I'm a sports fan

243
00:11:51,899 --> 00:11:55,139
in football coaches, uh, in soccer, uh.

244
00:11:55,139 --> 00:11:55,259
..

245
00:11:55,259 --> 00:11:56,139
Not football.

246
00:11:56,139 --> 00:11:58,699
 Uh, you know, people, people often talk about

247
00:11:58,699 --> 00:12:00,059
like, man management, right?

248
00:12:00,059 --> 00:12:02,039
Great coaches do, right?

249
00:12:02,039 --> 00:12:04,799
I think there is an element of that in our lives

250
00:12:04,799 --> 00:12:07,512
how do you get the best out of the people you work with?

251
00:12:07,512 --> 00:12:07,861
..

252
00:12:07,861 --> 00:12:09,735
. you know, at times you're working with people who

253
00:12:09,735 --> 00:12:13,835
who are so committed to achieving, if they've done something wrong

254
00:12:13,835 --> 00:12:15,835
they feel it more than you, uh

255
00:12:15,835 --> 00:12:16,855
you do, right?

256
00:12:16,855 --> 00:12:19,790
So you treat them differently than.

257
00:12:19,790 --> 00:12:19,956
..

258
00:12:19,956 --> 00:12:23,195
You know, occasionally there are people who you need to clearly let them know

259
00:12:23,195 --> 00:12:25,035
like, "That wasn't okay," or whatever it is.

260
00:12:25,035 --> 00:12:27,975
But I've often found that not to be the case.

261
00:12:27,975 --> 00:12:31,555
And sometimes the right words at the right time

262
00:12:31,555 --> 00:12:35,415
spoken firmly can reverberate through time.

263
00:12:35,415 --> 00:12:37,515
Also sometimes the unspoken words.

264
00:12:37,515 --> 00:12:39,816
You know, people can sometimes see that

265
00:12:39,816 --> 00:12:43,535
like, you know, you're unhappy without you saying it.

266
00:12:43,535 --> 00:12:46,355
And so sometimes the silence can, uh

267
00:12:46,355 --> 00:12:47,715
deliver that message even more.

268
00:12:47,715 --> 00:12:49,455
Sometimes less is more.

269
00:12:49,455 --> 00:12:52,356
Um, who's the greatest, uh, soccer player of all time

270
00:12:52,356 --> 00:12:55,435
Messi or Ronaldo or Pele or Maradona?

271
00:12:55,435 --> 00:12:59,099
I'm gonna make, uh, you know, in this question- Is this going to be a political answer so no.

272
00:12:59,099 --> 00:12:59,115
..

273
00:12:59,115 --> 00:12:59,475
 Oh, no, no, no.

274
00:12:59,475 --> 00:13:01,695
I, I, I will tell a truthful answer- The truth.

275
00:13:01,695 --> 00:13:01,695
 .

276
00:13:01,695 --> 00:13:01,695
 .

277
00:13:01,695 --> 00:13:03,495
because, uh, the, the- So it's Messi.

278
00:13:03,495 --> 00:13:03,521
.

279
00:13:03,521 --> 00:13:03,575
..

280
00:13:03,575 --> 00:13:04,095
truthful answer- Okay.

281
00:13:04,095 --> 00:13:04,101
.

282
00:13:04,101 --> 00:13:04,115
..

283
00:13:04,115 --> 00:13:04,775
no, it is.

284
00:13:04,775 --> 00:13:09,455
You know, it's been interesting because my son is a big Cristiano Ron- Ronaldo fan.

285
00:13:09,455 --> 00:13:14,435
And, uh, so we've had to watch El Clasicos together

286
00:13:14,435 --> 00:13:17,175
you know, wi- with that dynamic in there.

287
00:13:17,175 --> 00:13:20,475
I so admire CR7's.

288
00:13:20,475 --> 00:13:20,715
..

289
00:13:20,715 --> 00:13:25,015
I mean, I've never seen an athlete more committed to that kind of excellence.

290
00:13:25,015 --> 00:13:27,315
And so he's one of the all-time greats

291
00:13:27,315 --> 00:13:30,855
but, you know, for me, Messi is it.

292
00:13:30,855 --> 00:13:33,395
Yeah, when I see Lon- uh, Lionel Messi

293
00:13:33,395 --> 00:13:39,755
you just are in awe that humans are able to achieve that level of greatness and genius and artistry.

294
00:13:39,755 --> 00:13:41,695
When we talk, we'll talk about AI

295
00:13:41,695 --> 00:13:44,995
maybe robotics and this kind of stuff, that level of genius

296
00:13:44,995 --> 00:13:49,215
I'm not sure you can possibly match by AI in a long time.

297
00:13:49,215 --> 00:13:50,955
It's just an example of greatness.

298
00:13:50,955 --> 00:13:53,175
And you have that kind of greatness in other disciplines

299
00:13:53,175 --> 00:13:57,035
but in sport, you get to visually see it

300
00:13:57,035 --> 00:14:00,695
unlike anything else, and just the, the timing

301
00:14:00,695 --> 00:14:03,915
the movement, uh, it's just genius.

302
00:14:03,915 --> 00:14:05,855
I had the chance to see him couple weeks ago.

303
00:14:05,855 --> 00:14:07,755
He played in, uh, San Jose.

304
00:14:07,755 --> 00:14:07,835
Mm-hmm.

305
00:14:07,835 --> 00:14:11,575
So, um, against the Quakes, so I went to see it

306
00:14:11,575 --> 00:14:12,355
see the game.

307
00:14:12,355 --> 00:14:13,675
I was a fan on the.

308
00:14:13,675 --> 00:14:13,815
..

309
00:14:13,815 --> 00:14:17,175
 Had good seats, knew where he would play in the second half hopefully

310
00:14:17,175 --> 00:14:21,155
and, uh, even at his age, just watching him when he gets the ball

311
00:14:21,155 --> 00:14:22,095
that movement.

312
00:14:22,095 --> 00:14:24,635
You know, you're right, that special quality

313
00:14:24,635 --> 00:14:27,435
it's tough to describe, but you feel it when you see it

314
00:14:27,435 --> 00:14:27,635
yeah.

315
00:14:27,635 --> 00:14:29,075
He's still got it.

316
00:14:29,075 --> 00:14:34,675
Uh, if we rank all the technological innovations throughout human history

317
00:14:34,675 --> 00:14:39,335
let's go back, uh, maybe the history of human civilizations 12

318
00:14:39,335 --> 00:14:42,888
000 years ago, and you rank them by the.

319
00:14:42,888 --> 00:14:43,035
..

320
00:14:43,035 --> 00:14:46,535
how much of a productivity multiplier they've been.

321
00:14:46,535 --> 00:14:53,055
So, uh, we can go to electricity or the labor mechanization for the Industrial Revolution

322
00:14:53,055 --> 00:14:55,895
or we can go back to the first Agricultural Revolution 12

323
00:14:55,895 --> 00:14:56,795
000 years ago.

324
00:14:56,795 --> 00:15:01,495
In that long list of inventions, do you think AI

325
00:15:01,495 --> 00:15:04,295
when history is written 1,000 years from now

326
00:15:04,295 --> 00:15:08,175
do you think it has a chance to be the number one productivity multiplier?

327
00:15:08,175 --> 00:15:09,195
It's a great question.

328
00:15:09,195 --> 00:15:12,375
Look, many years ago, I think it might have been 2017 or 2018

329
00:15:12,375 --> 00:15:14,955
um, you know, I, I said at that time

330
00:15:14,955 --> 00:15:18,595
like, you know, "AI is the most profound technology humanity will ever work on.

331
00:15:18,595 --> 00:15:21,536
It'll be more profound than fire or electricity.

332
00:15:21,536 --> 00:15:23,655
" So I have to back myself, I.

333
00:15:23,655 --> 00:15:23,755
..

334
00:15:23,755 --> 00:15:26,195
You know, I still think, uh, that's the case.

335
00:15:26,195 --> 00:15:28,155
You know, when you asked this question

336
00:15:28,155 --> 00:15:31,435
um, I was thinking, "Well, do we have a recency bias

337
00:15:31,435 --> 00:15:31,855
" right?

338
00:15:31,855 --> 00:15:36,235
You know, like in sports, it's very tempting to call the current person you're seeing the greatest- Yes.

339
00:15:36,235 --> 00:15:36,308
.

340
00:15:36,308 --> 00:15:36,455
..

341
00:15:36,455 --> 00:15:37,175
player, right?

342
00:15:37,175 --> 00:15:40,415
And, and so is there a recency bias?

343
00:15:40,415 --> 00:15:44,955
And, you know, I do think, uh

344
00:15:44,955 --> 00:15:49,695
from first principles, I would argue AI will be bigger than all of those.

345
00:15:49,695 --> 00:15:51,475
I didn't live through those moments.

346
00:15:51,475 --> 00:15:59,295
You know, two years ago, I had to go through a surgery and then I processed that there was a point in time people didn't have anesthesia when they went through these procedures.

347
00:15:59,295 --> 00:16:03,115
At that moment, I was like, "That has got to be the greatest invention-"  Yeah.

348
00:16:03,115 --> 00:16:03,115
".

349
00:16:03,115 --> 00:16:03,115
".

350
00:16:03,115 --> 00:16:05,215
humanity has ever, ever done," right?

351
00:16:05,215 --> 00:16:05,255
Yeah.

352
00:16:05,255 --> 00:16:08,195
So, look, we, we don't know what it is to have

353
00:16:08,195 --> 00:16:10,175
uh, uh, lived through those times.

354
00:16:10,175 --> 00:16:18,635
But, you know, and many of what you're talking about were kind of this general things which pretty much affected everything

355
00:16:18,635 --> 00:16:21,315
you know, electricity or internet, et cetera.

356
00:16:21,315 --> 00:16:28,615
But I don't think we've ever dealt with a technology both which is progressing so fast

357
00:16:28,615 --> 00:16:33,375
becoming so capable, it's not clear what the ceiling is.

358
00:16:33,375 --> 00:16:36,699
And the main unique.

359
00:16:36,699 --> 00:16:36,775
..

360
00:16:36,775 --> 00:16:38,995
it's recursively self-improving, right?

361
00:16:38,995 --> 00:16:40,155
It's capable of that.

362
00:16:40,155 --> 00:16:42,645
And so the fact it is going.

363
00:16:42,645 --> 00:16:42,735
..

364
00:16:42,735 --> 00:16:48,555
it's the first technology will kind of dramatically accelerate creation itself

365
00:16:48,555 --> 00:16:52,255
like creating things, building new things, can

366
00:16:52,255 --> 00:16:54,615
can improve and achieve things on its own

367
00:16:54,615 --> 00:16:55,935
right?

368
00:16:55,935 --> 00:16:58,535
I think, like, puts it in a different league

369
00:16:58,535 --> 00:16:58,995
right?

370
00:16:58,995 --> 00:16:59,683
And so.

371
00:16:59,683 --> 00:16:59,955
..

372
00:16:59,955 --> 00:17:00,715
a different league.

373
00:17:00,715 --> 00:17:04,035
And so I think the impact it'll end up having

374
00:17:04,035 --> 00:17:07,575
uh, will far surpass everything we've seen before.

375
00:17:07,575 --> 00:17:10,734
Uh, obviously with that comes a lot of

376
00:17:10,734 --> 00:17:13,115
uh, important things to think and wrestle with

377
00:17:13,115 --> 00:17:15,355
but I definitely think that'll end up being the case.

378
00:17:15,355 --> 00:17:20,755
Especially if you guess the point of where we can achieve superhuman performance on the AI research itself.

379
00:17:20,755 --> 00:17:23,435
So it's a technology that may.

380
00:17:23,435 --> 00:17:23,595
..

381
00:17:23,595 --> 00:17:31,815
this is an open question, but it may be able to achieve a level to where the technology itself can create itself better than it

382
00:17:31,815 --> 00:17:32,815
it could yesterday.

383
00:17:32,815 --> 00:17:37,135
It's like the Move 37 of AlphaResearch or whatever it is

384
00:17:37,135 --> 00:17:37,375
right?

385
00:17:37,375 --> 00:17:37,395
Yeah.

386
00:17:37,395 --> 00:17:38,763
Like, you know, and when, when.

387
00:17:38,763 --> 00:17:38,895
..

388
00:17:38,895 --> 00:17:45,115
yeah, you're right, when, when it can do novel self-directed research.

389
00:17:45,115 --> 00:17:50,515
Obviously, for a long time, we'll, we'll have hopefully always humans in the loop and all that stuff

390
00:17:50,515 --> 00:17:53,475
and these are complex questions to talk about.

391
00:17:53,475 --> 00:17:55,448
But yes, I think the underlying technology.

392
00:17:55,448 --> 00:17:55,575
..

393
00:17:55,575 --> 00:17:57,967
You know, I've said this, like, if you watched.

394
00:17:57,967 --> 00:17:58,075
..

395
00:17:58,075 --> 00:18:03,715
seeing AlphaGo start from scratch, be clueless

396
00:18:03,715 --> 00:18:07,355
and like become better through the course of a day

397
00:18:07,355 --> 00:18:09,815
you know, like,  you know, kind of like

398
00:18:09,815 --> 00:18:13,442
kind of like, you know, really hits you when you see that happen.

399
00:18:13,442 --> 00:18:13,696
..

400
00:18:13,696 --> 00:18:15,924
. even our, like, the VO3 models

401
00:18:15,924 --> 00:18:17,563
if you sample the models when they were

402
00:18:17,563 --> 00:18:21,483
like, 30% done and 60% done and looked at what they were generating

403
00:18:21,483 --> 00:18:25,803
and you kind of see how it all comes together

404
00:18:25,803 --> 00:18:28,523
it's kind of like, I would say

405
00:18:28,523 --> 00:18:32,003
it's kind of inspiring, a little bit unsettling

406
00:18:32,003 --> 00:18:33,724
right, as a, as a human.

407
00:18:33,724 --> 00:18:35,323
So all of that is true, I think.

408
00:18:35,323 --> 00:18:38,944
Well, the interesting thing of the Industrial Revolution

409
00:18:38,944 --> 00:18:41,643
electricity, like you mentioned, it can go back to the

410
00:18:41,643 --> 00:18:45,183
again, the agriculture, the first Agriculture Revolution.

411
00:18:45,183 --> 00:18:50,843
There is, um, what's called the Neolithic package of the first Agriculture Revolution.

412
00:18:50,843 --> 00:18:57,023
It wasn't just that the nomads settled down and started planting food

413
00:18:57,023 --> 00:19:03,563
but all this other kind of technology was born from that and it's included in this package.

414
00:19:03,563 --> 00:19:05,064
There wasn't one piece of technology.

415
00:19:05,064 --> 00:19:09,204
It's, there's these ripple effects, second and third order effects that happen.

416
00:19:09,204 --> 00:19:12,503
Everything from something silly, like, "silly," profound

417
00:19:12,503 --> 00:19:14,963
like pottery that can store-  .

418
00:19:14,963 --> 00:19:14,963
..

419
00:19:14,963 --> 00:19:19,263
liquids and food, uh, to s- something we kind of take for granted

420
00:19:19,263 --> 00:19:23,783
but social hierarchies, uh, and political hierarchies.

421
00:19:23,783 --> 00:19:25,823
So, like, early government was formed.

422
00:19:25,823 --> 00:19:31,103
'Cause it turns out if humans stop moving and have some surplus food

423
00:19:31,103 --> 00:19:37,883
they start coming up with, uh, they get bored  and they start coming up with interesting systems and then trade emerges

424
00:19:37,883 --> 00:19:40,643
which turns out to be a really profound thing.

425
00:19:40,643 --> 00:19:41,823
And like I said, government.

426
00:19:41,823 --> 00:19:46,063
There, I mean there's just, uh, second and third order effects from that

427
00:19:46,063 --> 00:19:50,784
included in that package is incredible and probably extremely difficult if

428
00:19:50,784 --> 00:19:54,283
if you'll ask one of the people in the nomadic tribes to predict that

429
00:19:54,283 --> 00:19:55,323
it would be impossible.

430
00:19:55,323 --> 00:19:56,504
It's difficult to predict.

431
00:19:56,504 --> 00:20:03,543
But all that said, wha- what do you think are some of the early things we might see in the

432
00:20:03,543 --> 00:20:05,054
quote-unquote, "AI package?

433
00:20:05,054 --> 00:20:09,283
" I mean, most of it probably we don't know today.

434
00:20:09,283 --> 00:20:13,363
But, like, you know, the one thing which we can tangibly start seeing now is

435
00:20:13,363 --> 00:20:17,943
you know, obviously with the coding progress you got a sense of it

436
00:20:17,943 --> 00:20:20,703
it's going to be so easy to imagine

437
00:20:20,703 --> 00:20:24,563
like thoughts in your head, translating that into things that exist.

438
00:20:24,563 --> 00:20:26,464
That'll be part of the package, right?

439
00:20:26,464 --> 00:20:33,163
Like, it's gonna empower almost all of humanity to kind of express themselves.

440
00:20:33,163 --> 00:20:36,503
Maybe in the past you could have expressed with words

441
00:20:36,503 --> 00:20:41,684
but, like, you could kind of build things into existence 

442
00:20:41,684 --> 00:20:41,963
right?

443
00:20:41,963 --> 00:20:43,524
You know, maybe not fully today.

444
00:20:43,524 --> 00:20:46,203
We are at the early stages of vibe coding.

445
00:20:46,203 --> 00:20:49,923
You know, I've been amazed at what people have put out online with VO3.

446
00:20:49,923 --> 00:20:50,244
Mm-hmm.

447
00:20:50,244 --> 00:20:51,423
But it takes a bit of work, right?

448
00:20:51,423 --> 00:20:53,403
You have to stitch together a set of prompts.

449
00:20:53,403 --> 00:20:55,884
But all this is gonna get better.

450
00:20:55,884 --> 00:20:59,643
The thing I always think about, this is the worst it'll ever be.

451
00:20:59,643 --> 00:21:00,463
 Right?

452
00:21:00,463 --> 00:21:01,944
Like at any  given moment in time.

453
00:21:01,944 --> 00:21:05,343
Yeah, it's interesting you went there as kind of a first thought.

454
00:21:05,343 --> 00:21:11,604
So the exponential increase of access to creativity.

455
00:21:11,604 --> 00:21:16,203
Software, creation, uh, are you creating a program

456
00:21:16,203 --> 00:21:19,263
a piece of content for, to be shared with others?

457
00:21:19,263 --> 00:21:21,783
Games down the line.

458
00:21:21,783 --> 00:21:25,683
All of that, like, just becomes infinitely more possible.

459
00:21:25,683 --> 00:21:26,943
Well, I think the big thing is that

460
00:21:26,943 --> 00:21:29,283
uh, it makes it accessible.

461
00:21:29,283 --> 00:21:33,003
It unlocks the cognitive capabilities of the entire eight billion.

462
00:21:33,003 --> 00:21:33,763
No, I agree.

463
00:21:33,763 --> 00:21:40,303
Look, think about 40 years ago, maybe in the US there were five people who could do what you are doing.

464
00:21:40,303 --> 00:21:40,683
Mm-hmm.

465
00:21:40,683 --> 00:21:43,383
Like go do a interview, you know

466
00:21:43,383 --> 00:21:44,203
and, uh, you know.

467
00:21:44,203 --> 00:21:47,303
But today think about with YouTube and other

468
00:21:47,303 --> 00:21:50,423
other products, et cetera, like how many more people are doing it.

469
00:21:50,423 --> 00:21:54,243
So I think this is what technology does

470
00:21:54,243 --> 00:21:54,563
right?

471
00:21:54,563 --> 00:21:57,823
Like when the internet created blogs, you know

472
00:21:57,823 --> 00:21:59,483
you heard from so many more people.

473
00:21:59,483 --> 00:22:01,068
So I think.

474
00:22:01,068 --> 00:22:01,203
..

475
00:22:01,203 --> 00:22:07,283
But, but with AI, I think that number won't be in the few hundreds of thousands.

476
00:22:07,283 --> 00:22:11,583
It'll be tens of millions of people, maybe even a billion people

477
00:22:11,583 --> 00:22:15,283
like, putting out things into the world in a

478
00:22:15,283 --> 00:22:16,863
a, a deeper way.

479
00:22:16,863 --> 00:22:19,823
And I think it'll change the landscape of creativity

480
00:22:19,823 --> 00:22:21,763
and it makes a lot of people nervous.

481
00:22:21,763 --> 00:22:25,083
Like for example, uh, whatever, Fox

482
00:22:25,083 --> 00:22:27,923
M- MSNBC, CNN are really nervous about this part.

483
00:22:27,923 --> 00:22:30,903
Like, y- you mean this dude in a suit could just do this?

484
00:22:30,903 --> 00:22:33,263
And, and you, and YouTube and, and

485
00:22:33,263 --> 00:22:35,483
and thousands of others, tens of thousands

486
00:22:35,483 --> 00:22:37,443
millions of other creators can do the same kind of thing?

487
00:22:37,443 --> 00:22:38,643
That makes me nervous.

488
00:22:38,643 --> 00:22:46,423
And now y- you get a podcast from NotebookLM that's about five to 10 times better than any podcasts I've ever  done.

489
00:22:46,423 --> 00:22:47,823
Uh- Not true, but yeah.

490
00:22:47,823 --> 00:22:49,043
I'm, I'm, I'm joking at this time

491
00:22:49,043 --> 00:22:50,423
but maybe not, and that changes.

492
00:22:50,423 --> 00:22:51,363
You have to evolve.

493
00:22:51,363 --> 00:22:59,683
Because I, on the podcasting front, I'm a fan of podcasts much more than I am a fan of being a host or whatever.

494
00:22:59,683 --> 00:23:02,423
If there's great podcasts that are both AIs

495
00:23:02,423 --> 00:23:04,223
I'll just stop doing this podcast.

496
00:23:04,223 --> 00:23:05,403
I'll listen to that podcast.

497
00:23:05,403 --> 00:23:07,203
But you have to evolve and you have to change

498
00:23:07,203 --> 00:23:08,863
and that makes people really nervous, I think.

499
00:23:08,863 --> 00:23:11,203
But it's also a really exciting future.

500
00:23:11,203 --> 00:23:13,503
The only thing I may say is I do think

501
00:23:13,503 --> 00:23:15,783
like, in a world in which there are two AI

502
00:23:15,783 --> 00:23:19,260
I think people value and, uh, choose.

503
00:23:19,260 --> 00:23:19,443
..

504
00:23:19,443 --> 00:23:27,743
Just like in chess, you and I would never watch Stockfish 10 or whatever and AlphaGo play against each o- like

505
00:23:27,743 --> 00:23:29,383
it would be boring for us to watch.

506
00:23:29,383 --> 00:23:34,823
But Magnus Carlsen and Gukesh, that game would be much more fascinating to watch.

507
00:23:34,823 --> 00:23:36,343
So it's tough to say.

508
00:23:36,343 --> 00:23:42,683
Like, one way to say is you'll have a lot more content and so you will be listening to AI generated content- Mm-hmm.

509
00:23:42,683 --> 00:23:42,696
.

510
00:23:42,696 --> 00:23:42,723
..

511
00:23:42,723 --> 00:23:44,303
because sometimes it's efficient, et cetera.

512
00:23:44,303 --> 00:23:50,503
But the premium experiences you value might be a version of

513
00:23:50,503 --> 00:23:52,763
like, the human essence wherever it comes through.

514
00:23:52,763 --> 00:23:56,623
Going back to what we talked earlier about watching Messi dribble the ball.

515
00:23:56,623 --> 00:23:59,443
I don't know, one day I'm sure a machine will dribble much better  than Messi

516
00:23:59,443 --> 00:24:03,343
but I don't know whether it would evoke that same emotion in us.

517
00:24:03,343 --> 00:24:05,323
So I think that'll be fascinating to see.

518
00:24:05,323 --> 00:24:14,243
I think the element of podcasting or audiobooks that is about information gathering- Yeah.

519
00:24:14,243 --> 00:24:14,296
.

520
00:24:14,296 --> 00:24:14,403
..

521
00:24:14,403 --> 00:24:20,643
that part might be removed or that might be more efficiently and in a compelling way done by AI

522
00:24:20,643 --> 00:24:25,263
but then it'll be just nice to hear humans struggle with the information .

523
00:24:25,263 --> 00:24:28,839
 Contend with the information, try to internalize it

524
00:24:28,839 --> 00:24:33,059
combine it with the complexity of our own emotions and consciousness and all that kind of stuff.

525
00:24:33,059 --> 00:24:37,379
But if you actually want to find out about a piece of hi- history

526
00:24:37,379 --> 00:24:39,020
you go to Gemini.

527
00:24:39,020 --> 00:24:43,220
If you want to see Lex struggle with that history

528
00:24:43,220 --> 00:24:44,162
then you look.

529
00:24:44,162 --> 00:24:44,220
..

530
00:24:44,220 --> 00:24:46,579
Or other humans, you look, you, you look at that.

531
00:24:46,579 --> 00:24:49,380
But, uh, the point is, it's going to change the nature

532
00:24:49,380 --> 00:24:53,940
uh, continue to change the nature of how we discover information

533
00:24:53,940 --> 00:24:56,279
how we consume the information, how we create that information

534
00:24:56,279 --> 00:24:59,519
the same way that YouTube changed everything completely

535
00:24:59,519 --> 00:25:00,379
changed news.

536
00:25:00,379 --> 00:25:01,059
It changed.

537
00:25:01,059 --> 00:25:01,100
..

538
00:25:01,100 --> 00:25:03,220
And that's something our society is struggling with.

539
00:25:03,220 --> 00:25:03,560
Yeah.

540
00:25:03,560 --> 00:25:03,907
YouTube.

541
00:25:03,907 --> 00:25:03,979
..

542
00:25:03,979 --> 00:25:05,231
Look, YouTube enabled.

543
00:25:05,231 --> 00:25:05,400
..

544
00:25:05,400 --> 00:25:07,380
I mean, you know this better than anyone else

545
00:25:07,380 --> 00:25:09,499
it's enabled so many creators.

546
00:25:09,499 --> 00:25:12,319
There is no doubt in me that, like

547
00:25:12,319 --> 00:25:16,159
we will enable more filmmakers than, than have ever been

548
00:25:16,159 --> 00:25:16,319
right?

549
00:25:16,319 --> 00:25:18,359
You're going to empower a lot more people.

550
00:25:18,359 --> 00:25:23,340
Um, so I think there's an expansionary aspect of this

551
00:25:23,340 --> 00:25:24,859
which is underestimated, I think.

552
00:25:24,859 --> 00:25:28,299
I think it'll unleash human creativity in a way

553
00:25:28,299 --> 00:25:30,439
uh, that hasn't been seen before.

554
00:25:30,439 --> 00:25:32,159
It's tough to internalize.

555
00:25:32,159 --> 00:25:37,600
The only way is if you, if you brought someone from the '50s or '40s and just put them in front of YouTube

556
00:25:37,600 --> 00:25:40,060
 you know, I think it would blow their mind away.

557
00:25:40,060 --> 00:25:44,819
Similarly, I think we would get blown away by what's possible in a 10 to 20 year timeframe.

558
00:25:44,819 --> 00:25:46,559
Uh, do you think there's a future

559
00:25:46,559 --> 00:25:48,200
how many years out is it, that

560
00:25:48,200 --> 00:25:50,079
let's say, let's put a mark on it

561
00:25:50,079 --> 00:25:53,860
50% of content, in a comp- good content

562
00:25:53,860 --> 00:25:58,139
50% of good content is generated by VO4

563
00:25:58,139 --> 00:25:58,940
5, 6?

564
00:25:58,940 --> 00:26:02,459
You know, I think depends on what it is for.

565
00:26:02,459 --> 00:26:07,419
Um, like, you know, maybe if you look at movies today with CGI

566
00:26:07,419 --> 00:26:10,439
there are great filmmakers, like used to look at

567
00:26:10,439 --> 00:26:12,800
like, who the directors are and who use it.

568
00:26:12,800 --> 00:26:14,699
There are filmmakers who don't use it at all

569
00:26:14,699 --> 00:26:16,239
you value that.

570
00:26:16,239 --> 00:26:18,620
There are people who use it incredibly.

571
00:26:18,620 --> 00:26:20,919
You know, think about somebody like a James Cameron

572
00:26:20,919 --> 00:26:23,899
like what he would do with these tools in his hands.

573
00:26:23,899 --> 00:26:26,759
But I think there'll be a lot more content created.

574
00:26:26,759 --> 00:26:33,139
Like, just like writers today use Google Docs and not think about the fact that they're using a tool like that

575
00:26:33,139 --> 00:26:36,479
like people will be using the future versions of these things

576
00:26:36,479 --> 00:26:39,179
like it won't be a big deal at all to them.

577
00:26:39,179 --> 00:26:44,299
I've gotten a chance to get to know Darren Aronofsky well.

578
00:26:44,299 --> 00:26:47,245
He's been really  leaning in and trying to figure.

579
00:26:47,245 --> 00:26:47,299
..

580
00:26:47,299 --> 00:26:54,999
It's, it's fun to watch a genius who came up before any of this was even remotely possible

581
00:26:54,999 --> 00:26:57,219
he created Pi, one of my favorite movies

582
00:26:57,219 --> 00:27:00,799
and from there just continued to create a really interesting variety of movies.

583
00:27:00,799 --> 00:27:05,859
And now he's trying to see how can AI be used to create compelling films.

584
00:27:05,859 --> 00:27:08,625
You have people like that, you have people.

585
00:27:08,625 --> 00:27:08,799
..

586
00:27:08,799 --> 00:27:10,519
Uh, I've gotten a chance to know

587
00:27:10,519 --> 00:27:13,699
uh, edgier folks, uh, that are AI first

588
00:27:13,699 --> 00:27:14,639
like Dorr Brothers.

589
00:27:14,639 --> 00:27:21,459
Both Aronofsky and Dorr Brothers create at the edge of the Overton Window of society

590
00:27:21,459 --> 00:27:24,499
you know, they push, whether it's, uh

591
00:27:24,499 --> 00:27:28,779
sexuality or, or violence, it's edgy like artists are

592
00:27:28,779 --> 00:27:31,079
but it's still classy, doesn't cross that line

593
00:27:31,079 --> 00:27:33,559
uh, whatever that line is.

594
00:27:33,559 --> 00:27:34,779
You know, um, Hunter S.

595
00:27:34,779 --> 00:27:38,479
Thompson has this line that the, uh

596
00:27:38,479 --> 00:27:40,799
the only way to find out where the edge

597
00:27:40,799 --> 00:27:42,459
where the line is, is by crossing it

598
00:27:42,459 --> 00:27:44,779
uh, and I think for artists that's true

599
00:27:44,779 --> 00:27:48,179
that's kind of their purpose sometimes, comedians and artists just cross that line.

600
00:27:48,179 --> 00:27:53,219
I wonder if you can comment on the weird place that puts Google

601
00:27:53,219 --> 00:27:58,679
 because Google's line is probably different than some of these artists.

602
00:27:58,679 --> 00:28:00,464
What, what's your.

603
00:28:00,464 --> 00:28:00,619
..

604
00:28:00,619 --> 00:28:03,599
How do you think about specifically VO, um

605
00:28:03,599 --> 00:28:07,179
and Flow, about like how to allow artists to get

606
00:28:07,179 --> 00:28:11,199
do crazy shit, but also like the responsibility of like

607
00:28:11,199 --> 00:28:14,859
um, not, for it not to be too crazy?

608
00:28:14,859 --> 00:28:16,159
I mean, it's a great question.

609
00:28:16,159 --> 00:28:16,695
Look, part of.

610
00:28:16,695 --> 00:28:16,799
..

611
00:28:16,799 --> 00:28:20,359
You mentioned Darren, uh, you know, he's a clear visionary

612
00:28:20,359 --> 00:28:20,579
right?

613
00:28:20,579 --> 00:28:30,719
Part of the reason we wor- started working with them early on VO is he's one of those people who's able to kind of see that future

614
00:28:30,719 --> 00:28:37,859
get inspired by it, and kind of showing the way for how creative people can express themselves with it.

615
00:28:37,859 --> 00:28:42,099
Look, I think when it comes to allowing artistic free expression

616
00:28:42,099 --> 00:28:45,419
that is one of the most important values in a society

617
00:28:45,419 --> 00:28:45,899
right?

618
00:28:45,899 --> 00:28:50,079
I think, uh, you know, artists have always been the ones to push

619
00:28:50,079 --> 00:28:53,079
push boundaries, expand the frontiers of thought

620
00:28:53,079 --> 00:28:55,839
uh, and so, look, I think

621
00:28:55,839 --> 00:28:58,779
I think that's going to be an important value we have.

622
00:28:58,779 --> 00:29:08,499
So I think we will provide tools and put it in the hands of artists for them to use and put out their work.

623
00:29:08,499 --> 00:29:13,659
Those APIs, I mean, I almost think of that as infrastructure

624
00:29:13,659 --> 00:29:16,539
just like when you provide electricity to people or something

625
00:29:16,539 --> 00:29:20,639
you want them to use it and like you're not thinking about the use cases on top of it

626
00:29:20,639 --> 00:29:21,659
so- It's a paintbrush.

627
00:29:21,659 --> 00:29:22,159
Yeah.

628
00:29:22,159 --> 00:29:24,439
And, and so I think that's how

629
00:29:24,439 --> 00:29:27,759
obviously there have to be some things, and

630
00:29:27,759 --> 00:29:30,979
you know, society needs to decide at a fundamental level what's okay

631
00:29:30,979 --> 00:29:31,679
what's not.

632
00:29:31,679 --> 00:29:35,099
Uh, we'll be responsible with it, um

633
00:29:35,099 --> 00:29:39,459
but I do think, you know, when it comes to artistic free expression

634
00:29:39,459 --> 00:29:43,519
I think that's one of those values we should work hard to defend.

635
00:29:43,519 --> 00:29:46,919
Uh, I wonder if you can comment on

636
00:29:46,919 --> 00:29:55,359
uh, maybe earlier versions of Gemini were a little bit careful on the kind of things you would be willing to answer.

637
00:29:55,359 --> 00:29:58,579
I just want to comment on, I was really surprised and

638
00:29:58,579 --> 00:30:02,519
uh, pleasantly surprised and en- enjoyed the fact that Gemini 2.

639
00:30:02,519 --> 00:30:05,219
5 Pro is a lot less careful in a good sense.

640
00:30:05,219 --> 00:30:09,639
Don't ask me why, but I've been doing a lot of research on Genghis Khan- .

641
00:30:09,639 --> 00:30:09,639
.

642
00:30:09,639 --> 00:30:09,639
.

643
00:30:09,639 --> 00:30:12,179
and the, uh, the Aztecs, uh

644
00:30:12,179 --> 00:30:14,239
so there's a lot of violence there in that history

645
00:30:14,239 --> 00:30:15,559
it's a very violent history.

646
00:30:15,559 --> 00:30:18,939
I've also been doing a lot of research on World War I and World War II

647
00:30:18,939 --> 00:30:22,739
and earlier versions of Gemini were very, um

648
00:30:22,739 --> 00:30:26,579
basically this kind of sense, "Are you sure you want to learn about this?

649
00:30:26,579 --> 00:30:29,691
" And now it's actually very factual-.

650
00:30:29,691 --> 00:30:29,907
..

651
00:30:29,907 --> 00:30:33,887
objective, uh, talks about very difficult parts of human history

652
00:30:33,887 --> 00:30:36,367
and does so with nuance and depth.

653
00:30:36,367 --> 00:30:37,567
It's, it's been really nice.

654
00:30:37,567 --> 00:30:40,967
But there's a line there that I guess Google has to kinda walk.

655
00:30:40,967 --> 00:30:41,601
I wonder if it's.

656
00:30:41,601 --> 00:30:41,687
..

657
00:30:41,687 --> 00:30:43,688
And it's also an engineering challenge, how to

658
00:30:43,688 --> 00:30:48,827
how to do that at scale across all the weird queries  that people ask.

659
00:30:48,827 --> 00:30:49,455
What, um.

660
00:30:49,455 --> 00:30:49,607
..

661
00:30:49,607 --> 00:30:51,228
Can you just speak to that challenge?

662
00:30:51,228 --> 00:30:54,607
How do you allow Gemini to say, again

663
00:30:54,607 --> 00:30:58,247
forgive, pardon my French, crazy shit, but not too

664
00:30:58,247 --> 00:30:59,447
not, not too crazy?

665
00:30:59,447 --> 00:31:02,627
I think one of the good insights here has been

666
00:31:02,627 --> 00:31:08,407
as the models are getting more capable, the models are really good at this stuff

667
00:31:08,407 --> 00:31:08,767
right?

668
00:31:08,767 --> 00:31:08,847
Mm-hmm.

669
00:31:08,847 --> 00:31:11,127
And so, I think in some ways

670
00:31:11,127 --> 00:31:13,447
maybe a year ago, the models weren't fully there

671
00:31:13,447 --> 00:31:16,067
so they would also do stupid things more often.

672
00:31:16,067 --> 00:31:21,728
And so, you know, you're trying to handle those edge cases

673
00:31:21,728 --> 00:31:25,387
but then you make a mistake in how you handle those edge cases and it compounds.

674
00:31:25,387 --> 00:31:27,037
But I think with 2.

675
00:31:27,037 --> 00:31:33,227
5, what we particularly found is, once the models cross a certain level of intelligence and sophistication

676
00:31:33,227 --> 00:31:37,467
you know, they are, they are able to reason through these nuanced issues pretty well.

677
00:31:37,467 --> 00:31:37,487
Mm-hmm.

678
00:31:37,487 --> 00:31:39,247
And I think users really want that, right?

679
00:31:39,247 --> 00:31:43,627
Like, you know, uh, you want as much access to the raw model as possible

680
00:31:43,627 --> 00:31:44,147
right?

681
00:31:44,147 --> 00:31:46,627
But I think it's a great area to think about

682
00:31:46,627 --> 00:31:49,147
like, you know, over time, you know

683
00:31:49,147 --> 00:31:52,927
we should allow more and more closer access to it.

684
00:31:52,927 --> 00:31:57,247
Maybe l- may, uh, obviously let people custom prompts if they wanted to

685
00:31:57,247 --> 00:31:59,007
and like, you know, and, you know

686
00:31:59,007 --> 00:32:00,627
experiment with it, et cetera.

687
00:32:00,627 --> 00:32:03,987
Uh, I, I think that's an important direction.

688
00:32:03,987 --> 00:32:06,627
But look, the first principles we wanna think about it is

689
00:32:06,627 --> 00:32:11,773
you know, from a scientific standpoint, like making sure the models.

690
00:32:11,773 --> 00:32:11,907
..

691
00:32:11,907 --> 00:32:13,367
And I'm saying scientific in the sense of

692
00:32:13,367 --> 00:32:16,107
like, how you would approach math or physics or something like that.

693
00:32:16,107 --> 00:32:16,707
Mm-hmm.

694
00:32:16,707 --> 00:32:21,007
From first principles, having the models reason about the world

695
00:32:21,007 --> 00:32:24,887
be nuanced, et cetera, uh, you know

696
00:32:24,887 --> 00:32:27,447
from the ground up is the right way to build these things

697
00:32:27,447 --> 00:32:27,787
right?

698
00:32:27,787 --> 00:32:33,947
Not like some subset of humans kinda hard coding things on top of it.

699
00:32:33,947 --> 00:32:37,327
Uh, so I think it's the direction we've been taking

700
00:32:37,327 --> 00:32:39,587
and I think you'll see us continue to push in that direction.

701
00:32:39,587 --> 00:32:41,532
Yeah, I actually asked.

702
00:32:41,532 --> 00:32:41,667
..

703
00:32:41,667 --> 00:32:43,147
Uh, I gave these notes.

704
00:32:43,147 --> 00:32:46,207
I took extensive notes and I  gave them to Gemini and said

705
00:32:46,207 --> 00:32:49,804
"Can you ask a novel question that's not in these notes?

706
00:32:49,804 --> 00:32:52,055
" And it wrote me.

707
00:32:52,055 --> 00:32:52,127
..

708
00:32:52,127 --> 00:32:56,367
Gemini continues to really surprise me, really surprise me.

709
00:32:56,367 --> 00:32:57,567
It's been really beautiful.

710
00:32:57,567 --> 00:32:59,027
It's an incredible model.

711
00:32:59,027 --> 00:33:02,047
Uh, the, the question its, it

712
00:33:02,047 --> 00:33:05,407
it generated was, "You," meaning Sundar

713
00:33:05,407 --> 00:33:09,487
"told the world Gemini's churning out 480 trillion tokens a month.

714
00:33:09,487 --> 00:33:14,527
Uh, what's the most life-changing five-word sentence hiding in that haystack?

715
00:33:14,527 --> 00:33:15,967
" That's a Gemini question.

716
00:33:15,967 --> 00:33:17,782
 But it made me, it gave me a sense.

717
00:33:17,782 --> 00:33:17,847
..

718
00:33:17,847 --> 00:33:19,507
I don't think you can answer that, but it gave me

719
00:33:19,507 --> 00:33:19,860
it may.

720
00:33:19,860 --> 00:33:19,927
..

721
00:33:19,927 --> 00:33:27,747
it woke me up to like, all of these tokens are providing little aha moments for people across the globe.

722
00:33:27,747 --> 00:33:30,373
So that's like learning, that those tokens are.

723
00:33:30,373 --> 00:33:30,467
..

724
00:33:30,467 --> 00:33:35,207
People are curious, they ask a question and they find something out

725
00:33:35,207 --> 00:33:37,047
and it truly could be life changing.

726
00:33:37,047 --> 00:33:38,047
Oh, it is.

727
00:33:38,047 --> 00:33:38,197
I.

728
00:33:38,197 --> 00:33:38,307
..

729
00:33:38,307 --> 00:33:40,907
Look, you know, I had the same feeling about search many

730
00:33:40,907 --> 00:33:41,827
many years ago.

731
00:33:41,827 --> 00:33:44,050
You, you, you know, you, you definitely.

732
00:33:44,050 --> 00:33:44,207
..

733
00:33:44,207 --> 00:33:49,047
You know, these tokens per month has like grown 50 times in the last 12 months

734
00:33:49,047 --> 00:33:49,967
so- Is that accurate, by the way?

735
00:33:49,967 --> 00:33:50,707
The four- Yeah, it is.

736
00:33:50,707 --> 00:33:50,807
Okay.

737
00:33:50,807 --> 00:33:51,447
 You know, it is, you know

738
00:33:51,447 --> 00:33:52,327
it is, it is accurate.

739
00:33:52,327 --> 00:33:53,807
I'm glad it got it right.

740
00:33:53,807 --> 00:33:57,480
Um, but you know, that number was 9.

741
00:33:57,480 --> 00:33:59,847
7 trillion tokens per month, 12 months ago

742
00:33:59,847 --> 00:34:00,707
right?

743
00:34:00,707 --> 00:34:02,507
It's gone, gone up to 480.

744
00:34:02,507 --> 00:34:04,447
Uh, you know, it's a 50X increase.

745
00:34:04,447 --> 00:34:08,027
So there's no limit to human cur- curiosity

746
00:34:08,027 --> 00:34:11,067
uh, and I think it's, it's one of those moments.

747
00:34:11,067 --> 00:34:13,627
Uh, maybe, I don't think it is there today

748
00:34:13,627 --> 00:34:21,507
but maybe one day there's a five-word phrase which says what the actual universe is or something like that

749
00:34:21,507 --> 00:34:22,627
and something very meaningful.

750
00:34:22,627 --> 00:34:23,987
But I don't think we are quite there yet.

751
00:34:23,987 --> 00:34:28,347
Do you think the scaling laws are holding strong on

752
00:34:28,347 --> 00:34:28,687
um.

753
00:34:28,687 --> 00:34:28,887
..

754
00:34:28,887 --> 00:34:32,467
There's a lot of ways to describe the scaling laws for AI

755
00:34:32,467 --> 00:34:34,867
but on the pre-training, on the post-training fronts.

756
00:34:34,867 --> 00:34:40,107
So the flip side of that, do you anticipate AI progress will hit a wall?

757
00:34:40,107 --> 00:34:41,587
Is there a wall?

758
00:34:41,587 --> 00:34:44,746
You know, it's a cherished micro kitchen conversation.

759
00:34:44,746 --> 00:34:44,806
 Yes.

760
00:34:44,806 --> 00:34:46,467
Once in a while I have it, uh

761
00:34:46,467 --> 00:34:49,387
you know, like when Demis is visiting or

762
00:34:49,387 --> 00:34:52,266
you know, if Demis, Koray, Geoff

763
00:34:52,266 --> 00:34:53,947
Noam, Sergey, a bunch of our people

764
00:34:53,947 --> 00:34:54,089
like.

765
00:34:54,089 --> 00:34:54,147
..

766
00:34:54,147 --> 00:34:54,187
Yeah.

767
00:34:54,187 --> 00:34:54,187
.

768
00:34:54,187 --> 00:34:54,187
.

769
00:34:54,187 --> 00:34:55,647
you know, we sit and, uh, you know

770
00:34:55,647 --> 00:34:57,067
sh- uh, uh, you know, talk about this

771
00:34:57,067 --> 00:34:57,367
right?

772
00:34:57,367 --> 00:35:00,287
And, um, look, I.

773
00:35:00,287 --> 00:35:00,447
..

774
00:35:00,447 --> 00:35:02,727
We see a lot of headroom ahead, right?

775
00:35:02,727 --> 00:35:07,287
I think, uh, we've been able to optimize and improve on all fronts

776
00:35:07,287 --> 00:35:07,707
right?

777
00:35:07,707 --> 00:35:12,747
Uh, pre-training, post-training, test time compute

778
00:35:12,747 --> 00:35:17,907
tool use, right, over time, making these more agentic.

779
00:35:17,907 --> 00:35:22,807
So, getting these models to be more general world models in that direction

780
00:35:22,807 --> 00:35:29,947
like VO3, uh, you know, the physics understanding is dramatically better than what the VO1 or something like that was.

781
00:35:29,947 --> 00:35:33,107
So you kind of see on all those dimensions

782
00:35:33,107 --> 00:35:36,667
I, I feel, you know, progress is very obvious to see

783
00:35:36,667 --> 00:35:42,647
and I feel like there is significant headroom.

784
00:35:42,647 --> 00:35:49,207
More importantly, you know, I'm fortunate to work with some of the best researchers on the planet 

785
00:35:49,207 --> 00:35:49,467
right?

786
00:35:49,467 --> 00:35:53,027
They think, uh, there is more headroom to be had here.

787
00:35:53,027 --> 00:35:56,107
Uh, and so I think we have an exciting trajectory ahead.

788
00:35:56,107 --> 00:35:59,175
It's tougher to say, you know.

789
00:35:59,175 --> 00:35:59,307
..

790
00:35:59,307 --> 00:36:01,067
Each year I sit and say, "Okay

791
00:36:01,067 --> 00:36:04,227
we are gonna throw 10X more compute over the course of next year at it

792
00:36:04,227 --> 00:36:06,279
and, like, will we see progress?

793
00:36:06,279 --> 00:36:11,167
" Sitting here today, I feel like the year ahead will have a lot of progress.

794
00:36:11,167 --> 00:36:14,367
And do you feel any limitations like, uh

795
00:36:14,367 --> 00:36:17,407
that, uh, the bottlenecks, compute limited

796
00:36:17,407 --> 00:36:19,367
uh, data limited, idea limited.

797
00:36:19,367 --> 00:36:21,287
Do you feel any of those limitations?

798
00:36:21,287 --> 00:36:23,367
Or is it full steam ahead on all fronts?

799
00:36:23,367 --> 00:36:25,847
I think it's compute limited in this sense

800
00:36:25,847 --> 00:36:26,127
right?

801
00:36:26,127 --> 00:36:27,167
Like, you know, we can all.

802
00:36:27,167 --> 00:36:27,327
..

803
00:36:27,327 --> 00:36:30,147
Part of the reason you've seen us do Flash

804
00:36:30,147 --> 00:36:32,414
NanoFlash and Pro models.

805
00:36:32,414 --> 00:36:32,714
..

806
00:36:32,714 --> 00:36:33,013
..

807
00:36:33,013 --> 00:36:34,444
. but not an ultra model.

808
00:36:34,444 --> 00:36:39,523
It's like for each generation we feel like we've been able to get the pro model at like

809
00:36:39,523 --> 00:36:43,103
I don't know, 80, 90% of ultra's capability.

810
00:36:43,103 --> 00:36:46,744
But ultra would be a, a lot more

811
00:36:46,744 --> 00:36:51,403
uh, like slow and a lot more expensive to sell.

812
00:36:51,403 --> 00:37:00,683
But what we've been able to do is to go to the next generation and make the next generation's pro as good as the previous generation's ultra- Yeah.

813
00:37:00,683 --> 00:37:00,743
.

814
00:37:00,743 --> 00:37:00,863
..

815
00:37:00,863 --> 00:37:04,603
but be able to serve it in a way that it's fast and you can use it and so on.

816
00:37:04,603 --> 00:37:07,183
So I do think scaling laws are working

817
00:37:07,183 --> 00:37:14,164
but it's tough to get at any given time the models we all use the most.

818
00:37:14,164 --> 00:37:20,963
This may be like a few months behind the maximum capability we can deliver

819
00:37:20,963 --> 00:37:22,123
right?

820
00:37:22,123 --> 00:37:25,963
Because that won't be the fastest, easiest to use

821
00:37:25,963 --> 00:37:26,463
et cetera.

822
00:37:26,463 --> 00:37:30,183
Also, that's in terms of intelligence, it becomes harder and harder to measure

823
00:37:30,183 --> 00:37:34,343
uh, "performance" in quotes, because, you know

824
00:37:34,343 --> 00:37:41,463
you could argue Gemini Flash is much more impactful than Pro just because of the latency

825
00:37:41,463 --> 00:37:42,903
it's super intelligent already.

826
00:37:42,903 --> 00:37:48,803
I mean, sometimes like latencies, uh, may be more important than intelligence

827
00:37:48,803 --> 00:37:52,923
 especially when the intelligence is just a little bit less and Flash not.

828
00:37:52,923 --> 00:37:53,043
..

829
00:37:53,043 --> 00:37:54,523
it's still incredibly smart model.

830
00:37:54,523 --> 00:37:54,863
Yeah.

831
00:37:54,863 --> 00:38:04,143
And so you, you have to now start measuring impact and then it feels like benchmarks are less and less capable of capturing the intelligence of models

832
00:38:04,143 --> 00:38:07,663
the effectiveness of models, the usefulness, the real world usefulness of models.

833
00:38:07,663 --> 00:38:09,283
Uh, another kitchen question.

834
00:38:09,283 --> 00:38:11,163
So lots of folks are talking about, uh

835
00:38:11,163 --> 00:38:16,123
timelines for AGI or ASI, artificial super intelligence.

836
00:38:16,123 --> 00:38:24,563
So AGI, loosely defined, is basically human expert level at a lot of the main fields of pursuit for humans.

837
00:38:24,563 --> 00:38:32,263
And ASI is what AGI becomes presumably quickly by being able to self-improve

838
00:38:32,263 --> 00:38:36,943
so becoming far superior in intelligence across all these disciplines than humans.

839
00:38:36,943 --> 00:38:39,103
When do you think we'll have AGI?

840
00:38:39,103 --> 00:38:40,583
Is 2030 a possibility?

841
00:38:40,583 --> 00:38:43,663
Uh, there's one other term we should throw in there.

842
00:38:43,663 --> 00:38:45,323
I don't know who, who used it first.

843
00:38:45,323 --> 00:38:46,523
Maybe Karpathy did.

844
00:38:46,523 --> 00:38:47,523
AJI.

845
00:38:47,523 --> 00:38:49,283
Have you, have you heard AJI?

846
00:38:49,283 --> 00:38:51,203
The artificial jagged intelligence.

847
00:38:51,203 --> 00:38:53,223
Sometimes feels that way, right?

848
00:38:53,223 --> 00:38:56,023
Both, there are progress and you see what they can do.

849
00:38:56,023 --> 00:39:00,703
And then like you can trivially find they make numeric letters or like

850
00:39:00,703 --> 00:39:05,683
you know, counting R's in strawberry or something which seems to trip up most models or whatever it is

851
00:39:05,683 --> 00:39:05,983
right?

852
00:39:05,983 --> 00:39:09,943
So, uh, so maybe we should throw that term in there.

853
00:39:09,943 --> 00:39:14,763
I, I feel like we are in the AJI phase where like dramatic progress

854
00:39:14,763 --> 00:39:16,823
some things don't work well, but overall

855
00:39:16,823 --> 00:39:19,123
you know, you're seeing, uh, lots of progress.

856
00:39:19,123 --> 00:39:23,143
But if your question is will, will it happen by 2030?

857
00:39:23,143 --> 00:39:28,643
Look, we constantly move the line of what it means to be AGI.

858
00:39:28,643 --> 00:39:37,303
There are moments today, you know, like sitting in a Waymo in a San Francisco street with all the crowds and the people and kind of work its way through.

859
00:39:37,303 --> 00:39:39,803
I see glimpses of it there.

860
00:39:39,803 --> 00:39:43,463
The car is sometimes kind of impatient, trying to work its way

861
00:39:43,463 --> 00:39:46,723
uh, using Astra like in Gemini Live or seeing

862
00:39:46,723 --> 00:39:49,203
uh, you know, asking questions about the world.

863
00:39:49,203 --> 00:39:51,703
What's this skinny building doing in my neighborhood?

864
00:39:51,703 --> 00:39:54,303
It's a streetlight, not a building.

865
00:39:54,303 --> 00:39:56,003
You, you see glimpses.

866
00:39:56,003 --> 00:40:01,823
That's why I use the word AGI because then you see stuff which obviously

867
00:40:01,823 --> 00:40:04,123
you know, we are far from AGI too.

868
00:40:04,123 --> 00:40:07,063
So you have both experiences simultaneously happening to you.

869
00:40:07,063 --> 00:40:09,683
I'll answer your question, but I'll also throw out this.

870
00:40:09,683 --> 00:40:11,783
I almost feel the term doesn't matter.

871
00:40:11,783 --> 00:40:15,223
What I know is by 2030 there'll be such dramatic progress.

872
00:40:15,223 --> 00:40:20,883
We'll be dealing with the consequences of that progress

873
00:40:20,883 --> 00:40:28,823
both the positives, uh, both the positive externalities and the negative externalities that come with it in a big way by 2030.

874
00:40:28,823 --> 00:40:31,123
So that I strongly feel, right?

875
00:40:31,123 --> 00:40:37,323
Whatever we may be arguing about the term or maybe Gemini can answer what that moment is in time in 2030

876
00:40:37,323 --> 00:40:40,983
but I think the progress will be dramatic

877
00:40:40,983 --> 00:40:41,243
right?

878
00:40:41,243 --> 00:40:42,303
So that I believe in.

879
00:40:42,303 --> 00:40:45,503
Will the AI think it has reached AGI by 2030?

880
00:40:45,503 --> 00:40:49,243
I would say we will just fall short of that timeline

881
00:40:49,243 --> 00:40:49,543
right?

882
00:40:49,543 --> 00:40:50,863
So I think it'll take a bit longer.

883
00:40:50,863 --> 00:40:54,223
It's amazing, in the early days of Google DeepMind in 2010

884
00:40:54,223 --> 00:40:56,663
they talked about a 20-year timeframe to achieve

885
00:40:56,663 --> 00:40:58,443
uh, AGI.

886
00:40:58,443 --> 00:41:00,583
So which is, which is kind of fascinating to see.

887
00:41:00,583 --> 00:41:03,793
But, you know, I.

888
00:41:03,793 --> 00:41:03,963
..

889
00:41:03,963 --> 00:41:11,363
For me the whole thing, seeing what Google Brain did in 2012 and when we acquired DeepMind in 2014

890
00:41:11,363 --> 00:41:15,543
uh, right close to where we are sitting in 2012

891
00:41:15,543 --> 00:41:19,603
you know, Jeff Dean showed the image of when the neural networks could recognize

892
00:41:19,603 --> 00:41:21,383
uh, a picture of a cat, right?

893
00:41:21,383 --> 00:41:22,383
And identify it.

894
00:41:22,383 --> 00:41:24,383
You know, this is the early versions of Brain

895
00:41:24,383 --> 00:41:24,743
right?

896
00:41:24,743 --> 00:41:28,583
And so, you know, we all talked about couple decades.

897
00:41:28,583 --> 00:41:31,483
I don't think we'll quite get there by 2030.

898
00:41:31,483 --> 00:41:34,623
So my sense is it's slightly after that.

899
00:41:34,623 --> 00:41:37,123
But I, I would stress it doesn't matter

900
00:41:37,123 --> 00:41:44,843
like what the definition is because you will have mind-blowing progress on many dimensions.

901
00:41:44,843 --> 00:41:46,823
Maybe AI can create videos.

902
00:41:46,823 --> 00:41:49,447
We have to figure out as a society how do we.

903
00:41:49,447 --> 00:41:49,543
..

904
00:41:49,543 --> 00:41:57,883
We need some system by which we all agree that this is AI generated and we have to disclose it in a certain way because how do you distinguish reality otherwise?

905
00:41:57,883 --> 00:42:00,083
Yeah, there's so many interesting things you said.

906
00:42:00,083 --> 00:42:02,283
So first of all, just looking back at this recent

907
00:42:02,283 --> 00:42:05,823
now feels like distant history, uh, with Google Brain.

908
00:42:05,823 --> 00:42:10,043
I mean, that was before TensorFlow, before TensorFlow was made public and open sourced.

909
00:42:10,043 --> 00:42:14,543
So the tooling matters too combined with GitHub ability to share code.

910
00:42:14,543 --> 00:42:19,423
Then you have the ideas of attention transformers and the diffusion now

911
00:42:19,423 --> 00:42:25,143
and then there might be a new idea that seems simple in retrospect but will change everything

912
00:42:25,143 --> 00:42:28,403
and that could be the post-training, the inference time innovations.

913
00:42:28,403 --> 00:42:36,003
And I think ShadCN tweeted that Google is just one great UI from completely winning the AI race.

914
00:42:36,003 --> 00:42:37,755
Meaning like-.

915
00:42:37,755 --> 00:42:38,099
..

916
00:42:38,099 --> 00:42:39,820
UI is a huge part of it.

917
00:42:39,820 --> 00:42:43,699
Like, how that intelligence, uh, uh

918
00:42:43,699 --> 00:42:46,899
I think Logan Cooper project likes to talk about this right now as an LLM

919
00:42:46,899 --> 00:42:53,859
but it become, like, when is it going to become a system where you're sh- talking about shipping systems versus shipping a particular model?

920
00:42:53,859 --> 00:42:55,739
Yeah, it, that matters too.

921
00:42:55,739 --> 00:43:00,659
How the system is, um, manifest itself and how it presents itself to the world.

922
00:43:00,659 --> 00:43:01,999
That really, really matters.

923
00:43:01,999 --> 00:43:04,119
Oh, hugely so.

924
00:43:04,119 --> 00:43:08,139
There are simple UI innovations which have changed the world

925
00:43:08,139 --> 00:43:08,399
right?

926
00:43:08,399 --> 00:43:11,420
And, uh, I absolutely think so.

927
00:43:11,420 --> 00:43:18,279
Um, we will see a lot more progress in the next couple of years as I think AI itself

928
00:43:18,279 --> 00:43:21,259
uh, on a self-improving track for UI itself.

929
00:43:21,259 --> 00:43:22,879
Like, you know, today- Yes.

930
00:43:22,879 --> 00:43:22,979
.

931
00:43:22,979 --> 00:43:23,179
..

932
00:43:23,179 --> 00:43:25,860
we are, like, constraining the models.

933
00:43:25,860 --> 00:43:29,460
The models can't quite express themselves in terms of the UI to

934
00:43:29,460 --> 00:43:30,419
to people.

935
00:43:30,419 --> 00:43:33,720
Um, but that is, uh, like

936
00:43:33,720 --> 00:43:34,939
you know, if you think about it

937
00:43:34,939 --> 00:43:36,899
we've kind of boxed them in that way.

938
00:43:36,899 --> 00:43:41,139
But given these models can code, uh

939
00:43:41,139 --> 00:43:45,699
you know, they should be able to write the best interfaces to express their ideas- Mm-hmm.

940
00:43:45,699 --> 00:43:45,799
.

941
00:43:45,799 --> 00:43:45,999
..

942
00:43:45,999 --> 00:43:47,059
over time, right?

943
00:43:47,059 --> 00:43:48,759
That is incredible idea.

944
00:43:48,759 --> 00:43:50,919
So the APIs are already open.

945
00:43:50,919 --> 00:43:59,759
So you can  you create a really nice agentic system that continuously improves the way you can be talking to an AI.

946
00:43:59,759 --> 00:44:00,399
Yeah.

947
00:44:00,399 --> 00:44:03,839
But it, a lot of that is the interface and then

948
00:44:03,839 --> 00:44:08,519
of course, the incredible multimodal aspect of the interface that Google's been pushing.

949
00:44:08,519 --> 00:44:10,599
These models are natively multimodal.

950
00:44:10,599 --> 00:44:13,379
They can easily take content from any format

951
00:44:13,379 --> 00:44:14,639
put it in any format.

952
00:44:14,639 --> 00:44:17,099
They can write a good user interface.

953
00:44:17,099 --> 00:44:20,219
They probably understand your preferences better than over time.

954
00:44:20,219 --> 00:44:23,979
Like, you know, and so, so all this is

955
00:44:23,979 --> 00:44:26,079
like, the evolution ahead, right?

956
00:44:26,079 --> 00:44:30,279
And so, um, that goes back to where we started the conversation.

957
00:44:30,279 --> 00:44:33,619
I, like, I think there'll be dramatic evolutions in the years ahead.

958
00:44:33,619 --> 00:44:35,939
Maybe one more kitchen question.

959
00:44:35,939 --> 00:44:41,319
Uh, this e- even further ridiculous concept of PDOOM.

960
00:44:41,319 --> 00:44:51,699
So the philosophically minded folks in the AI community think about the probability that AGI and then ASI might destroy all of human civilization.

961
00:44:51,699 --> 00:44:55,259
I would say my PDOOM is about 10%.

962
00:44:55,259 --> 00:45:01,019
Do you ever think about this kind of long-term threat of ASI?

963
00:45:01,019 --> 00:45:02,899
And what would your PDOOM be?

964
00:45:02,899 --> 00:45:05,119
Look, I mean, for sure.

965
00:45:05,119 --> 00:45:06,939
Look, I've, uh, both been, uh

966
00:45:06,939 --> 00:45:10,499
very excited about AI, uh, but I've always felt

967
00:45:10,499 --> 00:45:13,199
uh, this is a technology, you know

968
00:45:13,199 --> 00:45:17,739
we have to actively think about the risks and work very

969
00:45:17,739 --> 00:45:20,979
very hard to harness it in a way that it

970
00:45:20,979 --> 00:45:22,519
it all works out well.

971
00:45:22,519 --> 00:45:25,359
Um, on the PDOOM question, look, it's

972
00:45:25,359 --> 00:45:29,779
uh, you know, won't surprise you to say that's probably another micro kitchen conversation that pops up once in a while

973
00:45:29,779 --> 00:45:30,139
right?

974
00:45:30,139 --> 00:45:34,419
And given how powerful the technology is, maybe stepping back

975
00:45:34,419 --> 00:45:36,079
you know, when you're running a large organization

976
00:45:36,079 --> 00:45:39,499
if you can kinda align the incentives of the organization

977
00:45:39,499 --> 00:45:41,819
you can achieve pretty much anything, right?

978
00:45:41,819 --> 00:45:45,919
Like, you know, if you can get kind of people all marching in towards like a goal

979
00:45:45,919 --> 00:45:48,519
uh, in a very focused way, in a mission-driven way

980
00:45:48,519 --> 00:45:49,859
you can pretty much achieve anything.

981
00:45:49,859 --> 00:45:54,319
But it's very tough to organize all of humanity that way.

982
00:45:54,319 --> 00:45:57,239
But I think if PDOOM is actually high

983
00:45:57,239 --> 00:46:00,399
at some point all of humanity is, like

984
00:46:00,399 --> 00:46:02,799
aligned in making sure that's not the case

985
00:46:02,799 --> 00:46:03,139
right?

986
00:46:03,139 --> 00:46:06,139
And so we'll actually make more progress against it

987
00:46:06,139 --> 00:46:06,559
I think.

988
00:46:06,559 --> 00:46:12,559
So the irony is, so there is a self-modulating aspect there.

989
00:46:12,559 --> 00:46:16,859
Like, I think if humanity collectively puts their mind to solving a problem

990
00:46:16,859 --> 00:46:18,879
whatever it is, I think we can get there.

991
00:46:18,879 --> 00:46:22,259
So because of that, you know, I

992
00:46:22,259 --> 00:46:26,639
I w-, I, I, I think I'm optimistic on the PDOOM scenarios

993
00:46:26,639 --> 00:46:28,999
but that doesn't mean.

994
00:46:28,999 --> 00:46:29,119
..

995
00:46:29,119 --> 00:46:31,659
I think the underlying risk is actually pretty high.

996
00:46:31,659 --> 00:46:37,839
But I'm, uh, you know, I have a lot of faith in humanity kind of rising up to the

997
00:46:37,839 --> 00:46:39,059
to meet that moment.

998
00:46:39,059 --> 00:46:40,859
That's really, really well put.

999
00:46:40,859 --> 00:46:43,919
I mean, as the threat becomes more concrete and real

1000
00:46:43,919 --> 00:46:47,839
humans do really come together and get their shit together.

1001
00:46:47,839 --> 00:46:54,139
Well, the other thing I think people don't often talk about is probability of doom without AI.

1002
00:46:54,139 --> 00:46:58,279
So there's all these other ways that humans can destroy themselves and it's very possible

1003
00:46:58,279 --> 00:47:03,399
at least I believe so, that AI will help us become smarter

1004
00:47:03,399 --> 00:47:07,679
kinder to each other, uh, more efficient.

1005
00:47:07,679 --> 00:47:13,459
Uh, it, it'll help more parts of the world flourish where it would be less resource constrained

1006
00:47:13,459 --> 00:47:17,739
which is often the source of military conflict and tensions and so on.

1007
00:47:17,739 --> 00:47:23,139
So we a- also have to load into that what's the PDOOM without AI?

1008
00:47:23,139 --> 00:47:30,199
With AI, PDOOM with AI and PDOOM without AI  'cause it's very possible that AI will be the thing that saves us

1009
00:47:30,199 --> 00:47:32,339
saves human civilizations from all the other threats.

1010
00:47:32,339 --> 00:47:33,399
I agree with you.

1011
00:47:33,399 --> 00:47:34,879
I think, I think it's insightful.

1012
00:47:34,879 --> 00:47:41,119
Uh, look, I felt, like, to make progress on some of the toughest problems would be good to have AI

1013
00:47:41,119 --> 00:47:43,859
like, pair helping you, right?

1014
00:47:43,859 --> 00:47:47,919
And, and, like, you know, so that resonates with me for sure.

1015
00:47:47,919 --> 00:47:48,199
Yeah.

1016
00:47:48,199 --> 00:47:48,959
Quick pause.

1017
00:47:48,959 --> 00:47:49,579
Bathroom break?

1018
00:47:49,579 --> 00:47:50,339
Uh, you know.

1019
00:47:50,339 --> 00:47:51,959
  Let's do that.

1020
00:47:51,959 --> 00:47:53,299
Let's do that.

1021
00:47:53,299 --> 00:47:55,299
If Notebook LM was the same com- like

1022
00:47:55,299 --> 00:47:58,839
what I saw today with Beam, if it was compelling in the same kind of way.

1023
00:47:58,839 --> 00:48:02,679
Blew my mind.

1024
00:48:02,679 --> 00:48:03,779
It was incredible.

1025
00:48:03,779 --> 00:48:04,719
I didn't think it's possible.

1026
00:48:04,719 --> 00:48:06,599
I didn't think it's possible before.

1027
00:48:06,599 --> 00:48:07,239
One day, my hope is, like, can you imagine

1028
00:48:07,239 --> 00:48:14,859
like, the US president and the Chinese president being able to do something like Beam with the LiveMe translation working well?

1029
00:48:14,859 --> 00:48:19,119
So they're both sitting and talking, make progress a bit more.

1030
00:48:19,119 --> 00:48:21,959
 Uh, yeah, just, uh, for people listening

1031
00:48:21,959 --> 00:48:25,159
we took a quick bathroom break and now we're talking about the demo I did.

1032
00:48:25,159 --> 00:48:28,659
We'll probably post it somewhere somehow, maybe here.

1033
00:48:28,659 --> 00:48:31,759
The, I got a chance to experience Beam and it was

1034
00:48:31,759 --> 00:48:39,019
uh, i- it's hard to, it's hard to describe it in words how real it felt with just

1035
00:48:39,019 --> 00:48:40,709
what is it, six cameras.

1036
00:48:40,709 --> 00:48:42,315
It's incredible, it's incredible.

1037
00:48:42,315 --> 00:48:44,512
It's, it's one of the toughest products of.

1038
00:48:44,512 --> 00:48:44,656
..

1039
00:48:44,656 --> 00:48:46,695
You can't quite describe it to people.

1040
00:48:46,695 --> 00:48:49,395
Even when we show i- it in slides

1041
00:48:49,395 --> 00:48:51,676
it's like you don't know what it is.

1042
00:48:51,676 --> 00:48:53,755
You have to kind of experience it.

1043
00:48:53,755 --> 00:48:56,495
On the world leaders front, um, politics

1044
00:48:56,495 --> 00:49:02,416
geopolitics, there- there's something really special, again with studying World War II and

1045
00:49:02,416 --> 00:49:07,096
uh, how much could've been saved if Chamberlain met Stalin in person.

1046
00:49:07,096 --> 00:49:10,535
And I sometimes also struggle explaining to people

1047
00:49:10,535 --> 00:49:14,916
articulating why I believe meeting in person for world leaders is powerful.

1048
00:49:14,916 --> 00:49:25,795
It just seems naive to say that, but there is something there in person and with Beam I- I felt that same thing and then I'm unable to explain.

1049
00:49:25,795 --> 00:49:28,735
All I kept doing is what like a child does-  .

1050
00:49:28,735 --> 00:49:28,775
..

1051
00:49:28,775 --> 00:49:31,075
you look real  you know?

1052
00:49:31,075 --> 00:49:36,396
And I mean, I don't know if that makes meetings more productive or so on

1053
00:49:36,396 --> 00:49:39,831
but it certainly makes them more, uh.

1054
00:49:39,831 --> 00:49:39,975
..

1055
00:49:39,975 --> 00:49:44,015
The- the same reason you wanna show up to work versus remote sometimes

1056
00:49:44,015 --> 00:49:45,435
that human connection.

1057
00:49:45,435 --> 00:49:46,735
I don't know what that is.

1058
00:49:46,735 --> 00:49:48,715
It's hard to- it's hard to put into words.

1059
00:49:48,715 --> 00:50:03,595
Um, there's some- there's something beautiful about great teams collaborating on a thing that's- that's not captured by the productivity of that team or whatever on paper.

1060
00:50:03,595 --> 00:50:06,935
Some of the most beautiful moments you experience in life is at work.

1061
00:50:06,935 --> 00:50:11,435
Pursuing a difficult thing together for many months- Oh.

1062
00:50:11,435 --> 00:50:11,521
.

1063
00:50:11,521 --> 00:50:11,693
..

1064
00:50:11,693 --> 00:50:13,055
there's nothing like it.

1065
00:50:13,055 --> 00:50:14,835
You're in the trenches and- Yeah.

1066
00:50:14,835 --> 00:50:14,860
.

1067
00:50:14,860 --> 00:50:14,910
..

1068
00:50:14,910 --> 00:50:17,075
yeah, you do form bonds that way, for sure.

1069
00:50:17,075 --> 00:50:18,775
And to be able to do that, like

1070
00:50:18,775 --> 00:50:21,415
somewhat, uh, remotely in that same personal touch

1071
00:50:21,415 --> 00:50:23,075
I don't know, that's a deeply fulfilling thing.

1072
00:50:23,075 --> 00:50:28,395
I know a lot of peop- I- I personally hate meetings because a significant percent of meetings when done

1073
00:50:28,395 --> 00:50:32,535
uh, poorly are- are- don't- don't serve a clear purpose.

1074
00:50:32,535 --> 00:50:36,755
So- but that's a meeting problem, that's not a communication problem.

1075
00:50:36,755 --> 00:50:39,955
If you can improve the communication for the meetings that are useful

1076
00:50:39,955 --> 00:50:41,855
it's just incredible.

1077
00:50:41,855 --> 00:50:49,075
So yeah, I was blown away by the great engineering behind it and then we- we get to see what impact that has that's really interesting

1078
00:50:49,075 --> 00:50:50,515
but just incredible engineering.

1079
00:50:50,515 --> 00:50:51,315
Really impressive.

1080
00:50:51,315 --> 00:50:55,975
Oh, it is, and obviously we'll work hard over the years to make it more and more accessible.

1081
00:50:55,975 --> 00:50:59,435
But yeah, even on a personal front outside of work meetings

1082
00:50:59,435 --> 00:51:04,935
you know, a grandmother who's far away from her grandchild and being able to

1083
00:51:04,935 --> 00:51:07,575
you know, have that kind of an interaction

1084
00:51:07,575 --> 00:51:08,535
right?

1085
00:51:08,535 --> 00:51:10,849
All that I think will end up being very.

1086
00:51:10,849 --> 00:51:10,935
..

1087
00:51:10,935 --> 00:51:13,255
I mean, nothing substitutes being in person

1088
00:51:13,255 --> 00:51:15,515
but you know, it's not always possible.

1089
00:51:15,515 --> 00:51:20,015
You know, you could be a soldier deployed try- trying to talk to your loved one.

1090
00:51:20,015 --> 00:51:24,175
So I think, uh, you know, so that's what inspires us.

1091
00:51:24,175 --> 00:51:29,775
When you and I hung out last year and took a walk

1092
00:51:29,775 --> 00:51:33,915
I remember, I don't think we talked about this but  but I remember

1093
00:51:33,915 --> 00:51:39,975
uh, you know, outside of that seeing dozens of articles written by analysts and experts and so on that

1094
00:51:39,975 --> 00:51:48,875
um, Sundar Pichai should step down because the perception was that Google was definitively losing the AI race

1095
00:51:48,875 --> 00:51:52,415
has lost its magic touch in the, uh

1096
00:51:52,415 --> 00:51:56,175
rapidly evolving, uh, technological, uh, landscape.

1097
00:51:56,175 --> 00:51:59,075
And now a year later, it's crazy

1098
00:51:59,075 --> 00:52:03,615
you show this plot of all the things that were shipped over the past year

1099
00:52:03,615 --> 00:52:08,275
it's incredible, and Gemini Pro is winning across many benchmarks and products

1100
00:52:08,275 --> 00:52:10,015
uh, as we sit here today.

1101
00:52:10,015 --> 00:52:13,915
So take me through that experience when there's all these articles saying

1102
00:52:13,915 --> 00:52:17,675
"You're the wrong guy to lead Google through this.

1103
00:52:17,675 --> 00:52:20,781
Google is lost, it's done, it's over.

1104
00:52:20,781 --> 00:52:23,855
" To today where Google is winning again.

1105
00:52:23,855 --> 00:52:26,255
What were some low points during that time?

1106
00:52:26,255 --> 00:52:28,555
Look, I, um.

1107
00:52:28,555 --> 00:52:28,795
..

1108
00:52:28,795 --> 00:52:31,935
I mean, lots to unpack.

1109
00:52:31,935 --> 00:52:36,775
Uh, you know, obviously, like, I mean

1110
00:52:36,775 --> 00:52:40,375
the- the main bet I made as a CEO was to really

1111
00:52:40,375 --> 00:52:46,015
uh, you know, make sure the company was approaching everything in a AI first way

1112
00:52:46,015 --> 00:52:50,835
really, you know, setting ourselves up to develop AGI responsibly

1113
00:52:50,835 --> 00:52:51,395
right?

1114
00:52:51,395 --> 00:52:54,815
And- and- and make sure we're putting out products

1115
00:52:54,815 --> 00:52:58,715
uh, which- which embodies that, things that are very

1116
00:52:58,715 --> 00:52:59,875
very useful for people.

1117
00:52:59,875 --> 00:53:04,795
So look, I- I knew even through moments like that last year

1118
00:53:04,795 --> 00:53:11,115
uh, you know, I had a good sense of what we were building internally

1119
00:53:11,115 --> 00:53:11,735
right?

1120
00:53:11,735 --> 00:53:16,775
So I'd already made, you know, many important decisions

1121
00:53:16,775 --> 00:53:23,095
you know, bringing together teams of the caliber of Brain and DeepMind and setting up Google DeepMind.

1122
00:53:23,095 --> 00:53:28,935
There were things like we made the decision to invest in TPUs 10 years ago

1123
00:53:28,935 --> 00:53:32,495
so we knew we were scaling up and building big models.

1124
00:53:32,495 --> 00:53:37,615
Anytime you're in a situation like that, a few aspects.

1125
00:53:37,615 --> 00:53:41,675
Uh, I'm good at tuning out noise

1126
00:53:41,675 --> 00:53:41,875
right?

1127
00:53:41,875 --> 00:53:43,275
Separating signal from noise.

1128
00:53:43,275 --> 00:53:45,115
Do you scuba dive?

1129
00:53:45,115 --> 00:53:46,235
Like have you- No.

1130
00:53:46,235 --> 00:53:46,475
No.

1131
00:53:46,475 --> 00:53:49,115
You know, it's amazing like I'm not good at it

1132
00:53:49,115 --> 00:53:53,855
but I've done it a few times, but- but sometimes you jump in the ocean

1133
00:53:53,855 --> 00:53:59,295
it's so choppy, but you go down one feet under

1134
00:53:59,295 --> 00:54:02,315
it's the calmest thing in the entire, uh

1135
00:54:02,315 --> 00:54:03,215
universe, right?

1136
00:54:03,215 --> 00:54:05,955
So there's a version of that, right?

1137
00:54:05,955 --> 00:54:09,655
Like, you know, uh, running Google

1138
00:54:09,655 --> 00:54:13,655
you know, you may as well be coaching Barcelona or Real Madrid

1139
00:54:13,655 --> 00:54:13,995
right?

1140
00:54:13,995 --> 00:54:14,635
 Like, you know?

1141
00:54:14,635 --> 00:54:14,935
Yeah.

1142
00:54:14,935 --> 00:54:16,095
You have a bad season.

1143
00:54:16,095 --> 00:54:18,355
So there are aspects to that, but you know

1144
00:54:18,355 --> 00:54:21,755
like, look, I- I'm good at tuning out the noise.

1145
00:54:21,755 --> 00:54:23,695
I do watch out for signals, you know?

1146
00:54:23,695 --> 00:54:25,735
It's important to separate the signal from the noise.

1147
00:54:25,735 --> 00:54:30,735
So there are good people sometimes making good points outside so you want to listen to it

1148
00:54:30,735 --> 00:54:32,195
you want to take that feedback in.

1149
00:54:32,195 --> 00:54:36,175
But, you know, internally, like, you know

1150
00:54:36,175 --> 00:54:39,315
you're making a set of consequential decisions, right?

1151
00:54:39,315 --> 00:54:42,575
As leaders, you're making a lot of decisions

1152
00:54:42,575 --> 00:54:46,175
many of them are like inconsequential.

1153
00:54:46,175 --> 00:54:46,375
..

1154
00:54:46,375 --> 00:54:48,795
. like it feels like, but over time you learn that.

1155
00:54:48,795 --> 00:54:53,315
Most of the decisions you're making on a day-to-day basis doesn't matter.

1156
00:54:53,315 --> 00:54:58,215
Like, you have to make them and you're making them just to keep things moving.

1157
00:54:58,215 --> 00:55:01,135
But you have to make a few consequential decisions

1158
00:55:01,135 --> 00:55:01,475
right?

1159
00:55:01,475 --> 00:55:08,235
And, and, uh, we had set up the right teams

1160
00:55:08,235 --> 00:55:13,176
right leaders, we had world-class researchers, we

1161
00:55:13,176 --> 00:55:14,675
uh, were training Gemini.

1162
00:55:14,675 --> 00:55:17,635
Internally there are factors which were, for example

1163
00:55:17,635 --> 00:55:19,475
outside people may not have appreciated.

1164
00:55:19,475 --> 00:55:23,915
I mean, T-TPUs are amazing, but we had to ramp up TPUs too.

1165
00:55:23,915 --> 00:55:26,515
That took time, right?

1166
00:55:26,515 --> 00:55:32,296
And, and, uh, to scale actually having enough TPUs to get the compute needed.

1167
00:55:32,296 --> 00:55:36,616
But I could see internally the trajectory we were on

1168
00:55:36,616 --> 00:55:42,855
and, and, be, you know, I was so excited internally about the possibility.

1169
00:55:42,855 --> 00:55:42,955
..

1170
00:55:42,955 --> 00:55:48,375
To me, this moment felt like one of the biggest opportunities ahead for us as a company.

1171
00:55:48,375 --> 00:55:49,136
Mm-hmm.

1172
00:55:49,136 --> 00:55:52,475
That the opportunity space ahead, or the next decade

1173
00:55:52,475 --> 00:55:55,696
next 20 years has been given what has happened in the past.

1174
00:55:55,696 --> 00:55:58,656
Um, and I thought we were set up

1175
00:55:58,656 --> 00:56:02,975
like, better than most companies in the world to go

1176
00:56:02,975 --> 00:56:04,595
uh, realize that vision.

1177
00:56:04,595 --> 00:56:09,115
I mean, you had to make some consequential

1178
00:56:09,115 --> 00:56:10,095
bold decisions.

1179
00:56:10,095 --> 00:56:13,175
Like, you mentioned the merger of DeepMind and Brain.

1180
00:56:13,175 --> 00:56:18,476
Uh, maybe it's my perspective just knowing humans

1181
00:56:18,476 --> 00:56:20,436
I'm sure there's a lot of egos involved

1182
00:56:20,436 --> 00:56:25,115
it's very difficult to merge teams and I'm sure there were some hard decisions to be made.

1183
00:56:25,115 --> 00:56:28,115
Can you take me through your process of how you think through that?

1184
00:56:28,115 --> 00:56:30,735
Do you go to pull the trigger and make that decision?

1185
00:56:30,735 --> 00:56:33,136
Maybe what were some painful points?

1186
00:56:33,136 --> 00:56:36,055
How do you navigate those turbulent waters?

1187
00:56:36,055 --> 00:56:38,795
Look, we were fortunate to have two world-class teams.

1188
00:56:38,795 --> 00:56:41,955
Uh, but you're right, like, it's like somebody coming and telling to you

1189
00:56:41,955 --> 00:56:44,435
"Take Stanford and MIT-" Right.

1190
00:56:44,435 --> 00:56:44,495
".

1191
00:56:44,495 --> 00:56:44,555
..

1192
00:56:44,555 --> 00:56:46,902
and then put them together and create a great department.

1193
00:56:46,902 --> 00:56:47,276
" Right?

1194
00:56:47,276 --> 00:56:49,415
And, and easier said than done.

1195
00:56:49,415 --> 00:56:51,915
Um, but we were fortunate, you know

1196
00:56:51,915 --> 00:56:56,356
phenomenal teams, both had their strengths, you know

1197
00:56:56,356 --> 00:56:58,155
but they were run very differently, right?

1198
00:56:58,155 --> 00:57:03,435
Like, uh, Brain was kind of a lot of diverse projects

1199
00:57:03,435 --> 00:57:07,515
bottoms up, and out of it came a lot of important research breakthroughs.

1200
00:57:07,515 --> 00:57:15,475
DeepMind at the time had a strong vision of how you wanna build AGI and so they were pursuing their direction.

1201
00:57:15,475 --> 00:57:19,675
But I think through those moments, luckily tapping into

1202
00:57:19,675 --> 00:57:23,835
um, you know, Geoff had expressed a desire to be more

1203
00:57:23,835 --> 00:57:26,615
to go back to more of a scientific- Mm-hmm.

1204
00:57:26,615 --> 00:57:26,648
.

1205
00:57:26,648 --> 00:57:26,715
..

1206
00:57:26,715 --> 00:57:28,675
individual contributor routes.

1207
00:57:28,675 --> 00:57:31,776
You know, he felt like management was taking up too much of his time

1208
00:57:31,776 --> 00:57:35,155
uh, and, and, and Demis naturally

1209
00:57:35,155 --> 00:57:36,975
I think, uh, you know, uh

1210
00:57:36,975 --> 00:57:40,515
was running DeepMind and w- was a natural choice there.

1211
00:57:40,515 --> 00:57:42,028
But I think it was.

1212
00:57:42,028 --> 00:57:42,175
..

1213
00:57:42,175 --> 00:57:45,035
You're right, you know, it took us a while to bring the teams together

1214
00:57:45,035 --> 00:57:48,815
credit to Demis, Geoff, Koray, all the great people there.

1215
00:57:48,815 --> 00:57:55,455
They worked super hard to combine the best of both worlds when you set up that team.

1216
00:57:55,455 --> 00:57:59,535
A few sleepless nights here and there as we put that thing together.

1217
00:57:59,535 --> 00:58:05,815
We were patient in how we did it so that it works well for the long term

1218
00:58:05,815 --> 00:58:06,895
right?

1219
00:58:06,895 --> 00:58:09,475
And, and some of that in that moment

1220
00:58:09,475 --> 00:58:11,855
I think, yes, with things moving fast

1221
00:58:11,855 --> 00:58:14,955
uh, I think you definitely, uh, felt the pressure

1222
00:58:14,955 --> 00:58:17,135
but I think we pulled off that, uh

1223
00:58:17,135 --> 00:58:19,215
transition well, and, you know, I think

1224
00:58:19,215 --> 00:58:21,355
I think, uh, you know, they've obviously

1225
00:58:21,355 --> 00:58:26,475
uh, doing incredible work and there's a lot more incredible things I heard coming from them.

1226
00:58:26,475 --> 00:58:28,795
Like we talked about, you have a very calm

1227
00:58:28,795 --> 00:58:30,635
even-tempered, respectful demeanor.

1228
00:58:30,635 --> 00:58:37,375
During that time, whether it's the merger or just dealing with the noise

1229
00:58:37,375 --> 00:58:41,115
uh, did, w- were there times where frustration boiled over?

1230
00:58:41,115 --> 00:58:47,875
Like, did you, uh, have to go a bit more intense on everybody than you usually would?

1231
00:58:47,875 --> 00:58:50,695
Probably, y- you know, probably, right.

1232
00:58:50,695 --> 00:58:52,235
I think, I think in the sense that

1233
00:58:52,235 --> 00:58:54,835
you know, there was a moment where we were all driving hard

1234
00:58:54,835 --> 00:58:57,915
but when you're in the trenches working with passion

1235
00:58:57,915 --> 00:59:02,275
you're gonna have days, right, you disagree

1236
00:59:02,275 --> 00:59:05,295
you argue, but, like, all that

1237
00:59:05,295 --> 00:59:09,175
I mean, just part of the course of working intensely

1238
00:59:09,175 --> 00:59:10,055
right?

1239
00:59:10,055 --> 00:59:12,915
And, uh, you know, at the end of the day

1240
00:59:12,915 --> 00:59:16,495
all of us are doing what we are doing because- uh

1241
00:59:16,495 --> 00:59:19,595
the impact it can have, we are motivated by it.

1242
00:59:19,595 --> 00:59:21,995
It's like, uh, you know, for many of us

1243
00:59:21,995 --> 00:59:25,115
this has been a long-term, uh, journey.

1244
00:59:25,115 --> 00:59:30,515
And so it's been super exciting, the positive moments far outweigh the

1245
00:59:30,515 --> 00:59:31,755
kind of, stressful moments.

1246
00:59:31,755 --> 00:59:37,255
Just early this year, I had a chance to celebrate back to back over two days

1247
00:59:37,255 --> 00:59:40,415
like, uh, you know, Nobel Prize for Geoff Hinton

1248
00:59:40,415 --> 00:59:42,535
and the next day a Nobel Prize for

1249
00:59:42,535 --> 00:59:44,355
uh, Demis and John Jumper.

1250
00:59:44,355 --> 00:59:46,995
You know, you work with people like that

1251
00:59:46,995 --> 00:59:48,455
all that is super inspiring.

1252
00:59:48,455 --> 00:59:50,955
Is there something, like, with you where you had to

1253
00:59:50,955 --> 00:59:54,515
like, put your foot down, maybe with Les

1254
00:59:54,515 --> 01:00:00,815
uh, versus more, or, like, "I'm the CEO and we're ma- we're doing this"?

1255
01:00:00,815 --> 01:00:04,055
To my earlier point about consequential decisions you make

1256
01:00:04,055 --> 01:00:08,675
there are decisions you make, people can disagree pretty vehemently and.

1257
01:00:08,675 --> 01:00:08,875
..

1258
01:00:08,875 --> 01:00:11,215
But at some point, like, you know

1259
01:00:11,215 --> 01:00:16,675
you make a clear decision and you, you just ask people to commit

1260
01:00:16,675 --> 01:00:17,395
right?

1261
01:00:17,395 --> 01:00:21,875
Like, you know, you can disagree, but it's time to disagree and commit so- Yeah.

1262
01:00:21,875 --> 01:00:21,881
.

1263
01:00:21,881 --> 01:00:21,895
..

1264
01:00:21,895 --> 01:00:22,795
that we can get moving.

1265
01:00:22,795 --> 01:00:26,675
And whether it's put- putting the foot down or

1266
01:00:26,675 --> 01:00:30,035
you know, like, you know, it's a natural part of what all of us have to do

1267
01:00:30,035 --> 01:00:36,615
and, you know, I think you can do that calmly and  be very firm in the direction you're making the decision.

1268
01:00:36,615 --> 01:00:40,775
And I think if you're clear, actually people over time respect that

1269
01:00:40,775 --> 01:00:40,995
right?

1270
01:00:40,995 --> 01:00:43,115
Like, you know, if you can make decisions with clarity

1271
01:00:43,115 --> 01:00:50,841
I find it very effective in meetings where you're making such decisions to hear everyone out.

1272
01:00:50,841 --> 01:00:54,823
I think it's important when you can to hear everyone out.

1273
01:00:54,823 --> 01:01:00,603
Sometimes what you're hearing actually influences how you think about and you're wrestling with it and making a decision.

1274
01:01:00,603 --> 01:01:04,643
Sometimes you have a clear conviction and you state so

1275
01:01:04,643 --> 01:01:07,483
"Look, I, uh, I, you know

1276
01:01:07,483 --> 01:01:09,823
this is how I feel and, you know

1277
01:01:09,823 --> 01:01:13,043
this is my conviction," and you kind of place the bet and you move on.

1278
01:01:13,043 --> 01:01:14,883
Are there big decisions like that?

1279
01:01:14,883 --> 01:01:19,263
I'm kind of intuitively assume the merger was the big one.

1280
01:01:19,263 --> 01:01:21,283
I think that was a very important decision

1281
01:01:21,283 --> 01:01:23,604
uh, you know, for, for the company to

1282
01:01:23,604 --> 01:01:25,303
to meet the moment.

1283
01:01:25,303 --> 01:01:26,983
I think we had to make sure we were

1284
01:01:26,983 --> 01:01:29,464
uh, we were doing that and doing that well.

1285
01:01:29,464 --> 01:01:31,684
I think that was a consequential decision.

1286
01:01:31,684 --> 01:01:32,944
There were many other things.

1287
01:01:32,944 --> 01:01:36,044
We set up a AI infrastructure team, like

1288
01:01:36,044 --> 01:01:39,903
to really go meet the moment, to scale up the compute we needed to

1289
01:01:39,903 --> 01:01:43,463
and really brought teams from disparate parts of the company

1290
01:01:43,463 --> 01:01:46,743
kind of created it to, to move forward.

1291
01:01:46,743 --> 01:01:51,463
Um, you know, bringing people, like

1292
01:01:51,463 --> 01:01:54,244
getting people to kind of work together physically

1293
01:01:54,244 --> 01:01:58,983
both in London with DeepMind and what we call Gradient Canopy

1294
01:01:58,983 --> 01:02:02,843
which is where the Mountain View Google DeepMind teams are.

1295
01:02:02,843 --> 01:02:06,504
But one of my favorite moments is I routinely walk

1296
01:02:06,504 --> 01:02:14,523
uh, like multiple times per week to the Gradient Canopy building where our top researchers are working on the models.

1297
01:02:14,523 --> 01:02:17,644
Sergey is often there amongst them, right?

1298
01:02:17,644 --> 01:02:19,303
 Like, you know, just, you know

1299
01:02:19,303 --> 01:02:23,403
loo- looking at, uh, you know, getting an update on the models

1300
01:02:23,403 --> 01:02:23,653
seeing ?

1301
01:02:23,653 --> 01:02:23,983
﹎s.

1302
01:02:23,983 --> 01:02:32,203
So all that, I think that cultural part of getting the teams together back with that energy I think ended up playing a big role too.

1303
01:02:32,203 --> 01:02:36,703
What about the decision to recently add AI mode?

1304
01:02:36,703 --> 01:02:40,843
So Google Search is the, uh, as they say

1305
01:02:40,843 --> 01:02:42,263
the front page of the internet.

1306
01:02:42,263 --> 01:02:48,143
It's like a legendary minimalist thing with 10 blue links

1307
01:02:48,143 --> 01:02:49,823
like, that's w- when people think internet

1308
01:02:49,823 --> 01:02:51,583
they think that page.

1309
01:02:51,583 --> 01:02:53,883
And now you're starting to mess with that.

1310
01:02:53,883 --> 01:02:56,363
So the AI mode, which is a separate tab

1311
01:02:56,363 --> 01:03:01,783
and then integrating AI in the results, I'm sure there were some battles in meetings on that one.

1312
01:03:01,783 --> 01:03:03,483
Look, uh, it, it, you know

1313
01:03:03,483 --> 01:03:06,783
in some ways when mobile came, you know

1314
01:03:06,783 --> 01:03:11,883
people wanted answers to more questions so we're kind of constantly evolving it.

1315
01:03:11,883 --> 01:03:13,523
But you're right, this moment, you know

1316
01:03:13,523 --> 01:03:18,883
that evolution, because the underlying technology is becoming much more capable

1317
01:03:18,883 --> 01:03:23,263
you know, you can have AI give a lot of context

1318
01:03:23,263 --> 01:03:23,823
you know.

1319
01:03:23,823 --> 01:03:27,043
But one of our important design goals though is when you come to Google Search

1320
01:03:27,043 --> 01:03:30,703
you're going to get a lot of context

1321
01:03:30,703 --> 01:03:33,683
but you're going to go and find a lot of things out on the web.

1322
01:03:33,683 --> 01:03:36,103
So that will be true in AI mode

1323
01:03:36,103 --> 01:03:37,723
in AI overviews, and so on.

1324
01:03:37,723 --> 01:03:43,263
But I think to our earlier conversation, we are still giving you access to links

1325
01:03:43,263 --> 01:03:47,963
but think of the AI as a layer which is giving you context

1326
01:03:47,963 --> 01:03:53,443
summary, maybe in AI mode you can have a dialogue with it back and forth- Mm-hmm.

1327
01:03:53,443 --> 01:03:53,509
.

1328
01:03:53,509 --> 01:03:53,643
..

1329
01:03:53,643 --> 01:03:56,003
on your journey, right?

1330
01:03:56,003 --> 01:03:59,983
And, but through it all, you're kind of learning what's out there in the world.

1331
01:03:59,983 --> 01:04:07,323
So those core principles don't change, but I think AI mode allows us to push the f- we have our best models there

1332
01:04:07,323 --> 01:04:08,323
right?

1333
01:04:08,323 --> 01:04:14,663
Uh, models which are using search as a deep tool really for every query you're asking

1334
01:04:14,663 --> 01:04:17,463
kind of fanning out, doing multiple searches

1335
01:04:17,463 --> 01:04:22,703
like, kind of assembling that knowledge in a way so you can go and consume what you want to

1336
01:04:22,703 --> 01:04:23,043
right?

1337
01:04:23,043 --> 01:04:24,783
And, and, and that's how we think about it.

1338
01:04:24,783 --> 01:04:26,883
I got a chance to listen to a bunch of

1339
01:04:26,883 --> 01:04:29,043
uh, Elizabeth "Liz" Reid- Yeah.

1340
01:04:29,043 --> 01:04:29,096
.

1341
01:04:29,096 --> 01:04:29,203
..

1342
01:04:29,203 --> 01:04:32,123
describe this, two things stood out to me that she mentioned.

1343
01:04:32,123 --> 01:04:35,443
One thing is what you were talking about is the query fan-out

1344
01:04:35,443 --> 01:04:40,163
which I didn't even think about before, uh

1345
01:04:40,163 --> 01:04:45,543
is the, the powerful aspect of integrating a bunch of stuff on the web for you in one place.

1346
01:04:45,543 --> 01:04:51,743
So yes, it provides that context so that you can decide which page to then go on to.

1347
01:04:51,743 --> 01:04:59,683
The other really, really big thing speaks to the earlier in terms of productivity multiplier that we were talking about that she mentioned was

1348
01:04:59,683 --> 01:05:00,843
um, language.

1349
01:05:00,843 --> 01:05:03,963
So one of the things you don't quite understand is it

1350
01:05:03,963 --> 01:05:09,503
through AI mode, you make, for non-English speakers

1351
01:05:09,503 --> 01:05:15,363
you make sort of, let's say, English language websites accessible by

1352
01:05:15,363 --> 01:05:20,443
in the reasoning process as you try to figure out what you're looking for.

1353
01:05:20,443 --> 01:05:22,083
Of course, once you show up to a page

1354
01:05:22,083 --> 01:05:23,563
you can use a basic translate.

1355
01:05:23,563 --> 01:05:23,783
Yeah.

1356
01:05:23,783 --> 01:05:31,103
But that process of figuring it out, if you empathize with a large part of the world that doesn't speak English

1357
01:05:31,103 --> 01:05:37,263
their, like, web, uh, is much smaller in that original language.

1358
01:05:37,263 --> 01:05:42,508
And so it unlocks, again, unlocks that huge cognitive capacity there that we don't.

1359
01:05:42,508 --> 01:05:42,583
..

1360
01:05:42,583 --> 01:05:47,163
You know, you take for granted here with all the bloggers and the journalists writing about AI mode

1361
01:05:47,163 --> 01:05:50,343
you forget that this now unlocks, um

1362
01:05:50,343 --> 01:05:53,303
'c- c- 'cause Gemini is really good at translation.

1363
01:05:53,303 --> 01:05:54,623
Oh, it is.

1364
01:05:54,623 --> 01:05:57,423
I mean, uh, the multimodality, the translation

1365
01:05:57,423 --> 01:06:02,083
uh, its ability to reason, we are dramatically improving tool use.

1366
01:06:02,083 --> 01:06:06,363
Uh, like as, as of putting that power in the flow of search

1367
01:06:06,363 --> 01:06:08,043
I, I think.

1368
01:06:08,043 --> 01:06:08,183
..

1369
01:06:08,183 --> 01:06:12,443
Look, I'm, I'm super excited with the AI overviews we've

1370
01:06:12,443 --> 01:06:15,863
we've seen the product has gotten much better.

1371
01:06:15,863 --> 01:06:18,743
We, you know, we measure it using all kinds of user metrics.

1372
01:06:18,743 --> 01:06:22,183
It's obviously driven strong growth of the product.

1373
01:06:22,183 --> 01:06:26,603
Uh, and, you know, we've been testing AI mode

1374
01:06:26,603 --> 01:06:33,043
you know, it's now in the hands of millions of people and the early metrics are very encouraging.

1375
01:06:33,043 --> 01:06:36,403
So, look, I, I, I'm excited about this next chap- chapter of Search.

1376
01:06:36,403 --> 01:06:38,923
For people who are not thinking through or are aware of this

1377
01:06:38,923 --> 01:06:44,503
so there's the 10 blue links with the AI overview on top that provides a nice summarization.

1378
01:06:44,503 --> 01:06:45,343
You can expand it.

1379
01:06:45,343 --> 01:06:47,703
And you have sources and links- Links.

1380
01:06:47,703 --> 01:06:47,709
.

1381
01:06:47,709 --> 01:06:47,723
..

1382
01:06:47,723 --> 01:06:48,443
now- Yep.

1383
01:06:48,443 --> 01:06:48,469
.

1384
01:06:48,469 --> 01:06:48,523
..

1385
01:06:48,523 --> 01:06:49,263
embedded.

1386
01:06:49,263 --> 01:06:49,443
Yeah.

1387
01:06:49,443 --> 01:06:51,383
I believe, at least Liz said so

1388
01:06:51,383 --> 01:06:56,083
I actually didn't notice it, but there's ads in the AI overview also.

1389
01:06:56,083 --> 01:06:59,653
I don't think there's ads in AI mode.

1390
01:06:59,653 --> 01:07:00,121
..

1391
01:07:00,121 --> 01:07:02,715
. uh, when ads in AI mode.

1392
01:07:02,715 --> 01:07:03,215
So no.

1393
01:07:03,215 --> 01:07:04,275
 When, when do you think.

1394
01:07:04,275 --> 01:07:04,375
..

1395
01:07:04,375 --> 01:07:05,175
I mean, it's.

1396
01:07:05,175 --> 01:07:05,295
..

1397
01:07:05,295 --> 01:07:08,835
Okay, we should say that in the '90s

1398
01:07:08,835 --> 01:07:15,716
I remember the animated GIFs, banner GIFs that take you to some shady websites that have nothing to do with anything.

1399
01:07:15,716 --> 01:07:18,075
AdSense revolutionized advertisement.

1400
01:07:18,075 --> 01:07:20,695
It's one of the greatest inventions, um

1401
01:07:20,695 --> 01:07:28,635
in, in, in recent history because it allows us for free to have access to all these kinds of services.

1402
01:07:28,635 --> 01:07:31,136
So ads fuel a lot of really powerful services.

1403
01:07:31,136 --> 01:07:36,416
And in, at its best, it's showing you relevant ads

1404
01:07:36,416 --> 01:07:39,435
but also very importantly, in a way that's not super annoying.

1405
01:07:39,435 --> 01:07:39,695
Mm-hmm.

1406
01:07:39,695 --> 01:07:40,335
Right?

1407
01:07:40,335 --> 01:07:41,256
In a classy way.

1408
01:07:41,256 --> 01:07:47,475
So, uh, when do you think it's possible to add ads into AI mode

1409
01:07:47,475 --> 01:07:50,615
and what does that look like from a classy

1410
01:07:50,615 --> 01:07:51,835
non-annoying perspective?

1411
01:07:51,835 --> 01:07:53,095
Two things.

1412
01:07:53,095 --> 01:07:58,895
Early part of AI mode, uh, we'll obviously focus more on the organic experience to make sure we are getting it right.

1413
01:07:58,895 --> 01:08:07,475
I think the fundamental value of ads are it enables access to deploy the services to billions of people.

1414
01:08:07,475 --> 01:08:09,488
Second is ads are.

1415
01:08:09,488 --> 01:08:09,635
..

1416
01:08:09,635 --> 01:08:14,815
The reason we've always taken ads seriously is we view ads as commercial information

1417
01:08:14,815 --> 01:08:19,295
but it's still information, and so we bring the same quality metrics to it.

1418
01:08:19,295 --> 01:08:22,910
I think with AI mode, to our earlier conversation about.

1419
01:08:22,910 --> 01:08:23,015
..

1420
01:08:23,015 --> 01:08:27,635
I think AI itself will help us over time figure out

1421
01:08:27,635 --> 01:08:29,495
you know, the best way to do it.

1422
01:08:29,495 --> 01:08:34,095
I, I think given we are giving context around everything

1423
01:08:34,095 --> 01:08:34,251
we.

1424
01:08:34,251 --> 01:08:34,354
..

1425
01:08:34,354 --> 01:08:37,354
I think it'll give us more opportunities to also explain

1426
01:08:37,354 --> 01:08:45,515
"Okay, here's some commercial information," like today as a podcaster you do it at certain spots and you probably figure out what's best in your podcast.

1427
01:08:45,515 --> 01:08:50,055
Um, I, I think so there are aspects of that

1428
01:08:50,055 --> 01:08:55,415
but I think, you know, I think the underlying need of people value commercial information

1429
01:08:55,415 --> 01:09:01,455
businesses are trying to connect to users, all that doesn't change in a AI moment.

1430
01:09:01,455 --> 01:09:04,175
But look, we will rethink it.

1431
01:09:04,175 --> 01:09:09,194
You've seen us in YouTube now do a mixture of subscription and ads.

1432
01:09:09,194 --> 01:09:14,675
Like obviously, you know, we, we are now introducing subscription offerings

1433
01:09:14,675 --> 01:09:16,635
uh, across everything.

1434
01:09:16,635 --> 01:09:19,923
And so as part of that we can optimi-.

1435
01:09:19,923 --> 01:09:20,035
..

1436
01:09:20,035 --> 01:09:23,015
The optimization point will end up being a different place as well.

1437
01:09:23,015 --> 01:09:31,795
Do you see a trajectory in the possible future where AI mode completely replaces the 10 blue links plus AI overview?

1438
01:09:31,795 --> 01:09:38,555
Our current plan is AI mode is going to be there as a separate tab for people who really want to experience that

1439
01:09:38,555 --> 01:09:43,675
but it's not yet at the level where our main search page is.

1440
01:09:43,675 --> 01:09:48,475
But as features work, we'll keep migrating it to the main page

1441
01:09:48,475 --> 01:09:51,435
and so you can view it as a continuum.

1442
01:09:51,435 --> 01:09:54,515
AI mode will offer you the bleeding edge experience

1443
01:09:54,515 --> 01:09:56,030
but it'll.

1444
01:09:56,030 --> 01:09:56,135
..

1445
01:09:56,135 --> 01:10:01,095
Things that work will keep overflowing to AI overviews in the main

1446
01:10:01,095 --> 01:10:01,915
main experience.

1447
01:10:01,915 --> 01:10:04,635
And the idea that AI mode will still take you to the web

1448
01:10:04,635 --> 01:10:05,955
to the human-created web.

1449
01:10:05,955 --> 01:10:08,755
Yes, that's going to be a core design principle for us.

1450
01:10:08,755 --> 01:10:10,735
So really users decide, right?

1451
01:10:10,735 --> 01:10:11,855
They drive this.

1452
01:10:11,855 --> 01:10:12,195
Yeah.

1453
01:10:12,195 --> 01:10:17,175
It's just exciting, a little bit scary that it might change the internet

1454
01:10:17,175 --> 01:10:18,995
because you.

1455
01:10:18,995 --> 01:10:19,115
..

1456
01:10:19,115 --> 01:10:27,975
Google has been dominating with a very specific look and idea of what it means to have the internet and to

1457
01:10:27,975 --> 01:10:31,755
as you move to AI mode, I mean

1458
01:10:31,755 --> 01:10:33,515
I, it's just a different experience.

1459
01:10:33,515 --> 01:10:35,415
Um, I think Liz was talking about

1460
01:10:35,415 --> 01:10:39,175
I think you've mentioned that you ask more questions

1461
01:10:39,175 --> 01:10:41,775
you ask longer questions.

1462
01:10:41,775 --> 01:10:43,735
Dramatically different types of questions.

1463
01:10:43,735 --> 01:10:43,975
Yeah.

1464
01:10:43,975 --> 01:10:45,975
Like it actually fuels curiosity.

1465
01:10:45,975 --> 01:10:46,843
Like I think it's.

1466
01:10:46,843 --> 01:10:46,935
..

1467
01:10:46,935 --> 01:10:53,615
For me, I've been asking just a much larger number of questions of this black box machine

1468
01:10:53,615 --> 01:10:54,095
let's say.

1469
01:10:54,095 --> 01:10:54,255
Yeah.

1470
01:10:54,255 --> 01:10:54,995
Whatever it is.

1471
01:10:54,995 --> 01:10:58,555
And, and with AI overview, it's interesting

1472
01:10:58,555 --> 01:10:58,805
like I.

1473
01:10:58,805 --> 01:10:58,855
..

1474
01:10:58,855 --> 01:11:00,785
'cause I still value the human.

1475
01:11:00,785 --> 01:11:00,955
..

1476
01:11:00,955 --> 01:11:05,855
I still ultimately want to end up on the human-created web

1477
01:11:05,855 --> 01:11:07,315
but I.

1478
01:11:07,315 --> 01:11:07,435
..

1479
01:11:07,435 --> 01:11:09,615
like you said, the context really helps.

1480
01:11:09,615 --> 01:11:13,735
It helps us deliver higher quality referrals, right?

1481
01:11:13,735 --> 01:11:15,169
You know, where people are like.

1482
01:11:15,169 --> 01:11:15,255
..

1483
01:11:15,255 --> 01:11:18,215
They, they have much higher likelihood of finding what they're looking for.

1484
01:11:18,215 --> 01:11:22,975
They're exploring, they're curious, their intent is getting satisfied more.

1485
01:11:22,975 --> 01:11:23,361
So all.

1486
01:11:23,361 --> 01:11:23,435
..

1487
01:11:23,435 --> 01:11:24,835
That's what all our metrics show.

1488
01:11:24,835 --> 01:11:27,755
It makes the humans that create the web nervous.

1489
01:11:27,755 --> 01:11:29,032
The journalists are getting ner-.

1490
01:11:29,032 --> 01:11:29,095
..

1491
01:11:29,095 --> 01:11:31,555
They've already been nervous, like we mentioned.

1492
01:11:31,555 --> 01:11:33,875
CNN is nervous because of podcasts.

1493
01:11:33,875 --> 01:11:36,735
Um, it makes people nervous.

1494
01:11:36,735 --> 01:11:42,135
Look, I, I, I think news and journalism will play an important role

1495
01:11:42,135 --> 01:11:44,535
you know, in the future.

1496
01:11:44,535 --> 01:11:46,255
Uh, we are pretty committed to it

1497
01:11:46,255 --> 01:11:46,755
right?

1498
01:11:46,755 --> 01:11:50,308
And, uh, so I think making sure that ecosystem.

1499
01:11:50,308 --> 01:11:50,415
..

1500
01:11:50,415 --> 01:11:56,755
In fact, I think we'll be able to differentiate ourselves as a company over time because of our commitment there.

1501
01:11:56,755 --> 01:11:58,735
So it's, it's, it's something I think

1502
01:11:58,735 --> 01:12:00,775
you know, I definitely value a lot and

1503
01:12:00,775 --> 01:12:04,635
and as we are designing, we'll continue prioritizing approaches.

1504
01:12:04,635 --> 01:12:11,195
I'm sure for the people who want, they can have a fine-tuned AI model that's clickbait hit pieces

1505
01:12:11,195 --> 01:12:13,575
uh, that will replace current journalism.

1506
01:12:13,575 --> 01:12:15,495
Uh, that's a shot at journalism.

1507
01:12:15,495 --> 01:12:16,195
Forgive me.

1508
01:12:16,195 --> 01:12:21,155
Uh, but I, I find that if you're looking for really strong criticism of things

1509
01:12:21,155 --> 01:12:23,395
that Gemini is very good at providing that.

1510
01:12:23,395 --> 01:12:24,455
Oh, absolutely.

1511
01:12:24,455 --> 01:12:25,881
It's better than anything the.

1512
01:12:25,881 --> 01:12:25,995
..

1513
01:12:25,995 --> 01:12:30,215
For now, I mean, people are concerned that there will be bias that's introduced

1514
01:12:30,215 --> 01:12:33,315
that as the A- AI systems become more and more powerful

1515
01:12:33,315 --> 01:12:40,995
there's incentive from sponsors, uh, to roll in and try to control the output of the AI models.

1516
01:12:40,995 --> 01:12:45,795
Uh, but for now, the objective criticism that's provided is way better than journalism.

1517
01:12:45,795 --> 01:12:48,495
Of course, the argument is the journalists are still valuable

1518
01:12:48,495 --> 01:12:55,015
but then, uh, I don't know, the crowdsourced journalism that we get on the open internet is also very

1519
01:12:55,015 --> 01:12:55,635
very powerful.

1520
01:12:55,635 --> 01:12:58,855
I feel like they're all super important things.

1521
01:12:58,855 --> 01:13:04,021
I think it's good that you get a lot of crowdsourced information coming in.

1522
01:13:04,021 --> 01:13:04,625
..

1523
01:13:04,625 --> 01:13:10,127
. but I feel like there is real value for high-quality j- journalism

1524
01:13:10,127 --> 01:13:10,747
right?

1525
01:13:10,747 --> 01:13:14,628
And, and I think these are all complementary.

1526
01:13:14,628 --> 01:13:19,267
I think, like, I view it as I find myself constantly seeking out also

1527
01:13:19,267 --> 01:13:21,808
like, trying to find objective reporting on

1528
01:13:21,808 --> 01:13:29,067
on things too, uh, and, and sometimes you get more context from the crowdfunded sources you read online

1529
01:13:29,067 --> 01:13:31,387
but I think both end up playing a super important role.

1530
01:13:31,387 --> 01:13:34,067
So there's, uh, you've spoken a little about

1531
01:13:34,067 --> 01:13:34,727
about this.

1532
01:13:34,727 --> 01:13:37,168
Dennis talked about this, sort of the

1533
01:13:37,168 --> 01:13:43,408
the slice of the web that will increasingly become about providing information for agents

1534
01:13:43,408 --> 01:13:44,707
so what we can think about is, like

1535
01:13:44,707 --> 01:13:46,787
two layers of the web.

1536
01:13:46,787 --> 01:13:49,227
One is for humans, one is for agents.

1537
01:13:49,227 --> 01:13:52,807
Do you see the AI agents, do you

1538
01:13:52,807 --> 01:13:56,467
do you see the one that's for AI agents growing over time?

1539
01:13:56,467 --> 01:13:59,787
Do you see there still being long-term, five

1540
01:13:59,787 --> 01:14:05,887
ten years' value for the human-created, human-created-for-the-purpose-of-human-consumption web

1541
01:14:05,887 --> 01:14:08,327
or will it all be agents in the end?

1542
01:14:08,327 --> 01:14:12,887
Today, like, not everyone does, but

1543
01:14:12,887 --> 01:14:16,327
you know, you, you go to a sh- you go to a big retail store

1544
01:14:16,327 --> 01:14:18,467
you love walking the aisle, you love shopping

1545
01:14:18,467 --> 01:14:22,187
uh, or grocery store, picking out food

1546
01:14:22,187 --> 01:14:22,747
et cetera.

1547
01:14:22,747 --> 01:14:25,087
But you're also online shopping and they're delivering

1548
01:14:25,087 --> 01:14:25,707
right?

1549
01:14:25,707 --> 01:14:29,907
So both are complementary, and, like, that's true for restaurants

1550
01:14:29,907 --> 01:14:30,487
et cetera.

1551
01:14:30,487 --> 01:14:35,827
So I do feel like over time, websites will also get better for humans.

1552
01:14:35,827 --> 01:14:37,767
They will be better designed.

1553
01:14:37,767 --> 01:14:41,247
Uh, AI might actually design them better for humans.

1554
01:14:41,247 --> 01:14:46,887
So I, I expect the web to get a lot richer and more interesting and

1555
01:14:46,887 --> 01:14:48,267
uh, better to use.

1556
01:14:48,267 --> 01:14:56,067
At the same time, I think there'll be an agentic web which is also making a lot of progress

1557
01:14:56,067 --> 01:15:02,387
and you have to solve the business value and the incentives to make that work well

1558
01:15:02,387 --> 01:15:02,807
right?

1559
01:15:02,807 --> 01:15:04,407
Like for people to participate in it.

1560
01:15:04,407 --> 01:15:12,487
But I think both will co-exist, and obviously the agents may not need the sa- I mean

1561
01:15:12,487 --> 01:15:15,347
not may not, they won't need the same design and

1562
01:15:15,347 --> 01:15:19,467
uh, UI paradigms which humans need to interact with.

1563
01:15:19,467 --> 01:15:22,047
Uh, but I think both will, both will be there.

1564
01:15:22,047 --> 01:15:24,867
I have to ask you about Chrome.

1565
01:15:24,867 --> 01:15:27,087
Uh, I have to say for me personally

1566
01:15:27,087 --> 01:15:28,783
Google Chrome was probably.

1567
01:15:28,783 --> 01:15:28,907
..

1568
01:15:28,907 --> 01:15:31,427
I don't know.

1569
01:15:31,427 --> 01:15:33,487
I, I, I'd like to see where I would rank it

1570
01:15:33,487 --> 01:15:37,227
but in this temptation, and this is not recency bias

1571
01:15:37,227 --> 01:15:40,247
although it might be a little bit, but I think it's up there

1572
01:15:40,247 --> 01:15:44,067
top three, maybe the number one piece of software for me of all time.

1573
01:15:44,067 --> 01:15:45,327
This is incredible.

1574
01:15:45,327 --> 01:15:46,627
It's really incredible.

1575
01:15:46,627 --> 01:15:49,007
The browser is our window to the web

1576
01:15:49,007 --> 01:15:57,027
and Chrome really continued for many years, but even initially to push the innovation on that front when it was stale

1577
01:15:57,027 --> 01:16:00,947
and it continues to challenge, it continues to make it more

1578
01:16:00,947 --> 01:16:04,147
uh, performant, so efficient, and just innovate constantly.

1579
01:16:04,147 --> 01:16:07,187
Uh, and the, the, the Chromium aspect of it.

1580
01:16:07,187 --> 01:16:11,767
Anyway,  uh, you were one of the pioneers of Chrome

1581
01:16:11,767 --> 01:16:14,687
pushing for it when it was an insane idea.

1582
01:16:14,687 --> 01:16:20,347
Probably one of the ideas that was criticized and doubted and so on.

1583
01:16:20,347 --> 01:16:25,987
So can you tell me, um, the story of what it took to push for Chrome?

1584
01:16:25,987 --> 01:16:27,087
What was your vision?

1585
01:16:27,087 --> 01:16:34,187
Look, it was a, such a dynamic t- time

1586
01:16:34,187 --> 01:16:38,867
uh, you know, around 2004, 2005 with Ajax

1587
01:16:38,867 --> 01:16:42,987
the web suddenly becoming dynamic in a matter of few months.

1588
01:16:42,987 --> 01:16:49,767
Flickr, Gmail, Google Maps, all kind of came into existence

1589
01:16:49,767 --> 01:16:50,067
right?

1590
01:16:50,067 --> 01:16:53,087
Like the fact that you have an interactive dynamic web

1591
01:16:53,087 --> 01:16:55,987
the web was evolving from simple, uh

1592
01:16:55,987 --> 01:17:00,727
text pages, simple HTML to rich dynamic applications.

1593
01:17:00,727 --> 01:17:07,527
But at the same time, you could see the browser was never meant for that world

1594
01:17:07,527 --> 01:17:08,007
right?

1595
01:17:08,007 --> 01:17:10,247
Like JavaScript execution was super slow.

1596
01:17:10,247 --> 01:17:18,887
Uh, you know, the browser was far away from being an operating system for that rich modern web which is coming into

1597
01:17:18,887 --> 01:17:20,547
uh, coming into place.

1598
01:17:20,547 --> 01:17:23,727
So that's the opportunity we saw, like

1599
01:17:23,727 --> 01:17:26,067
uh, you know, it's an amazing early team.

1600
01:17:26,067 --> 01:17:33,807
I still remember the day we got a shell on WebKit running and how fast it was.

1601
01:17:33,807 --> 01:17:39,867
Uh, you know, we had the clear vision for building a browser

1602
01:17:39,867 --> 01:17:43,827
like we wanted to bring core OS principles into the browser

1603
01:17:43,827 --> 01:17:44,147
right?

1604
01:17:44,147 --> 01:17:48,887
Like, so we built a secure browser sandbox.

1605
01:17:48,887 --> 01:17:51,387
Each tab was its own, but n- these things are common now

1606
01:17:51,387 --> 01:17:54,427
but at the time, like, it was pretty unique.

1607
01:17:54,427 --> 01:17:58,027
Uh, we found an amazing team in Aarhus

1608
01:17:58,027 --> 01:18:01,747
Denmark, uh, with the leader who built the J- V8

1609
01:18:01,747 --> 01:18:03,007
the JavaScript VM- Mm-hmm.

1610
01:18:03,007 --> 01:18:03,073
.

1611
01:18:03,073 --> 01:18:03,207
..

1612
01:18:03,207 --> 01:18:06,187
which at the time was 25 times faster than

1613
01:18:06,187 --> 01:18:09,407
uh, e- e- any other JavaScript VM out there.

1614
01:18:09,407 --> 01:18:12,507
And by the way, you're right, we open sourced it all and

1615
01:18:12,507 --> 01:18:14,327
you know, and put it in Chromium too

1616
01:18:14,327 --> 01:18:18,547
but we really thought the web could work much better

1617
01:18:18,547 --> 01:18:23,647
um, uh, you know, much faster and you could be much safer browsing the web.

1618
01:18:23,647 --> 01:18:28,227
And the name Chrome came was because we literally felt people were like

1619
01:18:28,227 --> 01:18:32,547
the, the, the chrome of the browser was getting clunkier.

1620
01:18:32,547 --> 01:18:34,387
We wanted to minimize it.

1621
01:18:34,387 --> 01:18:36,347
And so that was the origins of the project.

1622
01:18:36,347 --> 01:18:42,887
Definitely obviously a highly biased person here talking about Chrome.

1623
01:18:42,887 --> 01:18:45,667
Uh, but, eh, you know, it's the most fun I've had

1624
01:18:45,667 --> 01:18:48,087
uh, building a product from the ground up and

1625
01:18:48,087 --> 01:18:50,307
you know, it, it was an extraordinary team.

1626
01:18:50,307 --> 01:18:53,627
Uh, I had, uh, my co-founders in the project

1627
01:18:53,627 --> 01:18:56,127
like, perfect, so definitely fond memories.

1628
01:18:56,127 --> 01:18:58,387
So for people who don't know, Sundar

1629
01:18:58,387 --> 01:19:01,647
I mean, it is probably fair to say that y- you're the reason we have Chrome.

1630
01:19:01,647 --> 01:19:03,987
Yes, I know there's a lot of incredible engineers

1631
01:19:03,987 --> 01:19:07,127
but pushing forward inside a company that probably was opposing it.

1632
01:19:07,127 --> 01:19:07,394
..

1633
01:19:07,394 --> 01:19:07,661
..

1634
01:19:07,661 --> 01:19:08,995
. c- 'cause it's a crazy idea.

1635
01:19:08,995 --> 01:19:13,315
Because, um, as everybody probably knows, it's incredibly difficult to build a browser.

1636
01:19:13,315 --> 01:19:16,335
Yeah, look, I, uh, Eric, who was the CEO at the time

1637
01:19:16,335 --> 01:19:18,715
I think i- i- it was less that he was opposed to it

1638
01:19:18,715 --> 01:19:22,695
he kind of first time knew w- what a crazy thing it is to go build a browser.

1639
01:19:22,695 --> 01:19:26,075
And so he definitely was like, this is

1640
01:19:26,075 --> 01:19:30,036
you know, the, uh, there was a crazy aspect to actually wanting to go build a browser.

1641
01:19:30,036 --> 01:19:33,935
But, um, he was very supportive.

1642
01:19:33,935 --> 01:19:36,445
Uh, you know, everyone, uh, all the founders where.

1643
01:19:36,445 --> 01:19:36,575
..

1644
01:19:36,575 --> 01:19:38,575
I think once we started, you know

1645
01:19:38,575 --> 01:19:41,835
building something and we could use it and see how much better

1646
01:19:41,835 --> 01:19:43,755
from then on, like, you know, you're

1647
01:19:43,755 --> 01:19:46,155
you're really tinkering with the product and making it better.

1648
01:19:46,155 --> 01:19:48,395
It came to life pretty fast.

1649
01:19:48,395 --> 01:19:50,635
What, uh, wisdom do you draw from that?

1650
01:19:50,635 --> 01:19:56,376
From, um, pushing through on a crazy idea in the early days that ends up being revolutionary?

1651
01:19:56,376 --> 01:20:00,035
W- what, for future crazy ideas like it?

1652
01:20:00,035 --> 01:20:05,295
I mean, this, this is something Larry and Sergey have articulated clearly

1653
01:20:05,295 --> 01:20:08,175
I really interna- internalized this early on, which is

1654
01:20:08,175 --> 01:20:12,235
you know, their whole feeling around working on moonshots

1655
01:20:12,235 --> 01:20:16,315
like, as a way, when you work on something very ambitious

1656
01:20:16,315 --> 01:20:18,955
first of all, it attracts the best people

1657
01:20:18,955 --> 01:20:19,775
right?

1658
01:20:19,775 --> 01:20:21,075
So that's an advantage you get.

1659
01:20:21,075 --> 01:20:26,415
Number two, because it's so ambitious, you don't have others working on something crazy

1660
01:20:26,415 --> 01:20:28,295
so you pretty much have the path to yourselves

1661
01:20:28,295 --> 01:20:28,935
right?

1662
01:20:28,935 --> 01:20:30,295
It's like Waymo and self-driving.

1663
01:20:30,295 --> 01:20:37,415
Number three, it is, even if you end up quite not accomplishing what you set out to do and you end up doing 60%

1664
01:20:37,415 --> 01:20:40,455
80% of it, it'll end up being a terrific success.

1665
01:20:40,455 --> 01:20:44,915
So, so, you know, that's the advice I would give people

1666
01:20:44,915 --> 01:20:45,115
right?

1667
01:20:45,115 --> 01:20:51,555
I think like, you know, it's just aiming for big ideas has all these advantages and

1668
01:20:51,555 --> 01:20:56,215
and it's risky, but it also has all these advantages which people

1669
01:20:56,215 --> 01:20:57,475
I don't think, fully internalize.

1670
01:20:57,475 --> 01:20:59,415
I mean, you mentioned one of the craziest

1671
01:20:59,415 --> 01:21:01,295
biggest moonshots, which is Waymo.

1672
01:21:01,295 --> 01:21:04,595
Uh, it's one when I first saw

1673
01:21:04,595 --> 01:21:07,615
over a decade ago, a Waymo vehicle

1674
01:21:07,615 --> 01:21:10,615
a Google self-driving car vehicle.

1675
01:21:10,615 --> 01:21:13,455
I, it was, it was, for me

1676
01:21:13,455 --> 01:21:15,815
it was an a-ha moment for robotics.

1677
01:21:15,815 --> 01:21:19,535
It made me fall in love with robotics even more than before

1678
01:21:19,535 --> 01:21:20,815
it gave me a glimpse into the future.

1679
01:21:20,815 --> 01:21:21,555
So it's incredible.

1680
01:21:21,555 --> 01:21:24,995
I'm truly grateful for that project, for what it symbolizes.

1681
01:21:24,995 --> 01:21:26,835
But it's also a crazy moonshot.

1682
01:21:26,835 --> 01:21:30,755
'Cause for, for a long time,  Waymo's been just

1683
01:21:30,755 --> 01:21:32,895
uh, like you mentioned with scuba diving

1684
01:21:32,895 --> 01:21:38,515
just not listening to anybody, just calmly improving the system better and better

1685
01:21:38,515 --> 01:21:43,635
more testing, just expanding, uh, the operational domain more and more.

1686
01:21:43,635 --> 01:21:47,415
First of all, congrats on, uh, 10 million paid robotaxi rides.

1687
01:21:47,415 --> 01:21:53,275
Uh, what lessons do you take from Waymo about

1688
01:21:53,275 --> 01:21:56,155
like, the, the perseverance, the persistence on that project?

1689
01:21:56,155 --> 01:21:58,615
Well, c- really proud of the progress

1690
01:21:58,615 --> 01:22:00,195
uh, we have had with Waymo.

1691
01:22:00,195 --> 01:22:03,215
One of the things I think we were very committed to

1692
01:22:03,215 --> 01:22:05,463
you know, the final 20% can look like.

1693
01:22:05,463 --> 01:22:05,515
..

1694
01:22:05,515 --> 01:22:06,935
I mean, uh, we always say, right

1695
01:22:06,935 --> 01:22:11,255
the first 80% is easy, the final 20% takes 80% of the time.

1696
01:22:11,255 --> 01:22:16,555
I think we definitely, we're, we're working through that phase with Waymo

1697
01:22:16,555 --> 01:22:18,475
but I was aware of that, so.

1698
01:22:18,475 --> 01:22:20,295
But, you know, we knew we were at that stage.

1699
01:22:20,295 --> 01:22:23,480
We knew we were.

1700
01:22:23,480 --> 01:22:23,635
..

1701
01:22:23,635 --> 01:22:25,207
the technology gap between.

1702
01:22:25,207 --> 01:22:25,275
..

1703
01:22:25,275 --> 01:22:28,675
while there were many people, many other self-driving companies

1704
01:22:28,675 --> 01:22:30,335
we knew the technology gap was there.

1705
01:22:30,335 --> 01:22:36,095
In fact, we- uh, right at the moment when others were doubting Waymo is when

1706
01:22:36,095 --> 01:22:39,935
uh, I made the decision to invest more in Waymo

1707
01:22:39,935 --> 01:22:40,335
right?

1708
01:22:40,335 --> 01:22:40,707
Because.

1709
01:22:40,707 --> 01:22:40,795
..

1710
01:22:40,795 --> 01:22:43,495
 so, uh, so in some ways it's

1711
01:22:43,495 --> 01:22:45,875
it's counterintuitive, uh, but I think.

1712
01:22:45,875 --> 01:22:45,935
..

1713
01:22:45,935 --> 01:22:48,935
Look, we've always been a deep technology company and like

1714
01:22:48,935 --> 01:22:55,915
uh, you know, Waymo is a version of kinda building a AI robot that works well.

1715
01:22:55,915 --> 01:22:58,115
And so we get attracted to problems like that

1716
01:22:58,115 --> 01:23:01,055
the caliber of the teams there, uh

1717
01:23:01,055 --> 01:23:03,015
you know, uh, phenomenal teams.

1718
01:23:03,015 --> 01:23:06,535
And so, I know you follow the space super closely

1719
01:23:06,535 --> 01:23:09,015
uh, you know, I'm talking to someone who knows the space well

1720
01:23:09,015 --> 01:23:12,715
but it was very obvious it's gonna get there

1721
01:23:12,715 --> 01:23:15,155
and, you know, there's still more work to do.

1722
01:23:15,155 --> 01:23:22,795
But we, you know, it's a good example where we always prioritized being ambitious and safety at the same time

1723
01:23:22,795 --> 01:23:23,735
right?

1724
01:23:23,735 --> 01:23:23,755
Mm-hmm.

1725
01:23:23,755 --> 01:23:27,655
And, and, and equally committed to both and

1726
01:23:27,655 --> 01:23:31,975
and pushed hard and, you know, c- couldn't be more thrilled with

1727
01:23:31,975 --> 01:23:34,815
uh, how it's working, uh, how much people love

1728
01:23:34,815 --> 01:23:35,975
love the experience.

1729
01:23:35,975 --> 01:23:39,855
And it, this year has definitely, we have scaled up a lot

1730
01:23:39,855 --> 01:23:41,655
and we'll continue scaling up in '26.

1731
01:23:41,655 --> 01:23:45,555
That said, uh, the competition is heating up.

1732
01:23:45,555 --> 01:23:48,675
You've been, uh, friendly with Elon, uh

1733
01:23:48,675 --> 01:23:53,395
even though technically he's a competitor, but you've been friendly with a lot of tech CEOs in that way

1734
01:23:53,395 --> 01:23:55,135
just showing respect towards them and so on.

1735
01:23:55,135 --> 01:23:58,435
What, what do you think about the robotaxi efforts that Tesla is doing?

1736
01:23:58,435 --> 01:23:59,775
Do you see it as competition?

1737
01:23:59,775 --> 01:24:00,335
What do you think?

1738
01:24:00,335 --> 01:24:01,535
Do you like the competition?

1739
01:24:01,535 --> 01:24:06,495
We are one of the earliest and biggest backers of SpaceX

1740
01:24:06,495 --> 01:24:09,355
uh,  as Google, uh, right?

1741
01:24:09,355 --> 01:24:12,035
So, uh, you know, uh, thrilled with

1742
01:24:12,035 --> 01:24:15,495
uh, what SpaceX is doing and fortunate to be

1743
01:24:15,495 --> 01:24:18,495
uh, investors as a company there, right?

1744
01:24:18,495 --> 01:24:21,515
And, and look, we don't compete with Tesla directly

1745
01:24:21,515 --> 01:24:22,955
we are not making cars, et cetera

1746
01:24:22,955 --> 01:24:23,175
right?

1747
01:24:23,175 --> 01:24:25,815
We are building L4, 5 autonomy.

1748
01:24:25,815 --> 01:24:31,175
We are building a Waymo driver, which is general purpose and can be used in many settings.

1749
01:24:31,175 --> 01:24:35,795
They're obviously working on making Tesla self-driving too.

1750
01:24:35,795 --> 01:24:40,895
I'm just assumed it's a de facto that Elon would succeed in whatever he does.

1751
01:24:40,895 --> 01:24:42,095
 So, like, you know, I

1752
01:24:42,095 --> 01:24:43,475
I, you know, that, that, that is

1753
01:24:43,475 --> 01:24:45,575
uh, not something I question.

1754
01:24:45,575 --> 01:24:48,572
So, but I think we are so far from.

1755
01:24:48,572 --> 01:24:48,715
..

1756
01:24:48,715 --> 01:24:52,135
these spaces are such vast spaces.

1757
01:24:52,135 --> 01:24:55,375
Like, I, I think, think about transportation

1758
01:24:55,375 --> 01:25:02,755
the opportunity space, the Waymo driver is a general-purpose technology we can apply in many situations.

1759
01:25:02,755 --> 01:25:09,571
So you have a vast green space, um-In all future scenarios

1760
01:25:09,571 --> 01:25:12,471
I see Tesla doing well and, you know

1761
01:25:12,471 --> 01:25:13,431
Waymo doing well.

1762
01:25:13,431 --> 01:25:17,931
Like we mentioned with the Neolithic package, I think it's very possible that in the

1763
01:25:17,931 --> 01:25:20,172
quote-unquote, "AI package," when the history is written

1764
01:25:20,172 --> 01:25:26,852
autonomous vehicles, self-driving cars is, like, the big thing that changes everything.

1765
01:25:26,852 --> 01:25:31,211
Imagine over a period of, uh, a decade or two

1766
01:25:31,211 --> 01:25:34,672
just a complete transition from manually driven to autonomous.

1767
01:25:34,672 --> 01:25:37,271
In ways we went- we might not predict

1768
01:25:37,271 --> 01:25:41,051
it might change the way we move about the world completely

1769
01:25:41,051 --> 01:25:43,452
so that, you know, the possibility of that

1770
01:25:43,452 --> 01:25:47,051
and then the second and third order effects

1771
01:25:47,051 --> 01:25:51,131
as you're seeing now with Tesla, very possibly you would see some

1772
01:25:51,131 --> 01:25:52,327
um.

1773
01:25:52,327 --> 01:25:52,512
..

1774
01:25:52,512 --> 01:25:56,891
Internally with Alphabet, maybe Waymo, maybe some of the Gemini robotics stuff

1775
01:25:56,891 --> 01:26:04,272
it might lead you into the other domains of robotics 'cause we should remember that Waymo's a robot.

1776
01:26:04,272 --> 01:26:04,772
Mm-hmm.

1777
01:26:04,772 --> 01:26:07,091
It just happens to be on four wheels.

1778
01:26:07,091 --> 01:26:10,191
So you, you said that the next big thing

1779
01:26:10,191 --> 01:26:13,331
we can also throw that in the AI package

1780
01:26:13,331 --> 01:26:17,291
the big aha moment might be in the space of robotics.

1781
01:26:17,291 --> 01:26:19,751
What do you think th- that would look like?

1782
01:26:19,751 --> 01:26:23,551
Demis and the Google DeepMind team is very focused on Gemini robotics

1783
01:26:23,551 --> 01:26:23,711
right?

1784
01:26:23,711 --> 01:26:24,111
So we are- Yeah.

1785
01:26:24,111 --> 01:26:24,111
.

1786
01:26:24,111 --> 01:26:24,111
.

1787
01:26:24,111 --> 01:26:30,891
definitely building the underlying models well, so we have a lot of investments there

1788
01:26:30,891 --> 01:26:33,111
and I think we are also pretty cutting edge in our

1789
01:26:33,111 --> 01:26:34,251
uh, research there.

1790
01:26:34,251 --> 01:26:36,951
So we are definitely driving that direction.

1791
01:26:36,951 --> 01:26:41,411
We obviously are thinking about applications in robotics.

1792
01:26:41,411 --> 01:26:43,311
We'll, we'll kind of work �.

1793
01:26:43,311 --> 01:26:43,411
..

1794
01:26:43,411 --> 01:26:45,471
We are partnering with a few companies today

1795
01:26:45,471 --> 01:26:48,151
but it's an area I would say stay tuned.

1796
01:26:48,151 --> 01:26:51,951
We are, you know, we are yet to fully articulate our plans outside

1797
01:26:51,951 --> 01:26:55,191
but it's an area we are definitely committed to driving a

1798
01:26:55,191 --> 01:27:00,031
a lot of progress, but I think AI ends up driving that massive progress in robotics.

1799
01:27:00,031 --> 01:27:02,731
The field has been held back, uh

1800
01:27:02,731 --> 01:27:03,891
uh, for, for a while.

1801
01:27:03,891 --> 01:27:06,891
I mean, the hardware has made extraordinary

1802
01:27:06,891 --> 01:27:07,811
uh, progress.

1803
01:27:07,811 --> 01:27:10,571
Uh, the software had been the challenge

1804
01:27:10,571 --> 01:27:12,171
but, you know, with AI now and

1805
01:27:12,171 --> 01:27:15,731
uh, and, and the, and the generalized models we are building

1806
01:27:15,731 --> 01:27:18,331
uh, you know, we are building these models

1807
01:27:18,331 --> 01:27:21,651
getting them to work in the real world in a safe way

1808
01:27:21,651 --> 01:27:25,431
in a generalized way is the frontier we're pushing pretty hard on.

1809
01:27:25,431 --> 01:27:32,611
Well, it's really nice to see the d- the models and the different teams integrated to where all of them are pushing towards one world model that's being built.

1810
01:27:32,611 --> 01:27:35,415
So from all these different angles, multimodal.

1811
01:27:35,415 --> 01:27:35,551
..

1812
01:27:35,551 --> 01:27:39,579
You're ultimately trying to get Gemini.

1813
01:27:39,579 --> 01:27:39,771
..

1814
01:27:39,771 --> 01:27:46,091
S- s- the same thing that would make AI mode really effective in answering your questions

1815
01:27:46,091 --> 01:27:52,431
which requires a kind of world model, is the same kind of thing that would help a robot be useful in the physical world

1816
01:27:52,431 --> 01:27:53,971
so everything's aligned.

1817
01:27:53,971 --> 01:27:59,671
That, that is what makes this moment so unique because running a company for the first time

1818
01:27:59,671 --> 01:28:04,371
you can do one investment in a very deep horizontal way.

1819
01:28:04,371 --> 01:28:06,571
On top of it you can, like

1820
01:28:06,571 --> 01:28:09,511
drive multiple businesses forward, right?

1821
01:28:09,511 --> 01:28:14,011
And, you know, and that's, that's effectively what we are doing in Google and Alphabet

1822
01:28:14,011 --> 01:28:14,351
right?

1823
01:28:14,351 --> 01:28:17,191
Yeah, it's all coming together like it was planned ahead of time

1824
01:28:17,191 --> 01:28:18,051
but it's not, of course.

1825
01:28:18,051 --> 01:28:18,971
It's all distributed.

1826
01:28:18,971 --> 01:28:24,391
I mean, if, uh, Gmail and Sheets and all these other incredible services

1827
01:28:24,391 --> 01:28:26,151
I can sing Gmail praises for years.

1828
01:28:26,151 --> 01:28:31,986
I mean, just, just revolutionized email, but the moment you start to integrate AI Gemini into Gmail.

1829
01:28:31,986 --> 01:28:32,111
..

1830
01:28:32,111 --> 01:28:34,111
I mean, that's the other thing.

1831
01:28:34,111 --> 01:28:37,171
Speaking of productivity multiplier, people complain about email

1832
01:28:37,171 --> 01:28:38,231
but that changed everything.

1833
01:28:38,231 --> 01:28:40,751
Email, like, the invention of email changed everything

1834
01:28:40,751 --> 01:28:42,291
and it's been ripe.

1835
01:28:42,291 --> 01:28:42,451
..

1836
01:28:42,451 --> 01:28:45,231
There's been a few folks trying to revolutionize email

1837
01:28:45,231 --> 01:28:47,611
some of them on top of Gmail, but that's

1838
01:28:47,611 --> 01:28:51,171
like, ripe for innovation, not just spam filtering

1839
01:28:51,171 --> 01:28:53,411
but  you, uh, you.

1840
01:28:53,411 --> 01:28:53,471
..

1841
01:28:53,471 --> 01:28:56,651
They demoed a really nice demo of- Personalized responses

1842
01:28:56,651 --> 01:28:56,791
right?

1843
01:28:56,791 --> 01:28:56,797
.

1844
01:28:56,797 --> 01:28:56,811
..

1845
01:28:56,811 --> 01:28:58,131
personalized responses, and it.

1846
01:28:58,131 --> 01:28:58,251
..

1847
01:28:58,251 --> 01:28:59,328
At first I was like.

1848
01:28:59,328 --> 01:28:59,431
..

1849
01:28:59,431 --> 01:29:03,151
At first I felt really bad about that

1850
01:29:03,151 --> 01:29:08,557
but then I realized that there's nothing wrong to feel bad about because when you.

1851
01:29:08,557 --> 01:29:08,671
..

1852
01:29:08,671 --> 01:29:11,471
Uh, the example you gave is when a friend asks

1853
01:29:11,471 --> 01:29:14,351
you know, you went to whatever hiking location

1854
01:29:14,351 --> 01:29:15,651
uh, "Would you.

1855
01:29:15,651 --> 01:29:15,671
..

1856
01:29:15,671 --> 01:29:16,598
Have any advice?

1857
01:29:16,598 --> 01:29:22,151
" And they just search just through all your information to give them good advice and then you put the cherry on top

1858
01:29:22,151 --> 01:29:24,935
maybe some love or whatever, camaraderie, but it.

1859
01:29:24,935 --> 01:29:25,031
..

1860
01:29:25,031 --> 01:29:28,591
The informational aspect, the knowledge transfer it does for you.

1861
01:29:28,591 --> 01:29:31,991
I think there'll be important moments, like it should be imp- Like today

1862
01:29:31,991 --> 01:29:35,791
if you write a card in your own handwriting and send it to someone

1863
01:29:35,791 --> 01:29:36,931
that's a special thing.

1864
01:29:36,931 --> 01:29:39,191
Similarly, there'll be a time, I mean

1865
01:29:39,191 --> 01:29:43,451
to, to your friends, maybe your friend wrote and said he's not doing well or something

1866
01:29:43,451 --> 01:29:48,011
and those are moments you want to save your times for writing something

1867
01:29:48,011 --> 01:29:51,531
reaching out, but, you know, like saying

1868
01:29:51,531 --> 01:29:54,297
"Give me all the details of the trip you took

1869
01:29:54,297 --> 01:29:56,871
" uh, you know, to me makes a lot of sense for

1870
01:29:56,871 --> 01:29:58,671
uh, a AI assistant to help you

1871
01:29:58,671 --> 01:29:59,131
right?

1872
01:29:59,131 --> 01:30:02,011
And, uh, uh, so I think both are important

1873
01:30:02,011 --> 01:30:02,631
but I think.

1874
01:30:02,631 --> 01:30:02,731
..

1875
01:30:02,731 --> 01:30:04,951
Uh, uh, I think I'm excited about that direction.

1876
01:30:04,951 --> 01:30:09,211
Yeah, I think ultimately it gives more time for us humans to do the things we humans find meaningful

1877
01:30:09,211 --> 01:30:14,831
and, uh, I think it scares a lot of people because we're gonna have to ask ourselves the hard question of like

1878
01:30:14,831 --> 01:30:16,171
what do we find meaningful?

1879
01:30:16,171 --> 01:30:19,831
And I'm sure there's answers, and it's the old question of the meaning

1880
01:30:19,831 --> 01:30:23,531
meaning of existence is you, you have to try to figure that out.

1881
01:30:23,531 --> 01:30:29,611
That might be ultimately, uh, parenting or being creative in some domains of art or writing

1882
01:30:29,611 --> 01:30:32,155
and it, it challenges to, to, to.

1883
01:30:32,155 --> 01:30:32,251
..

1884
01:30:32,251 --> 01:30:34,711
Like, you know, it's a good question of to ask yourself

1885
01:30:34,711 --> 01:30:39,514
like, "In my life, what is the thing that brings me most joy and fulfillment?

1886
01:30:39,514 --> 01:30:42,951
" And if I'm able to actually focus more time on that

1887
01:30:42,951 --> 01:30:44,631
that's really powerful.

1888
01:30:44,631 --> 01:30:46,771
I think that's the, uh, you know

1889
01:30:46,771 --> 01:30:49,151
that's the holy grail, if you get this right.

1890
01:30:49,151 --> 01:30:51,651
I think it allows more people, uh

1891
01:30:51,651 --> 01:30:52,391
to find that.

1892
01:30:52,391 --> 01:30:54,891
I have to ask you, on the programming front

1893
01:30:54,891 --> 01:30:57,151
uh, AI is getting really good at programming.

1894
01:30:57,151 --> 01:31:00,571
Gemini, both the agentic and just the LLM has been incredible

1895
01:31:00,571 --> 01:31:04,556
so a lot of programmers are really worried that their jobs.

1896
01:31:04,556 --> 01:31:04,751
..

1897
01:31:04,751 --> 01:31:06,751
They will lose their jobs.

1898
01:31:06,751 --> 01:31:16,664
Uh, how worried should they be, and how should they adjust so they can be thriving in this new world where more and more code is written by AI?

1899
01:31:16,664 --> 01:31:17,915
I think a few things.

1900
01:31:17,915 --> 01:31:24,355
Looking at Google, um, you know, we've given various stats around like

1901
01:31:24,355 --> 01:31:29,775
you know, 30% of, uh, code now uses

1902
01:31:29,775 --> 01:31:31,995
like, AI-generated suggestions or whatever it is.

1903
01:31:31,995 --> 01:31:35,215
But the most important metric, and we carefully measure this

1904
01:31:35,215 --> 01:31:43,755
like, how much has our engineering velocity increased as a company due to AI

1905
01:31:43,755 --> 01:31:44,096
right?

1906
01:31:44,096 --> 01:31:47,015
And it's, like, tough to measure and we kind of rigorously try to measure it.

1907
01:31:47,015 --> 01:31:51,316
And our estimates are at, that number is now at 10%

1908
01:31:51,316 --> 01:31:51,676
right?

1909
01:31:51,676 --> 01:32:05,375
Like, now across the company we've accomplished a 10% engineering velocity increase using AI but we plan to hire engineers

1910
01:32:05,375 --> 01:32:07,016
more engineers next year, right?

1911
01:32:07,016 --> 01:32:14,215
So you, uh, because the opportunity space of what we can do is expanding too

1912
01:32:14,215 --> 01:32:14,575
right?

1913
01:32:14,575 --> 01:32:14,776
Mm-hmm.

1914
01:32:14,776 --> 01:32:18,555
And so I think hopefully, you know

1915
01:32:18,555 --> 01:32:21,635
for, uh, at least in the near to mid-term

1916
01:32:21,635 --> 01:32:26,675
for many engineers, it frees up more and more of the

1917
01:32:26,675 --> 01:32:29,835
uh, you know, even in engineering and coding

1918
01:32:29,835 --> 01:32:33,155
there are aspects which are so much fun.

1919
01:32:33,155 --> 01:32:36,135
You're designing, you're architecting, you're solving a problem.

1920
01:32:36,135 --> 01:32:39,235
There's a lot of grunt work, you know

1921
01:32:39,235 --> 01:32:40,635
which all goes hand-in-hand.

1922
01:32:40,635 --> 01:32:43,655
But it hopefully takes a lot of that away

1923
01:32:43,655 --> 01:32:48,835
makes it even more fun to code, frees you up more time to create

1924
01:32:48,835 --> 01:32:51,955
problem solve, brainstorm with your fellow colleagues and so on

1925
01:32:51,955 --> 01:32:52,215
right?

1926
01:32:52,215 --> 01:32:55,255
So that's, that's the opportunity there.

1927
01:32:55,255 --> 01:32:57,855
And second, I think, like, you know

1928
01:32:57,855 --> 01:32:58,835
it'll attract.

1929
01:32:58,835 --> 01:32:58,975
..

1930
01:32:58,975 --> 01:33:04,415
uh, it'll put the creative power in more people's hands

1931
01:33:04,415 --> 01:33:05,995
which means people create more.

1932
01:33:05,995 --> 01:33:08,555
That means there'll be more engineers doing more things.

1933
01:33:08,555 --> 01:33:11,055
So it's tough to fully predict, but

1934
01:33:11,055 --> 01:33:13,675
you know, I think in general in this moment

1935
01:33:13,675 --> 01:33:17,375
it feels like, you know, you know

1936
01:33:17,375 --> 01:33:21,335
people, uh, adopt these tools and be better programmers.

1937
01:33:21,335 --> 01:33:24,995
Like, there are more people playing chess now than ever before

1938
01:33:24,995 --> 01:33:25,355
right?

1939
01:33:25,355 --> 01:33:29,055
 So, uh, you know, it feels positive that way to me

1940
01:33:29,055 --> 01:33:32,315
at least speaking from within a Google context

1941
01:33:32,315 --> 01:33:35,315
uh, is how I would, you know

1942
01:33:35,315 --> 01:33:36,215
talk to them about it.

1943
01:33:36,215 --> 01:33:42,055
I still, I just know anecdotally, a lot of great programmers are generating a lot of code.

1944
01:33:42,055 --> 01:33:42,435
Yeah.

1945
01:33:42,435 --> 01:33:45,615
So their productivity, they're not always using all the code just.

1946
01:33:45,615 --> 01:33:45,695
..

1947
01:33:45,695 --> 01:33:47,035
you know, it's, there's still a lot of editing.

1948
01:33:47,035 --> 01:33:52,015
But like even for me, I'm still programming as a side thing

1949
01:33:52,015 --> 01:33:55,675
I think I'm like 5X more productive.

1950
01:33:55,675 --> 01:33:56,385
I don't.

1951
01:33:56,385 --> 01:33:56,515
..

1952
01:33:56,515 --> 01:33:58,111
I think that's, uh.

1953
01:33:58,111 --> 01:33:58,375
..

1954
01:33:58,375 --> 01:34:03,155
even for a large code base that's touching a lot of users like Google's does

1955
01:34:03,155 --> 01:34:09,095
I'm imagining like very soon that productivity should be going up even more.

1956
01:34:09,095 --> 01:34:14,915
Oh, the big unlock will be as we make the agented capabilities much more robust

1957
01:34:14,915 --> 01:34:15,455
right?

1958
01:34:15,455 --> 01:34:15,575
Mm-hmm.

1959
01:34:15,575 --> 01:34:17,855
I think that's what unlocks that next big wave.

1960
01:34:17,855 --> 01:34:20,855
I think the 10% is, like, a massive number.

1961
01:34:20,855 --> 01:34:23,035
Like, you know, if tomorrow like I showed up and said like

1962
01:34:23,035 --> 01:34:27,039
"You can improve like a large organization's productivity by 10%.

1963
01:34:27,039 --> 01:34:30,655
" When you have tens of thousands of engineers

1964
01:34:30,655 --> 01:34:32,395
that's a phenomenal number.

1965
01:34:32,395 --> 01:34:36,955
Uh, and, you know, that's different than what others cite as statistics saying like

1966
01:34:36,955 --> 01:34:40,800
you know, like, "This percentage of code is now written by AI.

1967
01:34:40,800 --> 01:34:42,547
" I'm talking more about like overall.

1968
01:34:42,547 --> 01:34:42,655
..

1969
01:34:42,655 --> 01:34:43,495
Actual productivity.

1970
01:34:43,495 --> 01:34:43,508
.

1971
01:34:43,508 --> 01:34:43,535
..

1972
01:34:43,535 --> 01:34:44,935
uh, actual productivity, right?

1973
01:34:44,935 --> 01:34:47,555
Engineering productivity, which is two different things

1974
01:34:47,555 --> 01:34:50,395
and, uh, which is the more important

1975
01:34:50,395 --> 01:34:51,355
uh, metric.

1976
01:34:51,355 --> 01:34:54,335
And, but I think it'll get better

1977
01:34:54,335 --> 01:34:54,875
right?

1978
01:34:54,875 --> 01:35:00,595
And like, you know, uh, I think there's no engineer who tomorrow if you magically became 2X more productive

1979
01:35:00,595 --> 01:35:04,775
you're just gonna create more things, you're gonna create more value-added things.

1980
01:35:04,775 --> 01:35:08,115
And so I think they, you'll, you'll find more satisfaction in your job

1981
01:35:08,115 --> 01:35:08,375
right?

1982
01:35:08,375 --> 01:35:08,615
So.

1983
01:35:08,615 --> 01:35:09,855
And there's a lot of aspects.

1984
01:35:09,855 --> 01:35:14,415
I mean, the actual Google code base might just improve because it'll become more standardized

1985
01:35:14,415 --> 01:35:18,995
more, um, easier for people to move about the code base because AI will help with that.

1986
01:35:18,995 --> 01:35:23,475
And therefore, that will also allow the AI to understand the entire code base better

1987
01:35:23,475 --> 01:35:25,375
which makes the engineering aspect.

1988
01:35:25,375 --> 01:35:27,415
And so I've been using Cursor a lot- Yeah.

1989
01:35:27,415 --> 01:35:27,515
.

1990
01:35:27,515 --> 01:35:27,715
..

1991
01:35:27,715 --> 01:35:30,735
uh, as, as a way to program with Gemini and other models.

1992
01:35:30,735 --> 01:35:32,235
It's like, it.

1993
01:35:32,235 --> 01:35:32,315
..

1994
01:35:32,315 --> 01:35:36,095
one of its powerful things is it's aware of the entire code base

1995
01:35:36,095 --> 01:35:38,255
and that allows you to ask questions of it

1996
01:35:38,255 --> 01:35:42,915
it allows the agents to move about that code base in a really powerful way.

1997
01:35:42,915 --> 01:35:44,335
I mean, that's a huge unlock.

1998
01:35:44,335 --> 01:35:47,755
Think about, like, you know, migrations

1999
01:35:47,755 --> 01:35:49,895
refactoring old code bases.

2000
01:35:49,895 --> 01:35:50,595
Refactoring, yeah.

2001
01:35:50,595 --> 01:35:50,915
Yeah.

2002
01:35:50,915 --> 01:35:52,595
I mean, th- think about like, you know

2003
01:35:52,595 --> 01:35:54,815
once we can do all this in a much better

2004
01:35:54,815 --> 01:35:56,615
more robust way than where we are today.

2005
01:35:56,615 --> 01:35:59,895
I think in the end everything will be written in JavaScript and run

2006
01:35:59,895 --> 01:36:00,795
run in Chrome.

2007
01:36:00,795 --> 01:36:01,135
.

2008
01:36:01,135 --> 01:36:02,755
I think  it's all going to that

2009
01:36:02,755 --> 01:36:03,555
uh, direction.

2010
01:36:03,555 --> 01:36:06,875
I mean, just for fun, Google has legendary coding

2011
01:36:06,875 --> 01:36:13,175
coding interviews, uh, like rigorous interviews for the engineers.

2012
01:36:13,175 --> 01:36:13,748
How.

2013
01:36:13,748 --> 01:36:13,835
..

2014
01:36:13,835 --> 01:36:16,975
can you comment on how that has changed in the era of AI?

2015
01:36:16,975 --> 01:36:18,400
 It's just such a weird.

2016
01:36:18,400 --> 01:36:18,535
..

2017
01:36:18,535 --> 01:36:21,235
 uh, you know, the whiteboard interview

2018
01:36:21,235 --> 01:36:24,115
uh, I assume is now allowed to have some prompts.

2019
01:36:24,115 --> 01:36:26,755
Such a, a, a good question.

2020
01:36:26,755 --> 01:36:31,055
Look, I do think, you know, we're making sure

2021
01:36:31,055 --> 01:36:37,915
you know, we'll, we'll introduce at least one round of in-person interviews for people-  Yeah.

2022
01:36:37,915 --> 01:36:38,015
.

2023
01:36:38,015 --> 01:36:38,215
..

2024
01:36:38,215 --> 01:36:40,115
just to make sure the fundamentals are there.

2025
01:36:40,115 --> 01:36:41,575
I think they'll end up being important.

2026
01:36:41,575 --> 01:36:43,515
But it's an equally important skill.

2027
01:36:43,515 --> 01:36:46,755
Look, if you can use these tools to generate better code

2028
01:36:46,755 --> 01:36:48,355
uh, like, you know, I think

2029
01:36:48,355 --> 01:36:49,375
I think that's an asset.

2030
01:36:49,375 --> 01:36:52,655
And so, uh, you know, I think

2031
01:36:52,655 --> 01:36:52,867
uh.

2032
01:36:52,867 --> 01:36:52,955
..

2033
01:36:52,955 --> 01:36:55,615
so overall I think it's a, it's a massive positive.

2034
01:36:55,615 --> 01:36:57,435
Vibe coding engineer.

2035
01:36:57,435 --> 01:37:00,215
Uh, do you recommend, uh, pe- people

2036
01:37:00,215 --> 01:37:03,395
uh, students interested in programming still get an education

2037
01:37:03,395 --> 01:37:05,795
uh, in computer science, a college education?

2038
01:37:05,795 --> 01:37:06,775
What do you think?

2039
01:37:06,775 --> 01:37:07,355
I do.

2040
01:37:07,355 --> 01:37:09,015
If you have a passion for computer science

2041
01:37:09,015 --> 01:37:09,515
I would.

2042
01:37:09,515 --> 01:37:12,255
You know, computer science is obviously a lot more than programming alone.

2043
01:37:12,255 --> 01:37:13,475
So I would.

2044
01:37:13,475 --> 01:37:19,035
I still don't think I would change what you pursue

2045
01:37:19,035 --> 01:37:26,179
um-I think AI will horizontally allow impact every field.

2046
01:37:26,179 --> 01:37:29,360
It's pretty tough to predict in what ways.

2047
01:37:29,360 --> 01:37:34,919
So any education in which you're learning good first principles thinking

2048
01:37:34,919 --> 01:37:36,939
I think is good education.

2049
01:37:36,939 --> 01:37:39,320
You've revolutionized web browsing.

2050
01:37:39,320 --> 01:37:41,599
You've re- revolutionized a lot of things over the years.

2051
01:37:41,599 --> 01:37:45,039
Um, Android changed the game.

2052
01:37:45,039 --> 01:37:47,299
It's an incredible, uh, operating system.

2053
01:37:47,299 --> 01:37:48,999
We could talk for hours about Android.

2054
01:37:48,999 --> 01:37:51,039
What does the future of Android look like?

2055
01:37:51,039 --> 01:37:56,520
Is it, is it possible it becomes more and more AI-centric

2056
01:37:56,520 --> 01:38:01,920
uh, especially now that you throw into the mix Android XR with

2057
01:38:01,920 --> 01:38:05,359
uh, being able to do augmented reality and

2058
01:38:05,359 --> 01:38:08,679
um, mixed reality and virtual reality in the physical world?

2059
01:38:08,679 --> 01:38:12,799
You know, the best innovations in computing have come when your

2060
01:38:12,799 --> 01:38:15,959
uh, through a paradigm IO change, right?

2061
01:38:15,959 --> 01:38:17,308
Like, you know, when, when.

2062
01:38:17,308 --> 01:38:17,400
..

2063
01:38:17,400 --> 01:38:25,319
with GWE and then with the graphic user interface and then with multi-touch in the context of mobile voice later on.

2064
01:38:25,319 --> 01:38:28,199
Similarly, I feel like, uh, you know

2065
01:38:28,199 --> 01:38:30,859
AR is that next paradigm.

2066
01:38:30,859 --> 01:38:37,359
I think it was held back both the system integration challenges of making good AR is very

2067
01:38:37,359 --> 01:38:38,019
very hard.

2068
01:38:38,019 --> 01:38:41,895
The second thing is you need AI to actually kind of.

2069
01:38:41,895 --> 01:38:42,079
..

2070
01:38:42,079 --> 01:38:44,859
otherwise the IO is too complicated.

2071
01:38:44,859 --> 01:38:48,259
For you to have a natural seamless IO to that

2072
01:38:48,259 --> 01:38:52,660
uh, uh, paradigm, AI ends up being super important.

2073
01:38:52,660 --> 01:38:58,639
And so this is why Project Astra ends up being super critical for that Android

2074
01:38:58,639 --> 01:38:59,939
uh, XR world.

2075
01:38:59,939 --> 01:39:02,099
Uh, but it is.

2076
01:39:02,099 --> 01:39:04,520
I think when you use glasses and, you know

2077
01:39:04,520 --> 01:39:09,400
always been amazed like at the, at the how useful these things are going to be.

2078
01:39:09,400 --> 01:39:10,509
So I.

2079
01:39:10,509 --> 01:39:10,599
..

2080
01:39:10,599 --> 01:39:13,099
look, I think it's a real opportunity for Android.

2081
01:39:13,099 --> 01:39:17,659
I think XR is one way it will kind of really come to life.

2082
01:39:17,659 --> 01:39:21,119
But I think there's an opportunity to rethink the mobile OS too

2083
01:39:21,119 --> 01:39:21,559
right?

2084
01:39:21,559 --> 01:39:23,919
I think we've been kind of living in this paradigm of

2085
01:39:23,919 --> 01:39:27,779
like, apps and shortcuts, all that won't go away.

2086
01:39:27,779 --> 01:39:34,079
But again, like if you're trying to get stuff done at an operating system level

2087
01:39:34,079 --> 01:39:40,239
you know, it needs to be more agentic so that you can kind of describe what you want to do or

2088
01:39:40,239 --> 01:39:43,039
like, it proactively understands what you're trying to do

2089
01:39:43,039 --> 01:39:47,259
learns from how you're doing things over and over again and kind of is adapting to you.

2090
01:39:47,259 --> 01:39:50,339
All that is kind of, like, the unlock we need to go and do.

2091
01:39:50,339 --> 01:39:54,359
Uh, with a basic efficient, minimalist, uh

2092
01:39:54,359 --> 01:39:57,799
UI, I've gotten a chance to try the glasses and they're incredible.

2093
01:39:57,799 --> 01:39:58,679
It's the little stuff.

2094
01:39:58,679 --> 01:40:01,799
It's hard to put into words, but no latency.

2095
01:40:01,799 --> 01:40:02,999
It just works.

2096
01:40:02,999 --> 01:40:09,619
Even that little map demo where you look down and you look up and there's a very smooth transition between the two

2097
01:40:09,619 --> 01:40:11,605
and useful.

2098
01:40:11,605 --> 01:40:11,779
..

2099
01:40:11,779 --> 01:40:15,959
very small amount of useful information is shown to you.

2100
01:40:15,959 --> 01:40:19,199
Enough not to distract from the world outside

2101
01:40:19,199 --> 01:40:22,019
but enough to provide a bit of context when you need it.

2102
01:40:22,019 --> 01:40:25,479
And some of that be-.

2103
01:40:25,479 --> 01:40:25,579
..

2104
01:40:25,579 --> 01:40:33,459
in order to bring that into reality, you have to solve a lot of the OS problems to make sure it works when you're integrating the AI into the whole thing.

2105
01:40:33,459 --> 01:40:39,059
So every- everything you do launches an agent that answers some basic question for you.

2106
01:40:39,059 --> 01:40:39,799
It's a good moonshot.

2107
01:40:39,799 --> 01:40:40,419
You know, I lo- like that.

2108
01:40:40,419 --> 01:40:40,559
 Yeah.

2109
01:40:40,559 --> 01:40:41,059
It's crazy.

2110
01:40:41,059 --> 01:40:42,799
No, but, but, you know, I think

2111
01:40:42,799 --> 01:40:44,724
uh, we are, you know.

2112
01:40:44,724 --> 01:40:44,759
..

2113
01:40:44,759 --> 01:40:48,739
but it's much closer to reality than, uh

2114
01:40:48,739 --> 01:40:49,659
other moonshots.

2115
01:40:49,659 --> 01:40:55,059
You know, we expect to have glasses in the hands of developers later this year and

2116
01:40:55,059 --> 01:40:57,579
and i- i- you know, in consumer science next year.

2117
01:40:57,579 --> 01:40:58,659
So it's an exciting time.

2118
01:40:58,659 --> 01:40:59,419
Yeah.

2119
01:40:59,419 --> 01:41:01,099
Well ex- extremely well executed.

2120
01:41:01,099 --> 01:41:01,513
Beam.

2121
01:41:01,513 --> 01:41:01,679
..

2122
01:41:01,679 --> 01:41:03,299
a- all the stuff, you know, 'cause I.

2123
01:41:03,299 --> 01:41:03,399
..

2124
01:41:03,399 --> 01:41:06,499
sometimes you don't know, like somebody commented on a

2125
01:41:06,499 --> 01:41:10,019
uh, a top comment on one of the demos of Beam.

2126
01:41:10,019 --> 01:41:18,167
Uh, they said, uh, "This will either be killed off in five weeks or revolutionize all meetings in five years.

2127
01:41:18,167 --> 01:41:19,681
" And th- there's very much.

2128
01:41:19,681 --> 01:41:19,779
..

2129
01:41:19,779 --> 01:41:27,399
Google tries so many things and sometimes sadly kills off very promising projects be- because there's so many other things to focus on.

2130
01:41:27,399 --> 01:41:29,379
I use, I use so many Google products.

2131
01:41:29,379 --> 01:41:30,839
Google Voice I still use.

2132
01:41:30,839 --> 01:41:33,059
I'm so glad that's not being killed off.

2133
01:41:33,059 --> 01:41:34,099
That's still alive.

2134
01:41:34,099 --> 01:41:36,239
Thank you, f- whoever is defending that

2135
01:41:36,239 --> 01:41:37,479
'cause it's, it's awesome.

2136
01:41:37,479 --> 01:41:39,459
And i- it's great they keep innovating.

2137
01:41:39,459 --> 01:41:41,919
I- I just want to list off just as a big thank you.

2138
01:41:41,919 --> 01:41:44,699
So Search, obviously Google revolutionized.

2139
01:41:44,699 --> 01:41:47,819
Chrome and all of these could be multi-hour conversations.

2140
01:41:47,819 --> 01:41:52,179
Gmail, I've been singing Gmail praises forever.

2141
01:41:52,179 --> 01:41:56,599
Maps, incredible technological innovation on revolutionizing mapping.

2142
01:41:56,599 --> 01:41:58,159
Android, like we talked about.

2143
01:41:58,159 --> 01:41:59,399
YouTube, like we talked about.

2144
01:41:59,399 --> 01:42:07,305
AdSense, uh, Google  Translate for the academic mind of Google Scholar is incredible when the.

2145
01:42:07,305 --> 01:42:07,359
..

2146
01:42:07,359 --> 01:42:09,319
with the b- and also the scanning of the books.

2147
01:42:09,319 --> 01:42:12,339
So making all the world's knowledge, um

2148
01:42:12,339 --> 01:42:15,919
accessible even when that knowledge is a kind of niche thing

2149
01:42:15,919 --> 01:42:17,099
which Google Scholar is.

2150
01:42:17,099 --> 01:42:19,959
Uh, and then obviously with DeepMind, uh

2151
01:42:19,959 --> 01:42:22,459
with AlphaZero, AlphaFold, AlphaEvolve.

2152
01:42:22,459 --> 01:42:25,079
I could talk forever about AlphaEvolve.

2153
01:42:25,079 --> 01:42:27,239
That's mind-blowing all of that  released.

2154
01:42:27,239 --> 01:42:32,199
And as part of that, uh, set of things you've released in this year when

2155
01:42:32,199 --> 01:42:36,079
uh, those brilliant articles were written about Google is done.

2156
01:42:36,079 --> 01:42:41,639
Uh, and, uh, like we talked about pioneering self-driving cars and quantum computing

2157
01:42:41,639 --> 01:42:49,039
which could be another thing that is low key as scuba diving its way to changing the world forever.

2158
01:42:49,039 --> 01:42:56,119
Uh, so another potheads/microkitchen question,  if you build AGI

2159
01:42:56,119 --> 01:42:59,659
what kind of question would you ask it?

2160
01:42:59,659 --> 01:43:02,719
Wh- what would you, what would you want to talk about?

2161
01:43:02,719 --> 01:43:09,359
Definitively, Google has created AGI that can basically answer any question.

2162
01:43:09,359 --> 01:43:11,319
What topic are you going to?

2163
01:43:11,319 --> 01:43:11,830
What.

2164
01:43:11,830 --> 01:43:11,939
..

2165
01:43:11,939 --> 01:43:13,512
 where, where's the, where's the.

2166
01:43:13,512 --> 01:43:13,559
..

2167
01:43:13,559 --> 01:43:14,139
where are you going?

2168
01:43:14,139 --> 01:43:15,919
It's a great question.

2169
01:43:15,919 --> 01:43:23,899
Um, maybe it's proactive by then and shou- should tell me a few things I should know.

2170
01:43:23,899 --> 01:43:29,827
But I think if I were to ask it-I think it'll help us understand ourselves much better

2171
01:43:29,827 --> 01:43:33,047
um, in a way that'll surprise us

2172
01:43:33,047 --> 01:43:33,547
I think.

2173
01:43:33,547 --> 01:43:36,900
Um, and so maybe that's.

2174
01:43:36,900 --> 01:43:36,967
..

2175
01:43:36,967 --> 01:43:39,807
You already see people do it with the products.

2176
01:43:39,807 --> 01:43:40,275
And so.

2177
01:43:40,275 --> 01:43:40,428
..

2178
01:43:40,428 --> 01:43:42,267
But, you know, in the AGI context

2179
01:43:42,267 --> 01:43:43,447
I think that'll be pretty powerful.

2180
01:43:43,447 --> 01:43:45,987
On a personal level or, or general human nature?

2181
01:43:45,987 --> 01:43:49,447
On a personal level, like you talking to AGI.

2182
01:43:49,447 --> 01:43:49,508
Oh, wow.

2183
01:43:49,508 --> 01:43:49,667
Yeah.

2184
01:43:49,667 --> 01:43:52,907
I think, I think, you know, uh

2185
01:43:52,907 --> 01:43:56,707
there is some chance it'll, it'll kinda understand you in a

2186
01:43:56,707 --> 01:43:58,807
in a very deep way, uh, I think

2187
01:43:58,807 --> 01:44:00,608
uh, you know, in a profound way

2188
01:44:00,608 --> 01:44:01,587
that's a possibility.

2189
01:44:01,587 --> 01:44:05,488
Uh, I, I think there's also the obvious thing of

2190
01:44:05,488 --> 01:44:08,368
like, maybe it helps us understand the universe better

2191
01:44:08,368 --> 01:44:15,607
um, you, you know, in a way that expands the frontiers of our understanding of the world.

2192
01:44:15,607 --> 01:44:18,008
Uh, that is something super exciting.

2193
01:44:18,008 --> 01:44:23,008
But, look, I, I really don't know.

2194
01:44:23,008 --> 01:44:26,768
I think, you know, I haven't, I haven't had access to something that powerful yet

2195
01:44:26,768 --> 01:44:29,687
but I think those are all possibilities.

2196
01:44:29,687 --> 01:44:34,437
I think that on the personal level, asking questions about yourself could.

2197
01:44:34,437 --> 01:44:34,567
..

2198
01:44:34,567 --> 01:44:38,384
a sequence of questions like that about, "What makes me happy?

2199
01:44:38,384 --> 01:44:42,028
" I think we'd be very surprised to learn the.

2200
01:44:42,028 --> 01:44:42,108
..

2201
01:44:42,108 --> 01:44:43,647
those kind of the, um.

2202
01:44:43,647 --> 01:44:43,727
..

2203
01:44:43,727 --> 01:44:51,168
a sequence of questions and answers we might explore some profound truths in the way that sometimes art reveals to us

2204
01:44:51,168 --> 01:44:54,707
great books reveal to us, great conversations with loved ones reveal

2205
01:44:54,707 --> 01:44:57,647
uh, things that are obvious in retrospect

2206
01:44:57,647 --> 01:44:59,767
but are nice when they're said.

2207
01:44:59,767 --> 01:45:03,627
Uh, but for me, number one question is about how many alien civilizations are there?

2208
01:45:03,627 --> 01:45:04,188
.

2209
01:45:04,188 --> 01:45:04,948
100%.

2210
01:45:04,948 --> 01:45:06,747
Are they- That's gonna be your first question?

2211
01:45:06,747 --> 01:45:06,988
.

2212
01:45:06,988 --> 01:45:10,247
Number one, how many living and dead alien civilizations?

2213
01:45:10,247 --> 01:45:12,188
Um, maybe a bunch of follow-ups, like

2214
01:45:12,188 --> 01:45:13,207
how close are they?

2215
01:45:13,207 --> 01:45:14,707
Are they dangerous?

2216
01:45:14,707 --> 01:45:18,727
Um, if, if there's no alien civilizations

2217
01:45:18,727 --> 01:45:19,267
why?

2218
01:45:19,267 --> 01:45:22,787
Uh, or if there's n- no advanced alien civilizations

2219
01:45:22,787 --> 01:45:25,708
but bacteria, like life everywhere, why?

2220
01:45:25,708 --> 01:45:28,027
What is the barrier preventing it from getting to that?

2221
01:45:28,027 --> 01:45:30,647
Uh, is it because that there's, uh.

2222
01:45:30,647 --> 01:45:30,887
..

2223
01:45:30,887 --> 01:45:33,808
that when you get sufficiently intelligent, you end up

2224
01:45:33,808 --> 01:45:40,507
uh, destroying ourselves because you need competition in order to develop an advanced civilization?

2225
01:45:40,507 --> 01:45:44,687
And when you have competition, it's going to lead to military conflict

2226
01:45:44,687 --> 01:45:46,247
and conflict eventually kills everybody.

2227
01:45:46,247 --> 01:45:46,847
I don't know.

2228
01:45:46,847 --> 01:45:47,828
I'm gonna have that kind of discussion.

2229
01:45:47,828 --> 01:45:49,307
Get a, get an answer to the Fermi paradox.

2230
01:45:49,307 --> 01:45:49,548
Yeah.

2231
01:45:49,548 --> 01:45:50,107
Exactly.

2232
01:45:50,107 --> 01:45:52,047
And, like, have a real discussion about it.

2233
01:45:52,047 --> 01:45:53,807
I'm not sure if it's a, um.

2234
01:45:53,807 --> 01:45:54,007
..

2235
01:45:54,007 --> 01:45:56,848
I'm realizing now with your answer, it's a more productive

2236
01:45:56,848 --> 01:45:59,847
um, answer 'cause I'm not sure what I'm gonna do with that information

2237
01:45:59,847 --> 01:46:03,727
but maybe it speaks to the general human curiosity that Liz talked about

2238
01:46:03,727 --> 01:46:05,708
that we're all just really curious.

2239
01:46:05,708 --> 01:46:12,467
And making the world's information accessible allows our curiosity to be satiated s- some.

2240
01:46:12,467 --> 01:46:17,967
With AI even more, we can be more and more curious and learn more about the world

2241
01:46:17,967 --> 01:46:18,847
about ourselves.

2242
01:46:18,847 --> 01:46:21,195
And in so doing, I always wonder if.

2243
01:46:21,195 --> 01:46:21,267
..

2244
01:46:21,267 --> 01:46:22,607
I don't know if you can comment on

2245
01:46:22,607 --> 01:46:26,453
like, is it possible to measure the.

2246
01:46:26,453 --> 01:46:26,687
..

2247
01:46:26,687 --> 01:46:30,427
not the GDP productivity increase, like we talked about

2248
01:46:30,427 --> 01:46:32,820
but maybe the.

2249
01:46:32,820 --> 01:46:32,887
..

2250
01:46:32,887 --> 01:46:41,567
whatever that increases, the, the breadth and depth of human knowledge that Google has unlocked with Google Search

2251
01:46:41,567 --> 01:46:45,287
and now with AI Mode, with Gemini

2252
01:46:45,287 --> 01:46:47,307
it's a difficult thing to measure.

2253
01:46:47,307 --> 01:46:48,957
Many years ago, there was a.

2254
01:46:48,957 --> 01:46:49,087
..

2255
01:46:49,087 --> 01:46:54,947
I think it was a MIT study, they just estimated the impact of Google Search

2256
01:46:54,947 --> 01:46:58,051
and they basically said it's the equivalent to.

2257
01:46:58,051 --> 01:46:58,147
..

2258
01:46:58,147 --> 01:47:02,727
on a per person basis, it's few thousands of dollars per year per person

2259
01:47:02,727 --> 01:47:03,287
right?

2260
01:47:03,287 --> 01:47:06,107
Uh, like it's value that got created per year

2261
01:47:06,107 --> 01:47:07,227
right?

2262
01:47:07,227 --> 01:47:08,480
And, and.

2263
01:47:08,480 --> 01:47:08,767
..

2264
01:47:08,767 --> 01:47:11,187
but it's, yeah, it's tough to capture these things

2265
01:47:11,187 --> 01:47:11,407
right?

2266
01:47:11,407 --> 01:47:14,587
Like, you kind of take it, take it for granted as these things come

2267
01:47:14,587 --> 01:47:17,607
uh, and, and the frontier keeps moving

2268
01:47:17,607 --> 01:47:22,547
but, uh, you know, how do you measure the value of something like AlphaFold over time

2269
01:47:22,547 --> 01:47:23,267
right?

2270
01:47:23,267 --> 01:47:24,887
And, and, and, and so on.

2271
01:47:24,887 --> 01:47:28,027
So it's- And also the increase in quality of life when you learn more.

2272
01:47:28,027 --> 01:47:30,607
I have to say, like, with, uh

2273
01:47:30,607 --> 01:47:33,727
some of the programming I do done by AI

2274
01:47:33,727 --> 01:47:35,947
for some reason I'm more excited to program.

2275
01:47:35,947 --> 01:47:36,187
Yeah.

2276
01:47:36,187 --> 01:47:38,487
Uh, and so the same with knowledge

2277
01:47:38,487 --> 01:47:42,067
with discovering things about the world, uh

2278
01:47:42,067 --> 01:47:43,727
it makes you more excited to be alive

2279
01:47:43,727 --> 01:47:44,947
it makes you more curious to.

2280
01:47:44,947 --> 01:47:45,027
..

2281
01:47:45,027 --> 01:47:45,717
And it keeps.

2282
01:47:45,717 --> 01:47:45,827
..

2283
01:47:45,827 --> 01:47:50,987
The more curious you are, the more exciting it is to live and experience the world.

2284
01:47:50,987 --> 01:47:51,791
And it's very hard to.

2285
01:47:51,791 --> 01:47:51,887
..

2286
01:47:51,887 --> 01:47:53,327
I don't know if that makes you more productive.

2287
01:47:53,327 --> 01:47:54,061
It probably.

2288
01:47:54,061 --> 01:47:54,167
..

2289
01:47:54,167 --> 01:47:58,087
not nearly as much as it makes you happy to be alive.

2290
01:47:58,087 --> 01:48:03,787
And that's a hard thing to measure the quality of life increase that some of these things do.

2291
01:48:03,787 --> 01:48:08,787
As AI continues to get better and better at everything that humans do

2292
01:48:08,787 --> 01:48:11,887
what do you think is the biggest thing that makes us humans special?

2293
01:48:11,887 --> 01:48:19,315
Look, I, I, I think it's tough to.

2294
01:48:19,315 --> 01:48:19,427
..

2295
01:48:19,427 --> 01:48:22,167
I mean, the essence of humanity, there's something about

2296
01:48:22,167 --> 01:48:26,807
uh, you know, the consciousness we have

2297
01:48:26,807 --> 01:48:30,107
what makes us uniquely human, maybe the lines will blur over time

2298
01:48:30,107 --> 01:48:32,747
uh, and, and it's tough to articulate

2299
01:48:32,747 --> 01:48:34,564
but, uh, I hope.

2300
01:48:34,564 --> 01:48:34,707
..

2301
01:48:34,707 --> 01:48:45,447
hopefully, you know, we live in a world where if you make resources more plentiful and make the world le- uh

2302
01:48:45,447 --> 01:48:48,747
lesser of a zero-sum game over time, right?

2303
01:48:48,747 --> 01:48:49,407
And, and.

2304
01:48:49,407 --> 01:48:49,467
..

2305
01:48:49,467 --> 01:48:53,487
which it's not, but, you know, in a resource constrained environment

2306
01:48:53,487 --> 01:48:54,847
people perceive it to be, right?

2307
01:48:54,847 --> 01:49:00,087
And, and, um, and so I hope the

2308
01:49:00,087 --> 01:49:03,507
the values of what makes us uniquely human

2309
01:49:03,507 --> 01:49:10,967
empathy, kindness, all that surfaces more is the aspirational hope I have.

2310
01:49:10,967 --> 01:49:14,827
Yeah, it multiplies the compassion, but also the curiosity.

2311
01:49:14,827 --> 01:49:19,407
Just the, the banter, the debates we'll have about the meaning of it all.

2312
01:49:19,407 --> 01:49:22,407
And I, I a- also think in the scientific domains

2313
01:49:22,407 --> 01:49:24,347
all the incredible work that DeepMind is doing

2314
01:49:24,347 --> 01:49:29,027
I think we'll still continue to, to play

2315
01:49:29,027 --> 01:49:33,647
to explore scientific questions, uh, mathematical questions

2316
01:49:33,647 --> 01:49:35,888
physics questions, even as AIs.

2317
01:49:35,888 --> 01:49:36,219
..

2318
01:49:36,219 --> 01:49:36,549
..

2319
01:49:36,549 --> 01:49:39,276
. gets better and better at helping us solve some of the questions.

2320
01:49:39,276 --> 01:49:42,295
Sometimes, the question itself is a really difficult thing.

2321
01:49:42,295 --> 01:49:46,976
Uh, both the right new questions to ask and the answers to them and

2322
01:49:46,976 --> 01:49:51,275
and, and the self-discovery process which it'll drive

2323
01:49:51,275 --> 01:49:51,835
I think.

2324
01:49:51,835 --> 01:49:58,456
You know, our early work with both CoScientists and AlphaEvolve just is just super exciting to see.

2325
01:49:58,456 --> 01:50:01,976
What gives you hope about the future of human civilization?

2326
01:50:01,976 --> 01:50:03,789
Look, I've always.

2327
01:50:03,789 --> 01:50:03,896
..

2328
01:50:03,896 --> 01:50:06,335
I'm, I'm, I'm an optimist and, you know

2329
01:50:06,335 --> 01:50:08,451
I, I, I look at, um.

2330
01:50:08,451 --> 01:50:08,715
..

2331
01:50:08,715 --> 01:50:13,795
Now, if you were to say you take the journey of human civilization

2332
01:50:13,795 --> 01:50:18,115
uh, it's been, you know, we've relentlessly ma- made the world better

2333
01:50:18,115 --> 01:50:20,255
right, in many ways.

2334
01:50:20,255 --> 01:50:24,015
At any given moment in time, there are big issues to work through.

2335
01:50:24,015 --> 01:50:27,855
It may look, but, you know, I always ask myself the question

2336
01:50:27,855 --> 01:50:31,121
"Would you have been born now or any other time in the past?

2337
01:50:31,121 --> 01:50:38,435
" I most often, not most often, almost always would rather be born now

2338
01:50:38,435 --> 01:50:38,895
right?

2339
01:50:38,895 --> 01:50:39,355
 You know?

2340
01:50:39,355 --> 01:50:44,575
And so that's the extraordinary thing that human civilization has accomplished

2341
01:50:44,575 --> 01:50:44,876
right?

2342
01:50:44,876 --> 01:50:46,135
And, like, you know, and we've

2343
01:50:46,135 --> 01:50:49,476
we've kind of constantly made the world a better place.

2344
01:50:49,476 --> 01:50:56,715
And so something tells me as humanity we always rise collectively to drive that

2345
01:50:56,715 --> 01:50:58,255
uh, frontier forward.

2346
01:50:58,255 --> 01:51:00,155
So I expect it to be no different in the future.

2347
01:51:00,155 --> 01:51:02,175
I agree with you totally.

2348
01:51:02,175 --> 01:51:04,595
I'm truly grateful to be alive in this moment

2349
01:51:04,595 --> 01:51:06,795
and I'm also really excited for the future.

2350
01:51:06,795 --> 01:51:13,735
And the work, uh, you and the incredible teams here are doing is one of the big reasons I'm excited for the future.

2351
01:51:13,735 --> 01:51:14,456
So thank you.

2352
01:51:14,456 --> 01:51:16,935
Thank you for all the cool products you've built

2353
01:51:16,935 --> 01:51:18,935
and please don't kill Google Voice.

2354
01:51:18,935 --> 01:51:21,335
  Uh, the- Thank you, Sundar.

2355
01:51:21,335 --> 01:51:22,435
We won't, yeah.

2356
01:51:22,435 --> 01:51:23,835
  Thank you for talking today.

2357
01:51:23,835 --> 01:51:24,535
This was incredible.

2358
01:51:24,535 --> 01:51:24,735
Thank you.

2359
01:51:24,735 --> 01:51:25,356
Real pleasure.

2360
01:51:25,356 --> 01:51:25,976
Appreciate it.

2361
01:51:25,976 --> 01:51:29,516
Thanks for listening to this conversation with Sundar Pichai.

2362
01:51:29,516 --> 01:51:35,617
To support this podcast, please check out our sponsors in the description or at lexfridman.

2363
01:51:35,617 --> 01:51:37,015
com/sponsors.

2364
01:51:37,015 --> 01:51:43,155
Shortly before this conversation, I got a chance to get a couple of demos that frankly blew my mind.

2365
01:51:43,155 --> 01:51:46,095
The engineering was really impressive.

2366
01:51:46,095 --> 01:51:53,075
The first demo was Google Beam, and the second demo was the XR glasses.

2367
01:51:53,075 --> 01:51:55,815
And some of it was caught on video.

2368
01:51:55,815 --> 01:51:58,795
So I thought I would include here some of those

2369
01:51:58,795 --> 01:52:00,555
uh, video clips.

2370
01:52:00,555 --> 01:52:02,195
Hey, Lex.

2371
01:52:02,195 --> 01:52:02,215
Hi there.

2372
01:52:02,215 --> 01:52:02,955
My name's Andrew.

2373
01:52:02,955 --> 01:52:05,935
I lead the Google Beam team, and we're gonna be excited to show you a demo.

2374
01:52:05,935 --> 01:52:08,255
We're gonna show you, I think, a glimpse of something new.

2375
01:52:08,255 --> 01:52:10,455
So that's the idea, a way to connect

2376
01:52:10,455 --> 01:52:13,175
a way to feel present from anywhere with anybody you care about.

2377
01:52:13,175 --> 01:52:15,715
Here's Google Beam.

2378
01:52:15,715 --> 01:52:17,935
This is a development platform that we've built.

2379
01:52:17,935 --> 01:52:20,695
So there's a prototype here of Google Beam.

2380
01:52:20,695 --> 01:52:22,195
There's one right down the hallway.

2381
01:52:22,195 --> 01:52:24,395
I'm gonna go down and turn that on in a second.

2382
01:52:24,395 --> 01:52:25,575
We're gonna experience it together.

2383
01:52:25,575 --> 01:52:26,935
We'll be back in the same room.

2384
01:52:26,935 --> 01:52:27,475
Wonderful.

2385
01:52:27,475 --> 01:52:30,555
Whoa.

2386
01:52:30,555 --> 01:52:30,955
Okay.

2387
01:52:30,955 --> 01:52:31,535
Hey, Lex.

2388
01:52:31,535 --> 01:52:32,335
Here we are.

2389
01:52:32,335 --> 01:52:32,975
All right.

2390
01:52:32,975 --> 01:52:33,135
Okay.

2391
01:52:33,135 --> 01:52:34,395
 This is real already.

2392
01:52:34,395 --> 01:52:35,075
Wow.

2393
01:52:35,075 --> 01:52:35,675
This is real.

2394
01:52:35,675 --> 01:52:36,075
Wow.

2395
01:52:36,075 --> 01:52:36,755
Good to see you.

2396
01:52:36,755 --> 01:52:37,775
This is Google Beam.

2397
01:52:37,775 --> 01:52:38,195
Uh-huh.

2398
01:52:38,195 --> 01:52:41,235
We're trying to make it feel like you and I could be anywhere in the world

2399
01:52:41,235 --> 01:52:44,075
but when these magic windows open, we're back together.

2400
01:52:44,075 --> 01:52:47,235
I see you exactly the same way you see me.

2401
01:52:47,235 --> 01:52:50,415
It's almost like we're sitting at the table sharing a table together.

2402
01:52:50,415 --> 01:52:51,275
I could- Yeah.

2403
01:52:51,275 --> 01:52:51,275
.

2404
01:52:51,275 --> 01:52:51,275
.

2405
01:52:51,275 --> 01:52:52,995
you know, learn from you, talk to you

2406
01:52:52,995 --> 01:52:54,675
share a meal with you, get to know you.

2407
01:52:54,675 --> 01:52:55,995
So you can feel the depth of this.

2408
01:52:55,995 --> 01:52:56,255
Uh, or just use your hand.

2409
01:52:56,255 --> 01:52:57,595
Yeah, great for me too.

2410
01:52:57,595 --> 01:52:58,035
Wow.

2411
01:52:58,035 --> 01:52:59,495
Wow.

2412
01:52:59,495 --> 01:53:02,275
So for people who probably can't even imagine what this looks like

2413
01:53:02,275 --> 01:53:03,855
there's a, there's a 3D version.

2414
01:53:03,855 --> 01:53:04,675
It looks real.

2415
01:53:04,675 --> 01:53:05,975
You look real.

2416
01:53:05,975 --> 01:53:06,015
Yeah.

2417
01:53:06,015 --> 01:53:07,495
It looks, to me, it looks real to you.

2418
01:53:07,495 --> 01:53:09,015
It looks like you're coming out of the screen.

2419
01:53:09,015 --> 01:53:13,995
And we, we quickly believe, um, once we're in Beam that we're just together.

2420
01:53:13,995 --> 01:53:14,155
Yeah.

2421
01:53:14,155 --> 01:53:14,155
Yep.

2422
01:53:14,155 --> 01:53:15,195
Like, you settle into it.

2423
01:53:15,195 --> 01:53:17,995
You're naturally attuned to seeing the world like this

2424
01:53:17,995 --> 01:53:22,995
and you just get used to seeing people this way but literally from anywhere in the world with these magic screens.

2425
01:53:22,995 --> 01:53:23,735
This is incredible.

2426
01:53:23,735 --> 01:53:24,935
It's a neat technology.

2427
01:53:24,935 --> 01:53:25,675
Wow.

2428
01:53:25,675 --> 01:53:28,315
So I saw demos of this but they're

2429
01:53:28,315 --> 01:53:30,075
they don't come close to the experience of this.

2430
01:53:30,075 --> 01:53:30,375
Right.

2431
01:53:30,375 --> 01:53:33,215
Yeah, I think one of the top YouTube comments on one of the demos I saw was like

2432
01:53:33,215 --> 01:53:34,575
"Why would I want a high definition?

2433
01:53:34,575 --> 01:53:36,175
Like, I'm trying to turn off the camera.

2434
01:53:36,175 --> 01:53:36,755
" Yeah.

2435
01:53:36,755 --> 01:53:42,115
But this actually is, this feels like the camera's been turned off and we're just in the same room together.

2436
01:53:42,115 --> 01:53:43,535
This is really compelling.

2437
01:53:43,535 --> 01:53:44,775
That's right.

2438
01:53:44,775 --> 01:53:46,375
I, I know it's kind of late in the day too

2439
01:53:46,375 --> 01:53:48,875
so I brought you a snack just in case you're a little bit hungry.

2440
01:53:48,875 --> 01:53:49,263
But, um.

2441
01:53:49,263 --> 01:53:49,375
..

2442
01:53:49,375 --> 01:53:52,595
So what, uh, can you push it farther and it just becomes

2443
01:53:52,595 --> 01:53:52,935
ah- Yeah.

2444
01:53:52,935 --> 01:53:54,655
Let's, let's try to float it between rooms

2445
01:53:54,655 --> 01:53:54,795
you know?

2446
01:53:54,795 --> 01:53:56,435
It kind of fades it from my room into your world.

2447
01:53:56,435 --> 01:53:57,935
And then, and then you see my hand

2448
01:53:57,935 --> 01:53:58,935
the depth of my hand.

2449
01:53:58,935 --> 01:53:59,315
Yeah, of course.

2450
01:53:59,315 --> 01:53:59,535
Yeah.

2451
01:53:59,535 --> 01:53:59,775
Wow.

2452
01:53:59,775 --> 01:53:59,795
Wow.

2453
01:53:59,795 --> 01:54:00,155
Of course, yeah.

2454
01:54:00,155 --> 01:54:00,930
It feels like you've.

2455
01:54:00,930 --> 01:54:01,015
..

2456
01:54:01,015 --> 01:54:01,815
Um, try this.

2457
01:54:01,815 --> 01:54:04,575
Try giving me a high five, and there's almost a sensation of human touch.

2458
01:54:04,575 --> 01:54:04,835
Yeah.

2459
01:54:04,835 --> 01:54:06,115
You can almost feel- Yes.

2460
01:54:06,115 --> 01:54:06,148
.

2461
01:54:06,148 --> 01:54:06,215
..

2462
01:54:06,215 --> 01:54:07,875
because you're so attuned to, you know

2463
01:54:07,875 --> 01:54:11,415
that should be a high five, it feeling like you could connect with somebody that way.

2464
01:54:11,415 --> 01:54:11,515
Yeah.

2465
01:54:11,515 --> 01:54:12,775
So it's kind of a magical experience.

2466
01:54:12,775 --> 01:54:13,855
Oh, this is really nice.

2467
01:54:13,855 --> 01:54:14,715
How much does it cost?

2468
01:54:14,715 --> 01:54:15,155
 Yeah, you.

2469
01:54:15,155 --> 01:54:15,275
..

2470
01:54:15,275 --> 01:54:16,535
Would, would you.

2471
01:54:16,535 --> 01:54:16,695
..

2472
01:54:16,695 --> 01:54:18,495
 Got a lot of companies testing it.

2473
01:54:18,495 --> 01:54:22,075
We just announced that we're gonna be bringing it to offices soon as a set of products.

2474
01:54:22,075 --> 01:54:24,115
We've got some companies helping us build these screens.

2475
01:54:24,115 --> 01:54:26,815
Um, but eventually I think this will be in almost every screen.

2476
01:54:26,815 --> 01:54:29,175
There's nothing, I'm not wearing anything.

2477
01:54:29,175 --> 01:54:31,015
Well, I'm wearing a suit and tie- I don't think so.

2478
01:54:31,015 --> 01:54:31,015
.

2479
01:54:31,015 --> 01:54:31,015
.

2480
01:54:31,015 --> 01:54:31,795
to clarify.

2481
01:54:31,795 --> 01:54:32,315
 Yeah.

2482
01:54:32,315 --> 01:54:33,175
I am wearing clothes.

2483
01:54:33,175 --> 01:54:34,375
This is not CGI.

2484
01:54:34,375 --> 01:54:38,155
But outside of that, cool, and the audio's really good.

2485
01:54:38,155 --> 01:54:40,475
And you can see me in this same three-dimensional way.

2486
01:54:40,475 --> 01:54:41,135
Yeah.

2487
01:54:41,135 --> 01:54:41,215
Okay.

2488
01:54:41,215 --> 01:54:43,675
The audio's spatialized, so if I'm talking from here

2489
01:54:43,675 --> 01:54:45,375
of course it sounds like I'm talking from here.

2490
01:54:45,375 --> 01:54:47,535
You know, if I move to the other side of the room- Wow.

2491
01:54:47,535 --> 01:54:47,541
.

2492
01:54:47,541 --> 01:54:47,555
..

2493
01:54:47,555 --> 01:54:47,915
I'm here.

2494
01:54:47,915 --> 01:54:52,055
Uh, so these little subtle cues, these really matter to bring people together.

2495
01:54:52,055 --> 01:54:55,275
All the non-verbals, all the emotion, the things that are lost today

2496
01:54:55,275 --> 01:54:56,255
here it is.

2497
01:54:56,255 --> 01:54:57,835
We put it back into this system.

2498
01:54:57,835 --> 01:54:58,615
You pulled this off.

2499
01:54:58,615 --> 01:54:59,190
And it feels like you're.

2500
01:54:59,190 --> 01:54:59,235
..

2501
01:54:59,235 --> 01:55:00,475
Holy shit.

2502
01:55:00,475 --> 01:55:01,415
They pulled it off.

2503
01:55:01,415 --> 01:55:03,275
And integrated- Yeah.

2504
01:55:03,275 --> 01:55:03,275
.

2505
01:55:03,275 --> 01:55:03,275
.

2506
01:55:03,275 --> 01:55:05,075
into this, I saw the translation also.

2507
01:55:05,075 --> 01:55:05,555
Right?

2508
01:55:05,555 --> 01:55:06,875
This is the- Yeah, we've got a bunch of things.

2509
01:55:06,875 --> 01:55:08,655
Let, let me show you a couple kind of cool things.

2510
01:55:08,655 --> 01:55:10,015
Let's do a little bit of work together.

2511
01:55:10,015 --> 01:55:12,795
Maybe we could, um, critique one- Yeah.

2512
01:55:12,795 --> 01:55:12,795
.

2513
01:55:12,795 --> 01:55:12,795
.

2514
01:55:12,795 --> 01:55:13,767
of your latest, uh.

2515
01:55:13,767 --> 01:55:13,935
..

2516
01:55:13,935 --> 01:55:16,315
 Um, so, uh, you know

2517
01:55:16,315 --> 01:55:19,275
it's you and I work together, so of course we're in the same room

2518
01:55:19,275 --> 01:55:21,515
but with this superpower I can bring other teams- Look at that.

2519
01:55:21,515 --> 01:55:21,515
.

2520
01:55:21,515 --> 01:55:21,515
.

2521
01:55:21,515 --> 01:55:22,095
in here with me.

2522
01:55:22,095 --> 01:55:23,755
Um, and it's, it's nice.

2523
01:55:23,755 --> 01:55:26,035
It, you know, it's like we could sit together

2524
01:55:26,035 --> 01:55:27,995
we could watch something, we could work.

2525
01:55:27,995 --> 01:55:30,815
Um, we've shared meals as a team together in this system.

2526
01:55:30,815 --> 01:55:33,715
But once you do the presence aspect of this- Wow.

2527
01:55:33,715 --> 01:55:33,715
.

2528
01:55:33,715 --> 01:55:33,715
.

2529
01:55:33,715 --> 01:55:35,415
you wanna bring some other superpowers to it.

2530
01:55:35,415 --> 01:55:38,403
And so you can do, review code together.

2531
01:55:38,403 --> 01:55:39,636
Yeah, yeah, exactly.

2532
01:55:39,636 --> 01:55:41,455
I've got some, uh, slides I'm working on.

2533
01:55:41,455 --> 01:55:42,895
You know, maybe you could help me with this.

2534
01:55:42,895 --> 01:55:44,235
Keep your eyes on me for a second.

2535
01:55:44,235 --> 01:55:46,115
I'll slide back into the center.

2536
01:55:46,115 --> 01:55:49,795
I didn't really move, but the system just kind of puts us in the right spot and knows where we need to be.

2537
01:55:49,795 --> 01:55:51,856
Oh, so you just turn to your laptop

2538
01:55:51,856 --> 01:55:54,835
the system moves you, and then it does the overlay automatically?

2539
01:55:54,835 --> 01:55:57,276
It kind of morphs the room to put things- Yeah.

2540
01:55:57,276 --> 01:55:57,276
.

2541
01:55:57,276 --> 01:55:57,276
.

2542
01:55:57,276 --> 01:55:59,075
in the spot that they need to, to be in.

2543
01:55:59,075 --> 01:56:00,655
Everything has a place in the room.

2544
01:56:00,655 --> 01:56:03,995
Everything has a sense of presence or spatial consistency

2545
01:56:03,995 --> 01:56:06,815
and that kind of makes it feel like we're together with us and other things.

2546
01:56:06,815 --> 01:56:09,095
I, I should also say, you're not just three-dimensional.

2547
01:56:09,095 --> 01:56:13,515
It feels like you're leaning, like, out of the screen.

2548
01:56:13,515 --> 01:56:14,675
You f- you- I am?

2549
01:56:14,675 --> 01:56:16,295
You're like coming out of the screen.

2550
01:56:16,295 --> 01:56:16,335
Right.

2551
01:56:16,335 --> 01:56:18,895
You're not just in that world three-dimensional.

2552
01:56:18,895 --> 01:56:19,675
Yeah, exactly.

2553
01:56:19,675 --> 01:56:20,136
Right.

2554
01:56:20,136 --> 01:56:21,335
Holy crap.

2555
01:56:21,335 --> 01:56:22,095
All right.

2556
01:56:22,095 --> 01:56:22,915
So- Move back to center?

2557
01:56:22,915 --> 01:56:23,675
Okay, okay, yeah, yeah.

2558
01:56:23,675 --> 01:56:24,935
Let, let me tell you how this works.

2559
01:56:24,935 --> 01:56:26,935
You probably already have the, the premise of it

2560
01:56:26,935 --> 01:56:28,415
but there's two things.

2561
01:56:28,415 --> 01:56:28,595
Yeah.

2562
01:56:28,595 --> 01:56:30,436
Two really hard things that we put together.

2563
01:56:30,436 --> 01:56:33,136
One is a AI video model.

2564
01:56:33,136 --> 01:56:36,055
So there's a set of cameras, you asked kind of about those earlier.

2565
01:56:36,055 --> 01:56:39,995
There's six color cameras, just like webcams that we have today- Mm-hmm.

2566
01:56:39,995 --> 01:56:39,995
.

2567
01:56:39,995 --> 01:56:39,995
.

2568
01:56:39,995 --> 01:56:45,495
taking video streams and feeding them into our AI model and turning that into a 3D video of you and I.

2569
01:56:45,495 --> 01:56:45,515
Mm-hmm.

2570
01:56:45,515 --> 01:56:50,395
It's effectively a light field, so it's kind of an interactive 3D video that you can see from any perspective.

2571
01:56:50,395 --> 01:56:54,495
That's transmitted over to the second thing, and that's a light field display

2572
01:56:54,495 --> 01:56:56,175
and it's happening bidirectionally.

2573
01:56:56,175 --> 01:56:58,695
I see you and you see me both in our light field displays.

2574
01:56:58,695 --> 01:57:02,635
These are effectively flat televisions or flat displays

2575
01:57:02,635 --> 01:57:06,255
but they have this sense of dimensionality, depth

2576
01:57:06,255 --> 01:57:07,875
size is correct.

2577
01:57:07,875 --> 01:57:09,955
You can see shadows and lighting are correct.

2578
01:57:09,955 --> 01:57:10,255
Mm-hmm.

2579
01:57:10,255 --> 01:57:13,035
And everything's correct from your vantage point.

2580
01:57:13,035 --> 01:57:15,655
So if you move around ever so slightly and I hold still

2581
01:57:15,655 --> 01:57:17,615
you see a different perspective here.

2582
01:57:17,615 --> 01:57:20,235
You see kind of things that were occluded become revealed.

2583
01:57:20,235 --> 01:57:22,715
You see shadows that, you know, moving the way they should move.

2584
01:57:22,715 --> 01:57:27,195
All of that's computed and generated using our AI video model for you.

2585
01:57:27,195 --> 01:57:29,055
It's based on your eye position.

2586
01:57:29,055 --> 01:57:33,795
Where does the right scene need to be placed in this light field display for you just to feel present?

2587
01:57:33,795 --> 01:57:35,315
It's real time, no latency.

2588
01:57:35,315 --> 01:57:37,255
I'm not seeing late- you weren't freezing up at all.

2589
01:57:37,255 --> 01:57:38,695
No, no, I hope not.

2590
01:57:38,695 --> 01:57:40,255
I think it's-  It's you and I together

2591
01:57:40,255 --> 01:57:41,055
real time.

2592
01:57:41,055 --> 01:57:43,695
That's what you need for real communication, and at a

2593
01:57:43,695 --> 01:57:45,535
at a quality level that I think is- This is awesome.

2594
01:57:45,535 --> 01:57:45,635
.

2595
01:57:45,635 --> 01:57:45,835
..

2596
01:57:45,835 --> 01:57:46,935
realistic.

2597
01:57:46,935 --> 01:57:48,615
Is it possible to do three people?

2598
01:57:48,615 --> 01:57:50,275
Like, is that gonna move that way also?

2599
01:57:50,275 --> 01:57:50,775
Yeah.

2600
01:57:50,775 --> 01:57:51,895
Let me, let me kind of show you.

2601
01:57:51,895 --> 01:57:54,495
So if, if she enters the room with us- Mm-hmm.

2602
01:57:54,495 --> 01:57:54,561
.

2603
01:57:54,561 --> 01:57:54,695
..

2604
01:57:54,695 --> 01:57:56,295
you can see her, you can see me

2605
01:57:56,295 --> 01:58:00,215
and if we add more people, you eventually lose the sense of presence.

2606
01:58:00,215 --> 01:58:02,795
You kind of shrink people down, you lose a sense of scale.

2607
01:58:02,795 --> 01:58:05,775
So think of it as the window fits a certain number of people.

2608
01:58:05,775 --> 01:58:05,895
Mm-hmm.

2609
01:58:05,895 --> 01:58:08,135
If you want to fit a big group of people

2610
01:58:08,135 --> 01:58:09,875
you want, you know, the board room or the big room

2611
01:58:09,875 --> 01:58:11,775
you need like a much wider window.

2612
01:58:11,775 --> 01:58:13,335
Um, if you want to see, you know

2613
01:58:13,335 --> 01:58:15,935
just grandma and the kids, you can do smaller windows.

2614
01:58:15,935 --> 01:58:18,115
So everybody has a seat at the table

2615
01:58:18,115 --> 01:58:20,075
everybody has a sense of where they belong

2616
01:58:20,075 --> 01:58:21,915
and there's kind of this sense of presence that's obeyed.

2617
01:58:21,915 --> 01:58:25,555
If you have too many people, you kind of go back to like 2D metaphors that we're used to

2618
01:58:25,555 --> 01:58:27,015
people in tiles placed anywhere.

2619
01:58:27,015 --> 01:58:29,195
For the image I'm seeing, did you have to get scanned?

2620
01:58:29,195 --> 01:58:31,315
I, I mean, I see you without being scanned.

2621
01:58:31,315 --> 01:58:33,455
So it's just so much easier if you don't have to wear anything

2622
01:58:33,455 --> 01:58:34,675
you don't have to pre-scan.

2623
01:58:34,675 --> 01:58:34,795
Yeah.

2624
01:58:34,795 --> 01:58:39,155
You just do it the way it's supposed to happen without anybody having to learn anything or put anything on.

2625
01:58:39,155 --> 01:58:41,695
I thought you had to solve the, the scanning problem

2626
01:58:41,695 --> 01:58:42,855
but here you don't.

2627
01:58:42,855 --> 01:58:44,515
It's just cameras.

2628
01:58:44,515 --> 01:58:44,635
No, that's right.

2629
01:58:44,635 --> 01:58:45,295
It's just vision.

2630
01:58:45,295 --> 01:58:45,555
That's right.

2631
01:58:45,555 --> 01:58:47,415
It, it's video.

2632
01:58:47,415 --> 01:58:49,815
Yeah, we're not trying to kind of make an approximation of you

2633
01:58:49,815 --> 01:58:52,395
because everything you do every day matters, you know?

2634
01:58:52,395 --> 01:58:54,115
I cut myself shaving-  .

2635
01:58:54,115 --> 01:58:54,115
..

2636
01:58:54,115 --> 01:58:55,055
I put on a pin.

2637
01:58:55,055 --> 01:58:57,175
Um, all the little kind of, you know

2638
01:58:57,175 --> 01:58:59,455
aspects of you, those just happen.

2639
01:58:59,455 --> 01:59:02,935
Um, we don't have the time to scan or kind of capture those or dress avatars.

2640
01:59:02,935 --> 01:59:04,935
We, we kind of appear as we appear.

2641
01:59:04,935 --> 01:59:08,275
And so all that's transmitted truthfully as it's happening.

2642
01:59:08,275 --> 01:59:11,135
 Chris, still, how you doing?

2643
01:59:11,135 --> 01:59:11,895
Nice to meet you.

2644
01:59:11,895 --> 01:59:12,295
Nice to meet you.

2645
01:59:12,295 --> 01:59:14,835
So, um, as Max mentioned, we've got AI glasses here.

2646
01:59:14,835 --> 01:59:17,135
We start with, um, a foundation of great glasses

2647
01:59:17,135 --> 01:59:19,095
something stylish, lightweight, wearable.

2648
01:59:19,095 --> 01:59:22,218
Then we say, "How can we build great technology and experiences on top of that?

2649
01:59:22,218 --> 01:59:25,475
" Um, one of the core tenets of the Android XR platform

2650
01:59:25,475 --> 01:59:28,795
this idea of a multimodal conversational device, see what you see

2651
01:59:28,795 --> 01:59:29,335
hear what you hear.

2652
01:59:29,335 --> 01:59:31,495
So you've got a camera, you've got speakers

2653
01:59:31,495 --> 01:59:33,575
multiple microphones for speaker isolation.

2654
01:59:33,575 --> 01:59:36,415
Um, I'll give you a chance to try these yourself.

2655
01:59:36,415 --> 01:59:36,715
Yeah.

2656
01:59:36,715 --> 01:59:37,115
Sorry.

2657
01:59:37,115 --> 01:59:37,175
All right.

2658
01:59:37,175 --> 01:59:37,855
We'll put it up there.

2659
01:59:37,855 --> 01:59:39,895
Whoa.

2660
01:59:39,895 --> 01:59:40,295
Yeah.

2661
01:59:40,295 --> 01:59:41,855
So the first thing you see- Yeah.

2662
01:59:41,855 --> 01:59:41,855
.

2663
01:59:41,855 --> 01:59:41,855
.

2664
01:59:41,855 --> 01:59:45,135
is a super, uh, simple, straightforward home screen.

2665
01:59:45,135 --> 01:59:45,515
Yes.

2666
01:59:45,515 --> 01:59:47,475
So you probably see the time, the weather

2667
01:59:47,475 --> 01:59:49,055
calendar appointments there.

2668
01:59:49,055 --> 01:59:53,455
This is designed to be sort of your one-stop shop for quick glanceable information throughout the day.

2669
01:59:53,455 --> 01:59:53,495
Mm-hmm.

2670
01:59:53,495 --> 01:59:54,975
We want it to be something that's, um

2671
01:59:54,975 --> 01:59:57,695
easy to, you know, get what you need and then go back to what you're doing.

2672
01:59:57,695 --> 01:59:59,495
So you can imagine, you know, turning on the display

2673
01:59:59,495 --> 02:00:01,715
getting that quick glance, and then, um

2674
02:00:01,715 --> 02:00:02,755
continuing on your way.

2675
02:00:02,755 --> 02:00:04,275
You can be fully conversational at this point.

2676
02:00:04,275 --> 02:00:06,275
You can, um, ask questions, for example

2677
02:00:06,275 --> 02:00:06,955
about the paintings.

2678
02:00:06,955 --> 02:00:09,015
You can interrupt, you can ask follow-up questions.

2679
02:00:09,015 --> 02:00:11,095
And as I mentioned before, if you want to at any time pause

2680
02:00:11,095 --> 02:00:12,075
you just tap there on the right.

2681
02:00:12,075 --> 02:00:15,035
Gemini, how much does this painting cost?

2682
02:00:15,035 --> 02:00:18,215
The painting is called The Bridge of Les Filippes

2683
02:00:18,215 --> 02:00:20,915
1875 by Armand Guillaumin.

2684
02:00:20,915 --> 02:00:25,615
It appears to be a print, and it would be impossible to know its cost without finding the seller.

2685
02:00:25,615 --> 02:00:26,775
Okay, so this is fake.

2686
02:00:26,775 --> 02:00:27,555
I understand.

2687
02:00:27,555 --> 02:00:30,315
Why do humans pay so much money for paintings?

2688
02:00:30,315 --> 02:00:33,355
People pay a lot for paintings because of their artistic merit

2689
02:00:33,355 --> 02:00:37,055
the artist's fame, sense of rarity, and for the appreciation you can learn.

2690
02:00:37,055 --> 02:00:40,275
So, uh, a few more things that we want to show you just for

2691
02:00:40,275 --> 02:00:41,075
uh, sake of time.

2692
02:00:41,075 --> 02:00:42,835
Could you go ahead and long press on the side again to

2693
02:00:42,835 --> 02:00:43,915
to sleep Gemini there?

2694
02:00:43,915 --> 02:00:45,015
Um, there you go.

2695
02:00:45,015 --> 02:00:48,455
Um, did you catch Google I/O last week by any chance?

2696
02:00:48,455 --> 02:00:48,735
Yes.

2697
02:00:48,735 --> 02:00:50,475
So you might have seen on stage the

2698
02:00:50,475 --> 02:00:52,655
uh, Google Maps experience very briefly.

2699
02:00:52,655 --> 02:00:52,815
Mm-hmm.

2700
02:00:52,815 --> 02:00:55,575
I wanted to give you a chance to get a sense of what that feels like today.

2701
02:00:55,575 --> 02:00:57,295
You can imagine you're walking down the street.

2702
02:00:57,295 --> 02:00:59,915
If you look up like you're walking straight ahead

2703
02:00:59,915 --> 02:01:01,855
you get quick turn-by-turn directions.

2704
02:01:01,855 --> 02:01:03,795
So you have a sense of-  Well done.

2705
02:01:03,795 --> 02:01:03,795
.

2706
02:01:03,795 --> 02:01:03,795
.

2707
02:01:03,795 --> 02:01:05,255
what the next turn is like- Nice.

2708
02:01:05,255 --> 02:01:05,261
.

2709
02:01:05,261 --> 02:01:05,275
..

2710
02:01:05,275 --> 02:01:06,335
without taking your phone in your pocket.

2711
02:01:06,335 --> 02:01:07,755
Oh, that's so intuitive.

2712
02:01:07,755 --> 02:01:10,275
Sometimes you need that quick sense of which way- Yeah.

2713
02:01:10,275 --> 02:01:10,275
.

2714
02:01:10,275 --> 02:01:10,275
.

2715
02:01:10,275 --> 02:01:10,775
is the right way.

2716
02:01:10,775 --> 02:01:11,455
Sometimes.

2717
02:01:11,455 --> 02:01:11,715
Yeah.

2718
02:01:11,715 --> 02:01:13,535
So you, let's say you're coming out of the subway- Every time.

2719
02:01:13,535 --> 02:01:13,535
.

2720
02:01:13,535 --> 02:01:13,535
.

2721
02:01:13,535 --> 02:01:15,775
getting out of a cab, you can just glance down at your feet.

2722
02:01:15,775 --> 02:01:18,575
We have it set up to translate from Russian to English.

2723
02:01:18,575 --> 02:01:18,815
Mm-hmm.

2724
02:01:18,815 --> 02:01:20,155
I think I get to wear the glasses.

2725
02:01:20,155 --> 02:01:20,195
Mm-hmm.

2726
02:01:20,195 --> 02:01:22,375
And you speak to me if you don't mind.

2727
02:01:22,375 --> 02:01:24,715
I can speak Russian.

2728
02:01:24,715 --> 02:01:29,535
Uh, .

2729
02:01:29,535 --> 02:01:30,175
I'm doing well.

2730
02:01:30,175 --> 02:01:30,735
How are you doing?

2731
02:01:30,735 --> 02:01:35,475
Tempted to swear, tempted to say inappropriate things.

2732
02:01:35,475 --> 02:01:40,238
.

2733
02:01:40,238 --> 02:01:43,787
Uh, I see it transcribed in real time

2734
02:01:43,787 --> 02:01:46,447
and so obviously, you know, based on the

2735
02:01:46,447 --> 02:01:48,807
uh, different languages and sequence of subjects and verbs

2736
02:01:48,807 --> 02:01:52,687
there's a slight delay sometimes, but it's really just like subtitles for the real world.

2737
02:01:52,687 --> 02:01:53,108
Cool.

2738
02:01:53,108 --> 02:01:53,807
Thank you for this.

2739
02:01:53,807 --> 02:01:55,307
All right, back to me.

2740
02:01:55,307 --> 02:02:04,107
Hopefully watching videos of me having my mind blown like the apes in 2001: A Space Odyssey playing with a monolith was

2741
02:02:04,107 --> 02:02:05,168
uh, somewhat interesting.

2742
02:02:05,168 --> 02:02:07,407
Uh, like I said, I was very impressed.

2743
02:02:07,407 --> 02:02:12,947
And now I thought if it's okay, I could make a few additional comments about the episode and just in general.

2744
02:02:12,947 --> 02:02:19,028
In this conversation with Sundar Pichai, I discussed the concept of the Neolithic package

2745
02:02:19,028 --> 02:02:24,457
which is the set of innovations that came along with the first agricultural revolution about 12

2746
02:02:24,457 --> 02:02:28,367
000 years ago, which included the formation of social hierarchies

2747
02:02:28,367 --> 02:02:32,947
the early primitive forms of government, labor specialization

2748
02:02:32,947 --> 02:02:37,267
domestication of plants and animals, early forms of trade

2749
02:02:37,267 --> 02:02:41,647
large-scale cooperations of humans like that required to build

2750
02:02:41,647 --> 02:02:46,787
yes, the pyramids and temples like Göbekli Tepe.

2751
02:02:46,787 --> 02:02:52,427
I think this may be the right way to actually talk about the inventions that changed human history.

2752
02:02:52,427 --> 02:03:01,447
Not just as a single invention, but as a kind of network of innovations and transformations that came along with it.

2753
02:03:01,447 --> 02:03:06,507
And the productivity multiplier framework that I mentioned in the episode

2754
02:03:06,507 --> 02:03:13,027
I think is a nice way to try to concretize the impact of each of these inventions under consideration.

2755
02:03:13,027 --> 02:03:22,567
And, uh, we have to remember that each node in the network of the sort of fast follow-on inventions is in itself a productivity multiplier.

2756
02:03:22,567 --> 02:03:25,607
Some are additive, some are multiplicative.

2757
02:03:25,607 --> 02:03:36,987
So in some sense, the size of the network in the package is the thing that matters when you're trying to rank the impact of

2758
02:03:36,987 --> 02:03:40,067
uh, inventions on human history.

2759
02:03:40,067 --> 02:03:43,827
The easy picks for the period of biggest transformation

2760
02:03:43,827 --> 02:03:47,947
at least in sort of, uh, modern day discourse

2761
02:03:47,947 --> 02:03:51,247
is the Industrial Revolution, or even, uh

2762
02:03:51,247 --> 02:03:54,567
in the 20th century, the computer or the internet.

2763
02:03:54,567 --> 02:04:01,267
I think it's because it's easiest to intuit for modern day humans the impact

2764
02:04:01,267 --> 02:04:05,067
the exponential impact, uh, of those technologies.

2765
02:04:05,067 --> 02:04:07,647
But recently, and I suppose this changes week to week

2766
02:04:07,647 --> 02:04:11,727
but, uh, I have been doing a lot of reading on ancient human history

2767
02:04:11,727 --> 02:04:17,847
so recently my pick for the number one invention would have to be the first agricultural revolution

2768
02:04:17,847 --> 02:04:23,427
the Neolithic package that led to the formation of human civilizations.

2769
02:04:23,427 --> 02:04:28,807
That's what enabled the scaling of the collective intelligence machine of humanity

2770
02:04:28,807 --> 02:04:32,377
and for us to become the early bootloader for the next 10

2771
02:04:32,377 --> 02:04:35,627
000 years of technological progress, which, yes

2772
02:04:35,627 --> 02:04:39,467
includes AI and the tech that builds on top of AI.

2773
02:04:39,467 --> 02:04:46,687
And of course, it could be argued that the word invention doesn't properly apply to the agricultural revolution.

2774
02:04:46,687 --> 02:04:54,347
I think actually, uh, Yuval Noah Harari argues that it wasn't the humans who were the inventors

2775
02:04:54,347 --> 02:04:57,307
but, uh, a handful of plant species

2776
02:04:57,307 --> 02:04:59,887
namely wheat, rice, and potatoes.

2777
02:04:59,887 --> 02:05:04,887
This is strictly a fair perspective, but I'm having fun

2778
02:05:04,887 --> 02:05:06,567
like I said, with this discussion.

2779
02:05:06,567 --> 02:05:10,927
Here, I just think of the entire Earth as a system that continuously transforms

2780
02:05:10,927 --> 02:05:14,647
and I'm using the term invention in that context

2781
02:05:14,647 --> 02:05:21,707
asking the question of when was the biggest leap on the log scale plot of

2782
02:05:21,707 --> 02:05:22,887
uh, human progress?

2783
02:05:22,887 --> 02:05:27,847
Will AI, AGI, ASI eventually take the number one spot on this ranking?

2784
02:05:27,847 --> 02:05:30,727
I think it has a very good chance to do so

2785
02:05:30,727 --> 02:05:36,547
due, again, to the size of the network of inventions that will come along with it.

2786
02:05:36,547 --> 02:05:41,127
I think we discussed in this podcast, uh

2787
02:05:41,127 --> 02:05:45,007
the kind of things that would be included in the so-called AI package

2788
02:05:45,007 --> 02:05:48,267
but I think there's, uh, a lot more possibilities

2789
02:05:48,267 --> 02:05:52,507
including, uh, discussed in previous podcasts, in many previous podcasts

2790
02:05:52,507 --> 02:05:57,587
including with Dariam Aday, uh, talking on the biological innovation side

2791
02:05:57,587 --> 02:05:58,867
the science progress side.

2792
02:05:58,867 --> 02:06:04,407
In this podcast, I think we talk about something that I'm particularly excited about in the near term

2793
02:06:04,407 --> 02:06:12,527
which is unlocking the cognitive capacity of the entire landscape of brains that is the human species.

2794
02:06:12,527 --> 02:06:18,547
Making it more accessible through education and through machine translation

2795
02:06:18,547 --> 02:06:27,607
making information knowledge and the rapid learning and innovation process accessible to more humans

2796
02:06:27,607 --> 02:06:29,827
to the entire eight billion, if you will.

2797
02:06:29,827 --> 02:06:40,267
So I do think language or machine translation applied to all the different methods that we use on the internet to discover knowledge is

2798
02:06:40,267 --> 02:06:41,627
uh, a big unlock.

2799
02:06:41,627 --> 02:06:45,047
But there are a lot of other stuff in the so-called AI package

2800
02:06:45,047 --> 02:06:48,407
like discussed with Dario, curing all major human diseases.

2801
02:06:48,407 --> 02:06:53,627
He really focuses on that in the Machines of Love and Grace essay.

2802
02:06:53,627 --> 02:07:00,527
I think there will be huge leaps in productivity for human programmers and semi-autonomous human programmers

2803
02:07:00,527 --> 02:07:04,367
so humans in the loop, but most of the programming is done by AI agents

2804
02:07:04,367 --> 02:07:15,047
and then moving that towards a superhuman AI researcher that's doing the research that develops and programs the AI system in itself.

2805
02:07:15,047 --> 02:07:18,767
I think there will be huge transformative effects from autonomous vehicles.

2806
02:07:18,767 --> 02:07:25,127
These are the things that we maybe don't immediately understand or we understand from an economics perspective

2807
02:07:25,127 --> 02:07:32,287
but there will be a point when AI systems are able to interpret

2808
02:07:32,287 --> 02:07:43,147
understand, interact with the human world to a sufficient degree to where many of the manually controlled human in the loop systems we rely on become fully autonomous.

2809
02:07:43,147 --> 02:07:48,159
And I think mobility is such a big part of human civilization that.

2810
02:07:48,159 --> 02:07:48,387
..

2811
02:07:48,387 --> 02:07:51,999
There will be effects on that, that are not just economic

2812
02:07:51,999 --> 02:07:55,319
but are social, cultural, and so on.

2813
02:07:55,319 --> 02:07:58,839
And there's a lot more things I could talk about for a long time.

2814
02:07:58,839 --> 02:08:05,060
So obviously, the integration, u- utilization of AI in the creation of art

2815
02:08:05,060 --> 02:08:06,219
film, music.

2816
02:08:06,219 --> 02:08:16,299
I think the digitalization and, um, automating basic functions of government and then integrating AI into that process

2817
02:08:16,299 --> 02:08:21,119
thereby decreasing corruption and cost and increasing transparency and

2818
02:08:21,119 --> 02:08:22,179
uh, efficiency.

2819
02:08:22,179 --> 02:08:30,420
I think we as humans, individual humans will continue to transition further and further into cyborgs.

2820
02:08:30,420 --> 02:08:37,719
So the- there's already a AI in the loop of the human condition

2821
02:08:37,719 --> 02:08:41,799
and that will become increasingly so as the AI becomes

2822
02:08:41,799 --> 02:08:43,080
uh, more powerful.

2823
02:08:43,080 --> 02:08:47,059
The thing I'm obviously really excited about is major breakthroughs in science

2824
02:08:47,059 --> 02:08:49,059
and not just in the medical front, but

2825
02:08:49,059 --> 02:08:54,799
uh, in physics, fundamental physics, which would then lead to energy breakthroughs

2826
02:08:54,799 --> 02:09:00,320
increasing the chance that we become, we actually become a Kardashev type I civilization

2827
02:09:00,320 --> 02:09:07,619
and then enabling us in so doing to do interstellar exploration of space and colonization of space.

2828
02:09:07,619 --> 02:09:11,059
I think there- also, in the near term

2829
02:09:11,059 --> 02:09:20,919
much like with the, uh, Industrial Revolution that led to rapid specialization of skills and expertise

2830
02:09:20,919 --> 02:09:24,339
there might be a great sort of de-specialization.

2831
02:09:24,339 --> 02:09:29,859
So as the AI, AI system become superhuman experts at particular fields

2832
02:09:29,859 --> 02:09:37,079
there might be greater and greater value to being the integrator of AIs

2833
02:09:37,079 --> 02:09:40,999
for humans to be sort of generalists.

2834
02:09:40,999 --> 02:09:46,060
Uh, and so the great value of the human mind will come from the generalists

2835
02:09:46,060 --> 02:09:47,560
not the specialists.

2836
02:09:47,560 --> 02:09:51,259
That's a real possibility that that changes the way we are about the world

2837
02:09:51,259 --> 02:09:56,299
that we wanna know a little bit of a lot of things and move about the world in that way.

2838
02:09:56,299 --> 02:09:59,279
That could have, when passing a certain threshold

2839
02:09:59,279 --> 02:10:02,879
a complete shift in who we are as a collective intelligence

2840
02:10:02,879 --> 02:10:05,819
as a, as a human species.

2841
02:10:05,819 --> 02:10:10,559
Also, as an aside, when thought thinking about the invention that was the greatest in human history

2842
02:10:10,559 --> 02:10:15,619
again, for a bit of fun, we have to remember that all of them build on top of each other.

2843
02:10:15,619 --> 02:10:17,959
And so we need to look at the delta

2844
02:10:17,959 --> 02:10:20,579
the step change on the, I would say

2845
02:10:20,579 --> 02:10:24,359
impossibly to perfectly measured plot of exponential human progress.

2846
02:10:24,359 --> 02:10:28,159
Really, we can go back to the entire history of life on Earth

2847
02:10:28,159 --> 02:10:31,379
and, uh, previous podcast guest, Nick Lane

2848
02:10:31,379 --> 02:10:33,599
does a great job of this in his book

2849
02:10:33,599 --> 02:10:40,079
Life Ascending, listing these 10 major inventions throughout the evolution of life on Earth

2850
02:10:40,079 --> 02:10:45,499
like DNA, photosynthesis, complex cells, sex

2851
02:10:45,499 --> 02:10:48,379
movement, sight, all those kinds of things.

2852
02:10:48,379 --> 02:10:50,479
I forget the full list that's on there

2853
02:10:50,479 --> 02:10:55,919
but I think that's so far from the human experience that my intuition about

2854
02:10:55,919 --> 02:11:00,359
let's say, productivity multipliers of those particular invention completely

2855
02:11:00,359 --> 02:11:08,079
uh, breaks down, and, uh, a different framework is needed to understand the impact of these inventions of evolution.

2856
02:11:08,079 --> 02:11:12,399
The origin of life on Earth or even the Big Bang itself

2857
02:11:12,399 --> 02:11:18,519
of course, is the OG invention that set the stage for all the rest of it

2858
02:11:18,519 --> 02:11:25,679
and there are probably many more turtles under that which are yet to be discovered.

2859
02:11:25,679 --> 02:11:29,319
So anyway, we live in interesting times

2860
02:11:29,319 --> 02:11:30,299
fellow humans.

2861
02:11:30,299 --> 02:11:36,819
I do believe the set of positive trajectories for humanity outnumber the set of negative trajectories

2862
02:11:36,819 --> 02:11:38,319
but not by much.

2863
02:11:38,319 --> 02:11:41,039
So, uh, let's not mess this up.

2864
02:11:41,039 --> 02:11:47,039
And now, let me leave you with some words from French philosopher Jean de la Bruyère

2865
02:11:47,039 --> 02:11:51,109
"Out of difficulties grow miracles.

2866
02:11:51,109 --> 02:11:55,599
" Thank you for listening, and hope to see you next time.

