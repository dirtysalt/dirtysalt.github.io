1
00:00:00,080 --> 00:00:00,330
despite the fact that i was involved in inventing the transformer

2
00:00:00,340 --> 00:00:00,330
luckily

3
00:00:00,340 --> 00:00:00,330
luckily

4
00:00:00,340 --> 00:00:06,410
no one's been working on them as long as i have rights

5
00:00:06,420 --> 00:00:09,610
with maybe the exception of the other se seven authors.

6
00:00:09,620 --> 00:00:09,610
so i actually made the decision uh

7
00:00:09,620 --> 00:00:14,809
earlier this year

8
00:00:14,819 --> 00:00:18,970
that i'm going to drastically reduce the amounts of of research that i'm doing specifically on the transformer

9
00:00:18,980 --> 00:00:18,970
because of the feeling

10
00:00:18,980 --> 00:00:24,010
that i have

11
00:00:24,020 --> 00:00:24,010
that it's it's an oversaturated space, right? it's not

12
00:00:24,020 --> 00:00:29,689
that there's no more interesting things to be done with them.

13
00:00:29,699 --> 00:00:40,010
and i'm going to make use of the opportunity to do something different, right? to actually turn up the amount of exploration

14
00:00:40,020 --> 00:00:40,010
that i'm doing in my research.

15
00:00:43,360 --> 00:00:43,610
we just released the continuous thought machine.

16
00:00:43,620 --> 00:00:48,410
it's a spotlight at europe's 2025 this year.

17
00:00:48,420 --> 00:00:48,410
you should care about it because it has native adaptive compute.

18
00:00:53,520 --> 00:00:58,969
it's a new way of building a recurrent model that uses higher level concepts for neurons

19
00:00:58,979 --> 00:01:04,170
and a synchronization as a representation that lets us solve problems in ways that seem more human

20
00:01:04,180 --> 00:01:04,170
by being biologically and nature inspired.

21
00:01:10,320 --> 00:01:15,530
the atmosphere in ai research was actually quite different back during the transformer

22
00:01:15,540 --> 00:01:15,530
uh

23
00:01:15,540 --> 00:01:15,530
uh

24
00:01:15,540 --> 00:01:15,530
uh

25
00:01:15,540 --> 00:01:20,970
because it doesn't feel like something similar could actually happen right now.

26
00:01:20,980 --> 00:01:20,970
because of the reduced amount of freedom

27
00:01:20,980 --> 00:01:26,250
that we have, right? the transformers was very very bottom up, right? it's not

28
00:01:26,260 --> 00:01:32,650
that somebody had this grand plan

29
00:01:32,660 --> 00:01:36,729
that came down from on high that this is what we should be working on.

30
00:01:36,739 --> 00:01:41,770
it was a bunch of people talking over lunch, thinking about what the current problems are

31
00:01:41,780 --> 00:01:41,770
and how to solve them

32
00:01:41,780 --> 00:01:46,810
and having the freedom to have, you know,

33
00:01:46,820 --> 00:01:51,930
literally months to dedicate to just trying this idea and having this this new architecture fall out.

34
00:01:56,960 --> 00:01:57,210
we've spent hundreds of millions of dollars.

35
00:01:57,220 --> 00:02:02,409
the biggest sort of evolution based search is probably in the tens of thousands.

36
00:02:02,419 --> 00:02:02,409
we have all this compute.

37
00:02:07,200 --> 00:02:07,450
what happens? what happens if you scale up these search algorithms?

38
00:02:07,460 --> 00:02:12,170
and i'm sure you'll find something interesting, you know,

39
00:02:12,180 --> 00:02:16,010
when someone eventually does bite that bullet and really scale up these evolutionary sort of a life experiments

40
00:02:20,879 --> 00:02:25,930
because i pitched it in an environment where people were just going all in on this one technology.

41
00:02:25,940 --> 00:02:25,930
i got zero interest.

42
00:02:30,239 --> 00:02:30,489
so now i have my own company and i can pursue those directions.

43
00:02:36,080 --> 00:02:36,330
this podcast is supported by cyber fund. > > hey folks,

44
00:02:36,340 --> 00:02:36,330
i'm omar,

45
00:02:36,340 --> 00:02:42,330
product and design lead at google deepmind.

46
00:02:42,340 --> 00:02:46,010
we just launched a revamped vibe coding experience in ai studio that lets you mix

47
00:02:46,020 --> 00:02:50,410
and match ai capabilities to turn your ideas into reality faster than ever.

48
00:02:50,420 --> 00:02:50,410
just describe your app

49
00:02:50,420 --> 00:02:55,050
and gemini will automatically wire up the right models and apis for you.

50
00:02:55,060 --> 00:02:59,209
and if you need a spark, hit

51
00:02:59,219 --> 00:02:59,209
i'm feeling lucky and we'll help you get started.

52
00:02:59,219 --> 00:03:06,730
head to a. studio / build studio / build to create your first app. > >

53
00:03:06,740 --> 00:03:06,730
two for ai labs is a research lab based in zurich.

54
00:03:06,740 --> 00:03:12,330
they've got a team of amazing ml engineers and research scientists.

55
00:03:12,340 --> 00:03:15,930
they're doing some really cool stuff. if you look at their website, for example,

56
00:03:15,940 --> 00:03:19,450
you can see what their approach was for winning the arc agi 3 pub

57
00:03:19,460 --> 00:03:19,450
uh

58
00:03:19,460 --> 00:03:19,450
uh

59
00:03:19,460 --> 00:03:19,450
uh

60
00:03:19,460 --> 00:03:24,890
and they are hiring amazing ml engineers and research scientists.

61
00:03:24,900 --> 00:03:29,290
they also care deeply about ai safety.

62
00:03:29,300 --> 00:03:33,690
so if any of that is a fit for you, please go to two for aolabs

63
00:03:33,700 --> 00:03:33,690
and

64
00:03:33,700 --> 00:03:33,690
and

65
00:03:33,700 --> 00:03:33,690
and

66
00:03:33,700 --> 00:03:38,810
the audience will know i'm a huge fan of kenneth stanley's ideas.

67
00:03:38,820 --> 00:03:42,409
so his book, why greatness cannot be planned, changed my life.

68
00:03:42,419 --> 00:03:42,409
it was absolutely insane.

69
00:03:42,419 --> 00:03:42,409
it was absolutely insane.

70
00:03:42,419 --> 00:03:47,610
that we need to allow people to follow their own gradient of interest

71
00:03:47,620 --> 00:03:47,610
unfettered

72
00:03:47,620 --> 00:03:52,330
by objectives and committees and and so on.

73
00:03:52,340 --> 00:03:57,370
because that is how we do epistemic foraging.

74
00:03:57,380 --> 00:03:57,370
that

75
00:03:57,380 --> 00:04:02,090
when you have too many agendas involved in the mix, you kind of end up with a gray goo

76
00:04:02,100 --> 00:04:02,090
and you don't discover, you know,

77
00:04:02,100 --> 00:04:10,569
interesting novelty and diversity. and i suppose that's basically the thesis of of your company, sakana,

78
00:04:10,579 --> 00:04:10,569
is to lean into those ideas. > > yes, exactly.

79
00:04:10,579 --> 00:04:16,970
um, at the company, we're a massive fan of that book.

80
00:04:16,980 --> 00:04:21,290
we're we're hoping to have him come and talk at our company next week, actually.

81
00:04:21,300 --> 00:04:21,290
and um it's a philosophy

82
00:04:21,300 --> 00:04:27,290
that we we do talk about internally, right? we have copies of the books in

83
00:04:27,300 --> 00:04:27,290
including the

84
00:04:27,300 --> 00:04:31,130
the recent japanese translation. as you know,

85
00:04:31,140 --> 00:04:36,410
one of the co-founders, one of my my main jobs, one of the main things

86
00:04:36,420 --> 00:04:40,570
that i have to keep doing for this company is making sure

87
00:04:40,580 --> 00:04:40,570
that we protect the freedom that the researchers currently have, right?

88
00:04:45,600 --> 00:04:45,850
because it's

89
00:04:45,860 --> 00:04:45,850
it's

90
00:04:45,860 --> 00:04:45,850
it's

91
00:04:45,860 --> 00:04:49,210
it's a privilege really that we have the resources to be able to do that. and inevitably,

92
00:04:49,220 --> 00:04:49,210
as i've seen happen,

93
00:04:49,220 --> 00:04:53,530
as the company grows, more and more pressure comes in and it narrows the freedom.

94
00:04:53,540 --> 00:04:59,530
but i think

95
00:04:59,540 --> 00:05:05,370
because, you know, we we believe in this philosophy so strongly, i'm hoping

96
00:05:05,380 --> 00:05:05,370
that we can give people all the research freedom that we do now

97
00:05:05,380 --> 00:05:10,970
um for as long as possible.

98
00:05:10,980 --> 00:05:14,490
> > and what are those processes that curtail freedom as a company matures?

99
00:05:14,500 --> 00:05:14,490
i mean, how would you describe that?

100
00:05:14,500 --> 00:05:18,810
it's great that there's never been so much interest

101
00:05:18,820 --> 00:05:18,810
and people

102
00:05:18,820 --> 00:05:18,810
and people

103
00:05:18,820 --> 00:05:23,850
and resources

104
00:05:23,860 --> 00:05:23,850
and money in the industry, but unfortunately

105
00:05:23,860 --> 00:05:34,250
that just increases the amount of pressure people have in order to compete with all the other people working on it

106
00:05:34,260 --> 00:05:39,850
and trying to get the the value out of this technology and making money.

107
00:05:39,860 --> 00:05:43,690
and i think that's what just happens, right? as a startup, you have a a feeling of, you know,

108
00:05:43,700 --> 00:05:48,490
excitement and trying something new.

109
00:05:48,500 --> 00:05:48,490
and right at the beginning, you have a bit of a runway.

110
00:05:52,560 --> 00:05:52,810
so, you have the freedom to try different things.

111
00:05:52,820 --> 00:05:56,169
but inevitably, people are starting to ask for returns on their investments

112
00:05:56,179 --> 00:06:01,610
or they're expecting you to churn out some product.

113
00:06:01,620 --> 00:06:07,130
and this just unfortunately reduces the uh

114
00:06:07,140 --> 00:06:07,130
the

115
00:06:07,140 --> 00:06:07,130
the

116
00:06:07,140 --> 00:06:07,130
the

117
00:06:12,800 --> 00:06:13,050
the

118
00:06:13,060 --> 00:06:13,050
the

119
00:06:13,060 --> 00:06:13,050
the

120
00:06:13,060 --> 00:06:13,050
the

121
00:06:13,060 --> 00:06:18,169
or the pressure to to create technology that's actually useful for the products that we have goes up

122
00:06:18,179 --> 00:06:25,370
and so the feeling of autonomy i think starts to go down. but you know

123
00:06:25,380 --> 00:06:30,169
i literally tell people when they start working for the company

124
00:06:30,179 --> 00:06:34,569
i want you to work on what you think is interesting and important

125
00:06:34,579 --> 00:06:34,569
and i mean it there

126
00:06:34,579 --> 00:06:34,569
and i mean it there

127
00:06:34,579 --> 00:06:34,569
and i mean it there

128
00:06:34,579 --> 00:06:39,289
there's a phenomenon called audience capture

129
00:06:39,299 --> 00:06:42,410
> > right > >

130
00:06:42,420 --> 00:06:42,410
and i think there might be a phenomenon called technology capture

131
00:06:42,420 --> 00:06:46,810
which is that in the early days of google

132
00:06:46,820 --> 00:06:52,090
it was quite open-ended and i mean transformers is now the ubiquitous backbone of all ai technology

133
00:06:52,100 --> 00:06:56,090
and it's a huge achievement that that you're involved in

134
00:06:56,100 --> 00:07:00,490
but i mean there's a similar story with with open ai.

135
00:07:00,500 --> 00:07:04,169
they're now starting to see all of these commercialization opportunities. they they can

136
00:07:04,179 --> 00:07:04,169
i mean they're going to become linkedin.

137
00:07:04,179 --> 00:07:07,370
they're going to become an application platform.

138
00:07:07,380 --> 00:07:07,370
they're going to become a search platform.

139
00:07:07,380 --> 00:07:10,090
they're going to become a social network.

140
00:07:10,100 --> 00:07:10,090
and

141
00:07:10,100 --> 00:07:13,770
and i guess this could happen to you guys

142
00:07:13,780 --> 00:07:13,770
that there's a very strong chance, especially with your new paper

143
00:07:13,780 --> 00:07:17,930
that we're going to talk about today, this continuous thought machines.

144
00:07:17,940 --> 00:07:25,370
it it could be a revolutionary technology, but then it will become obvious how it could be commercialized.

145
00:07:25,380 --> 00:07:28,490
and that's how those pressures come in. > > i

146
00:07:28,500 --> 00:07:33,130
i like the i like the audience capture analogy. i think

147
00:07:33,140 --> 00:07:33,130
um

148
00:07:33,140 --> 00:07:33,130
um

149
00:07:33,140 --> 00:07:38,090
by large language models, right? they they worked so well that everyone wanted to work on them.

150
00:07:45,360 --> 00:07:52,169
and i'm really worried that we're kind of stuck in this local minimum now, right?

151
00:07:52,179 --> 00:07:52,169
and we sort of need to try to try to escape it.

152
00:07:52,179 --> 00:07:57,449
so, we spoke about the transformers, but there's a there's a time just before the transformers

153
00:07:57,459 --> 00:08:01,289
that i'd like to talk about

154
00:08:01,299 --> 00:08:01,289
because i think it's quite illustrative.

155
00:08:01,299 --> 00:08:05,370
so, of course, the

156
00:08:05,380 --> 00:08:08,330
the main technology before transformers was recurrent neural networks, right? and there was a similar feeling, right?

157
00:08:08,340 --> 00:08:13,289
when recurrent neural networks came in and we

158
00:08:13,299 --> 00:08:13,289
we

159
00:08:13,299 --> 00:08:13,289
we

160
00:08:13,299 --> 00:08:17,050
we discovered this new sort of sequence of sequence learning,

161
00:08:17,060 --> 00:08:17,050
that was also a massive breakthrough, right? the

162
00:08:17,060 --> 00:08:17,050
that was also a massive breakthrough, right? the

163
00:08:17,060 --> 00:08:17,050
that was also a massive breakthrough, right? the

164
00:08:22,080 --> 00:08:22,330
the

165
00:08:22,340 --> 00:08:22,330
the translation quality went up massively, right? um

166
00:08:22,340 --> 00:08:22,330
the translation quality went up massively, right? um

167
00:08:30,879 --> 00:08:31,129
uh

168
00:08:31,139 --> 00:08:35,529
quality went up massively. and there was this a similar sort of feeling then of like

169
00:08:35,539 --> 00:08:35,529
okay

170
00:08:35,539 --> 00:08:35,529
okay

171
00:08:35,539 --> 00:08:35,529
okay

172
00:08:35,539 --> 00:08:35,529
okay

173
00:08:35,539 --> 00:08:39,610
we found the technology and we just need to sort of perfect this technology

174
00:08:39,620 --> 00:08:39,610
and back then

175
00:08:39,620 --> 00:08:39,610
and back then

176
00:08:39,620 --> 00:08:39,610
and back then

177
00:08:39,620 --> 00:08:39,610
and back then

178
00:08:47,360 --> 00:08:47,610
character level language modeling

179
00:08:47,620 --> 00:08:47,610
right

180
00:08:47,620 --> 00:08:52,970
so every time a new rnn based character level language modeling paper came out

181
00:08:52,980 --> 00:08:52,970
i got quite excited

182
00:08:52,980 --> 00:08:52,970
i got quite excited

183
00:08:52,980 --> 00:08:52,970
i got quite excited

184
00:08:52,980 --> 00:08:52,970
i got quite excited

185
00:08:52,980 --> 00:08:52,970
i got quite excited

186
00:08:58,640 --> 00:08:58,890
i'd want to like

187
00:08:58,900 --> 00:08:58,890
quickly read the paper like okay

188
00:08:58,900 --> 00:08:58,890
quickly read the paper like okay

189
00:08:58,900 --> 00:08:58,890
quickly read the paper like okay

190
00:08:58,900 --> 00:09:01,529
how did they get the improvements

191
00:09:01,539 --> 00:09:01,529
but the papers were always these

192
00:09:01,539 --> 00:09:12,730
just these slight modifications on the same architecture, right? it was lstms and grus

193
00:09:12,740 --> 00:09:12,730
and maybe

194
00:09:12,740 --> 00:09:12,730
and maybe

195
00:09:12,740 --> 00:09:18,010
initializing it with the identity matrix to so that you could use the relu function

196
00:09:18,020 --> 00:09:18,010
or like

197
00:09:18,020 --> 00:09:18,010
or like

198
00:09:18,020 --> 00:09:21,610
or if you

199
00:09:21,620 --> 00:09:21,610
if you layer them in a slightly different way

200
00:09:21,620 --> 00:09:25,610
or if you had gating going upwards as well as as sideways.

201
00:09:25,620 --> 00:09:25,610
um,

202
00:09:25,620 --> 00:09:25,610
um,

203
00:09:25,620 --> 00:09:25,610
um,

204
00:09:31,360 --> 00:09:31,610
uh

205
00:09:31,620 --> 00:09:37,370
like hierarchical lstm where it would actually decide to compute or not compute the different layers.

206
00:09:37,380 --> 00:09:42,730
and if you trained on wikipedia and you looked at the structure of

207
00:09:42,740 --> 00:09:42,730
when it was decided to compute or not compute,

208
00:09:42,740 --> 00:09:46,490
it kind of looked like the structure of of the the sentences were actually being picked up

209
00:09:50,880 --> 00:09:51,130
by the model.

210
00:09:51,140 --> 00:09:51,130
and i used to love that sort of stuff, right? um,

211
00:09:51,140 --> 00:09:51,130
and i used to love that sort of stuff, right? um,

212
00:09:51,140 --> 00:09:51,130
and i used to love that sort of stuff, right? um,

213
00:09:56,320 --> 00:09:56,570
the

214
00:09:56,580 --> 00:10:02,410
the improvements were always like 1. 26 bits per character, 1. 25 bits per character, 1. 24.

215
00:10:02,420 --> 00:10:06,970
that was a result that was publishable, right? that was exciting. but then after the transformer

216
00:10:06,980 --> 00:10:11,209
the team that i went on to

217
00:10:11,219 --> 00:10:11,209
afterwards

218
00:10:11,219 --> 00:10:11,209
afterwards

219
00:10:11,219 --> 00:10:11,209
afterwards

220
00:10:11,219 --> 00:10:16,970
very deep transformer models

221
00:10:16,980 --> 00:10:16,970
decoder only transformer models to language modeling

222
00:10:21,839 --> 00:10:22,089
and we immediately got something like 1. 1

223
00:10:26,320 --> 00:10:26,570
uh

224
00:10:26,580 --> 00:10:26,570
right

225
00:10:26,580 --> 00:10:30,810
so so something that was so good that people actually come to our desk and politely tell us like

226
00:10:30,820 --> 00:10:30,810
uh

227
00:10:30,820 --> 00:10:30,810
uh

228
00:10:30,820 --> 00:10:30,810
uh

229
00:10:30,820 --> 00:10:35,850
like a calculation

230
00:10:35,860 --> 00:10:35,850
do you think it's nats

231
00:10:35,860 --> 00:10:35,850
do you think it's nats

232
00:10:35,860 --> 00:10:35,850
do you think it's nats

233
00:10:35,860 --> 00:10:35,850
do you think it's nats

234
00:10:40,480 --> 00:10:40,730
no

235
00:10:40,740 --> 00:10:40,730
we

236
00:10:40,740 --> 00:10:40,730
we

237
00:10:40,740 --> 00:10:40,730
we

238
00:10:40,740 --> 00:10:40,730
we

239
00:10:40,740 --> 00:10:40,730
we

240
00:10:40,740 --> 00:10:40,730
we

241
00:10:40,740 --> 00:10:45,850
what struck me later is that all of a sudden all of that research

242
00:10:45,860 --> 00:10:45,850
and to be clear

243
00:10:45,860 --> 00:10:55,690
very good research was suddenly made completely redundant. > > yes.

244
00:10:55,700 --> 00:10:55,690
> > right.

245
00:10:55,700 --> 00:10:58,570
all of those endless permutations to to rnn's were suddenly seemingly a waste of time.

246
00:11:04,240 --> 00:11:04,490
we're kind of in the situation right now

247
00:11:04,500 --> 00:11:09,209
where a lot of the papers are just taking the same architecture

248
00:11:09,219 --> 00:11:14,410
and making these endless amount of different tweaks of

249
00:11:14,420 --> 00:11:14,410
like

250
00:11:14,420 --> 00:11:18,170
you know where to put them normalization layer and slightly different ways of training them

251
00:11:18,180 --> 00:11:22,410
and and we might be wasting the time in exactly the same way

252
00:11:22,420 --> 00:11:22,410
right

253
00:11:22,420 --> 00:11:22,410
right

254
00:11:22,420 --> 00:11:22,410
right

255
00:11:26,320 --> 00:11:26,570
i don't think that this is the final architecture

256
00:11:26,580 --> 00:11:29,529
and we just need to keep scaling up

257
00:11:29,539 --> 00:11:29,529
there's some breakthrough that will occur at some point

258
00:11:29,539 --> 00:11:39,610
and then it will once again become obvious that we're kind of wasting a lot of time right now.

259
00:11:39,620 --> 00:11:42,970
> > yeah. so we are a victim of our own success

260
00:11:42,980 --> 00:11:42,970
and this basin of attraction

261
00:11:42,980 --> 00:11:42,970
and this basin of attraction

262
00:11:46,720 --> 00:11:50,250
sarah hooker spoke about the hardware lottery and this is a kind of architecture lottery

263
00:11:50,260 --> 00:11:50,250
and it it

264
00:11:50,260 --> 00:11:55,290
it actually made me think of the um agricultural revolution

265
00:11:55,300 --> 00:11:55,290
which is that this kind of phase change happened

266
00:11:55,300 --> 00:12:05,130
and all of the folks that had these skills that were so necessary, these diverse skills for living

267
00:12:05,140 --> 00:12:05,130
and surviving, they died out.

268
00:12:05,140 --> 00:12:10,089
and that's actually quite paradoxical because we need those skills to take the next step.

269
00:12:10,099 --> 00:12:14,570
m > > and so we

270
00:12:14,580 --> 00:12:14,570
we're now in this regime

271
00:12:14,580 --> 00:12:14,570
we're now in this regime

272
00:12:14,580 --> 00:12:24,010
and the implication is that you can do anything with a foundation model in the corporate world

273
00:12:24,020 --> 00:12:24,010
we used to have data scientists

274
00:12:24,020 --> 00:12:24,010
we used to have data scientists

275
00:12:24,020 --> 00:12:28,089
they were ml engineers doing these architectural tweaks

276
00:12:28,099 --> 00:12:28,089
even in

277
00:12:28,099 --> 00:12:28,089
even in

278
00:12:28,099 --> 00:12:28,089
even in

279
00:12:28,099 --> 00:12:33,610
and now we just have ai engineers who are just doing prompt engineering

280
00:12:33,620 --> 00:12:33,610
and so on.

281
00:12:33,620 --> 00:12:43,930
so you're saying that the fundamental skills that we need to be diverse to think of new solutions

282
00:12:43,940 --> 00:12:43,930
and new architectures, they're dying out.

283
00:12:43,940 --> 00:12:48,810
i think i'm going to disagree with that. i think the problem is we have plenty of very talented

284
00:12:56,959 --> 00:12:57,209
uh

285
00:12:57,219 --> 00:13:03,130
very creative researchers out there, but they're not using their talents. right? for example,

286
00:13:03,140 --> 00:13:03,130
you know,

287
00:13:03,140 --> 00:13:07,050
if you're in academia, there's pressure to publish, right?

288
00:13:07,060 --> 00:13:16,730
and if there's pressure to publish, you think to yourself, okay, well, i have this really cool idea, but it might not work.

289
00:13:16,740 --> 00:13:21,370
it might be too weird, right? it might be difficult to get it accepted

290
00:13:21,380 --> 00:13:21,370
because i have to sort of like sell the idea more.

291
00:13:25,680 --> 00:13:25,930
or i can just try this new positional embedding,

292
00:13:25,940 --> 00:13:32,250
right? the problem is that the current environment both in academia

293
00:13:32,260 --> 00:13:37,130
and in companies are not actually giving people the freedom

294
00:13:37,140 --> 00:13:42,250
that they need to do the research that they probably want to do.

295
00:13:42,260 --> 00:13:44,329
i > > mean there's also this interesting thing that even in spite of great new research

296
00:13:47,839 --> 00:13:48,089
i mean i was speaking to seb hoger

297
00:13:48,099 --> 00:13:53,450
and he's got all of these new architectural ideas and open ai aren't implementing them.

298
00:13:53,460 --> 00:13:57,209
i mean google are doing this diffusion language model

299
00:13:57,219 --> 00:13:57,209
which is quite cool.

300
00:13:57,219 --> 00:14:02,010
and i i'd like to know your opinion on why that is.

301
00:14:02,020 --> 00:14:05,850
so there's a few philosophies floating around like this concept of a universal representation

302
00:14:05,860 --> 00:14:10,810
that there are universal patterns and the the transformer representations resemble those in the brain.

303
00:14:10,820 --> 00:14:15,290
and it's rather led to this idea of

304
00:14:15,300 --> 00:14:15,290
well

305
00:14:15,300 --> 00:14:15,290
well

306
00:14:18,720 --> 00:14:18,970
because if we just have more scale and more compute

307
00:14:18,980 --> 00:14:22,649
then all roads lead to rome.

308
00:14:22,659 --> 00:14:25,610
so why would we bother doing it any differently? > > there's actually better

309
00:14:25,620 --> 00:14:30,250
right? there is actually already architectures that have been shown in the research to work better than transformers.

310
00:14:30,260 --> 00:14:30,250
okay.

311
00:14:36,160 --> 00:14:42,170
but not better enough in order to move the entire industry away from such an established architecture

312
00:14:42,180 --> 00:14:46,570
where you're familiar with it.

313
00:14:46,580 --> 00:14:46,570
you know how to train it.

314
00:14:46,580 --> 00:14:46,570
you know how to train it.

315
00:14:46,580 --> 00:14:49,930
you know how the internals work, right? you know how to fine-tune them.

316
00:14:49,940 --> 00:14:54,570
you have all this software is already set up for training transformers.

317
00:14:54,580 --> 00:14:54,570
fine gening transformers

318
00:14:58,160 --> 00:14:58,410
inference.

319
00:14:58,420 --> 00:15:02,089
so if you want to move the industry away from that,

320
00:15:02,099 --> 00:15:02,089
being better is not good enough.

321
00:15:05,360 --> 00:15:05,610
it has to be obviously crushingly better.

322
00:15:05,620 --> 00:15:12,170
transformers were that much better over rnns.

323
00:15:12,180 --> 00:15:12,170
okay, transformers

324
00:15:12,180 --> 00:15:16,889
where you just applied it to a new problem

325
00:15:16,899 --> 00:15:16,889
and it just was so so much faster to train

326
00:15:16,899 --> 00:15:21,690
and you just got such higher accuracy that you just had to move.

327
00:15:26,240 --> 00:15:26,490
and i think the deep

328
00:15:26,500 --> 00:15:29,610
the deep learning revolution was also another example of that, right? where you had plenty of skeptics

329
00:15:29,620 --> 00:15:35,610
and people were pushing um neural networks even back then and people are going, " no,

330
00:15:35,620 --> 00:15:39,529
we think symbolic stuff will work better.

331
00:15:39,539 --> 00:15:42,970
" but then they demonstrated it as being so much better that you couldn't ignore it.

332
00:15:42,980 --> 00:15:48,570
this fact makes finding the next thing even harder.

333
00:15:48,580 --> 00:15:54,089
right? that's the gravitational pole of always pull pulling you back to, oh, okay,

334
00:15:54,099 --> 00:15:58,250
but a transformer is good enough. and yeah, you made a cool little architecture over here that

335
00:15:58,260 --> 00:15:58,250
yeah, it looks like it's

336
00:15:58,260 --> 00:16:01,930
it's got better accuracy, but openai over here just made it 10 times bigger and it beats that.

337
00:16:05,600 --> 00:16:10,009
so, let's just keep going. may i also submit that there could be an additional reason

338
00:16:10,019 --> 00:16:10,009
which is

339
00:16:10,019 --> 00:16:10,009
which is

340
00:16:10,019 --> 00:16:14,089
i love that fractured entangled representations paper. um

341
00:16:14,099 --> 00:16:14,089
there's there's this shortcut learning problem

342
00:16:14,099 --> 00:16:18,730
and i think that there's a little bit of a mirage going on here and there

343
00:16:18,740 --> 00:16:22,810
there might be problems with these language models that we don't

344
00:16:22,820 --> 00:16:26,329
you know that we're not fully aware of

345
00:16:26,339 --> 00:16:29,930
and there's also this thing that we're seeing that we are starting to bastardize the architecture.

346
00:16:29,940 --> 00:16:37,529
so we know we need to have adaptive computation for reasoning. we know we want things like uncertainty quantification

347
00:16:37,539 --> 00:16:42,250
and what we're doing is is we're bolting these things on top rather

348
00:16:42,260 --> 00:16:42,250
than having an architecture which intrinsically does all of these things

349
00:16:45,279 --> 00:16:45,529
that we know we need.

350
00:16:45,539 --> 00:16:45,529
> > yeah.

351
00:16:45,539 --> 00:16:45,529
> > yeah.

352
00:16:45,539 --> 00:16:54,730
and i think the the our continuous thought machine is is an attempt at addressing those um more directly, right? which which luke will be able to tell you more about later.

353
00:16:58,959 --> 00:16:59,209
there's something still not quite right with this

354
00:16:59,219 --> 00:17:03,209
the current technology, right? i i think the the phrase

355
00:17:03,219 --> 00:17:09,530
that's becoming popular is jagged intelligence, right? that the fact that you can ask an llm something

356
00:17:09,540 --> 00:17:14,410
and it can solve literally like a phd level problem

357
00:17:14,420 --> 00:17:19,130
and then you know in the next sentence it can say something just so clearly obviously wrong that it

358
00:17:19,140 --> 00:17:19,130
it's

359
00:17:19,140 --> 00:17:19,130
it's

360
00:17:19,140 --> 00:17:28,970
right? and i think this is actually a reflection of something probably quite fundamentally wrong with the current architecture.

361
00:17:28,980 --> 00:17:33,130
as amazing as they are, the current technology is actually too good.

362
00:17:33,140 --> 00:17:33,130
okay.

363
00:17:40,320 --> 00:17:45,929
another reason why it's it's difficult to move away from them, right? so they're too good in in in the following sense.

364
00:17:45,939 --> 00:17:49,610
and you you spoke about the fact that we have these foundation models.

365
00:17:49,620 --> 00:17:49,610
that's okay.

366
00:17:49,620 --> 00:17:52,169
so that we have the foundation that we can do anything with them.

367
00:17:52,179 --> 00:18:02,090
yes, i think current neural networks are so powerful that if you have enough patience

368
00:18:02,100 --> 00:18:07,130
and enough compute and enough data, you can make them do anything.

369
00:18:07,140 --> 00:18:07,130
but i don't necessarily think

370
00:18:07,140 --> 00:18:16,250
that they want to, right? we're sort of forcing them like they're universal pro approximators

371
00:18:16,260 --> 00:18:21,770
but i think there are probably a space of you know function approximators

372
00:18:21,780 --> 00:18:26,809
that will more want to represent things in the way that a human represents them.

373
00:18:26,819 --> 00:18:32,330
so there's actually quite an obscure paper that is my poster child for this. it's called intelligence matrix exponentiation

374
00:18:38,400 --> 00:18:38,650
> > and i think it was actually rejected. so, you know, you can probably project

375
00:18:42,400 --> 00:18:42,650
uh

376
00:18:42,660 --> 00:18:42,650
the image of a figure one,

377
00:18:42,660 --> 00:18:51,610
but there's an image of it's solving, you know, the classical spiral data set of needing to separate the two classes in the spiral.

378
00:18:51,620 --> 00:18:53,610
> > yes. > >

379
00:18:53,620 --> 00:18:53,610
and it has the decision boundary for a for both a classic rnn

380
00:18:53,620 --> 00:18:53,610
and it has the decision boundary for a for both a classic rnn

381
00:18:53,620 --> 00:19:00,410
multi-layer perceptron and a tanh

382
00:19:00,420 --> 00:19:06,410
multi-layer perceptron. and you can see they both solve it, right? technically, they both solve the problem because they

383
00:19:06,420 --> 00:19:06,410
they

384
00:19:06,420 --> 00:19:10,570
they classify all the points correctly

385
00:19:10,580 --> 00:19:16,010
and get a very good test score on this on this very simple data set.

386
00:19:16,020 --> 00:19:21,610
and then they show you the decision boundary for the for the m layer

387
00:19:21,620 --> 00:19:21,610
that they built in this paper

388
00:19:21,620 --> 00:19:26,650
and it's a spiral. the layer represented the spiral as a spiral. sh

389
00:19:26,660 --> 00:19:26,650
shouldn't

390
00:19:26,660 --> 00:19:26,650
shouldn't

391
00:19:26,660 --> 00:19:26,650
shouldn't

392
00:19:26,660 --> 00:19:33,049
you know if the data is a spiral

393
00:19:33,059 --> 00:19:33,049
shouldn't we represent it as a spiral?

394
00:19:33,059 --> 00:19:36,809
and then if you look back at the decision boundaries for for the spiral

395
00:19:36,819 --> 00:19:46,169
and the classic relu multi-layer perceptron, it's clear that you just have these tiny little peacewise linear separations.

396
00:19:46,179 --> 00:19:46,169
um,

397
00:19:46,179 --> 00:19:46,169
um,

398
00:19:53,600 --> 00:19:53,850
if you know if you train these things enough

399
00:19:53,860 --> 00:20:03,450
and you push these little peacewise linear boundaries around enough, it can

400
00:20:03,460 --> 00:20:03,450
it can fit the spiral and get a high accuracy.

401
00:20:03,460 --> 00:20:03,450
it can fit the spiral and get a high accuracy.

402
00:20:10,240 --> 00:20:10,490
when i look at those

403
00:20:10,500 --> 00:20:15,850
that that image that the relu version actually understands that it is a spiral, right?

404
00:20:15,860 --> 00:20:15,850
and

405
00:20:15,860 --> 00:20:22,169
when you represent it as a spiral, it actually extrapolates correctly

406
00:20:22,179 --> 00:20:26,169
because the spiral just keeps going out.

407
00:20:26,179 --> 00:20:28,570
> > you're touching on something fascinating there because,

408
00:20:28,580 --> 00:20:28,570
you know,

409
00:20:28,580 --> 00:20:28,570
you know,

410
00:20:32,400 --> 00:20:32,650
adaptive computation.

411
00:20:32,660 --> 00:20:32,650
um

412
00:20:32,660 --> 00:20:37,530
i'm really inspired by randall bisreerero's spline theory of of neural networks

413
00:20:37,540 --> 00:20:37,530
and we we've had them on many times

414
00:20:37,540 --> 00:20:42,250
and you can look on the tensorflow playground.

415
00:20:42,260 --> 00:20:45,530
you can look what happens when you have a relu network on on this, you know, spiral manifold.

416
00:20:45,540 --> 00:20:53,770
and, you know, you'd be forgiven for thinking that these things are basically a locality sensitive hashing table, right?

417
00:20:53,780 --> 00:20:53,770
because they they do

418
00:20:53,780 --> 00:20:53,770
because they they do

419
00:20:53,780 --> 00:20:53,770
because they they do

420
00:20:53,780 --> 00:20:53,770
because they they do

421
00:20:53,780 --> 00:20:58,169
they partition the space and and they

422
00:20:58,179 --> 00:20:58,169
they can predict the spiral

423
00:20:58,179 --> 00:21:04,490
manifold, right? but we want to do something a little bit more different than that.

424
00:21:04,500 --> 00:21:07,929
and it also comes into this imposttor thing

425
00:21:07,939 --> 00:21:12,650
because just tracing the spiral manifold but not continuing the pattern

426
00:21:12,660 --> 00:21:12,650
there's a big difference between that.

427
00:21:12,660 --> 00:21:17,049
so from an imposttor perspective

428
00:21:17,059 --> 00:21:22,650
just just tracing the pattern is not learning it abstractly or constructively. right? if we learned it constructively

429
00:21:22,660 --> 00:21:22,650
so we

430
00:21:22,660 --> 00:21:27,289
you know

431
00:21:27,299 --> 00:21:27,289
you speak about this in your paper

432
00:21:27,299 --> 00:21:31,289
this complexification the abstract building blocks and you can do adaptive computation. you understand the spiral.

433
00:21:36,159 --> 00:21:40,490
that means that with adaptive computation, you can continue the spiral and then you can update the model's weights

434
00:21:40,500 --> 00:21:40,490
so it has adaptivity

435
00:21:40,500 --> 00:21:44,250
because that's so important for intelligence.

436
00:21:44,260 --> 00:21:49,610
so we know that we need models that can do these things. but for some reason they're

437
00:21:49,620 --> 00:21:49,610
they're so sick of fantic

438
00:21:53,760 --> 00:21:57,610
they're almost better than an adaptive intelligent system because they tell us exactly what we want to hear.

439
00:21:57,620 --> 00:22:02,010
they seem so intelligent, but we know that they're missing these fundamental properties.

440
00:22:02,020 --> 00:22:07,130
> > i'm still fairly skeptical when i see video generation models.

441
00:22:07,140 --> 00:22:11,450
you know, we went through a phase where you could detect them

442
00:22:11,460 --> 00:22:15,450
because of the number of fingers on somebody's hand, right?

443
00:22:15,460 --> 00:22:18,970
and yes, with more data, with more compute, with better training tricks,

444
00:22:18,980 --> 00:22:26,010
okay, they submit.

445
00:22:26,020 --> 00:22:26,010
and now they usually do have five fingers.

446
00:22:26,020 --> 00:22:26,010
and now they usually do have five fingers.

447
00:22:34,480 --> 00:22:34,730
or did we just use more brute force to just you know

448
00:22:34,740 --> 00:22:40,570
force the the neural network to know

449
00:22:40,580 --> 00:22:40,570
it's five fingers

450
00:22:40,580 --> 00:22:45,210
where something that actually had a much better kind of representation space.

451
00:22:45,220 --> 00:22:49,289
it's almost mad that it's controversial to say that we should represent a spiral

452
00:22:49,299 --> 00:22:49,289
like a spiral. but, you know,

453
00:22:49,299 --> 00:22:53,210
something that could do that

454
00:22:53,220 --> 00:22:53,210
generally

455
00:22:53,220 --> 00:22:53,210
generally

456
00:22:53,220 --> 00:22:57,850
if if it represented a human hand the way that, you know,

457
00:22:57,860 --> 00:23:01,530
maybe i represent a human hand, then maybe it would be much easier to count how many fingers are on a on a hand.

458
00:23:01,540 --> 00:23:05,929
it's unfortunate that they work so well.

459
00:23:05,939 --> 00:23:05,929
it's unfortunate that scaling works so well

460
00:23:05,939 --> 00:23:09,929
because it's too easy for people to just sweep these problems under the carpet.

461
00:23:09,939 --> 00:23:16,090
you guys have possibly created what i think might be the best paper of of the year.

462
00:23:16,100 --> 00:23:20,250
this could actually be the innovation which takes us to the next step.

463
00:23:20,260 --> 00:23:24,809
and you did you get the spotlight in europe as well? > > yeah.

464
00:23:26,799 --> 00:23:27,049
> > this year and congratulations on that.

465
00:23:27,059 --> 00:23:30,570
so i think that's testament to how amazing this paper is. > >

466
00:23:30,580 --> 00:23:30,570
the ctm

467
00:23:30,580 --> 00:23:30,570
the ctm

468
00:23:34,559 --> 00:23:39,610
it's actually not that far out outside of the local minimum that we're stuck in.

469
00:23:39,620 --> 00:23:43,929
right? it's not as if we went and found this completely new technology. right? we took quite a um

470
00:23:43,939 --> 00:23:51,929
a simple biologically inspired idea, right, of these of of the fact that neurons synchronize

471
00:23:51,939 --> 00:24:02,250
and not even necessarily in a biologically plausible way, right? brains don't literally have all their neurons wired together in a way

472
00:24:02,260 --> 00:24:02,250
that they work out their synchronization.

473
00:24:06,559 --> 00:24:10,730
but it's it's the sort of research that i want to encourage people to do.

474
00:24:10,740 --> 00:24:10,730
and the way to sell it is quite easy.

475
00:24:10,740 --> 00:24:16,890
i think at no point did we have to worry about being scooped, right?

476
00:24:16,900 --> 00:24:22,650
that stress was taken away from us completely.

477
00:24:22,660 --> 00:24:22,650
so, and there was

478
00:24:22,660 --> 00:24:28,250
so there was no pressure to sort of rush out with this with this idea

479
00:24:28,260 --> 00:24:31,610
because we're like, well, there's probably somebody else working on exactly this.

480
00:24:31,620 --> 00:24:35,529
and i think the reason that we, you know, we were able to get a spotlight is

481
00:24:35,539 --> 00:24:39,210
because we're able to create such a polished paper.

482
00:24:39,220 --> 00:24:44,090
we took the time to do the science properly to get the the base

483
00:24:44,100 --> 00:24:49,690
the baselines that we wanted and do all the the the the tasks that we wanted to try.

484
00:24:49,700 --> 00:24:59,289
encouraging researchers to take a little bit more of a of a risk, right? to try these slightly more speculative long-term ideas, i think

485
00:24:59,299 --> 00:24:59,289
is

486
00:24:59,299 --> 00:24:59,289
is

487
00:24:59,299 --> 00:25:04,570
i don't think it's necessarily a very difficult thing to sell.

488
00:25:04,580 --> 00:25:11,690
and i want to have the ctm as like a poster child of it works,

489
00:25:11,700 --> 00:25:11,690
right? it was a bit of a risk.

490
00:25:11,700 --> 00:25:15,690
we, you know, we didn't know if we were going to find something interesting, but you know, it was our first shot

491
00:25:15,700 --> 00:25:19,370
and we did find something interesting and it became a a successful paper.

492
00:25:19,380 --> 00:25:19,370
if

493
00:25:19,380 --> 00:25:28,730
if we do find a system which can acquire knowledge, design new architectures, do the the open-ended type of science

494
00:25:28,740 --> 00:25:28,730
that you're speaking to, can you see a future

495
00:25:32,880 --> 00:25:33,130
where at some point the locus of progress will be mostly driven

496
00:25:33,140 --> 00:25:38,090
by the models themselves? > > i think so.

497
00:25:38,100 --> 00:25:46,490
whether or not that's going to replace us completely, i go back and forth on. powerful algorithms are finding

498
00:25:46,500 --> 00:25:46,490
uh

499
00:25:46,500 --> 00:25:46,490
uh

500
00:25:46,500 --> 00:25:50,409
right? and i think it might just end up being a more powerful version of that.

501
00:25:50,419 --> 00:25:50,409
right? so i i know the the ai scientist

502
00:25:55,120 --> 00:25:55,370
that we we released, we showed

503
00:25:55,380 --> 00:26:02,010
that you could actually go end to end, right? go from seeding the system with an idea for a research paper

504
00:26:02,020 --> 00:26:07,690
and then just take your hands off and just let it go.

505
00:26:07,700 --> 00:26:13,289
think about the idea, write the code, run the code, collect the results, and write the paper. uh

506
00:26:13,299 --> 00:26:17,610
to the point that we were actually able to get it to

507
00:26:17,620 --> 00:26:17,610
um

508
00:26:17,620 --> 00:26:29,049
a 100 % ai generated paper accepted to to a workshop recently, right? but i think we did

509
00:26:29,059 --> 00:26:34,010
that to show that you could do it right as a sort of demonstration in a real system.

510
00:26:34,020 --> 00:26:42,809
i think i would want it to be much more interactive, right? i would want to be able to seed with an idea

511
00:26:42,819 --> 00:26:47,289
and then have it come back with more ideas, have a discussion with me, then go away to write the code.

512
00:26:47,299 --> 00:26:52,570
i want look at the code and check it and then discuss the results as they're coming out.

513
00:26:52,580 --> 00:26:56,730
so that's the sort of nearterm future that i that i would envision

514
00:26:56,740 --> 00:27:01,770
or how i would like to do research with an ai.

515
00:27:01,780 --> 00:27:01,770
> > and could you introspect on that? is it

516
00:27:04,799 --> 00:27:05,049
because you feel we need supervision because the models don't yet understand?

517
00:27:10,080 --> 00:27:14,090
you know there's this path dependence idea. so we need to do supervision because we have the path dependence

518
00:27:14,100 --> 00:27:17,370
so we can guide the generation of the language models.

519
00:27:17,380 --> 00:27:20,650
maybe in the future the language models will just understand better themselves. but there's also the output dimension

520
00:27:20,660 --> 00:27:24,570
which is that we want to produce artifacts that extend the fogyny of human interest.

521
00:27:24,580 --> 00:27:29,049
we want it to be human relevant.

522
00:27:29,059 --> 00:27:29,049
> > yeah.

523
00:27:29,059 --> 00:27:29,049
> > yeah.

524
00:27:29,059 --> 00:27:29,049
> > yeah.

525
00:27:32,080 --> 00:27:32,330
in that initial seed idea

526
00:27:32,340 --> 00:27:36,650
it's probably impossible to actually describe exactly what you want. it's exactly the same with, you know,

527
00:27:36,660 --> 00:27:41,929
when i have an intern. i can't just have an intern come into the company and i go,

528
00:27:41,939 --> 00:27:45,850
i have this mad idea

529
00:27:45,860 --> 00:27:49,370
and then just explain it to them and then just leave them alone for 4 months.

530
00:27:49,380 --> 00:27:49,370
there's a back and forth

531
00:27:49,380 --> 00:27:53,610
because i have a particular idea that i want to explore

532
00:27:53,620 --> 00:27:58,250
and i need to keep steering them in the direction that i i

533
00:27:58,260 --> 00:27:58,250
you know that i had in my my mind originally.

534
00:27:58,260 --> 00:28:02,570
so i think it's more like that

535
00:28:02,580 --> 00:28:02,570
basically.

536
00:28:02,580 --> 00:28:07,690
you have such a deep understanding. so you have this rich provenence and history and path dependence

537
00:28:07,700 --> 00:28:12,490
and that means you can take creative steps

538
00:28:12,500 --> 00:28:12,490
intuitive steps for you

539
00:28:12,500 --> 00:28:12,490
intuitive steps for you

540
00:28:12,500 --> 00:28:17,770
they respect all of this deep abstract understanding that you have

541
00:28:17,780 --> 00:28:17,770
and interns don't yet have that

542
00:28:21,919 --> 00:28:25,289
> > but maybe ai models in the future will have that. > > yeah

543
00:28:25,299 --> 00:28:25,289
sure.

544
00:28:25,299 --> 00:28:25,289
sure.

545
00:28:25,299 --> 00:28:29,289
if they get to the point where my inputs becomes detrimental

546
00:28:29,299 --> 00:28:29,289
then yeah

547
00:28:29,299 --> 00:28:29,289
then yeah

548
00:28:29,299 --> 00:28:33,690
it's kind of like chess, right? there was a point at which chess engine

549
00:28:33,700 --> 00:28:40,649
and human fusion actually beat chess engines.

550
00:28:40,659 --> 00:28:46,330
that's not that's not true anymore, right? adding a human into the mix actually makes the the bots worse.

551
00:28:49,039 --> 00:28:49,289
> > oh, interesting.

552
00:28:49,299 --> 00:28:49,289
i wasn't aware of that. > > yeah.

553
00:28:49,299 --> 00:28:49,289
i wasn't aware of that. > > yeah.

554
00:28:49,299 --> 00:28:53,610
when that day comes for ai scientists is a is a is a broader discussion.

555
00:28:53,620 --> 00:28:53,610
i think

556
00:28:58,880 --> 00:29:02,570
> > i think now is a good segue to talk about this paper in a little bit more detail.

557
00:29:02,580 --> 00:29:05,529
so this continuous thought machines you were just pointing to it before. luke

558
00:29:05,539 --> 00:29:05,529
first

559
00:29:05,539 --> 00:29:05,529
first

560
00:29:05,539 --> 00:29:05,529
first

561
00:29:05,539 --> 00:29:05,529
first

562
00:29:05,539 --> 00:29:10,809
introduce yourself > > and set this thing up for us. > > my name is luke.

563
00:29:10,819 --> 00:29:14,649
i am a research scientist at sakana ai and

564
00:29:14,659 --> 00:29:14,649
uh

565
00:29:14,659 --> 00:29:21,210
my primary sector of research is this continuous thought machines.

566
00:29:21,220 --> 00:29:26,010
it's took us somewhere in the region of about eight months working on this project with uh

567
00:29:26,020 --> 00:29:26,010
the whole team. um

568
00:29:26,020 --> 00:29:26,010
the whole team. um

569
00:29:26,020 --> 00:29:30,730
i did a lot of the work uh

570
00:29:30,740 --> 00:29:33,529
but we also had a lot of people in different areas and doing different parts of it that

571
00:29:33,539 --> 00:29:39,289
i think an 8-month life cycle for a paper seems a bit long for ai research at the moment.

572
00:29:39,299 --> 00:29:39,289
um

573
00:29:39,299 --> 00:29:39,289
um

574
00:29:39,299 --> 00:29:43,529
to the actual technical points of the paper.

575
00:29:43,539 --> 00:29:43,529
so we call it continuous thought machines.

576
00:29:43,539 --> 00:29:47,610
it originally had a different name. we called it asynchronous thought machines before

577
00:29:47,620 --> 00:29:47,610
but

578
00:29:50,880 --> 00:29:51,130
uh

579
00:29:51,140 --> 00:29:51,130
every single time people asked us what's the asynchronous part

580
00:29:51,140 --> 00:29:59,770
it became a bit confusing. so continuous thought machines basically depends on three novelties.

581
00:29:59,780 --> 00:29:59,770
uh

582
00:29:59,780 --> 00:30:04,409
the first one is having what we call a internal thought dimension

583
00:30:04,419 --> 00:30:09,130
and this is not necessarily something new. it's related conceptually to the ideas of latent reasoning. uh

584
00:30:09,140 --> 00:30:14,409
and it's essentially applying compute in a sequential dimension.

585
00:30:14,419 --> 00:30:14,409
and when you start thinking about ideas and problems

586
00:30:14,419 --> 00:30:14,409
and when you start thinking about ideas and problems

587
00:30:14,419 --> 00:30:23,770
in this domain and in in this framework, you start understanding that many problems that look like intell

588
00:30:23,780 --> 00:30:27,850
or solutions to problems that look intelligent

589
00:30:27,860 --> 00:30:27,850
are often solutions that have a sequential nature.

590
00:30:27,860 --> 00:30:32,090
so for instance,

591
00:30:32,100 --> 00:30:35,929
one of the primary tasks that we tested in the continuous thought machines was this maze solving task.

592
00:30:35,939 --> 00:30:42,330
and solving mazes for deep learning is is quite trivial.

593
00:30:42,340 --> 00:30:42,330
it's really easy to do if you make the task easy for machines.

594
00:30:42,340 --> 00:30:49,210
and one of the ways to do this is you give an image of a maze to a neural network

595
00:30:55,279 --> 00:30:55,529
like a convolutional neural network and it outputs a image

596
00:30:55,539 --> 00:30:55,529
uh

597
00:30:55,539 --> 00:31:01,450
same size of the maze

598
00:31:01,460 --> 00:31:01,450
and it's uh

599
00:31:01,460 --> 00:31:01,450
and it's uh

600
00:31:01,460 --> 00:31:05,929
and there's ones where there is a path.

601
00:31:05,939 --> 00:31:09,610
there's some really brilliant work showing how you can train these in a careful way

602
00:31:09,620 --> 00:31:09,610
and scale them up essentially indefinitely.

603
00:31:09,620 --> 00:31:13,450
and this is fascinating

604
00:31:13,460 --> 00:31:13,450
and

605
00:31:13,460 --> 00:31:13,450
and

606
00:31:13,460 --> 00:31:13,450
and

607
00:31:13,460 --> 00:31:19,130
however, when you take that uh approach out of the picture and you ask what is a more human

608
00:31:22,480 --> 00:31:22,730
way to solve this problem, it becomes a sequential problem. you have to say

609
00:31:22,740 --> 00:31:22,730
well

610
00:31:27,120 --> 00:31:27,370
go up, go right, go up, go left, whatever the case may be

611
00:31:27,380 --> 00:31:31,929
to trace a route from start to finish.

612
00:31:31,939 --> 00:31:36,570
and when you constrain that simple problem space

613
00:31:36,580 --> 00:31:36,570
uh

614
00:31:36,580 --> 00:31:42,409
and you you ask a machine learning system to solve it like that

615
00:31:42,419 --> 00:31:42,409
turns out to actually get much much more challenging.

616
00:31:42,419 --> 00:31:46,809
so this became our hello world problem for the ctm

617
00:31:46,819 --> 00:31:51,690
and applying an internal sequential thought dimension to this is how we went about solving this.

618
00:31:55,519 --> 00:31:55,769
uh

619
00:31:55,779 --> 00:31:55,769
two other novelties that we can touch on and talk about.

620
00:31:55,779 --> 00:31:55,769
two other novelties that we can touch on and talk about.

621
00:31:55,779 --> 00:31:55,769
two other novelties that we can touch on and talk about.

622
00:31:55,779 --> 00:32:00,490
we sort of rethought the idea of what neurons should be.

623
00:32:00,500 --> 00:32:04,570
there is a lot of excellent research in this world

624
00:32:04,580 --> 00:32:04,570
uh

625
00:32:04,580 --> 00:32:04,570
uh

626
00:32:04,580 --> 00:32:08,730
particularly exploring how neurons work in biological systems.

627
00:32:08,740 --> 00:32:12,730
and then we get on the other side of the scale how deep learning neurons work

628
00:32:12,740 --> 00:32:16,649
which the quintessential example is a relu.

629
00:32:16,659 --> 00:32:16,649
it's off or on in a sense.

630
00:32:16,659 --> 00:32:16,649
it's off or on in a sense.

631
00:32:16,659 --> 00:32:22,649
very high level abstraction of neurons in the brains feels a little bit myopic.

632
00:32:22,659 --> 00:32:27,370
so we approached this problem and said well

633
00:32:27,380 --> 00:32:27,370
let's let's on a neuron by neuron basis

634
00:32:27,380 --> 00:32:33,210
let this neuron be a little model itself.

635
00:32:33,220 --> 00:32:38,409
and this ended up doing a lot of interesting work on how to build dynamics in the system.

636
00:32:38,419 --> 00:32:38,409
the third novelty here is

637
00:32:38,419 --> 00:32:38,409
the third novelty here is

638
00:32:38,419 --> 00:32:48,169
we have this internal dimension over which thinking happens. we ask the question, well, what is the representation?

639
00:32:48,179 --> 00:32:52,649
what is the representation for a biological system when it's thinking?

640
00:32:52,659 --> 00:32:56,090
is it just the state of the neurons at any given time? does

641
00:32:56,100 --> 00:32:56,090
that capture a thought, if you wish,

642
00:32:56,100 --> 00:33:01,769
if i can be controversial and use the term thinking and thought

643
00:33:01,779 --> 00:33:01,769
and my philosophy with this is no,

644
00:33:01,779 --> 00:33:07,130
it doesn't. that the concept of a thought is something that exists over time.

645
00:33:07,140 --> 00:33:11,210
so, how do we capture that in in engineering speak?

646
00:33:11,220 --> 00:33:11,210
we

647
00:33:11,220 --> 00:33:16,250
instead of measuring the states of the model that is recurrent,

648
00:33:16,260 --> 00:33:21,690
we measure how it synchronizes how neurons synchronize in pairs along with other neurons.

649
00:33:21,700 --> 00:33:26,250
and this opens up the door to a huge array of things

650
00:33:26,260 --> 00:33:26,250
that we can do with this type of representation.

651
00:33:30,399 --> 00:33:36,409
> > you were talking about this um sort of sequential nature of of reasoning and devil's advocate.

652
00:33:36,419 --> 00:33:36,409
i mean there was that anthropic biology paper

653
00:33:36,419 --> 00:33:41,450
and they were talking about planning and thinking and and they they were

654
00:33:41,460 --> 00:33:46,090
they were saying that this thing is planning ahead

655
00:33:46,100 --> 00:33:46,090
because because i think your system

656
00:33:46,100 --> 00:33:46,090
because because i think your system

657
00:33:46,100 --> 00:33:49,210
we can say

658
00:33:49,220 --> 00:33:49,210
does planning it

659
00:33:49,220 --> 00:33:49,210
does planning it

660
00:33:49,220 --> 00:33:53,450
can you explain that > >

661
00:33:53,460 --> 00:33:53,450
yes

662
00:33:53,460 --> 00:33:56,809
i think the boundary in terms of computation from a a cheering machine perspective

663
00:33:56,819 --> 00:33:56,809
if you wish

664
00:33:56,819 --> 00:33:56,809
if you wish

665
00:33:56,819 --> 00:34:02,010
is really interesting because the notion of being able to write your tape

666
00:34:02,020 --> 00:34:02,010
uh

667
00:34:02,020 --> 00:34:06,970
read from a tape and then write again to be in a ting compute system

668
00:34:06,980 --> 00:34:06,970
ting

669
00:34:06,980 --> 00:34:11,050
complete system is

670
00:34:11,060 --> 00:34:11,050
uh

671
00:34:11,060 --> 00:34:11,050
uh

672
00:34:11,060 --> 00:34:22,010
and i think the primary difference with let's talk about transformers versus what we're trying to do with the ctm is

673
00:34:22,020 --> 00:34:27,770
that the process that the ctm thinks in

674
00:34:27,780 --> 00:34:27,770
we can apply that process that internal process to

675
00:34:27,780 --> 00:34:27,770
we can apply that process that internal process to

676
00:34:27,780 --> 00:34:32,730
breaking down a problem.

677
00:34:32,739 --> 00:34:32,730
so the problem itself can be a single

678
00:34:32,739 --> 00:34:37,050
there is a single solution to this problem

679
00:34:37,060 --> 00:34:37,050
and you could do that in one shot. you could

680
00:34:37,060 --> 00:34:37,050
and you could do that in one shot. you could

681
00:34:37,060 --> 00:34:41,050
with the maze

682
00:34:41,060 --> 00:34:41,050
you could just process that in one shot

683
00:34:41,060 --> 00:34:44,489
but there are certain phrasings of problems that are real problems

684
00:34:44,500 --> 00:34:49,770
that doing so becomes exponentially more challenging.

685
00:34:49,780 --> 00:34:54,570
so in the maze task, a really good example is that if you try to predict 100

686
00:34:54,580 --> 00:34:54,570
200 steps down the path in one shot,

687
00:34:54,580 --> 00:35:00,410
no models that we could train,

688
00:35:00,420 --> 00:35:00,410
not even our model could do that.

689
00:35:00,420 --> 00:35:03,930
and we needed to actually build an autocurriculum system where the model first predicted the first step and then

690
00:35:07,119 --> 00:35:07,369
when it could predict the first step,

691
00:35:07,379 --> 00:35:07,369
then it

692
00:35:07,379 --> 00:35:10,810
we started training it on the second and third and fourth step.

693
00:35:10,820 --> 00:35:14,570
and the sort of resultant behavior of this is where it gets interesting.

694
00:35:14,580 --> 00:35:19,050
one of the one of the ways that i like to do research

695
00:35:19,060 --> 00:35:19,050
and

696
00:35:19,060 --> 00:35:23,130
that i encourage people who work with me to do research is understand the if you wish the behavior of a model.

697
00:35:23,140 --> 00:35:23,130
we're getting to a point now

698
00:35:27,920 --> 00:35:32,810
where the models that we build are demonstrably intelligent in ways that keep surprising us

699
00:35:32,820 --> 00:35:32,810
and breaking that down into a single set of metrics

700
00:35:32,820 --> 00:35:38,970
or even a finite single metric about performance

701
00:35:38,980 --> 00:35:44,010
seems maybe not to be the right way to do it for me.

702
00:35:44,020 --> 00:35:47,849
and understanding the behavior and the actions that those models take

703
00:35:47,859 --> 00:35:51,210
when you put them in a system and train them in a certain way

704
00:35:51,220 --> 00:35:51,210
uh

705
00:35:51,220 --> 00:35:55,770
seems to reveal more about what's actually going on under the hood. > > very cool.

706
00:35:55,780 --> 00:35:57,609
and i think i didn't pick up on this. so so you're

707
00:35:57,619 --> 00:35:57,609
you're doing a fixed number of um steps

708
00:35:57,619 --> 00:36:02,089
so you have like a a context window

709
00:36:02,099 --> 00:36:02,089
and did you say that you've set that around 100 steps?

710
00:36:07,359 --> 00:36:07,609
> > so for the

711
00:36:07,619 --> 00:36:07,609
for the maze task

712
00:36:07,619 --> 00:36:07,609
for the maze task

713
00:36:07,619 --> 00:36:12,089
the model always observes the full image at every step.

714
00:36:12,099 --> 00:36:12,089
uh

715
00:36:12,099 --> 00:36:12,089
uh

716
00:36:12,099 --> 00:36:12,089
uh

717
00:36:12,099 --> 00:36:12,089
uh

718
00:36:12,099 --> 00:36:12,089
uh

719
00:36:16,640 --> 00:36:16,890
those images could be tokens from a language

720
00:36:16,900 --> 00:36:16,890
uh

721
00:36:16,900 --> 00:36:20,329
the output of a language model

722
00:36:20,339 --> 00:36:20,329
those

723
00:36:20,339 --> 00:36:20,329
those

724
00:36:20,339 --> 00:36:20,329
those

725
00:36:20,339 --> 00:36:24,570
whatever the case may be

726
00:36:24,580 --> 00:36:24,570
it should be agnostic to data

727
00:36:24,580 --> 00:36:24,570
it should be agnostic to data

728
00:36:27,680 --> 00:36:27,930
but in the maze task

729
00:36:27,940 --> 00:36:27,930
the model can continuously just observe the data

730
00:36:27,940 --> 00:36:27,930
the model can continuously just observe the data

731
00:36:27,940 --> 00:36:33,690
no matter where it can look at the whole image simultaneously

732
00:36:33,700 --> 00:36:36,810
but it uses attention to retrieve information from the data

733
00:36:36,820 --> 00:36:36,810
and it has

734
00:36:36,820 --> 00:36:42,970
let's call it 100 steps that it can think through. and what we do is we pick up

735
00:36:42,980 --> 00:36:42,970
at some point

736
00:36:47,839 --> 00:36:48,089
the model solves three steps through the maze.

737
00:36:48,099 --> 00:36:52,329
so it says i'm going to go up, up, and right. and then it's correct.

738
00:36:52,339 --> 00:36:52,329
but then it makes the wrong turn.

739
00:36:52,339 --> 00:36:55,530
and at that point, we stop supervision.

740
00:36:55,540 --> 00:36:55,530
we only train it to solve the fourth step.

741
00:36:55,540 --> 00:36:59,930
so one more than what it could. in practice, we do it five,

742
00:36:59,940 --> 00:37:03,130
but the principle holds.

743
00:37:03,140 --> 00:37:03,130
and when you do that, it's a self bootstrapping mechanism.

744
00:37:03,140 --> 00:37:11,609
and i think the uh intuitive listener will understand how that extends to other domains, other sequential domains

745
00:37:11,619 --> 00:37:11,609
for instance

746
00:37:11,619 --> 00:37:16,089
like uh language prediction, many tokens ahead, that sort of thing.

747
00:37:16,099 --> 00:37:21,210
> > so i'm really interested in this idea of adaptive computation.

748
00:37:21,220 --> 00:37:21,210
so i i guess

749
00:37:21,220 --> 00:37:26,650
the first question is how sensitive was the performance to the number of steps

750
00:37:26,660 --> 00:37:26,650
and then the next question would be

751
00:37:26,660 --> 00:37:31,050
could you have an arbitrary number of steps

752
00:37:31,060 --> 00:37:31,050
which means that you know

753
00:37:31,060 --> 00:37:35,050
perhaps based on uncertainty

754
00:37:35,060 --> 00:37:35,050
or you know some kind of criterion

755
00:37:35,060 --> 00:37:35,050
or you know some kind of criterion

756
00:37:35,060 --> 00:37:40,089
and then the final question is

757
00:37:40,099 --> 00:37:44,730
could you have potentially like an arbitrary or unbounded number of steps

758
00:37:44,740 --> 00:37:44,730
>

759
00:37:44,740 --> 00:37:44,730
>

760
00:37:44,740 --> 00:37:44,730
>

761
00:37:44,740 --> 00:37:44,730
>

762
00:37:44,740 --> 00:37:44,730
>

763
00:37:44,740 --> 00:37:52,730
i think that i think that i'll answer the uncertainty question first about the sensitivity to steps.

764
00:37:52,740 --> 00:37:56,490
so a very good example of this is we just trained the model on imageet classification

765
00:37:56,500 --> 00:38:00,650
and our last function is quite simple.

766
00:38:00,660 --> 00:38:00,650
what we do is we run it for for example

767
00:38:00,660 --> 00:38:00,650
what we do is we run it for for example

768
00:38:00,660 --> 00:38:05,690
and we pick up two points

769
00:38:05,700 --> 00:38:05,690
two distinct points. the first one is

770
00:38:05,700 --> 00:38:09,210
where is it performing the best

771
00:38:09,220 --> 00:38:09,210
i. e. where is the loss

772
00:38:09,220 --> 00:38:09,210
i. e. where is the loss

773
00:38:09,220 --> 00:38:09,210
i. e. where is the loss

774
00:38:09,220 --> 00:38:13,770
where is it most sure

775
00:38:13,780 --> 00:38:13,770
or where is it most certain

776
00:38:13,780 --> 00:38:13,770
or where is it most certain

777
00:38:13,780 --> 00:38:13,770
or where is it most certain

778
00:38:13,780 --> 00:38:18,890
between 0 and 49

779
00:38:18,900 --> 00:38:18,890
inclusive

780
00:38:18,900 --> 00:38:18,890
inclusive

781
00:38:18,900 --> 00:38:24,890
we just make the last the average of the cross entropy at those points.

782
00:38:24,900 --> 00:38:28,329
so what this does is it induces a behavior

783
00:38:28,339 --> 00:38:32,970
where easy examples are solved almost immediately in one or two steps

784
00:38:32,980 --> 00:38:36,810
whereas more challenging examples will naturally take more thinking

785
00:38:36,820 --> 00:38:36,810
and it enables the model to use the full breadth of time

786
00:38:36,820 --> 00:38:44,970
that it has available to it just in a natural fashion without having to force it to happen.

787
00:38:44,980 --> 00:38:44,970
so you've decided to model every neuron as an mlp

788
00:38:44,980 --> 00:38:49,690
which is really fascinating.

789
00:38:49,700 --> 00:38:49,690
talk about that

790
00:38:49,700 --> 00:38:49,690
talk about that

791
00:38:53,520 --> 00:38:57,770
and i think you use the inner product to determine the extent to which the parameters are are synchronized

792
00:38:57,780 --> 00:38:57,770
and this kind of unfills over over time

793
00:38:57,780 --> 00:39:04,730
as as the driving force. can you explain that in a bit more detail? > > absolutely.

794
00:39:04,740 --> 00:39:09,369
i think it's a it's a good point to explain the uh neuron level models as we call them in the paper

795
00:39:09,379 --> 00:39:09,369
or nlm first

796
00:39:09,379 --> 00:39:09,369
or nlm first

797
00:39:09,379 --> 00:39:14,810
so you can imagine a recurrent system is a state vector, a state vector

798
00:39:14,820 --> 00:39:19,609
that is being updated from step to step.

799
00:39:19,619 --> 00:39:24,010
we track that state vector and that state vector unfolds

800
00:39:24,020 --> 00:39:30,810
and for each individual neuron, each uh i neuron in the system, we have a unfolding time series.

801
00:39:30,820 --> 00:39:35,690
it's a continuous time series. well, it's discreet,

802
00:39:35,700 --> 00:39:35,690
but it's a continuous value.

803
00:39:35,700 --> 00:39:40,329
and those time series define what we call the activations over time.

804
00:39:40,339 --> 00:39:44,810
and synchronization is quite simply just measuring the dotproduct between two of these time series.

805
00:39:44,820 --> 00:39:49,130
so you have a system of d neurons

806
00:39:49,140 --> 00:39:55,290
and essentially you have d over two squared different synchronization pairs.

807
00:39:55,300 --> 00:39:59,690
so neuron one can be related to neuron 2 by how they synchronize

808
00:39:59,700 --> 00:39:59,690
and neuron

809
00:39:59,700 --> 00:39:59,690
and neuron

810
00:39:59,700 --> 00:39:59,690
and neuron

811
00:39:59,700 --> 00:39:59,690
and neuron

812
00:39:59,700 --> 00:40:05,050
the neuron level models

813
00:40:05,060 --> 00:40:05,050
they function by taking in a finite history

814
00:40:05,060 --> 00:40:11,130
like a foq of neuron of activations coming in

815
00:40:11,140 --> 00:40:11,130
and instead of being just a radio activation

816
00:40:11,140 --> 00:40:15,290
they use that history as information to uh

817
00:40:15,300 --> 00:40:15,290
process a single activation out

818
00:40:15,300 --> 00:40:21,369
and that is what moves from what we call pre-activations to post activations.

819
00:40:21,379 --> 00:40:30,410
and the principle here is that this might seem rather arbitrary and does it help for performance?

820
00:40:30,420 --> 00:40:34,730
turns out it does, but that's not really the catch all solution here. that's not what we're after. uh

821
00:40:34,740 --> 00:40:38,170
what we're after here is trying to do something biologically plausible. uh

822
00:40:38,180 --> 00:40:44,089
find the line somewhere between biology, which is how the brain implements things in the biological substrate

823
00:40:44,099 --> 00:40:44,089
that we have

824
00:40:47,599 --> 00:40:52,410
versus deep learning, which is highly parallelizable, super fast to learn, back propanable,

825
00:40:52,420 --> 00:40:55,530
all of the nice properties of that that have got us this far.

826
00:40:55,540 --> 00:40:55,530
and find a line somewhere

827
00:40:55,540 --> 00:40:59,369
where we can take some sprinkling of biological inspiration but still train it with deep learning.

828
00:40:59,379 --> 00:41:02,970
and it turns out that neuron level models is a nice interim that we can do this with.

829
00:41:02,980 --> 00:41:07,690
the concept of synchronization is applied on top of the outputs of those neuron level models.

830
00:41:07,700 --> 00:41:12,170
so on on this

831
00:41:12,180 --> 00:41:12,170
on the scaling

832
00:41:12,180 --> 00:41:12,170
on the scaling

833
00:41:12,180 --> 00:41:17,450
i think the time complexity is quadratic in respect of the dimension of the synchronization matrix

834
00:41:17,460 --> 00:41:17,450
right

835
00:41:17,460 --> 00:41:21,609
and in your paper

836
00:41:21,619 --> 00:41:21,609
you were talking about subsampling to improve the performance

837
00:41:21,619 --> 00:41:25,450
but how how did that affect the

838
00:41:25,460 --> 00:41:25,450
the stability and the you know

839
00:41:25,460 --> 00:41:29,609
like were there any things that that cost you doing that? yeah, it's a neat question.

840
00:41:29,619 --> 00:41:32,810
i think in terms of stability,

841
00:41:32,820 --> 00:41:32,810
what's what we found was kind of fun

842
00:41:32,820 --> 00:41:32,810
what's what we found was kind of fun

843
00:41:35,920 --> 00:41:40,410
that we had throughout the the experiments that we ran with this paper was it tended

844
00:41:40,420 --> 00:41:40,410
no matter what we tried it on it

845
00:41:40,420 --> 00:41:45,290
it just kind of worked with all spreads of hyperparameters. uh

846
00:41:45,300 --> 00:41:45,290
and this

847
00:41:45,300 --> 00:41:49,130
the problems that you have with back prop through time

848
00:41:49,140 --> 00:41:49,130
typically with recurrence models

849
00:41:49,140 --> 00:41:49,130
typically with recurrence models

850
00:41:49,140 --> 00:41:53,290
and lstms it's a challenge

851
00:41:53,300 --> 00:41:56,569
and you run for many internal ticks with the rnns or the lstms and the learning seems to break

852
00:41:56,579 --> 00:41:56,569
down

853
00:41:56,579 --> 00:41:56,569
down

854
00:41:56,579 --> 00:42:05,050
that we use synchronization in some sense touches all of the neurons through all of the time

855
00:42:05,060 --> 00:42:05,050
so it really helps with gradient propagation

856
00:42:05,060 --> 00:42:05,050
so it really helps with gradient propagation

857
00:42:05,060 --> 00:42:05,050
so it really helps with gradient propagation

858
00:42:09,200 --> 00:42:09,450
that's maybe a bit oblique to what you asked about synchronization

859
00:42:09,460 --> 00:42:14,010
is we have a system of d neurons

860
00:42:14,020 --> 00:42:16,890
and like i said earlier there d over two squared possible combinations.

861
00:42:20,000 --> 00:42:20,250
this essentially means that our underlying state

862
00:42:20,260 --> 00:42:23,930
or underlying representation to the system is quite a lot larger

863
00:42:23,940 --> 00:42:28,329
than what you would get with just taking those d neurons.

864
00:42:28,339 --> 00:42:32,569
and as to what that means in terms of downstream computation

865
00:42:32,579 --> 00:42:35,770
and performance and the things that we can do with this is what we're actively exploring right now.

866
00:42:35,780 --> 00:42:35,770
> > you guys used an exponential decay rate.

867
00:42:40,000 --> 00:42:40,250
> > you have the system that unfolds over time.

868
00:42:40,260 --> 00:42:49,849
it would be maybe a little bit too constrained if the synchronization between any two neurons depended on the same time scale.

869
00:42:49,859 --> 00:42:49,849
so for instance,

870
00:42:49,859 --> 00:42:54,010
there are neurons in your brain that are firing over very long time scales and very short time scales.

871
00:42:54,020 --> 00:42:57,770
the way that they fire together impacts other neurons and causes those neurons to fire.

872
00:42:57,780 --> 00:43:06,650
but everything in biological brains happens at diverse time scales. it's why we have uh

873
00:43:06,660 --> 00:43:06,650
different brain waves for different thinking states

874
00:43:06,660 --> 00:43:06,650
different brain waves for different thinking states

875
00:43:06,660 --> 00:43:06,650
different brain waves for different thinking states

876
00:43:06,660 --> 00:43:11,930
but beside

877
00:43:11,940 --> 00:43:16,089
that point, what we do with the exponential decay in the continuous thought machines is it allows us for a very sharp decay to say

878
00:43:16,099 --> 00:43:20,329
that these two neurons that are pairing together,

879
00:43:20,339 --> 00:43:24,490
what only really matters is how they fire together right now.

880
00:43:24,500 --> 00:43:24,490
right?

881
00:43:24,500 --> 00:43:24,490
right?

882
00:43:24,500 --> 00:43:24,490
right?

883
00:43:28,400 --> 00:43:31,849
that's capturing a global sense of how those neurons are firing over an extremely long period of time.

884
00:43:31,859 --> 00:43:31,849
so this was essentially a way of us

885
00:43:31,859 --> 00:43:31,849
so this was essentially a way of us

886
00:43:31,859 --> 00:43:37,609
capturing this idea of how different neurons could maybe fire together very quickly

887
00:43:37,619 --> 00:43:42,569
and other neurons can fire together very slowly or not at all.

888
00:43:42,579 --> 00:43:42,569
and this lets

889
00:43:42,579 --> 00:43:51,450
that representation space that i spoke about that d over2 squ representation space lets it again become more rich

890
00:43:51,460 --> 00:43:55,930
and we can enrich that space with more subtle tweaks to how we compute those representations.

891
00:43:58,560 --> 00:43:58,810
so we were speaking about this yesterday, luke, that um

892
00:43:58,820 --> 00:44:05,690
when folks apply transformers to things like the arc challenge or things that need reasoning.

893
00:44:05,700 --> 00:44:05,690
um

894
00:44:05,700 --> 00:44:05,690
um

895
00:44:05,700 --> 00:44:13,770
so the architects who were the winners of last year's challenge, they did um depth first search sampling

896
00:44:13,780 --> 00:44:17,849
and some folks have been experimenting with using language representations

897
00:44:17,859 --> 00:44:17,849
or you know

898
00:44:17,859 --> 00:44:17,849
or you know

899
00:44:17,859 --> 00:44:23,450
and some part of this is to do with the the

900
00:44:23,460 --> 00:44:23,450
the reachability um

901
00:44:23,460 --> 00:44:23,450
the reachability um

902
00:44:23,460 --> 00:44:23,450
the reachability um

903
00:44:23,460 --> 00:44:23,450
the reachability um

904
00:44:28,240 --> 00:44:28,490
which means you can kind of monotonically

905
00:44:28,500 --> 00:44:28,490
um increase

906
00:44:28,500 --> 00:44:31,770
but if i understand correctly

907
00:44:31,780 --> 00:44:35,050
your system might have some interesting properties for reasoning

908
00:44:35,060 --> 00:44:35,050
and for discrete and sparse domains and also for sample efficiency

909
00:44:39,440 --> 00:44:39,690
because we want

910
00:44:39,700 --> 00:44:43,050
we want to build a system that can actually do well on things like the arc challenge.

911
00:44:43,060 --> 00:44:46,890
but can you kind of explain in simple terms why you think this architecture could be significantly better

912
00:44:46,900 --> 00:44:51,290
than transformers for doing those things? > >

913
00:44:51,300 --> 00:44:54,410
i think a lot of the really fascinating work in the last few years

914
00:44:54,420 --> 00:45:02,970
that i found fascinating in the literature of language models has been related to what one can actually call a new scaling dimension.

915
00:45:02,980 --> 00:45:02,970
i in some sense see continue

916
00:45:02,980 --> 00:45:02,970
i in some sense see continue

917
00:45:02,980 --> 00:45:08,730
chain of thought reasoning as a way of adding more compute to a system.

918
00:45:08,740 --> 00:45:13,530
that's obviously just one small part of what that really is and what that really means.

919
00:45:13,540 --> 00:45:16,810
but i think it's quite a profound breakthrough

920
00:45:16,820 --> 00:45:16,810
uh

921
00:45:16,820 --> 00:45:16,810
uh

922
00:45:16,820 --> 00:45:16,810
uh

923
00:45:16,820 --> 00:45:27,770
that reasoning component be entirely internal yet still running in some sort of sequential manner.

924
00:45:27,780 --> 00:45:27,770
and i think that that's rather important.

925
00:45:27,780 --> 00:45:32,329
and you spoke earlier about gemini's diffusion language modeling

926
00:45:32,339 --> 00:45:37,210
and i think that there are a lot of different directions that are exploring this right now. uh

927
00:45:37,220 --> 00:45:43,690
i do think that the continuous thought machine with the ideas of synchronization

928
00:45:43,700 --> 00:45:48,970
and multi-hierarchical temporal representations gives a certain flexibility on that space that

929
00:45:48,980 --> 00:45:48,970
uh

930
00:45:48,980 --> 00:45:48,970
uh

931
00:45:48,980 --> 00:45:54,170
and that richness of that space

932
00:45:54,180 --> 00:45:58,010
being able to project the next step to solve the arc challenge

933
00:45:58,020 --> 00:45:58,010
and the next 100

934
00:45:58,020 --> 00:46:02,569
the next 200 steps to be able to break that down into a process that a model can

935
00:46:02,579 --> 00:46:02,569
then

936
00:46:02,579 --> 00:46:02,569
then

937
00:46:02,579 --> 00:46:02,569
then

938
00:46:02,579 --> 00:46:12,569
that process in its highdimensional latent case becomes something that feels like a good approach to take.

939
00:46:12,579 --> 00:46:12,569
> >

940
00:46:12,579 --> 00:46:15,530
do you see any relationship between this architecture and you know alex graves neuro touring machine?

941
00:46:19,920 --> 00:46:20,170
>

942
00:46:20,180 --> 00:46:20,170
>

943
00:46:20,180 --> 00:46:20,170
>

944
00:46:20,180 --> 00:46:20,170
>

945
00:46:20,180 --> 00:46:20,170
>

946
00:46:20,180 --> 00:46:26,170
i think that the one of the the most challenging parts about uh

947
00:46:26,180 --> 00:46:31,050
working with a neural neural touring machine is the concept of writing to memory and reading to memory

948
00:46:31,060 --> 00:46:34,490
because it is a discrete action.

949
00:46:34,500 --> 00:46:34,490
um

950
00:46:34,500 --> 00:46:34,490
um

951
00:46:34,500 --> 00:46:42,250
and yes

952
00:46:42,260 --> 00:46:42,250
uh

953
00:46:42,260 --> 00:46:47,130
i wouldn't go so far as to say that the continuous thought machine is definitively nearing tur

954
00:46:47,140 --> 00:46:47,130
incomplete

955
00:46:47,140 --> 00:46:52,890
but the notion of the notion of doing reasoning in a space that is uh latent

956
00:46:52,900 --> 00:46:58,170
and letting that space unfold in a way that is uh

957
00:46:58,180 --> 00:46:58,170
rich towards a different set of tasks.

958
00:46:58,180 --> 00:47:03,770
and this this actually brings me to a point that i find quite interesting

959
00:47:03,780 --> 00:47:03,770
um

960
00:47:03,780 --> 00:47:07,130
that i'd like to share with you.

961
00:47:07,140 --> 00:47:12,010
consider again the imageet task or any sort of uh classification task. it's

962
00:47:12,020 --> 00:47:12,010
it's a nice test bed.

963
00:47:12,020 --> 00:47:18,010
there are many images that are really easy and there are many images that are really difficult.

964
00:47:18,020 --> 00:47:18,010
when we train for instance

965
00:47:18,020 --> 00:47:21,930
a vit or a cnn

966
00:47:21,940 --> 00:47:21,930
uh

967
00:47:21,940 --> 00:47:28,650
to do this task, it has to nest all of that reasoning in the same space.

968
00:47:28,660 --> 00:47:33,930
it has to put all of its decision-m process for a very simple obvious cat versus some complex weird underrepresented class in

969
00:47:33,940 --> 00:47:38,970
that system in that data set

970
00:47:38,980 --> 00:47:38,970
and it has to nest it all in parallel in a way that is

971
00:47:38,980 --> 00:47:44,410
we get to the last layer and then we classify.

972
00:47:44,420 --> 00:47:44,410
um

973
00:47:44,420 --> 00:47:44,410
um

974
00:47:44,420 --> 00:47:50,170
breaking that down where you have different points in time

975
00:47:50,180 --> 00:47:50,170
where you can say

976
00:47:50,180 --> 00:47:50,170
where you can say

977
00:47:50,180 --> 00:47:50,170
where you can say

978
00:47:50,180 --> 00:47:55,530
versus now i'm done

979
00:47:55,540 --> 00:47:55,530
i can stop

980
00:47:55,540 --> 00:47:55,530
i can stop

981
00:47:55,540 --> 00:48:00,170
and actually naturally segment it into its easy to difficult components.

982
00:48:00,180 --> 00:48:05,050
and i think we know that curriculum learning

983
00:48:05,060 --> 00:48:09,210
and learning in this continuous sense again seems to be a good idea.

984
00:48:09,220 --> 00:48:09,210
it's it's how humans learn.

985
00:48:09,220 --> 00:48:13,369
and if we can get at that architecturally

986
00:48:13,379 --> 00:48:18,089
and just have that fall out in a model, again, this seems like a something worth exploring.

987
00:48:18,099 --> 00:48:18,089
uh

988
00:48:18,099 --> 00:48:21,609
i'm not sure if you know much about model calibration and how neural networks tend to be poorly calibrated.

989
00:48:24,640 --> 00:48:24,890
> > oh, go for it, tommy.

990
00:48:24,900 --> 00:48:24,890
um

991
00:48:24,900 --> 00:48:24,890
um

992
00:48:24,900 --> 00:48:29,050
it's a bit of an old finding, but if you train a neural network for long enough

993
00:48:29,060 --> 00:48:32,250
and it fits really really well and you've regularized it

994
00:48:32,260 --> 00:48:32,250
regularized it really really well,

995
00:48:32,260 --> 00:48:35,530
you'll find that the model is unccalibrated, which essentially means that it is very certain

996
00:48:35,540 --> 00:48:35,530
uh

997
00:48:35,540 --> 00:48:41,770
about some components

998
00:48:41,780 --> 00:48:45,530
where some classes where it's wrong and uncertain for some classes where it's correct.

999
00:48:45,540 --> 00:48:49,130
essentially what you want for a perfectly calibrated model is if it predicts a uh probability

1000
00:48:49,140 --> 00:48:49,130
that this is in class

1001
00:48:49,140 --> 00:48:53,690
the correct class with 50

1002
00:48:53,700 --> 00:48:53,690
%.

1003
00:48:53,700 --> 00:48:53,690
%.

1004
00:48:53,700 --> 00:48:53,690
%.

1005
00:48:59,200 --> 00:48:59,450
and so forth.

1006
00:48:59,460 --> 00:48:59,450
so a well-c calibrated model

1007
00:48:59,460 --> 00:49:03,050
if it's predicting a probability of 0. 9 that it is a cat

1008
00:49:03,060 --> 00:49:03,050
then 90 % of the time it should be correct.

1009
00:49:03,060 --> 00:49:14,089
and it's actually turns out that most models that you train for long enough get poorly calibrated.

1010
00:49:14,099 --> 00:49:14,089
and there are loads of post hawk tricks

1011
00:49:14,099 --> 00:49:14,089
and there are loads of post hawk tricks

1012
00:49:14,099 --> 00:49:21,930
to fixing this. we measured the calibration of the ctm after training and it was nearly perfectly calibrated

1013
00:49:21,940 --> 00:49:21,930
which is again a little bit of a smoking gun

1014
00:49:21,940 --> 00:49:26,089
that this actually seems to be probably a better way to do things.

1015
00:49:26,099 --> 00:49:30,569
the flavor of this kind of research is such that we didn't actually go out

1016
00:49:30,579 --> 00:49:35,930
and actually try to create a very well-c calibrated model, right?

1017
00:49:35,940 --> 00:49:40,970
and we didn't even try to create a model

1018
00:49:40,980 --> 00:49:46,089
that was necessarily going to be able to do some kind of adaptive computation time, right? um

1019
00:49:46,099 --> 00:49:46,089
i was

1020
00:49:46,099 --> 00:49:46,089
i was

1021
00:49:46,099 --> 00:49:46,089
i was

1022
00:49:46,099 --> 00:49:46,089
i was

1023
00:49:46,099 --> 00:49:46,089
i was

1024
00:49:46,099 --> 00:49:46,089
i was

1025
00:49:53,359 --> 00:49:53,609
adapted computation

1026
00:49:53,619 --> 00:49:53,609
time

1027
00:49:53,619 --> 00:49:53,609
time

1028
00:49:53,619 --> 00:49:53,609
time

1029
00:49:53,619 --> 00:49:53,609
time

1030
00:49:53,619 --> 00:49:59,530
it had a massive amount of hyperparameter sweeps in it

1031
00:49:59,540 --> 00:50:05,530
because in that paper he needed to have a loss on the amount of computation that was being done.

1032
00:50:10,079 --> 00:50:10,329
> > because anytime you try to do some sort of adaptive computation time research,

1033
00:50:16,000 --> 00:50:23,210
what you're fighting is the fact that neural networks are greedy, right?

1034
00:50:23,220 --> 00:50:27,450
because obviously the way to get the lowest loss is to use all the computation

1035
00:50:27,460 --> 00:50:27,450
that you have access to.

1036
00:50:27,460 --> 00:50:32,170
so unless you had like an extra loss that had a penalty that said

1037
00:50:32,180 --> 00:50:32,170
okay

1038
00:50:32,180 --> 00:50:32,170
okay

1039
00:50:32,180 --> 00:50:36,010
you're not allowed to use all the computation that's and and very very carefully balance loss

1040
00:50:36,020 --> 00:50:36,010
that's

1041
00:50:36,020 --> 00:50:46,089
when you actually got the interesting dynamic computation time behavior falling out of the the model in that paper.

1042
00:50:46,099 --> 00:50:51,050
but was really gratifying to see with the the continuous thought machine is that

1043
00:50:51,060 --> 00:50:55,849
because of the way that we set up the loss that luke described earlier,

1044
00:50:55,859 --> 00:51:00,970
adaptive computation times seem to just fall out naturally.

1045
00:51:00,980 --> 00:51:07,050
so that's more the way that i think research should go. > > okay?

1046
00:51:07,060 --> 00:51:11,450
because we we don't actually have like a specific goal

1047
00:51:11,460 --> 00:51:11,450
um

1048
00:51:11,460 --> 00:51:11,450
um

1049
00:51:11,460 --> 00:51:16,890
or a specific problem we're trying to fix like that or something we're trying to invent.

1050
00:51:16,900 --> 00:51:25,770
it's more that we have this interesting architecture and that we're just following the gradients of interestingness.

1051
00:51:25,780 --> 00:51:25,770
> > yes.

1052
00:51:25,780 --> 00:51:25,770
> > yes.

1053
00:51:25,780 --> 00:51:32,569
that point, i i think maybe the most exciting thing about your paper is, you know, we were talking about path dependence

1054
00:51:32,579 --> 00:51:32,569
and um

1055
00:51:32,579 --> 00:51:32,569
and um

1056
00:51:32,579 --> 00:51:38,329
which is built step by step, this process of complexification

1057
00:51:38,339 --> 00:51:38,329
and u

1058
00:51:38,339 --> 00:51:38,329
and u

1059
00:51:38,339 --> 00:51:44,170
this is um apppropo in in the theme of world models in in general

1060
00:51:44,180 --> 00:51:44,170
and also active inference

1061
00:51:48,720 --> 00:51:48,970
and i say active inference in big quotes

1062
00:51:48,980 --> 00:51:48,970
because it's not kl friston's active

1063
00:51:48,980 --> 00:51:52,250
you know

1064
00:51:52,260 --> 00:51:54,809
maybe adaptive inference or something like that but we want to build agents that can continue to learn

1065
00:51:54,819 --> 00:51:54,809
that can update their parameters

1066
00:51:54,819 --> 00:52:00,329
and most importantly can construct path dependent understanding

1067
00:52:00,339 --> 00:52:06,730
and because it that's completely different to just understanding what the thing is.

1068
00:52:06,740 --> 00:52:10,410
it's how you got there is very important

1069
00:52:10,420 --> 00:52:14,569
and this architecture potentially allows these agents using this algorithm to explore trajectories in spaces

1070
00:52:14,579 --> 00:52:24,730
find the best trajectories and actually construct an understanding which carves the world up by the joints.

1071
00:52:24,740 --> 00:52:24,730
yeah, that's a

1072
00:52:24,740 --> 00:52:28,329
that's a really neat perspective. i haven't actually thought about it like that, but yes, i think

1073
00:52:28,339 --> 00:52:28,329
um

1074
00:52:28,339 --> 00:52:34,010
that particular stance becomes really interesting when you think about ambiguous problems

1075
00:52:34,020 --> 00:52:45,050
because carving the world up in one way is as performant as carving it up in another way.

1076
00:52:45,060 --> 00:52:45,050
> > yeah.

1077
00:52:45,060 --> 00:52:45,050
> > yeah.

1078
00:52:45,060 --> 00:52:45,050
> > yeah.

1079
00:52:45,060 --> 00:52:47,130
perhaps the hallucination in language models is carving the world up in some fine way

1080
00:52:52,640 --> 00:52:52,890
but it's just not performance

1081
00:52:52,900 --> 00:52:52,890
in our measure of this is hallucination

1082
00:52:52,900 --> 00:52:56,650
and actually that's not true

1083
00:52:56,660 --> 00:52:56,650
but in some other

1084
00:52:56,660 --> 00:53:01,849
trace down the path of wanting to carve the world up through a auto

1085
00:53:01,859 --> 00:53:01,849
reggressive

1086
00:53:01,859 --> 00:53:01,849
reggressive

1087
00:53:01,859 --> 00:53:05,930
you end up in a different carve up of that world

1088
00:53:05,940 --> 00:53:05,930
and being able to train a model

1089
00:53:05,940 --> 00:53:10,490
that can be implicitly aware of the fact

1090
00:53:10,500 --> 00:53:14,890
that it is actually carving up the world in a different way

1091
00:53:14,900 --> 00:53:14,890
and and explore those manners, those

1092
00:53:14,900 --> 00:53:14,890
and and explore those manners, those

1093
00:53:14,900 --> 00:53:20,890
descents down the carve up is something that we're after

1094
00:53:20,900 --> 00:53:24,650
and i think it's quite an exciting approach to be trying to take a stance of

1095
00:53:24,660 --> 00:53:24,650
let's

1096
00:53:24,660 --> 00:53:31,369
let's break up this problem into small solvable parts and learn to do it like that

1097
00:53:31,379 --> 00:53:36,010
and how can we do this in a natural way without too many hacks.

1098
00:53:36,020 --> 00:53:40,730
yeah, it's something i've been thinking about

1099
00:53:40,740 --> 00:53:40,730
because um shalet

1100
00:53:40,740 --> 00:53:40,730
because um shalet

1101
00:53:40,740 --> 00:53:44,809
um ideas is for him adapting to novelty is getting the right answer

1102
00:53:44,819 --> 00:53:51,609
and the reason why you gave that answer is very

1103
00:53:51,619 --> 00:53:51,609
very important

1104
00:53:51,619 --> 00:53:51,609
very important

1105
00:53:51,619 --> 00:53:56,089
we have this problem that we

1106
00:53:56,099 --> 00:54:00,490
we come up with this kind of cost function that rather leads to this shortcut problem

1107
00:54:00,500 --> 00:54:04,569
but you know we could just build a symbolic system

1108
00:54:04,579 --> 00:54:04,569
we could be gi and and we could say

1109
00:54:04,579 --> 00:54:04,569
we could be gi and and we could say

1110
00:54:04,579 --> 00:54:10,010
we need do this um principled kind of construction of knowledge

1111
00:54:10,020 --> 00:54:10,010
maintaining semantics.

1112
00:54:10,020 --> 00:54:13,930
well, we're not doing that.

1113
00:54:13,940 --> 00:54:17,530
we're doing a hybrid system. but there must be some natural way of doing reasoning

1114
00:54:17,540 --> 00:54:17,530
where

1115
00:54:17,540 --> 00:54:23,210
in spite of the end objective being this cost function

1116
00:54:23,220 --> 00:54:23,210
that

1117
00:54:23,220 --> 00:54:27,609
because of the way that we traversed these open-ended spaces that we can actually have more confidence

1118
00:54:27,619 --> 00:54:27,609
mechanistically

1119
00:54:27,619 --> 00:54:32,170
that we're doing reasoning

1120
00:54:32,180 --> 00:54:32,170
which is aligned to the world.

1121
00:54:32,180 --> 00:54:37,369
i think that's a great way of seeing this particular uh avenue of research

1122
00:54:37,379 --> 00:54:42,089
and i think that obviously we're not the only people thinking like this

1123
00:54:42,099 --> 00:54:44,809
and we're not the only ones trying to do this.

1124
00:54:44,819 --> 00:54:44,809
um

1125
00:54:44,819 --> 00:54:44,809
um

1126
00:54:44,819 --> 00:54:50,410
and surprisingly

1127
00:54:50,420 --> 00:54:50,410
so it wasn't

1128
00:54:50,420 --> 00:54:50,410
so it wasn't

1129
00:54:50,420 --> 00:54:50,410
so it wasn't

1130
00:54:50,420 --> 00:54:55,849
it's not the goal to to do this type of research.

1131
00:54:55,859 --> 00:54:58,970
it's not the goal to be able to break the world down into these small uh chunks

1132
00:54:58,980 --> 00:55:03,930
that we can actually reason over in in a way that seems natural.

1133
00:55:03,940 --> 00:55:10,730
instead, what we did was pay respect to the brain, pay respect to nature and say, well,

1134
00:55:10,740 --> 00:55:15,849
if we build these inspired things, what what actually happens? what

1135
00:55:15,859 --> 00:55:15,849
what different ways of approaching a problem emerge?

1136
00:55:15,859 --> 00:55:27,050
and then when those different ways of approaching a problem emerge, what big philosophical and

1137
00:55:27,060 --> 00:55:27,050
uh

1138
00:55:27,060 --> 00:55:31,530
intelligence-based questions can we then start to ask? and that's where we're at right now.

1139
00:55:31,540 --> 00:55:31,530
so it might feel at times, especially for me, uh

1140
00:55:31,540 --> 00:55:36,170
too many questions and too few hands to answer those questions.

1141
00:55:36,180 --> 00:55:41,290
but i think the fun and exciting thing and the encouraging thing that i i can

1142
00:55:41,300 --> 00:55:41,290
you know

1143
00:55:41,300 --> 00:55:45,050
try to encourage other younger researchers out there is that

1144
00:55:45,060 --> 00:55:45,050
uh

1145
00:55:45,060 --> 00:55:45,050
uh

1146
00:55:45,060 --> 00:55:50,010
do what you're passion passionate about

1147
00:55:50,020 --> 00:55:55,130
and figure out how to build the things that you care about and then see what that does.

1148
00:55:55,140 --> 00:55:59,450
see what doors that opens up and see how to explore deeper into those domains.

1149
00:55:59,460 --> 00:56:02,569
> > we were talking about this yesterday, weren't we?

1150
00:56:02,579 --> 00:56:02,569
that you can think of language as being a kind of maze.

1151
00:56:07,040 --> 00:56:07,290
> > yes.

1152
00:56:07,300 --> 00:56:11,770
like what is to stop us from taking this architecture and building the next generation language model with it.

1153
00:56:11,780 --> 00:56:11,770
i mean that that's honestly

1154
00:56:11,780 --> 00:56:11,770
i mean that that's honestly

1155
00:56:16,720 --> 00:56:16,970
something that i am actively trying to explore right now

1156
00:56:16,980 --> 00:56:16,970
and uh

1157
00:56:16,980 --> 00:56:16,970
and uh

1158
00:56:16,980 --> 00:56:21,290
i think the maze

1159
00:56:21,300 --> 00:56:21,290
the maze task gets really interesting

1160
00:56:21,300 --> 00:56:26,569
when you add ambiguity to it when there are many ways to solve the maze

1161
00:56:26,579 --> 00:56:26,569
and honestly

1162
00:56:26,579 --> 00:56:29,609
this isn't something i've tried yet

1163
00:56:29,619 --> 00:56:29,609
and maybe it's something i should try next week

1164
00:56:29,619 --> 00:56:33,609
but it's essentially you can imagine an agent or the ctm

1165
00:56:33,619 --> 00:56:33,609
in this case

1166
00:56:33,619 --> 00:56:37,609
observing the maze and taking a trajectory

1167
00:56:37,619 --> 00:56:37,609
and surprisingly we saw this

1168
00:56:37,619 --> 00:56:42,490
we have a section in our recently updated paper on ar archive

1169
00:56:42,500 --> 00:56:42,490
the final camera ready version of this paper

1170
00:56:46,880 --> 00:56:50,730
where we added an extra supplementary section that is not in the main technical report

1171
00:56:50,740 --> 00:56:55,210
and that supplementary section is basically hey we saw this cool stuff happen

1172
00:56:55,220 --> 00:56:55,210
and we list

1173
00:56:55,220 --> 00:56:59,690
i think 14 different interesting things that happened while we were doing the research

1174
00:56:59,700 --> 00:56:59,690
um

1175
00:56:59,700 --> 00:57:03,210
that obviously didn't make it into the paper

1176
00:57:03,220 --> 00:57:06,730
but we wanted people to know about these strange things that happened

1177
00:57:06,740 --> 00:57:06,730
and this is one of the strange things where

1178
00:57:06,740 --> 00:57:06,730
and this is one of the strange things where

1179
00:57:11,359 --> 00:57:11,609
we watched during training

1180
00:57:11,619 --> 00:57:11,609
what was happening.

1181
00:57:11,619 --> 00:57:19,609
and at some time during training, maybe halfway through the training run, we could see what the model would do is it would start going one path in the maze

1182
00:57:19,619 --> 00:57:23,530
and then suddenly it would realize, oh no,

1183
00:57:23,540 --> 00:57:23,530
damn,

1184
00:57:23,540 --> 00:57:27,210
i'm wrong. and would backtrack and then take another path. but eventually it gets really good

1185
00:57:27,220 --> 00:57:31,609
and it does you some sort of distributed learning in this

1186
00:57:31,619 --> 00:57:35,450
because it's got a attention mechanism with multiple heads.

1187
00:57:35,460 --> 00:57:38,490
so it can actually just figure out how to do this pretty well and refine its solution.

1188
00:57:38,500 --> 00:57:43,450
but sometime early on in the the learning it descends multiple paths and comes back and backtracks.

1189
00:57:43,460 --> 00:57:47,609
we have a really fascinating set of experiments that also showed

1190
00:57:47,619 --> 00:57:47,609
and this

1191
00:57:47,619 --> 00:57:47,609
and this

1192
00:57:47,619 --> 00:57:51,369
we actually have some supplementary material online showing this where

1193
00:57:51,379 --> 00:57:51,369
uh

1194
00:57:51,379 --> 00:57:55,770
and i don't really know what this says. it's kind of a deep philosophical thing

1195
00:57:55,780 --> 00:57:58,490
but if you're trying to solve a maze

1196
00:57:58,500 --> 00:58:03,450
but you don't have enough time. turns out that there's a there's a foster algorithm to do it.

1197
00:58:03,460 --> 00:58:03,450
and this was

1198
00:58:03,460 --> 00:58:07,930
this blew my mind when i saw it.

1199
00:58:07,940 --> 00:58:07,930
so if we constrain the amount of thinking time that the model has

1200
00:58:07,940 --> 00:58:11,369
but still get it to try solve a long maze

1201
00:58:11,379 --> 00:58:11,369
instead of tracing out that maze,

1202
00:58:11,379 --> 00:58:15,770
what it does is it quickly jumps ahead to approximately

1203
00:58:15,780 --> 00:58:19,690
where it needs to be and it traces backwards and it fills in that path backwards

1204
00:58:19,700 --> 00:58:28,890
and then it jumps forward again leaprogs over the top and traces that section backwards and then leap frogs

1205
00:58:28,900 --> 00:58:32,650
and it does this fascinating leaprogging behavior that is based on the constraint of the system. and again,

1206
00:58:32,660 --> 00:58:32,650
you know,

1207
00:58:32,660 --> 00:58:37,130
this is just an observation we made

1208
00:58:37,140 --> 00:58:37,130
and what that means

1209
00:58:37,140 --> 00:58:37,130
and what that means

1210
00:58:37,140 --> 00:58:37,130
and what that means

1211
00:58:37,140 --> 00:58:37,130
and what that means

1212
00:58:37,140 --> 00:58:42,170
giving a model time to think versus not

1213
00:58:42,180 --> 00:58:42,170
and is it enough time to think?

1214
00:58:42,180 --> 00:58:42,170
and is it enough time to think?

1215
00:58:45,839 --> 00:58:46,089
what different algorithms does the model learn

1216
00:58:46,099 --> 00:58:49,930
when you constrain it in this way? i find that quite fascinating and an interesting thing to explore.

1217
00:58:49,940 --> 00:59:01,690
does it tell us something about how humans think? does it tell us something about how how we think under constrained settings versus open-ended settings? there's a number of cool questions you can ask on this front.

1218
00:59:04,079 --> 00:59:04,329
> > you

1219
00:59:04,339 --> 00:59:04,329
you guys are both huge fans of um

1220
00:59:04,339 --> 00:59:04,329
you guys are both huge fans of um

1221
00:59:04,339 --> 00:59:08,250
population methods and collective intelligence

1222
00:59:08,260 --> 00:59:08,250
and because we can

1223
00:59:08,260 --> 00:59:12,170
we can scale this thing up and we can scale it out

1224
00:59:12,180 --> 00:59:16,569
and what would it mean to scale this thing out not only just in a kind of um

1225
00:59:16,579 --> 00:59:20,410
what do they call it trivial paralization but in terms of having some kind of weight sharing between parallel models

1226
00:59:20,420 --> 00:59:20,410
and so on.

1227
00:59:20,420 --> 00:59:20,410
and so on.

1228
00:59:25,280 --> 00:59:25,530
what

1229
00:59:25,540 --> 00:59:25,530
what would

1230
00:59:25,540 --> 00:59:25,530
what would

1231
00:59:25,540 --> 00:59:28,089
what would that give you potentially? > >

1232
00:59:28,099 --> 00:59:28,089
uh

1233
00:59:28,099 --> 00:59:28,089
uh

1234
00:59:28,099 --> 00:59:28,089
uh

1235
00:59:28,099 --> 00:59:32,890
so one of the active things that we're trying to explore in our team is

1236
00:59:32,900 --> 00:59:32,890
uh

1237
00:59:32,900 --> 00:59:32,890
uh

1238
00:59:32,900 --> 00:59:37,049
long-term memory

1239
00:59:37,059 --> 00:59:40,569
and what what does this mean for a system like this? so an experiment that one can construct

1240
00:59:40,579 --> 00:59:40,569
for instance

1241
00:59:40,579 --> 00:59:46,329
is to put some agents in a maze and let them try

1242
00:59:46,339 --> 00:59:46,329
solve this maze

1243
00:59:46,339 --> 00:59:46,329
solve this maze

1244
00:59:46,339 --> 00:59:50,569
but in a very constrained setting where a agent can only see maybe a 5x5 region around it

1245
00:59:55,760 --> 00:59:56,010
and we give that agent some mechanism for saving and retrieving memories

1246
00:59:56,020 --> 01:00:02,010
and the task if you wish is to solve that maze

1247
01:00:02,020 --> 01:00:02,010
find your way to the end

1248
01:00:02,020 --> 01:00:09,770
and the model needs to learn how to construct memory such that it can get back to a point

1249
01:00:09,780 --> 01:00:09,770
where it's seen before

1250
01:00:09,780 --> 01:00:14,170
and know i did the wrong thing last time

1251
01:00:14,180 --> 01:00:18,250
and go a different route and you can then see this with uh

1252
01:00:18,260 --> 01:00:23,369
parallel agents in the same maze with a shared memory structure and see what actually happens

1253
01:00:23,379 --> 01:00:27,210
when you can all access that memory structure and have a shared global like

1254
01:00:27,220 --> 01:00:31,530
almost like a cultural memory that we can access and solve this global task

1255
01:00:31,540 --> 01:00:31,530
by having many agents trying to use this memory system

1256
01:00:36,799 --> 01:00:37,049
and i do think

1257
01:00:37,059 --> 01:00:41,049
that memory is going to be a very key element to what we need to do in the future for ai in general.

1258
01:00:41,059 --> 01:00:46,809
> > so the subject of uh reasoning came up just a second ago

1259
01:00:46,819 --> 01:00:54,089
and i think there's a perception that recently we made a lot of progress in reasoning right

1260
01:00:54,099 --> 01:00:58,490
because it's actually one of the main things that i think people are are working on.

1261
01:00:58,500 --> 01:01:04,650
we released a data set recently called uh sudoku bench

1262
01:01:04,660 --> 01:01:09,530
and i was actually quite happy to see it come up organically on your uh podcast a few weeks ago.

1263
01:01:09,540 --> 01:01:12,490
> > chris moore, > > right? > > yes.

1264
01:01:12,500 --> 01:01:15,530
so, i i wanted to tell you a little bit about this benchmark

1265
01:01:15,540 --> 01:01:20,569
because i think i've been having a little bit of issue promoting it

1266
01:01:20,579 --> 01:01:25,290
because it doesn't on the surface sound particularly interesting

1267
01:01:25,300 --> 01:01:30,970
because sudoku has a sort of a feeling

1268
01:01:30,980 --> 01:01:37,290
that it's already been solved, right? so, how interesting can a collection of of sudokus be for reasoning?

1269
01:01:37,300 --> 01:01:42,089
exactly. we're not talking about normal sedokus.

1270
01:01:42,099 --> 01:01:42,089
we're talking about variant sodokas.

1271
01:01:42,099 --> 01:01:48,569
and what variant sodokas are are usually normal sedokus,

1272
01:01:48,579 --> 01:01:53,930
right? so put the numbers one to nine in the row, the column,

1273
01:01:53,940 --> 01:01:58,490
and the box, but then literally any additional rules on top of that.

1274
01:02:04,880 --> 01:02:05,130
and they're all handcrafted.

1275
01:02:05,140 --> 01:02:16,650
they all have extremely different constraints. um constraints that actually require very strong natural language understanding.

1276
01:02:19,599 --> 01:02:19,849
so for example, there's one puzzle in the data set

1277
01:02:19,859 --> 01:02:29,450
where it tells you the constraints of the puzzle in natural language and then says, " oh,

1278
01:02:29,460 --> 01:02:29,450
by the way,

1279
01:02:29,460 --> 01:02:29,450
by the way,

1280
01:02:29,460 --> 01:02:38,569
" right? so you have to be able to meta reason about the rules themselves even before you start

1281
01:02:38,579 --> 01:02:38,569
uh

1282
01:02:38,579 --> 01:02:46,010
solving the puzzle. there are other puzzles where you have um

1283
01:02:46,020 --> 01:02:46,010
a maze overlaid on the sodoku

1284
01:02:46,020 --> 01:02:50,890
and the rat has to work out a way through the maze by following

1285
01:02:50,900 --> 01:02:50,890
uh

1286
01:02:50,900 --> 01:02:55,930
a path to the cheese.

1287
01:02:55,940 --> 01:02:59,530
but then there are constraints on the path that it takes of like what numbers

1288
01:02:59,540 --> 01:02:59,530
and what they can be add up to.

1289
01:02:59,540 --> 01:02:59,530
and what they can be add up to.

1290
01:02:59,540 --> 01:03:07,930
it's difficult to really describe how varied these these

1291
01:03:07,940 --> 01:03:07,930
uh

1292
01:03:07,940 --> 01:03:07,930
uh

1293
01:03:07,940 --> 01:03:17,849
and i think they're so varied that if anyone was actually be able to beat our benchmark

1294
01:03:17,859 --> 01:03:28,730
they would necessarily have to have created an extremely powerful reasoning system. right now, the best models um

1295
01:03:28,740 --> 01:03:37,369
get around 15 %, but they're only the very

1296
01:03:37,379 --> 01:03:37,369
very simplest and the very

1297
01:03:37,379 --> 01:03:37,369
very simplest and the very

1298
01:03:37,379 --> 01:03:37,369
very simplest and the very

1299
01:03:45,359 --> 01:03:45,609
um

1300
01:03:45,619 --> 01:03:51,450
we're going to be putting out a blog post about um gpt5's performance and it is a jump

1301
01:03:51,460 --> 01:03:51,450
but it's still completely unable to solve puzzles

1302
01:03:51,460 --> 01:03:51,450
but it's still completely unable to solve puzzles

1303
01:03:51,460 --> 01:03:58,730
you know

1304
01:03:58,740 --> 01:03:58,730
humans can can solve. and what i really like about this data

1305
01:04:03,599 --> 01:04:03,849
uh

1306
01:04:03,859 --> 01:04:03,849
data set

1307
01:04:03,859 --> 01:04:03,849
data set

1308
01:04:03,859 --> 01:04:09,049
and actually was the catalyst for me creating it in the first place

1309
01:04:09,059 --> 01:04:09,049
it was that there was a

1310
01:04:09,059 --> 01:04:09,049
it was that there was a

1311
01:04:09,059 --> 01:04:09,049
it was that there was a

1312
01:04:09,059 --> 01:04:09,049
it was that there was a

1313
01:04:09,059 --> 01:04:15,530
so we have all this data

1314
01:04:15,540 --> 01:04:15,530
it's from the internet

1315
01:04:18,799 --> 01:04:19,049
um

1316
01:04:19,059 --> 01:04:19,049
but what you really want

1317
01:04:19,059 --> 01:04:19,049
but what you really want

1318
01:04:19,059 --> 01:04:19,049
but what you really want

1319
01:04:19,059 --> 01:04:25,770
you wouldn't want all of the text that humans have ever created

1320
01:04:25,780 --> 01:04:35,849
you would actually want the thought traces in their head as they were creating the text, right? if you could actually learn from

1321
01:04:35,859 --> 01:04:35,849
that, then you would get something really powerful.

1322
01:04:40,160 --> 01:04:46,650
and i thought to myself, well, that data must exist somewhere. my first thought was maybe philosophy like

1323
01:04:46,660 --> 01:04:46,650
uh

1324
01:04:46,660 --> 01:04:46,650
uh

1325
01:04:46,660 --> 01:04:46,650
uh

1326
01:04:53,359 --> 01:04:56,890
there's a type of philosophy where you just write down your thoughts without thinking like just stream of consciousness.

1327
01:04:59,359 --> 01:04:59,609
i thought maybe that could work.

1328
01:04:59,619 --> 01:04:59,609
um,

1329
01:04:59,619 --> 01:05:04,170
but then when i wasn't thinking about it and i was, you know,

1330
01:05:04,180 --> 01:05:08,569
in my leisure time, i was watching a youtube channel called cracking the cryptic. > > yes.

1331
01:05:10,079 --> 01:05:10,329
> > where these

1332
01:05:10,339 --> 01:05:10,329
uh

1333
01:05:10,339 --> 01:05:16,170
these two british gentlemen will solve these extremely difficult sudoku puzzles for you. right.

1334
01:05:16,180 --> 01:05:16,170
sometimes their

1335
01:05:16,180 --> 01:05:21,690
their videos are four hours long and they they're professionals

1336
01:05:21,700 --> 01:05:25,609
like this is their job. and what was perfect

1337
01:05:25,619 --> 01:05:25,609
i realized is they tell you in agonizing detail

1338
01:05:32,079 --> 01:05:32,329
exactly what reasoning they used to solve those particular puzzles.

1339
01:05:39,359 --> 01:05:39,609
right? so we

1340
01:05:39,619 --> 01:05:45,049
with their permission took all of their videos which represents thousands of hours of very high quality human reasoning

1341
01:05:45,059 --> 01:05:57,609
like thought traces and scraped them and made that available for imitation learning.

1342
01:05:57,619 --> 01:05:57,609
right? um

1343
01:05:57,619 --> 01:05:57,609
right? um

1344
01:05:57,619 --> 01:05:57,609
right? um

1345
01:05:57,619 --> 01:06:08,730
that i did a little bit too much of a good job of really creating a very difficult benchmark.

1346
01:06:08,740 --> 01:06:12,329
right. so, we're still trying to get that stuff working and we'll publish it that if we

1347
01:06:12,339 --> 01:06:17,690
if we have some success. um, yeah,

1348
01:06:17,700 --> 01:06:17,690
i want to

1349
01:06:17,700 --> 01:06:17,690
i want to

1350
01:06:17,700 --> 01:06:21,049
that this this reasoning benchmark really is different, right? not only do you get something

1351
01:06:21,059 --> 01:06:25,369
that's super grounded, like you know exactly if it's right

1352
01:06:25,379 --> 01:06:25,369
or wrong,

1353
01:06:25,379 --> 01:06:29,289
so you can do rl to your heart's consent, but you can't generalize very easily.

1354
01:06:36,559 --> 01:06:36,809
each puzzle is deliberately designed

1355
01:06:36,819 --> 01:06:45,450
by hand to have a new and unique twist on the rules called a breakin

1356
01:06:45,460 --> 01:06:45,450
that you have to understand.

1357
01:06:45,460 --> 01:06:59,289
and right now, despite all the progress we've made, the current ai models can't take that leap.

1358
01:06:59,299 --> 01:07:04,650
they can't find these breakins, right? they'll fall back to, okay, i'll try

1359
01:07:04,660 --> 01:07:04,650
no,

1360
01:07:04,660 --> 01:07:08,730
i'll try five, i'll try six, i'll try seven, right? the

1361
01:07:08,740 --> 01:07:13,849
the reasoning becomes really boring and nothing like what you see in the transcripts that we've

1362
01:07:13,859 --> 01:07:16,970
we've open sourced from this from this youtube channel.

1363
01:07:16,980 --> 01:07:16,970
so i just want to put the challenge out there right that this

1364
01:07:16,980 --> 01:07:21,450
this is a a really difficult benchmark

1365
01:07:21,460 --> 01:07:26,250
and i think progress on this benchmark will really mean progress in ai generally.

1366
01:07:26,260 --> 01:07:26,250
> > could you reflect a bit so

1367
01:07:26,260 --> 01:07:31,130
after watching this um

1368
01:07:31,140 --> 01:07:36,089
cracking the cryptic youtube channel? how diverse were the patterns? because um

1369
01:07:36,099 --> 01:07:36,089
chris was saying to me, oh

1370
01:07:36,099 --> 01:07:36,089
chris was saying to me, oh

1371
01:07:36,099 --> 01:07:39,369
they go on discord

1372
01:07:39,379 --> 01:07:46,250
servers, they get these creative crazy ideas and i'm i'm obsessed. maybe it

1373
01:07:46,260 --> 01:07:49,930
maybe i'm just being idealistic, but i love this idea of there being a deductive closure of knowledge, right?

1374
01:07:49,940 --> 01:07:56,010
that that there's this big tree of of reasoning

1375
01:07:56,020 --> 01:07:59,930
and we're all in possession of different parts of the tree to different depths.

1376
01:07:59,940 --> 01:08:03,130
so the smarter and the more knowledgeable you are, the deeper down the tree you go.

1377
01:08:03,140 --> 01:08:09,049
but in this idealized form, there is one tree and all knowledge kind of, you know,

1378
01:08:09,059 --> 01:08:13,609
originates or emanates from these abstract principles.

1379
01:08:13,619 --> 01:08:19,130
and we could in principle build reasoning engines that could just reason from first principles and it might be

1380
01:08:19,140 --> 01:08:19,130
um

1381
01:08:19,140 --> 01:08:19,130
um

1382
01:08:24,000 --> 01:08:24,250
so so you have to perform all of the steps.

1383
01:08:24,260 --> 01:08:27,770
and it feels like because we're not in possession of the full tree.

1384
01:08:27,779 --> 01:08:32,089
what we need to do is kind of fish around. we fish around to find lego blocks.

1385
01:08:32,099 --> 01:08:32,089
oh, that's a good lego block.

1386
01:08:32,099 --> 01:08:35,929
i can apply that to this problem.

1387
01:08:35,939 --> 01:08:35,929
and maybe

1388
01:08:35,939 --> 01:08:40,810
that's just what we need to do in ai for the time being is is is we need to just acquire as much of the tree as possible.

1389
01:08:43,920 --> 01:08:44,170
but could

1390
01:08:44,180 --> 01:08:46,810
could we just do it all the way down? > > yeah, fascinating question.

1391
01:08:46,819 --> 01:08:54,569
that tree is probably massive, right? > >

1392
01:08:54,578 --> 01:08:54,569
and as a human is solving these puzzles, they're definitely learning in real time

1393
01:09:03,198 --> 01:09:03,448
and discovering new parts of this tree.

1394
01:09:03,459 --> 01:09:08,810
and it's it's sort of a meta task, right? because it's not just reasoning, you're reasoning about the reasoning.

1395
01:09:14,479 --> 01:09:14,729
and i don't think we can.

1396
01:09:14,740 --> 01:09:14,729
we have that in ai right now.

1397
01:09:14,740 --> 01:09:20,170
because if you watch the videos, they'll say something like, " okay, this looks like a parask

1398
01:09:20,180 --> 01:09:25,370
or this is a set theoretic problem

1399
01:09:25,380 --> 01:09:25,370
or, you know,

1400
01:09:25,380 --> 01:09:25,370
or, you know,

1401
01:09:25,380 --> 01:09:31,130
i should get my path tool out and trace this this around.

1402
01:09:31,140 --> 01:09:31,130
"

1403
01:09:31,140 --> 01:09:31,130
"

1404
01:09:31,140 --> 01:09:31,130
"

1405
01:09:31,140 --> 01:09:38,009
this already massive collection of reasoning

1406
01:09:38,019 --> 01:09:38,009
lego blocks

1407
01:09:38,019 --> 01:09:44,170
as you say in their head. so they'll recognize

1408
01:09:44,180 --> 01:09:44,170
okay

1409
01:09:44,180 --> 01:09:44,170
okay

1410
01:09:49,120 --> 01:09:49,370
it's actually fascinating to watch how good they are at

1411
01:09:49,380 --> 01:09:53,290
just intuitively knowing where you know

1412
01:09:53,300 --> 01:09:53,290
someone like me haven't solved as many needs

1413
01:09:53,300 --> 01:09:58,250
to spend a lot of time looking around like

1414
01:09:58,260 --> 01:09:58,250
okay

1415
01:09:58,260 --> 01:09:58,250
okay

1416
01:09:58,260 --> 01:10:01,290
or maybe i try this one. um,

1417
01:10:01,300 --> 01:10:01,290
but even they're not perfect.

1418
01:10:01,300 --> 01:10:06,810
so, you can watch them take a certain kind of reasoning and start building up.

1419
01:10:06,820 --> 01:10:12,010
okay, maybe we should solve it like this and then go and know that doesn't disambiguate it enough

1420
01:10:12,020 --> 01:10:18,810
and then backtrack and then go down another path.

1421
01:10:18,820 --> 01:10:24,489
again, something that we do not see current ai doing when they're trying to solve

1422
01:10:24,499 --> 01:10:24,489
uh

1423
01:10:24,499 --> 01:10:24,489
uh

1424
01:10:24,499 --> 01:10:30,010
the tree is very big

1425
01:10:30,020 --> 01:10:34,250
and i guess the phoggenetic distance between many of these motifs in in the tree is just so large.

1426
01:10:34,260 --> 01:10:38,330
so it's so difficult to jump between and and and i

1427
01:10:38,340 --> 01:10:38,330
and i think that's why as a collective intelligence

1428
01:10:38,340 --> 01:10:43,130
we work so well together

1429
01:10:43,140 --> 01:10:47,690
because we actually find ways to jump to different parts of the tree, > > right?

1430
01:10:47,700 --> 01:10:51,050
and i and i think that's probably why the rl

1431
01:10:51,060 --> 01:10:51,050
the

1432
01:10:51,060 --> 01:10:56,810
the current state of the rl algorithms that we're trying to apply to this just isn't working

1433
01:10:56,820 --> 01:11:07,770
because in order to learn how to get these breakthroughs to to understand what the sort of nuance reasoning is to get these puzzles, you have to sample them.

1434
01:11:07,780 --> 01:11:07,770
and that it's

1435
01:11:11,360 --> 01:11:11,610
it's such a rare space, you know,

1436
01:11:11,620 --> 01:11:11,610
it's

1437
01:11:11,620 --> 01:11:17,850
it's such an specific kind of reasoning that's required to get to the

1438
01:11:17,860 --> 01:11:23,530
the specific breakthrough that this kind of technique doesn't work, right?

1439
01:11:23,540 --> 01:11:27,370
and there's definitely a feeling in the community like, okay, this is how you just solve things now.

1440
01:11:27,380 --> 01:11:27,370
like we have rl,

1441
01:11:31,120 --> 01:11:31,370
yes,

1442
01:11:31,380 --> 01:11:31,370
we can get these language models to do what we want.

1443
01:11:31,380 --> 01:11:35,449
it doesn't work for this for this data set. > > guys,

1444
01:11:35,459 --> 01:11:38,969
it's been an absolute honor having you on the show.

1445
01:11:38,979 --> 01:11:38,969
just before we go, are you hiring? because we've got a

1446
01:11:38,979 --> 01:11:44,010
we've got a great audience of ml engineers and scientists

1447
01:11:44,020 --> 01:11:44,010
and um

1448
01:11:44,020 --> 01:11:49,610
i think working for zakano would be the dream job.

1449
01:11:49,620 --> 01:11:49,610
> > that's very kind of you. yes,

1450
01:11:49,620 --> 01:11:54,570
we are definitely hiring

1451
01:11:54,580 --> 01:12:03,929
and as i said earlier in this interview, i honestly want to give people as much research freedom as possible.

1452
01:12:03,939 --> 01:12:10,010
i'm willing to make that bet, right? i think things that are very interesting will come out of this.

1453
01:12:10,020 --> 01:12:13,449
and i think we've already seen plenty of interesting things coming out of this.

1454
01:12:13,459 --> 01:12:17,290
so if you want to work on what you think is interesting and important, come to japan.

1455
01:12:21,120 --> 01:12:21,370
> > and japan just happens to be the most civilized culture in the world.

1456
01:12:24,800 --> 01:12:27,130
> > all right. > > it might be the opportunity of a lifetime, folks.

1457
01:12:27,140 --> 01:12:27,130
so um yeah, get in touch, guys.

1458
01:12:27,140 --> 01:12:31,610
seriously, thank you so much.

1459
01:12:31,620 --> 01:12:31,610
it's been an honor having you both on the show.

1460
01:12:33,679 --> 01:12:33,929
> > thank you very much. > > thank you so much. it's been great.

