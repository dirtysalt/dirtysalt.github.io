1
00:00:00,120 --> 00:00:02,640
- I see the danger of this concentration of power

2
00:00:02,640 --> 00:00:06,030
through proprietary AI systems

3
00:00:06,030 --> 00:00:08,940
as a much bigger danger than everything else.

4
00:00:08,940 --> 00:00:11,430
What works against this

5
00:00:11,430 --> 00:00:15,180
is people who think that for reasons of security,

6
00:00:15,180 --> 00:00:18,600
we should keep AI systems under lock and key

7
00:00:18,600 --> 00:00:19,560
because it's too dangerous

8
00:00:19,560 --> 00:00:22,080
to put it in the hands of everybody.

9
00:00:22,080 --> 00:00:25,380
That would lead to a very bad future

10
00:00:25,380 --> 00:00:27,690
in which all of our information diet

11
00:00:27,690 --> 00:00:30,900
is controlled by a small number of companies

12
00:00:30,900 --> 00:00:32,400
through proprietary systems.

13
00:00:32,400 --> 00:00:34,350
- I believe that people are fundamentally good

14
00:00:34,350 --> 00:00:38,550
and so if AI, especially open source AI

15
00:00:38,550 --> 00:00:41,730
can make them smarter,

16
00:00:41,730 --> 00:00:44,250
it just empowers the goodness in humans.

17
00:00:44,250 --> 00:00:45,762
- So I share that feeling.

18
00:00:45,762 --> 00:00:46,740
Okay?

19
00:00:46,740 --> 00:00:50,280
I think people are fundamentally good. (laughing)

20
00:00:50,280 --> 00:00:52,530
And in fact a lot of doomers are doomers

21
00:00:52,530 --> 00:00:55,530
because they don't think that people are fundamentally good.

22
00:00:57,720 --> 00:01:01,110
- The following is a conversation with Yann LeCun,

23
00:01:01,110 --> 00:01:02,910
his third time on this podcast.

24
00:01:02,910 --> 00:01:05,550
He is the chief AI scientist at Meta,

25
00:01:05,550 --> 00:01:07,410
professor at NYU,

26
00:01:07,410 --> 00:01:08,820
Turing Award winner

27
00:01:08,820 --> 00:01:10,890
and one of the seminal figures

28
00:01:10,890 --> 00:01:13,230
in the history of artificial intelligence.

29
00:01:13,230 --> 00:01:15,720
He and Meta AI

30
00:01:15,720 --> 00:01:19,590
have been big proponents of open sourcing AI development,

31
00:01:19,590 --> 00:01:21,390
and have been walking the walk

32
00:01:21,390 --> 00:01:24,000
by open sourcing many of their biggest models,

33
00:01:24,000 --> 00:01:28,230
including LLaMA 2 and eventually LLaMA 3.

34
00:01:28,230 --> 00:01:31,920
Also, Yann has been an outspoken critic

35
00:01:31,920 --> 00:01:34,500
of those people in the AI community

36
00:01:34,500 --> 00:01:36,570
who warn about the looming danger

37
00:01:36,570 --> 00:01:39,690
and existential threat of AGI.

38
00:01:39,690 --> 00:01:43,620
He believes the AGI will be created one day,

39
00:01:43,620 --> 00:01:45,510
but it will be good.

40
00:01:45,510 --> 00:01:47,730
It will not escape human control

41
00:01:47,730 --> 00:01:52,200
nor will it dominate and kill all humans.

42
00:01:52,200 --> 00:01:54,390
At this moment of rapid AI development,

43
00:01:54,390 --> 00:01:58,860
this happens to be somewhat a controversial position.

44
00:01:58,860 --> 00:02:00,090
And so it's been fun

45
00:02:00,090 --> 00:02:02,670
seeing Yann get into a lot of intense

46
00:02:02,670 --> 00:02:04,930
and fascinating discussions online

47
00:02:05,820 --> 00:02:08,669
as we do in this very conversation.

48
00:02:08,669 --> 00:02:10,500
This is the Lex Fridman podcast.

49
00:02:10,500 --> 00:02:11,340
To support it,

50
00:02:11,340 --> 00:02:13,800
please check out our sponsors in the description.

51
00:02:13,800 --> 00:02:17,043
And now, dear friends, here's Yann LeCun.

52
00:02:18,030 --> 00:02:21,330
You've had some strong statements,

53
00:02:21,330 --> 00:02:22,470
technical statements

54
00:02:22,470 --> 00:02:25,620
about the future of artificial intelligence recently,

55
00:02:25,620 --> 00:02:28,605
throughout your career actually but recently as well.

56
00:02:28,605 --> 00:02:31,980
You've said that autoregressive LLMs

57
00:02:31,980 --> 00:02:36,810
are not the way we're going to make progress

58
00:02:36,810 --> 00:02:38,790
towards superhuman intelligence.

59
00:02:38,790 --> 00:02:41,100
These are the large language models

60
00:02:41,100 --> 00:02:44,280
like GPT-4, like LLaMA 2 and 3 soon and so on.

61
00:02:44,280 --> 00:02:45,113
How do they work

62
00:02:45,113 --> 00:02:47,790
and why are they not going to take us all the way?

63
00:02:47,790 --> 00:02:49,080
- For a number of reasons.

64
00:02:49,080 --> 00:02:51,840
The first is that there is a number of characteristics

65
00:02:51,840 --> 00:02:53,553
of intelligent behavior.

66
00:02:54,450 --> 00:02:58,860
For example, the capacity to understand the world,

67
00:02:58,860 --> 00:03:00,360
understand the physical world,

68
00:03:01,290 --> 00:03:05,523
the ability to remember and retrieve things,

69
00:03:06,406 --> 00:03:08,910
persistent memory,

70
00:03:08,910 --> 00:03:12,390
the ability to reason and the ability to plan.

71
00:03:12,390 --> 00:03:14,190
Those are four essential characteristic

72
00:03:14,190 --> 00:03:18,150
of intelligent systems or entities,

73
00:03:18,150 --> 00:03:19,173
humans, animals.

74
00:03:20,610 --> 00:03:23,100
LLMs can do none of those,

75
00:03:23,100 --> 00:03:26,610
or they can only do them in a very primitive way.

76
00:03:26,610 --> 00:03:29,730
And they don't really understand the physical world,

77
00:03:29,730 --> 00:03:31,350
they don't really have persistent memory,

78
00:03:31,350 --> 00:03:32,310
they can't really reason

79
00:03:32,310 --> 00:03:34,410
and they certainly can't plan.

80
00:03:34,410 --> 00:03:38,910
And so if you expect the system to become intelligent

81
00:03:38,910 --> 00:03:43,223
just without having the possibility of doing those things,

82
00:03:43,223 --> 00:03:44,793
you're making a mistake.

83
00:03:45,870 --> 00:03:50,820
That is not to say that autoregressive LLMs are not useful,

84
00:03:50,820 --> 00:03:52,070
they're certainly useful.

85
00:03:53,910 --> 00:03:55,530
That they're not interesting,

86
00:03:55,530 --> 00:03:56,580
that we can't build

87
00:03:56,580 --> 00:04:00,120
a whole ecosystem of applications around them,

88
00:04:00,120 --> 00:04:00,953
of course we can.

89
00:04:00,953 --> 00:04:05,609
But as a path towards human level intelligence,

90
00:04:05,609 --> 00:04:08,610
they're missing essential components.

91
00:04:08,610 --> 00:04:11,190
And then there is another tidbit or fact

92
00:04:11,190 --> 00:04:14,040
that I think is very interesting;

93
00:04:14,040 --> 00:04:16,560
those LLMs are trained on enormous amounts of text.

94
00:04:16,560 --> 00:04:18,930
Basically the entirety

95
00:04:18,930 --> 00:04:21,540
of all publicly available text on the internet, right?

96
00:04:21,540 --> 00:04:26,540
That's typically on the order of 10 to the 13 tokens.

97
00:04:26,580 --> 00:04:28,230
Each token is typically two bytes.

98
00:04:28,230 --> 00:04:31,820
So that's two 10 to the 13 bytes as training data.

99
00:04:31,820 --> 00:04:34,617
It would take you or me 170,000 years

100
00:04:34,617 --> 00:04:37,964
to just read through this at eight hours a day. (laughs)

101
00:04:37,964 --> 00:04:41,310
So it seems like an enormous amount of knowledge, right?

102
00:04:41,310 --> 00:04:43,010
That those systems can accumulate.

103
00:04:46,170 --> 00:04:48,270
But then you realize it's really not that much data.

104
00:04:48,270 --> 00:04:51,712
If you talk to developmental psychologists,

105
00:04:51,712 --> 00:04:53,850
and they tell you a 4-year-old

106
00:04:53,850 --> 00:04:57,573
has been awake for 16,000 hours in his or her life,

107
00:05:00,210 --> 00:05:01,410
and the amount of information

108
00:05:01,410 --> 00:05:06,274
that has reached the visual cortex of that child

109
00:05:06,274 --> 00:05:07,630
in four years

110
00:05:08,570 --> 00:05:12,090
is about 10 to 15 bytes.

111
00:05:12,090 --> 00:05:12,960
And you can compute this

112
00:05:12,960 --> 00:05:16,050
by estimating that the optical nerve

113
00:05:16,050 --> 00:05:19,650
carry about 20 megabytes per second, roughly.

114
00:05:19,650 --> 00:05:22,170
And so 10 to the 15 bytes for a 4-year-old

115
00:05:22,170 --> 00:05:25,410
versus two times 10 to the 13 bytes

116
00:05:25,410 --> 00:05:28,623
for 170,000 years worth of reading.

117
00:05:29,460 --> 00:05:33,780
What that tells you is that through sensory input,

118
00:05:33,780 --> 00:05:35,263
we see a lot more information

119
00:05:35,263 --> 00:05:37,593
than we do through language.

120
00:05:38,490 --> 00:05:40,950
And that despite our intuition,

121
00:05:40,950 --> 00:05:43,890
most of what we learn and most of our knowledge

122
00:05:43,890 --> 00:05:46,568
is through our observation and interaction

123
00:05:46,568 --> 00:05:47,970
with the real world,

124
00:05:47,970 --> 00:05:49,470
not through language.

125
00:05:49,470 --> 00:05:51,690
Everything that we learn in the first few years of life,

126
00:05:51,690 --> 00:05:54,870
and certainly everything that animals learn

127
00:05:54,870 --> 00:05:57,060
has nothing to do with language.

128
00:05:57,060 --> 00:05:57,893
- So it would be good

129
00:05:57,893 --> 00:06:00,240
to maybe push against some of the intuition

130
00:06:00,240 --> 00:06:01,680
behind what you're saying.

131
00:06:01,680 --> 00:06:05,525
So it is true there's several orders of magnitude

132
00:06:05,525 --> 00:06:10,525
more data coming into the human mind, much faster,

133
00:06:10,620 --> 00:06:13,020
and the human mind is able to learn very quickly from that,

134
00:06:13,020 --> 00:06:14,613
filter the data very quickly.

135
00:06:15,450 --> 00:06:16,620
Somebody might argue

136
00:06:16,620 --> 00:06:19,830
your comparison between sensory data versus language.

137
00:06:19,830 --> 00:06:23,250
That language is already very compressed.

138
00:06:23,250 --> 00:06:25,320
It already contains a lot more information

139
00:06:25,320 --> 00:06:27,300
than the bytes it takes to store them,

140
00:06:27,300 --> 00:06:29,370
if you compare it to visual data.

141
00:06:29,370 --> 00:06:31,110
So there's a lot of wisdom in language.

142
00:06:31,110 --> 00:06:33,870
There's words and the way we stitch them together,

143
00:06:33,870 --> 00:06:36,270
it already contains a lot of information.

144
00:06:36,270 --> 00:06:40,650
So is it possible that language alone

145
00:06:40,650 --> 00:06:45,650
already has enough wisdom and knowledge in there

146
00:06:47,010 --> 00:06:50,910
to be able to, from that language construct a world model

147
00:06:50,910 --> 00:06:52,640
and understanding of the world,

148
00:06:52,640 --> 00:06:54,720
an understanding of the physical world

149
00:06:54,720 --> 00:06:56,700
that you're saying LLMs lack?

150
00:06:56,700 --> 00:07:00,090
- So it's a big debate among philosophers

151
00:07:00,090 --> 00:07:01,800
and also cognitive scientists,

152
00:07:01,800 --> 00:07:05,820
like whether intelligence needs to be grounded in reality.

153
00:07:05,820 --> 00:07:07,230
I'm clearly in the camp

154
00:07:07,230 --> 00:07:10,770
that yes, intelligence cannot appear

155
00:07:10,770 --> 00:07:14,310
without some grounding in some reality.

156
00:07:14,310 --> 00:07:17,040
It doesn't need to be physical reality,

157
00:07:17,040 --> 00:07:18,030
it could be simulated

158
00:07:18,030 --> 00:07:20,910
but the environment is just much richer

159
00:07:20,910 --> 00:07:22,380
than what you can express in language.

160
00:07:22,380 --> 00:07:27,380
Language is a very approximate representation or percepts

161
00:07:27,600 --> 00:07:29,273
and or mental models, right?

162
00:07:29,273 --> 00:07:32,190
I mean, there's a lot of tasks that we accomplish

163
00:07:32,190 --> 00:07:37,190
where we manipulate a mental model of the situation at hand,

164
00:07:38,310 --> 00:07:40,650
and that has nothing to do with language.

165
00:07:40,650 --> 00:07:43,530
Everything that's physical, mechanical, whatever,

166
00:07:43,530 --> 00:07:44,850
when we build something,

167
00:07:44,850 --> 00:07:46,860
when we accomplish a task,

168
00:07:46,860 --> 00:07:50,220
a moderate task of grabbing something, et cetera,

169
00:07:50,220 --> 00:07:52,260
we plan our action sequences,

170
00:07:52,260 --> 00:07:53,093
and we do this

171
00:07:53,093 --> 00:07:55,710
by essentially imagining the result

172
00:07:55,710 --> 00:08:00,710
of the outcome of sequence of actions that we might imagine.

173
00:08:01,260 --> 00:08:03,930
And that requires mental models

174
00:08:03,930 --> 00:08:06,060
that don't have much to do with language.

175
00:08:06,060 --> 00:08:07,830
And that's, I would argue,

176
00:08:07,830 --> 00:08:09,900
most of our knowledge

177
00:08:09,900 --> 00:08:13,740
is derived from that interaction with the physical world.

178
00:08:13,740 --> 00:08:15,660
So a lot of my colleagues

179
00:08:15,660 --> 00:08:19,290
who are more interested in things like computer vision

180
00:08:19,290 --> 00:08:20,460
are really on that camp

181
00:08:20,460 --> 00:08:25,080
that AI needs to be embodied, essentially.

182
00:08:25,080 --> 00:08:28,380
And then other people coming from the NLP side

183
00:08:28,380 --> 00:08:32,508
or maybe some other motivation

184
00:08:32,508 --> 00:08:34,477
don't necessarily agree with that.

185
00:08:34,477 --> 00:08:37,082
And philosophers are split as well.

186
00:08:38,400 --> 00:08:42,770
And the complexity of the world is hard to imagine.

187
00:08:44,700 --> 00:08:49,700
It's hard to represent all the complexities

188
00:08:51,000 --> 00:08:53,520
that we take completely for granted in the real world

189
00:08:53,520 --> 00:08:55,680
that we don't even imagine require intelligence, right?

190
00:08:55,680 --> 00:08:57,960
This is the old Moravec's paradox

191
00:08:57,960 --> 00:09:01,200
from the pioneer of robotics, Hans Moravec,

192
00:09:01,200 --> 00:09:03,270
who said, how is it that with computers,

193
00:09:03,270 --> 00:09:05,760
it seems to be easy to do high level complex tasks

194
00:09:05,760 --> 00:09:08,400
like playing chess and solving integrals

195
00:09:08,400 --> 00:09:09,660
and doing things like that,

196
00:09:09,660 --> 00:09:13,486
whereas the thing we take for granted that we do every day,

197
00:09:13,486 --> 00:09:16,320
like, I don't know, learning to drive a car

198
00:09:16,320 --> 00:09:18,630
or grabbing an object,

199
00:09:18,630 --> 00:09:21,780
we can't do with computers. (laughs)

200
00:09:21,780 --> 00:09:26,780
And we have LLMs that can pass the bar exam,

201
00:09:28,320 --> 00:09:29,430
so they must be smart.

202
00:09:29,430 --> 00:09:33,000
But then they can't launch a drive in 20 hours

203
00:09:33,000 --> 00:09:35,430
like any 17-year-old.

204
00:09:35,430 --> 00:09:37,780
They can't learn to clear out the dinner table

205
00:09:38,640 --> 00:09:40,170
and fill out the dishwasher

206
00:09:40,170 --> 00:09:42,736
like any 10-year-old can learn in one shot.

207
00:09:42,736 --> 00:09:44,103
Why is that?

208
00:09:44,103 --> 00:09:45,480
Like what are we missing?

209
00:09:45,480 --> 00:09:47,670
What type of learning

210
00:09:47,670 --> 00:09:52,230
or reasoning architecture or whatever are we missing

211
00:09:52,230 --> 00:09:55,980
that basically prevent us

212
00:09:55,980 --> 00:09:58,890
from having level five self-driving cars

213
00:09:58,890 --> 00:10:00,870
and domestic robots?

214
00:10:00,870 --> 00:10:05,520
- Can a large language model construct a world model

215
00:10:05,520 --> 00:10:07,710
that does know how to drive

216
00:10:07,710 --> 00:10:09,270
and does know how to fill a dishwasher,

217
00:10:09,270 --> 00:10:10,103
but just doesn't know

218
00:10:10,103 --> 00:10:12,540
how to deal with visual data at this time?

219
00:10:12,540 --> 00:10:17,190
So it can operate in a space of concepts.

220
00:10:17,190 --> 00:10:19,950
- So yeah, that's what a lot of people are working on.

221
00:10:19,950 --> 00:10:20,783
So the answer,

222
00:10:20,783 --> 00:10:22,550
the short answer is no.

223
00:10:22,550 --> 00:10:24,120
And the more complex answer is

224
00:10:24,120 --> 00:10:26,303
you can use all kind of tricks

225
00:10:26,303 --> 00:10:31,303
to get an LLM to basically digest visual representations

226
00:10:35,987 --> 00:10:40,593
of images or video or audio for that matter.

227
00:10:42,360 --> 00:10:45,360
And a classical way of doing this

228
00:10:45,360 --> 00:10:48,513
is you train a vision system in some way,

229
00:10:49,380 --> 00:10:51,270
and we have a number of ways to train vision systems,

230
00:10:51,270 --> 00:10:53,760
either supervised, unsupervised, self-supervised,

231
00:10:53,760 --> 00:10:55,160
all kinds of different ways.

232
00:10:56,280 --> 00:11:01,053
That will turn any image into a high level representation.

233
00:11:02,160 --> 00:11:03,720
Basically, a list of tokens

234
00:11:03,720 --> 00:11:05,853
that are really similar to the kind of tokens

235
00:11:05,853 --> 00:11:10,650
that a typical LLM takes as an input.

236
00:11:10,650 --> 00:11:15,150
And then you just feed that to the LLM

237
00:11:15,150 --> 00:11:17,070
in addition to the text,

238
00:11:17,070 --> 00:11:21,540
and you just expect the LLM during training

239
00:11:21,540 --> 00:11:25,076
to kind of be able to use those representations

240
00:11:25,076 --> 00:11:27,120
to help make decisions.

241
00:11:27,120 --> 00:11:29,100
I mean, there's been work along those lines

242
00:11:29,100 --> 00:11:30,363
for quite a long time.

243
00:11:31,320 --> 00:11:32,640
And now you see those systems, right?

244
00:11:32,640 --> 00:11:36,630
I mean, there are LLMs that have some vision extension.

245
00:11:36,630 --> 00:11:37,710
But they're basically hacks

246
00:11:37,710 --> 00:11:39,636
in the sense that those things

247
00:11:39,636 --> 00:11:41,675
are not like trained to handle,

248
00:11:41,675 --> 00:11:43,800
to really understand the world.

249
00:11:43,800 --> 00:11:46,068
They're not trained with video, for example.

250
00:11:46,068 --> 00:11:48,930
They don't really understand intuitive physics,

251
00:11:48,930 --> 00:11:50,280
at least not at the moment.

252
00:11:51,120 --> 00:11:52,200
- So you don't think

253
00:11:52,200 --> 00:11:54,300
there's something special to you about intuitive physics,

254
00:11:54,300 --> 00:11:55,920
about sort of common sense reasoning

255
00:11:55,920 --> 00:11:58,789
about the physical space, about physical reality?

256
00:11:58,789 --> 00:12:00,690
That to you is a giant leap

257
00:12:00,690 --> 00:12:02,790
that LLMs are just not able to do?

258
00:12:02,790 --> 00:12:03,813
- We're not gonna be able to do this

259
00:12:03,813 --> 00:12:07,800
with the type of LLMs that we are working with today.

260
00:12:07,800 --> 00:12:09,210
And there's a number of reasons for this,

261
00:12:09,210 --> 00:12:10,900
but the main reason is

262
00:12:12,450 --> 00:12:16,530
the way LLMs are trained is that you take a piece of text,

263
00:12:16,530 --> 00:12:20,190
you remove some of the words in that text, you mask them,

264
00:12:20,190 --> 00:12:22,620
you replace them by black markers,

265
00:12:22,620 --> 00:12:24,210
and you train a gigantic neural net

266
00:12:24,210 --> 00:12:26,133
to predict the words that are missing.

267
00:12:26,969 --> 00:12:30,270
And if you build this neural net in a particular way

268
00:12:30,270 --> 00:12:32,570
so that it can only look at words

269
00:12:32,570 --> 00:12:36,090
that are to the left of the one it's trying to predict,

270
00:12:36,090 --> 00:12:37,143
then what you have is a system

271
00:12:37,143 --> 00:12:38,790
that basically is trying to predict

272
00:12:38,790 --> 00:12:40,020
the next word in a text, right?

273
00:12:40,020 --> 00:12:43,410
So then you can feed it a text, a prompt,

274
00:12:43,410 --> 00:12:45,840
and you can ask it to predict the next word.

275
00:12:45,840 --> 00:12:47,580
It can never predict the next word exactly.

276
00:12:47,580 --> 00:12:49,080
And so what it's gonna do

277
00:12:49,080 --> 00:12:52,225
is produce a probability distribution

278
00:12:52,225 --> 00:12:54,990
of all the possible words in the dictionary.

279
00:12:54,990 --> 00:12:56,190
In fact, it doesn't predict words,

280
00:12:56,190 --> 00:12:58,980
it predicts tokens that are kind of subword units.

281
00:12:58,980 --> 00:13:01,439
And so it's easy to handle the uncertainty

282
00:13:01,439 --> 00:13:02,760
in the prediction there

283
00:13:02,760 --> 00:13:04,072
because there's only a finite number

284
00:13:04,072 --> 00:13:07,320
of possible words in the dictionary,

285
00:13:07,320 --> 00:13:10,314
and you can just compute a distribution over them.

286
00:13:10,314 --> 00:13:12,080
Then what the system does

287
00:13:12,080 --> 00:13:16,830
is that it picks a word from that distribution.

288
00:13:16,830 --> 00:13:18,800
Of course, there's a higher chance of picking words

289
00:13:18,800 --> 00:13:21,390
that have a higher probability within that distribution.

290
00:13:21,390 --> 00:13:22,800
So you sample from that distribution

291
00:13:22,800 --> 00:13:24,393
to actually produce a word,

292
00:13:25,260 --> 00:13:27,460
and then you shift that word into the input.

293
00:13:28,350 --> 00:13:29,910
And so that allows the system now

294
00:13:29,910 --> 00:13:32,250
to predict the second word, right?

295
00:13:32,250 --> 00:13:33,083
And once you do this,

296
00:13:33,083 --> 00:13:35,250
you shift it into the input, et cetera.

297
00:13:35,250 --> 00:13:38,100
That's called autoregressive prediction,

298
00:13:38,100 --> 00:13:39,840
which is why those LLMs

299
00:13:39,840 --> 00:13:42,140
should be called autoregressive LLMs,

300
00:13:43,320 --> 00:13:46,230
but we just call them at LLMs.

301
00:13:46,230 --> 00:13:50,580
And there is a difference between this kind of process

302
00:13:50,580 --> 00:13:53,823
and a process by which before producing a word,

303
00:13:54,660 --> 00:13:55,493
when you talk.

304
00:13:55,493 --> 00:13:56,610
When you and I talk,

305
00:13:56,610 --> 00:13:58,470
you and I are bilinguals.

306
00:13:58,470 --> 00:14:00,390
We think about what we're gonna say,

307
00:14:00,390 --> 00:14:01,980
and it's relatively independent

308
00:14:01,980 --> 00:14:04,380
of the language in which we're gonna say.

309
00:14:04,380 --> 00:14:06,511
When we talk about like, I don't know,

310
00:14:06,511 --> 00:14:09,030
let's say a mathematical concept or something.

311
00:14:09,030 --> 00:14:10,860
The kind of thinking that we're doing

312
00:14:10,860 --> 00:14:13,270
and the answer that we're planning to produce

313
00:14:14,160 --> 00:14:16,530
is not linked to whether we're gonna say it

314
00:14:16,530 --> 00:14:19,380
in French or Russian or English.

315
00:14:19,380 --> 00:14:21,630
- Chomsky just rolled his eyes, but I understand.

316
00:14:21,630 --> 00:14:24,834
So you're saying that there's a bigger abstraction

317
00:14:24,834 --> 00:14:28,530
that goes before language-

318
00:14:28,530 --> 00:14:30,240
- [Yann] Yeah. - And maps onto language.

319
00:14:30,240 --> 00:14:31,080
- Right.

320
00:14:31,080 --> 00:14:33,481
It's certainly true for a lot of thinking that we do.

321
00:14:33,481 --> 00:14:35,700
- Is that obvious that we don't?

322
00:14:35,700 --> 00:14:39,090
Like you're saying your thinking is same in French

323
00:14:39,090 --> 00:14:40,320
as it is in English?

324
00:14:40,320 --> 00:14:42,000
- Yeah, pretty much.

325
00:14:42,000 --> 00:14:43,941
- Pretty much or is this...

326
00:14:43,941 --> 00:14:45,660
Like how flexible are you,

327
00:14:45,660 --> 00:14:48,344
like if there's a probability distribution?

328
00:14:48,344 --> 00:14:49,177
(both laugh)

329
00:14:49,177 --> 00:14:50,991
- Well, it depends what kind of thinking, right?

330
00:14:50,991 --> 00:14:53,811
If it's like producing puns,

331
00:14:53,811 --> 00:14:56,220
I get much better in French than English about that (laughs)

332
00:14:56,220 --> 00:14:58,470
or much worse-

333
00:14:58,470 --> 00:15:00,082
- Is there an abstract representation of puns?

334
00:15:00,082 --> 00:15:01,651
Like is your humor an abstract...

335
00:15:01,651 --> 00:15:03,270
Like when you tweet

336
00:15:03,270 --> 00:15:06,001
and your tweets are sometimes a little bit spicy,

337
00:15:06,001 --> 00:15:09,720
is there an abstract representation in your brain of a tweet

338
00:15:09,720 --> 00:15:11,790
before it maps onto English?

339
00:15:11,790 --> 00:15:13,380
- There is an abstract representation

340
00:15:13,380 --> 00:15:18,380
of imagining the reaction of a reader to that text.

341
00:15:18,720 --> 00:15:19,950
- Oh, you start with laughter

342
00:15:19,950 --> 00:15:22,161
and then figure out how to make that happen?

343
00:15:22,161 --> 00:15:25,726
- Figure out like a reaction you wanna cause

344
00:15:25,726 --> 00:15:26,953
and then figure out how to say it

345
00:15:26,953 --> 00:15:29,100
so that it causes that reaction.

346
00:15:29,100 --> 00:15:30,780
But that's like really close to language.

347
00:15:30,780 --> 00:15:34,200
But think about like a mathematical concept

348
00:15:34,200 --> 00:15:38,006
or imagining something you want to build out of wood

349
00:15:38,006 --> 00:15:40,020
or something like this, right?

350
00:15:40,020 --> 00:15:41,040
The kind of thinking you're doing

351
00:15:41,040 --> 00:15:43,470
has absolutely nothing to do with language, really.

352
00:15:43,470 --> 00:15:44,970
Like it's not like you have necessarily

353
00:15:44,970 --> 00:15:47,880
like an internal monologue in any particular language.

354
00:15:47,880 --> 00:15:51,893
You're imagining mental models of the thing, right?

355
00:15:51,893 --> 00:15:54,450
I mean, if I ask you to like imagine

356
00:15:54,450 --> 00:15:56,460
what this water bottle will look like

357
00:15:56,460 --> 00:15:59,048
if I rotate it 90 degrees,

358
00:15:59,048 --> 00:16:01,413
that has nothing to do with language.

359
00:16:02,520 --> 00:16:04,650
And so clearly

360
00:16:04,650 --> 00:16:08,612
there is a more abstract level of representation

361
00:16:08,612 --> 00:16:11,172
in which we do most of our thinking

362
00:16:11,172 --> 00:16:13,890
and we plan what we're gonna say

363
00:16:13,890 --> 00:16:18,870
if the output is uttered words

364
00:16:19,830 --> 00:16:24,830
as opposed to an output being muscle actions, right?

365
00:16:26,610 --> 00:16:29,299
We plan our answer before we produce it.

366
00:16:29,299 --> 00:16:30,390
And LLMs don't do that,

367
00:16:30,390 --> 00:16:32,973
they just produce one word after the other,

368
00:16:33,840 --> 00:16:35,184
instinctively if you want.

369
00:16:35,184 --> 00:16:40,184
It's a bit like the subconscious actions where you don't...

370
00:16:41,940 --> 00:16:42,840
Like you're distracted.

371
00:16:42,840 --> 00:16:43,673
You're doing something,

372
00:16:43,673 --> 00:16:45,000
you're completely concentrated

373
00:16:45,000 --> 00:16:47,850
and someone comes to you and asks you a question.

374
00:16:47,850 --> 00:16:49,680
And you kind of answer the question.

375
00:16:49,680 --> 00:16:51,450
You don't have time to think about the answer,

376
00:16:51,450 --> 00:16:52,440
but the answer is easy

377
00:16:52,440 --> 00:16:54,030
so you don't need to pay attention

378
00:16:54,030 --> 00:16:55,980
and you sort of respond automatically.

379
00:16:55,980 --> 00:16:58,560
That's kind of what an LLM does, right?

380
00:16:58,560 --> 00:17:01,170
It doesn't think about its answer, really.

381
00:17:01,170 --> 00:17:04,500
It retrieves it because it's accumulated a lot of knowledge,

382
00:17:04,500 --> 00:17:06,119
so it can retrieve some things,

383
00:17:06,119 --> 00:17:10,950
but it's going to just spit out one token after the other

384
00:17:10,950 --> 00:17:13,020
without planning the answer.

385
00:17:13,020 --> 00:17:17,250
- But you're making it sound just one token after the other,

386
00:17:17,250 --> 00:17:22,250
one token at a time generation is bound to be simplistic.

387
00:17:25,109 --> 00:17:28,200
But if the world model is sufficiently sophisticated,

388
00:17:28,200 --> 00:17:30,213
that one token at a time,

389
00:17:31,251 --> 00:17:35,430
the most likely thing it generates as a sequence of tokens

390
00:17:35,430 --> 00:17:39,150
is going to be a deeply profound thing.

391
00:17:39,150 --> 00:17:39,983
- Okay.

392
00:17:39,983 --> 00:17:42,780
But then that assumes that those systems

393
00:17:42,780 --> 00:17:44,880
actually possess an internal world model.

394
00:17:44,880 --> 00:17:46,500
- So it really goes to the...

395
00:17:46,500 --> 00:17:48,780
I think the fundamental question is

396
00:17:48,780 --> 00:17:53,780
can you build a really complete world model?

397
00:17:54,150 --> 00:17:54,983
Not complete,

398
00:17:54,983 --> 00:17:58,590
but one that has a deep understanding of the world.

399
00:17:58,590 --> 00:17:59,423
- Yeah.

400
00:17:59,423 --> 00:18:03,600
So can you build this first of all by prediction?

401
00:18:03,600 --> 00:18:04,433
- [Lex] Right.

402
00:18:04,433 --> 00:18:06,083
- And the answer is probably yes.

403
00:18:06,083 --> 00:18:10,710
Can you build it by predicting words?

404
00:18:10,710 --> 00:18:14,190
And the answer is most probably no,

405
00:18:14,190 --> 00:18:17,490
because language is very poor in terms of...

406
00:18:17,490 --> 00:18:19,320
Or weak or low bandwidth if you want,

407
00:18:19,320 --> 00:18:21,390
there's just not enough information there.

408
00:18:21,390 --> 00:18:26,200
So building world models means observing the world

409
00:18:27,150 --> 00:18:32,150
and understanding why the world is evolving the way it is.

410
00:18:33,600 --> 00:18:38,600
And then the extra component of a world model

411
00:18:38,670 --> 00:18:41,370
is something that can predict

412
00:18:41,370 --> 00:18:42,750
how the world is going to evolve

413
00:18:42,750 --> 00:18:45,570
as a consequence of an action you might take, right?

414
00:18:45,570 --> 00:18:47,070
So one model really is,

415
00:18:47,070 --> 00:18:49,200
here is my idea of the state of the world at time T,

416
00:18:49,200 --> 00:18:51,060
here is an action I might take.

417
00:18:51,060 --> 00:18:53,970
What is the predicted state of the world

418
00:18:53,970 --> 00:18:55,710
at time T plus one?

419
00:18:55,710 --> 00:18:57,397
Now, that state of the world

420
00:18:57,397 --> 00:19:01,170
does not need to represent everything about the world,

421
00:19:01,170 --> 00:19:02,370
it just needs to represent

422
00:19:02,370 --> 00:19:06,120
enough that's relevant for this planning of the action,

423
00:19:06,120 --> 00:19:08,460
but not necessarily all the details.

424
00:19:08,460 --> 00:19:09,701
Now, here is the problem.

425
00:19:09,701 --> 00:19:11,782
You're not going to be able to do this

426
00:19:11,782 --> 00:19:14,880
with generative models.

427
00:19:14,880 --> 00:19:16,860
So a generative model that's trained on video,

428
00:19:16,860 --> 00:19:18,570
and we've tried to do this for 10 years.

429
00:19:18,570 --> 00:19:20,370
You take a video,

430
00:19:20,370 --> 00:19:22,440
show a system a piece of video

431
00:19:22,440 --> 00:19:25,770
and then ask you to predict the reminder of the video.

432
00:19:25,770 --> 00:19:27,870
Basically predict what's gonna happen.

433
00:19:27,870 --> 00:19:29,150
- One frame at a time.

434
00:19:29,150 --> 00:19:33,330
Do the same thing as sort of the autoregressive LLMs do,

435
00:19:33,330 --> 00:19:34,230
but for video.

436
00:19:34,230 --> 00:19:35,063
- Right.

437
00:19:35,063 --> 00:19:37,489
Either one frame at a time or a group of frames at a time.

438
00:19:37,489 --> 00:19:42,489
But yeah, a large video model, if you want. (laughing)

439
00:19:43,650 --> 00:19:45,480
The idea of doing this

440
00:19:45,480 --> 00:19:46,890
has been floating around for a long time.

441
00:19:46,890 --> 00:19:48,603
And at FAIR,

442
00:19:49,658 --> 00:19:51,060
some colleagues and I

443
00:19:51,060 --> 00:19:53,393
have been trying to do this for about 10 years.

444
00:19:54,870 --> 00:19:58,500
And you can't really do the same trick as with LLMs,

445
00:19:58,500 --> 00:20:02,070
because LLMs, as I said,

446
00:20:02,070 --> 00:20:05,160
you can't predict exactly which word is gonna follow

447
00:20:05,160 --> 00:20:06,870
a sequence of words,

448
00:20:06,870 --> 00:20:09,540
but you can predict the distribution of the words.

449
00:20:09,540 --> 00:20:11,580
Now, if you go to video,

450
00:20:11,580 --> 00:20:12,413
what you would have to do

451
00:20:12,413 --> 00:20:13,560
is predict the distribution

452
00:20:13,560 --> 00:20:16,500
of all possible frames in a video.

453
00:20:16,500 --> 00:20:19,309
And we don't really know how to do that properly.

454
00:20:19,309 --> 00:20:21,956
We do not know how to represent distributions

455
00:20:21,956 --> 00:20:24,660
over high dimensional continuous spaces

456
00:20:24,660 --> 00:20:25,893
in ways that are useful.

457
00:20:27,300 --> 00:20:31,350
And there lies the main issue.

458
00:20:31,350 --> 00:20:33,060
And the reason we can do this

459
00:20:33,060 --> 00:20:34,170
is because the world

460
00:20:34,170 --> 00:20:38,130
is incredibly more complicated and richer

461
00:20:38,130 --> 00:20:40,530
in terms of information than text.

462
00:20:40,530 --> 00:20:41,613
Text is discreet.

463
00:20:42,960 --> 00:20:45,000
Video is high dimensional and continuous.

464
00:20:45,000 --> 00:20:47,250
A lot of details in this.

465
00:20:47,250 --> 00:20:49,833
So if I take a video of this room,

466
00:20:50,785 --> 00:20:54,543
and the video is a camera panning around,

467
00:20:56,850 --> 00:20:57,960
there is no way I can predict

468
00:20:57,960 --> 00:21:00,060
everything that's gonna be in the room as I pan around,

469
00:21:00,060 --> 00:21:02,190
the system cannot predict what's gonna be in the room

470
00:21:02,190 --> 00:21:03,483
as the camera is panning.

471
00:21:04,410 --> 00:21:06,030
Maybe it's gonna predict,

472
00:21:06,030 --> 00:21:08,640
this is a room where there's a light and there is a wall

473
00:21:08,640 --> 00:21:09,473
and things like that.

474
00:21:09,473 --> 00:21:11,670
It can't predict what the painting of the wall looks like

475
00:21:11,670 --> 00:21:14,160
or what the texture of the couch looks like.

476
00:21:14,160 --> 00:21:16,140
Certainly not the texture of the carpet.

477
00:21:16,140 --> 00:21:19,200
So there's no way it can predict all those details.

478
00:21:19,200 --> 00:21:22,290
So the way to handle this

479
00:21:22,290 --> 00:21:24,900
is one way to possibly to handle this,

480
00:21:24,900 --> 00:21:26,430
which we've been working for a long time,

481
00:21:26,430 --> 00:21:29,790
is to have a model that has what's called a latent variable.

482
00:21:29,790 --> 00:21:33,060
And the latent variable is fed to a neural net,

483
00:21:33,060 --> 00:21:34,410
and it's supposed to represent

484
00:21:34,410 --> 00:21:35,343
all the information about the world

485
00:21:35,343 --> 00:21:37,950
that you don't perceive yet.

486
00:21:37,950 --> 00:21:42,327
And that you need to augment the system

487
00:21:43,396 --> 00:21:47,220
for the prediction to do a good job at predicting pixels,

488
00:21:47,220 --> 00:21:52,220
including the fine texture of the carpet and the couch

489
00:21:53,520 --> 00:21:54,993
and the painting on the wall.

490
00:21:56,595 --> 00:22:00,210
That has been a complete failure, essentially.

491
00:22:00,210 --> 00:22:01,380
And we've tried lots of things.

492
00:22:01,380 --> 00:22:03,870
We tried just straight neural nets,

493
00:22:03,870 --> 00:22:04,740
we tried GANs,

494
00:22:04,740 --> 00:22:08,345
we tried VAEs,

495
00:22:08,345 --> 00:22:10,304
all kinds of regularized auto encoders,

496
00:22:10,304 --> 00:22:13,980
we tried many things.

497
00:22:13,980 --> 00:22:15,750
We also tried those kind of methods

498
00:22:15,750 --> 00:22:20,750
to learn good representations of images or video

499
00:22:20,767 --> 00:22:24,390
that could then be used as input

500
00:22:24,390 --> 00:22:26,613
for example, an image classification system.

501
00:22:27,513 --> 00:22:29,610
And that also has basically failed.

502
00:22:29,610 --> 00:22:33,600
Like all the systems that attempt to predict missing parts

503
00:22:33,600 --> 00:22:34,870
of an image or a video

504
00:22:37,615 --> 00:22:40,260
from a corrupted version of it, basically.

505
00:22:40,260 --> 00:22:41,670
So, right, take an image or a video,

506
00:22:41,670 --> 00:22:44,100
corrupt it or transform it in some way,

507
00:22:44,100 --> 00:22:47,520
and then try to reconstruct the complete video or image

508
00:22:47,520 --> 00:22:48,933
from the corrupted version.

509
00:22:50,730 --> 00:22:52,170
And then hope that internally,

510
00:22:52,170 --> 00:22:54,900
the system will develop good representations of images

511
00:22:54,900 --> 00:22:57,060
that you can use for object recognition,

512
00:22:57,060 --> 00:22:58,510
segmentation, whatever it is.

513
00:22:59,610 --> 00:23:01,860
That has been essentially a complete failure.

514
00:23:02,700 --> 00:23:04,470
And it works really well for text.

515
00:23:04,470 --> 00:23:07,140
That's the principle that is used for LLMs, right?

516
00:23:07,140 --> 00:23:08,808
- So where's the failure exactly?

517
00:23:08,808 --> 00:23:11,320
Is it that it is very difficult to form

518
00:23:12,270 --> 00:23:14,243
a good representation of an image,

519
00:23:14,243 --> 00:23:16,020
like a good embedding

520
00:23:16,020 --> 00:23:19,350
of all the important information in the image?

521
00:23:19,350 --> 00:23:21,180
Is it in terms of the consistency

522
00:23:21,180 --> 00:23:24,030
of image to image to image to image that forms the video?

523
00:23:26,049 --> 00:23:28,699
If we do a highlight reel of all the ways you failed.

524
00:23:29,550 --> 00:23:30,630
What's that look like?

525
00:23:30,630 --> 00:23:31,463
- Okay.

526
00:23:31,463 --> 00:23:35,310
So the reason this doesn't work is...

527
00:23:35,310 --> 00:23:37,230
First of all, I have to tell you exactly what doesn't work

528
00:23:37,230 --> 00:23:40,020
because there is something else that does work.

529
00:23:40,020 --> 00:23:41,670
So the thing that does not work

530
00:23:41,670 --> 00:23:46,670
is training the system to learn representations of images

531
00:23:47,760 --> 00:23:51,643
by training it to reconstruct a good image

532
00:23:51,643 --> 00:23:53,610
from a corrupted version of it.

533
00:23:53,610 --> 00:23:54,443
Okay.

534
00:23:54,443 --> 00:23:55,650
That's what doesn't work.

535
00:23:55,650 --> 00:23:58,601
And we have a whole slew of techniques for this

536
00:23:58,601 --> 00:24:02,490
that are variant of then using auto encoders.

537
00:24:02,490 --> 00:24:03,540
Something called MAE,

538
00:24:03,540 --> 00:24:05,610
developed by some of my colleagues at FAIR,

539
00:24:05,610 --> 00:24:06,990
masked autoencoder.

540
00:24:06,990 --> 00:24:11,818
So it's basically like the LLMs or things like this

541
00:24:11,818 --> 00:24:13,665
where you train the system by corrupting text,

542
00:24:13,665 --> 00:24:15,270
except you corrupt images.

543
00:24:15,270 --> 00:24:16,407
You remove patches from it

544
00:24:16,407 --> 00:24:19,470
and you train a gigantic neural network to reconstruct.

545
00:24:19,470 --> 00:24:20,940
The features you get are not good.

546
00:24:20,940 --> 00:24:22,260
And you know they're not good

547
00:24:22,260 --> 00:24:25,500
because if you now train the same architecture,

548
00:24:25,500 --> 00:24:30,060
but you train it to supervise with label data,

549
00:24:30,060 --> 00:24:34,020
with textual descriptions of images, et cetera,

550
00:24:34,020 --> 00:24:35,760
you do get good representations.

551
00:24:35,760 --> 00:24:39,690
And the performance on recognition tasks is much better

552
00:24:39,690 --> 00:24:41,970
than if you do this self supervised free training.

553
00:24:41,970 --> 00:24:44,550
- So the architecture is good.

554
00:24:44,550 --> 00:24:45,383
- The architecture is good.

555
00:24:45,383 --> 00:24:47,670
The architecture of the encoder is good.

556
00:24:47,670 --> 00:24:48,503
Okay?

557
00:24:48,503 --> 00:24:51,390
But the fact that you train the system to reconstruct images

558
00:24:51,390 --> 00:24:53,910
does not lead it to produce

559
00:24:53,910 --> 00:24:56,280
long good generic features of images.

560
00:24:56,280 --> 00:24:58,380
- [Lex] When you train it in a self supervised way.

561
00:24:58,380 --> 00:25:00,360
- Self supervised by reconstruction.

562
00:25:00,360 --> 00:25:01,380
- [Lex] Yeah, by reconstruction.

563
00:25:01,380 --> 00:25:02,920
- Okay, so what's the alternative?

564
00:25:02,920 --> 00:25:04,380
(both laugh)

565
00:25:04,380 --> 00:25:07,500
The alternative is joint embedding.

566
00:25:07,500 --> 00:25:08,880
- What is joint embedding?

567
00:25:08,880 --> 00:25:11,250
What are these architectures that you're so excited about?

568
00:25:11,250 --> 00:25:13,410
- Okay, so now instead of training a system

569
00:25:13,410 --> 00:25:14,640
to encode the image

570
00:25:14,640 --> 00:25:17,762
and then training it to reconstruct the full image

571
00:25:17,762 --> 00:25:20,070
from a corrupted version,

572
00:25:20,070 --> 00:25:21,540
you take the full image,

573
00:25:21,540 --> 00:25:25,410
you take the corrupted or transformed version,

574
00:25:25,410 --> 00:25:27,533
you run them both through encoders,

575
00:25:27,533 --> 00:25:30,813
which in general are identical but not necessarily.

576
00:25:31,800 --> 00:25:36,800
And then you train a predictor on top of those encoders

577
00:25:37,474 --> 00:25:42,474
to predict the representation of the full input

578
00:25:42,480 --> 00:25:45,543
from the representation of the corrupted one.

579
00:25:46,433 --> 00:25:47,820
Okay?

580
00:25:47,820 --> 00:25:48,690
So joint embedding,

581
00:25:48,690 --> 00:25:51,150
because you're taking the full input

582
00:25:51,150 --> 00:25:54,180
and the corrupted version or transformed version,

583
00:25:54,180 --> 00:25:55,320
run them both through encoders

584
00:25:55,320 --> 00:25:57,210
so you get a joint embedding.

585
00:25:57,210 --> 00:25:59,160
And then you're saying

586
00:25:59,160 --> 00:26:02,040
can I predict the representation of the full one

587
00:26:02,040 --> 00:26:04,770
from the representation of the corrupted one?

588
00:26:04,770 --> 00:26:05,603
Okay?

589
00:26:06,690 --> 00:26:07,640
And I call this a JEPA,

590
00:26:07,640 --> 00:26:09,900
so that means joint embedding predictive architecture

591
00:26:09,900 --> 00:26:11,280
because there's joint embedding

592
00:26:11,280 --> 00:26:12,180
and there is this predictor

593
00:26:12,180 --> 00:26:13,410
that predicts the representation

594
00:26:13,410 --> 00:26:15,363
of the good guy from the bad guy.

595
00:26:16,520 --> 00:26:18,330
And the big question is

596
00:26:18,330 --> 00:26:20,730
how do you train something like this?

597
00:26:20,730 --> 00:26:23,464
And until five years ago or six years ago,

598
00:26:23,464 --> 00:26:26,340
we didn't have particularly good answers

599
00:26:26,340 --> 00:26:27,720
for how you train those things,

600
00:26:27,720 --> 00:26:31,923
except for one called contrastive learning.

601
00:26:34,890 --> 00:26:36,570
And the idea of contrastive learning

602
00:26:36,570 --> 00:26:38,790
is you take a pair of images

603
00:26:38,790 --> 00:26:42,480
that are, again, an image and a corrupted version

604
00:26:42,480 --> 00:26:44,310
or degraded version somehow

605
00:26:44,310 --> 00:26:47,220
or transformed version of the original one.

606
00:26:47,220 --> 00:26:49,950
And you train the predicted representation

607
00:26:49,950 --> 00:26:51,229
to be the same as that.

608
00:26:51,229 --> 00:26:52,770
If you only do this,

609
00:26:52,770 --> 00:26:53,940
this system collapses.

610
00:26:53,940 --> 00:26:55,710
It basically completely ignores the input

611
00:26:55,710 --> 00:26:58,060
and produces representations that are constant.

612
00:27:00,030 --> 00:27:02,790
So the contrastive methods avoid this.

613
00:27:02,790 --> 00:27:05,110
And those things have been around since the early '90s,

614
00:27:05,110 --> 00:27:07,173
I had a paper on this in 1993,

615
00:27:08,760 --> 00:27:13,360
is you also show pairs of images that you know are different

616
00:27:14,460 --> 00:27:17,550
and then you push away the representations from each other.

617
00:27:17,550 --> 00:27:20,430
So you say not only do representations of things

618
00:27:20,430 --> 00:27:22,020
that we know are the same,

619
00:27:22,020 --> 00:27:23,940
should be the same or should be similar,

620
00:27:23,940 --> 00:27:25,770
but representation of things that we know are different

621
00:27:25,770 --> 00:27:26,770
should be different.

622
00:27:27,780 --> 00:27:29,010
And that prevents the collapse,

623
00:27:29,010 --> 00:27:30,180
but it has some limitation.

624
00:27:30,180 --> 00:27:31,890
And there's a whole bunch of techniques

625
00:27:31,890 --> 00:27:35,700
that have appeared over the last six, seven years

626
00:27:35,700 --> 00:27:38,637
that can revive this type of method.

627
00:27:38,637 --> 00:27:40,350
Some of them from FAIR,

628
00:27:40,350 --> 00:27:44,002
some of them from Google and other places.

629
00:27:44,002 --> 00:27:47,310
But there are limitations to those contrastive methods.

630
00:27:47,310 --> 00:27:51,930
What has changed in the last three, four years

631
00:27:51,930 --> 00:27:54,960
is now we have methods that are non-contrastive.

632
00:27:54,960 --> 00:27:59,010
So they don't require those negative contrastive samples

633
00:27:59,010 --> 00:28:01,650
of images that we know are different.

634
00:28:01,650 --> 00:28:04,350
You train them only with images

635
00:28:04,350 --> 00:28:06,420
that are different versions

636
00:28:06,420 --> 00:28:08,270
or different views of the same thing.

637
00:28:09,150 --> 00:28:10,770
And you rely on some other tweaks

638
00:28:10,770 --> 00:28:12,690
to prevent the system from collapsing.

639
00:28:12,690 --> 00:28:16,020
And we have half a dozen different methods for this now.

640
00:28:16,020 --> 00:28:17,940
- So what is the fundamental difference

641
00:28:17,940 --> 00:28:22,380
between joint embedding architectures and LLMs?

642
00:28:22,380 --> 00:28:26,861
So can JEPA take us to AGI?

643
00:28:26,861 --> 00:28:31,860
Whether we should say that you don't like the term AGI

644
00:28:31,860 --> 00:28:33,090
and we'll probably argue,

645
00:28:33,090 --> 00:28:34,920
I think every single time I've talked to you

646
00:28:34,920 --> 00:28:36,870
we've argued about the G in AGI.

647
00:28:36,870 --> 00:28:38,147
- [Yann] Yes.

648
00:28:38,147 --> 00:28:40,260
- I get it, I get it, I get it. (laughing)

649
00:28:40,260 --> 00:28:42,360
Well we'll probably continue to argue about it.

650
00:28:42,360 --> 00:28:43,193
It's great.

651
00:28:45,859 --> 00:28:48,111
Because you're like French,

652
00:28:48,111 --> 00:28:51,840
and ami is I guess friend in French-

653
00:28:51,840 --> 00:28:52,673
- [Yann] Yes.

654
00:28:52,673 --> 00:28:55,890
- And AMI stands for advanced machine intelligence-

655
00:28:55,890 --> 00:28:56,723
- [Yann] Right.

656
00:28:57,600 --> 00:29:00,540
- But either way, can JEPA take us to that,

657
00:29:00,540 --> 00:29:02,640
towards that advanced machine intelligence?

658
00:29:02,640 --> 00:29:04,680
- Well, so it's a first step.

659
00:29:04,680 --> 00:29:05,513
Okay?

660
00:29:05,513 --> 00:29:07,320
So first of all, what's the difference

661
00:29:07,320 --> 00:29:10,835
with generative architectures like LLMs?

662
00:29:10,835 --> 00:29:15,835
So LLMs or vision systems that are trained by reconstruction

663
00:29:17,670 --> 00:29:20,100
generate the inputs, right?

664
00:29:20,100 --> 00:29:22,950
They generate the original input

665
00:29:22,950 --> 00:29:27,022
that is non-corrupted, non-transformed, right?

666
00:29:27,022 --> 00:29:28,922
So you have to predict all the pixels.

667
00:29:29,970 --> 00:29:33,450
And there is a huge amount of resources spent in the system

668
00:29:33,450 --> 00:29:36,809
to actually predict all those pixels, all the details.

669
00:29:36,809 --> 00:29:40,530
In a JEPA, you're not trying to predict all the pixels,

670
00:29:40,530 --> 00:29:42,330
you're only trying to predict

671
00:29:42,330 --> 00:29:47,070
an abstract representation of the inputs, right?

672
00:29:47,070 --> 00:29:49,500
And that's much easier in many ways.

673
00:29:49,500 --> 00:29:50,730
So what the JEPA system

674
00:29:50,730 --> 00:29:52,200
when it's being trained is trying to do,

675
00:29:52,200 --> 00:29:56,160
is extract as much information as possible from the input,

676
00:29:56,160 --> 00:29:58,200
but yet only extract information

677
00:29:58,200 --> 00:30:00,543
that is relatively easily predictable.

678
00:30:01,740 --> 00:30:02,573
Okay.

679
00:30:02,573 --> 00:30:03,406
So there's a lot of things in the world

680
00:30:03,406 --> 00:30:04,560
that we cannot predict.

681
00:30:04,560 --> 00:30:07,274
Like for example, if you have a self driving car

682
00:30:07,274 --> 00:30:08,874
driving down the street or road.

683
00:30:09,780 --> 00:30:13,410
There may be trees around the road.

684
00:30:13,410 --> 00:30:14,730
And it could be a windy day,

685
00:30:14,730 --> 00:30:17,400
so the leaves on the tree are kind of moving

686
00:30:17,400 --> 00:30:19,650
in kind of semi chaotic random ways

687
00:30:19,650 --> 00:30:22,050
that you can't predict and you don't care,

688
00:30:22,050 --> 00:30:23,700
you don't want to predict.

689
00:30:23,700 --> 00:30:25,350
So what you want is your encoder

690
00:30:25,350 --> 00:30:27,330
to basically eliminate all those details.

691
00:30:27,330 --> 00:30:28,800
It'll tell you there's moving leaves,

692
00:30:28,800 --> 00:30:30,030
but it's not gonna keep the details

693
00:30:30,030 --> 00:30:32,048
of exactly what's going on.

694
00:30:32,048 --> 00:30:35,970
And so when you do the prediction in representation space,

695
00:30:35,970 --> 00:30:37,290
you're not going to have to predict

696
00:30:37,290 --> 00:30:38,940
every single pixel of every leaf.

697
00:30:39,900 --> 00:30:43,590
And that not only is a lot simpler,

698
00:30:43,590 --> 00:30:45,750
but also it allows the system

699
00:30:45,750 --> 00:30:49,338
to essentially learn an abstract representation of the world

700
00:30:49,338 --> 00:30:54,338
where what can be modeled and predicted is preserved

701
00:30:54,720 --> 00:30:57,510
and the rest is viewed as noise

702
00:30:57,510 --> 00:30:59,160
and eliminated by the encoder.

703
00:30:59,160 --> 00:31:00,890
So it kind of lifts the level of abstraction

704
00:31:00,890 --> 00:31:02,340
of the representation.

705
00:31:02,340 --> 00:31:03,173
If you think about this,

706
00:31:03,173 --> 00:31:05,490
this is something we do absolutely all the time.

707
00:31:05,490 --> 00:31:07,020
Whenever we describe a phenomenon,

708
00:31:07,020 --> 00:31:10,110
we describe it at a particular level of abstraction.

709
00:31:10,110 --> 00:31:13,470
And we don't always describe every natural phenomenon

710
00:31:13,470 --> 00:31:15,300
in terms of quantum field theory, right?

711
00:31:15,300 --> 00:31:17,490
That would be impossible, right?

712
00:31:17,490 --> 00:31:19,723
So we have multiple levels of abstraction

713
00:31:19,723 --> 00:31:22,170
to describe what happens in the world.

714
00:31:22,170 --> 00:31:24,120
Starting from quantum field theory

715
00:31:24,120 --> 00:31:27,840
to like atomic theory and molecules in chemistry,

716
00:31:27,840 --> 00:31:29,169
materials,

717
00:31:29,169 --> 00:31:33,930
all the way up to kind of concrete objects in the real world

718
00:31:33,930 --> 00:31:34,763
and things like that.

719
00:31:34,763 --> 00:31:39,620
So we can't just only model everything at the lowest level.

720
00:31:40,470 --> 00:31:44,580
And that's what the idea of JEPA is really about.

721
00:31:44,580 --> 00:31:49,384
Learn abstract representation in a self supervised manner.

722
00:31:49,384 --> 00:31:52,110
And you can do it hierarchically as well.

723
00:31:52,110 --> 00:31:54,510
So that I think is an essential component

724
00:31:54,510 --> 00:31:56,310
of an intelligent system.

725
00:31:56,310 --> 00:31:58,590
And in language, we can get away without doing this

726
00:31:58,590 --> 00:32:02,610
because language is already to some level abstract

727
00:32:02,610 --> 00:32:05,490
and already has eliminated a lot of information

728
00:32:05,490 --> 00:32:07,110
that is not predictable.

729
00:32:07,110 --> 00:32:11,040
And so we can get away without doing the joint embedding,

730
00:32:11,040 --> 00:32:13,354
without lifting the abstraction level

731
00:32:13,354 --> 00:32:15,453
and by directly predicting words.

732
00:32:16,380 --> 00:32:17,730
- So joint embedding.

733
00:32:17,730 --> 00:32:20,010
It's still generative,

734
00:32:20,010 --> 00:32:23,340
but it's generative in this abstract representation space.

735
00:32:23,340 --> 00:32:24,173
- [Yann] Yeah.

736
00:32:24,173 --> 00:32:25,980
- And you're saying language,

737
00:32:25,980 --> 00:32:27,360
we were lazy with language

738
00:32:27,360 --> 00:32:30,420
'cause we already got the abstract representation for free

739
00:32:30,420 --> 00:32:32,040
and now we have to zoom out,

740
00:32:32,040 --> 00:32:34,620
actually think about generally intelligent systems,

741
00:32:34,620 --> 00:32:37,828
we have to deal with the full mess

742
00:32:37,828 --> 00:32:40,170
of physical of reality, of reality.

743
00:32:40,170 --> 00:32:42,930
And you do have to do this step

744
00:32:42,930 --> 00:32:47,730
of jumping from the full, rich, detailed reality

745
00:32:50,821 --> 00:32:54,870
to an abstract representation of that reality

746
00:32:54,870 --> 00:32:56,400
based on what you can then reason

747
00:32:56,400 --> 00:32:57,360
and all that kind of stuff.

748
00:32:57,360 --> 00:32:58,528
- Right.

749
00:32:58,528 --> 00:33:00,840
And the thing is those self supervised algorithms

750
00:33:00,840 --> 00:33:02,343
that learn by prediction,

751
00:33:03,210 --> 00:33:04,773
even in representation space,

752
00:33:06,210 --> 00:33:09,270
they learn more concept

753
00:33:09,270 --> 00:33:12,000
if the input data you feed them is more redundant.

754
00:33:12,000 --> 00:33:14,070
The more redundancy there is in the data,

755
00:33:14,070 --> 00:33:15,540
the more they're able to capture

756
00:33:15,540 --> 00:33:17,820
some internal structure of it.

757
00:33:17,820 --> 00:33:18,653
And so there,

758
00:33:18,653 --> 00:33:21,160
there is way more redundancy in the structure

759
00:33:22,291 --> 00:33:26,130
in perceptual inputs, sensory input like vision,

760
00:33:26,130 --> 00:33:28,500
than there is in text,

761
00:33:28,500 --> 00:33:30,000
which is not nearly as redundant.

762
00:33:30,000 --> 00:33:32,520
This is back to the question you were asking

763
00:33:32,520 --> 00:33:33,360
a few minutes ago.

764
00:33:33,360 --> 00:33:35,520
Language might represent more information really

765
00:33:35,520 --> 00:33:36,960
because it's already compressed,

766
00:33:36,960 --> 00:33:38,458
you're right about that.

767
00:33:38,458 --> 00:33:40,200
But that means it's also less redundant.

768
00:33:40,200 --> 00:33:43,680
And so self supervised only will not work as well.

769
00:33:43,680 --> 00:33:45,198
- Is it possible to join

770
00:33:45,198 --> 00:33:49,981
the self supervised training on visual data

771
00:33:49,981 --> 00:33:53,880
and self supervised training on language data?

772
00:33:53,880 --> 00:33:56,520
There is a huge amount of knowledge

773
00:33:56,520 --> 00:34:00,210
even though you talk down about those 10 to the 13 tokens.

774
00:34:00,210 --> 00:34:01,890
Those 10 to the 13 tokens

775
00:34:01,890 --> 00:34:03,393
represent the entirety,

776
00:34:04,500 --> 00:34:08,253
a large fraction of what us humans have figured out.

777
00:34:09,210 --> 00:34:11,370
Both the shit talk on Reddit

778
00:34:11,370 --> 00:34:14,130
and the contents of all the books and the articles

779
00:34:14,130 --> 00:34:18,534
and the full spectrum of human intellectual creation.

780
00:34:18,534 --> 00:34:22,230
So is it possible to join those two together?

781
00:34:22,230 --> 00:34:23,730
- Well, eventually, yes,

782
00:34:23,730 --> 00:34:27,840
but I think if we do this too early,

783
00:34:27,840 --> 00:34:30,330
we run the risk of being tempted to cheat.

784
00:34:30,330 --> 00:34:32,159
And in fact, that's what people are doing at the moment

785
00:34:32,159 --> 00:34:33,540
with vision language model.

786
00:34:33,540 --> 00:34:35,190
We're basically cheating.

787
00:34:35,190 --> 00:34:38,219
We are using language as a crutch

788
00:34:38,219 --> 00:34:42,989
to help the deficiencies of our vision systems

789
00:34:42,989 --> 00:34:46,469
to kind of learn good representations from images and video.

790
00:34:46,469 --> 00:34:47,840
And the problem with this

791
00:34:47,840 --> 00:34:52,840
is that we might improve our vision language system a bit,

792
00:34:53,790 --> 00:34:58,110
I mean our language models by feeding them images.

793
00:34:58,110 --> 00:34:59,520
But we're not gonna get to the level

794
00:34:59,520 --> 00:35:01,260
of even the intelligence

795
00:35:01,260 --> 00:35:03,247
or level of understanding of the world

796
00:35:03,247 --> 00:35:06,663
of a cat or a dog which doesn't have language.

797
00:35:07,590 --> 00:35:08,610
They don't have language

798
00:35:08,610 --> 00:35:12,060
and they understand the world much better than any LLM.

799
00:35:12,060 --> 00:35:14,140
They can plan really complex actions

800
00:35:15,000 --> 00:35:17,970
and sort of imagine the result of a bunch of actions.

801
00:35:17,970 --> 00:35:20,430
How do we get machines to learn that

802
00:35:20,430 --> 00:35:22,920
before we combine that with language?

803
00:35:22,920 --> 00:35:24,363
Obviously, if we combine this with language,

804
00:35:24,363 --> 00:35:26,193
this is gonna be a winner,

805
00:35:28,410 --> 00:35:30,390
but before that we have to focus

806
00:35:30,390 --> 00:35:33,270
on like how do we get systems to learn how the world works?

807
00:35:33,270 --> 00:35:37,209
- So this kind of joint embedding predictive architecture,

808
00:35:37,209 --> 00:35:40,050
for you, that's gonna be able to learn

809
00:35:40,050 --> 00:35:41,370
something like common sense,

810
00:35:41,370 --> 00:35:43,440
something like what a cat uses

811
00:35:43,440 --> 00:35:47,792
to predict how to mess with its owner most optimally

812
00:35:47,792 --> 00:35:49,499
by knocking over a thing.

813
00:35:49,499 --> 00:35:51,000
- That's the hope.

814
00:35:51,000 --> 00:35:54,615
In fact, the techniques we're using are non-contrastive.

815
00:35:54,615 --> 00:35:57,690
So not only is the architecture non-generative,

816
00:35:57,690 --> 00:36:00,750
the learning procedures we're using are non-contrastive.

817
00:36:00,750 --> 00:36:02,880
We have two sets of techniques.

818
00:36:02,880 --> 00:36:05,104
One set is based on distillation

819
00:36:05,104 --> 00:36:10,104
and there's a number of methods that use this principle.

820
00:36:10,260 --> 00:36:12,033
One by DeepMind called BYOL.

821
00:36:13,380 --> 00:36:14,730
A couple by FAIR,

822
00:36:14,730 --> 00:36:19,730
one called VICReg and another one called I-JEPA.

823
00:36:20,070 --> 00:36:21,420
And VICReg, I should say,

824
00:36:21,420 --> 00:36:23,580
is not a distillation method actually,

825
00:36:23,580 --> 00:36:25,620
but I-JEPA and BYOL certainly are.

826
00:36:25,620 --> 00:36:28,873
And there's another one also called DINO or Dino,

827
00:36:28,873 --> 00:36:31,740
also produced at FAIR.

828
00:36:31,740 --> 00:36:32,573
And the idea of those things

829
00:36:32,573 --> 00:36:35,760
is that you take the full input, let's say an image.

830
00:36:35,760 --> 00:36:37,773
You run it through an encoder,

831
00:36:38,814 --> 00:36:41,280
produces a representation.

832
00:36:41,280 --> 00:36:43,470
And then you corrupt that input or transform it,

833
00:36:43,470 --> 00:36:46,500
run it through essentially what amounts to the same encoder

834
00:36:46,500 --> 00:36:48,510
with some minor differences.

835
00:36:48,510 --> 00:36:50,400
And then train a predictor.

836
00:36:50,400 --> 00:36:51,870
Sometimes a predictor is very simple,

837
00:36:51,870 --> 00:36:53,070
sometimes it doesn't exist.

838
00:36:53,070 --> 00:36:55,230
But train a predictor to predict a representation

839
00:36:55,230 --> 00:37:00,230
of the first uncorrupted input from the corrupted input.

840
00:37:02,070 --> 00:37:04,913
But you only train the second branch.

841
00:37:04,913 --> 00:37:07,500
You only train the part of the network

842
00:37:07,500 --> 00:37:10,800
that is fed with the corrupted input.

843
00:37:10,800 --> 00:37:12,780
The other network, you don't train.

844
00:37:12,780 --> 00:37:14,220
But since they share the same weight,

845
00:37:14,220 --> 00:37:15,960
when you modify the first one,

846
00:37:15,960 --> 00:37:18,350
it also modifies the second one.

847
00:37:18,350 --> 00:37:19,620
And with various tricks,

848
00:37:19,620 --> 00:37:21,793
you can prevent the system from collapsing

849
00:37:21,793 --> 00:37:24,147
with the collapse of the type I was explaining before

850
00:37:24,147 --> 00:37:26,583
where the system basically ignores the input.

851
00:37:28,170 --> 00:37:31,050
So that works very well.

852
00:37:31,050 --> 00:37:33,724
The two techniques we've developed at FAIR,

853
00:37:33,724 --> 00:37:38,724
DINO and I-JEPA work really well for that.

854
00:37:39,270 --> 00:37:41,760
- So what kind of data are we talking about here?

855
00:37:41,760 --> 00:37:43,484
- So there's several scenarios.

856
00:37:43,484 --> 00:37:47,280
One scenario is you take an image,

857
00:37:47,280 --> 00:37:52,280
you corrupt it by changing the cropping, for example,

858
00:37:52,735 --> 00:37:54,300
changing the size a little bit,

859
00:37:54,300 --> 00:37:56,610
maybe changing the orientation, blurring it,

860
00:37:56,610 --> 00:37:58,230
changing the colors,

861
00:37:58,230 --> 00:38:00,000
doing all kinds of horrible things to it-

862
00:38:00,000 --> 00:38:01,560
- But basic horrible things.

863
00:38:01,560 --> 00:38:02,580
- Basic horrible things

864
00:38:02,580 --> 00:38:04,200
that sort of degrade the quality a little bit

865
00:38:04,200 --> 00:38:05,433
and change the framing,

866
00:38:06,360 --> 00:38:08,313
crop the image.

867
00:38:09,703 --> 00:38:12,150
And in some cases, in the case of I-JEPA,

868
00:38:12,150 --> 00:38:13,620
you don't need to do any of this,

869
00:38:13,620 --> 00:38:16,137
you just mask some parts of it, right?

870
00:38:16,137 --> 00:38:19,470
You just basically remove some regions

871
00:38:19,470 --> 00:38:21,840
like a big block, essentially.

872
00:38:21,840 --> 00:38:24,677
And then run through the encoders

873
00:38:24,677 --> 00:38:26,490
and train the entire system,

874
00:38:26,490 --> 00:38:27,630
encoder and predictor,

875
00:38:27,630 --> 00:38:29,490
to predict the representation of the good one

876
00:38:29,490 --> 00:38:31,740
from the representation of the corrupted one.

877
00:38:33,630 --> 00:38:35,403
So that's the I-JEPA.

878
00:38:35,403 --> 00:38:38,280
It doesn't need to know that it's an image, for example,

879
00:38:38,280 --> 00:38:39,540
because the only thing it needs to know

880
00:38:39,540 --> 00:38:40,923
is how to do this masking.

881
00:38:42,360 --> 00:38:43,350
Whereas with DINO,

882
00:38:43,350 --> 00:38:44,310
you need to know it's an image

883
00:38:44,310 --> 00:38:45,420
because you need to do things

884
00:38:45,420 --> 00:38:48,202
like geometry transformation and blurring

885
00:38:48,202 --> 00:38:51,600
and things like that that are really image specific.

886
00:38:51,600 --> 00:38:53,790
A more recent version of this that we have is called V-JEPA.

887
00:38:53,790 --> 00:38:56,820
So it's basically the same idea as I-JEPA

888
00:38:56,820 --> 00:38:59,160
except it's applied to video.

889
00:38:59,160 --> 00:39:00,720
So now you take a whole video

890
00:39:00,720 --> 00:39:02,670
and you mask a whole chunk of it.

891
00:39:02,670 --> 00:39:04,920
And what we mask is actually kind of a temporal tube.

892
00:39:04,920 --> 00:39:07,941
So like a whole segment of each frame in the video

893
00:39:07,941 --> 00:39:10,260
over the entire video.

894
00:39:10,260 --> 00:39:12,870
- And that tube is like statically positioned

895
00:39:12,870 --> 00:39:14,130
throughout the frames?

896
00:39:14,130 --> 00:39:15,906
It's literally just a straight tube?

897
00:39:15,906 --> 00:39:17,130
- Throughout the tube, yeah.

898
00:39:17,130 --> 00:39:18,870
Typically it's 16 frames or something,

899
00:39:18,870 --> 00:39:22,350
and we mask the same region over the entire 16 frames.

900
00:39:22,350 --> 00:39:24,758
It's a different one for every video, obviously.

901
00:39:24,758 --> 00:39:28,530
And then again, train that system

902
00:39:28,530 --> 00:39:31,290
so as to predict the representation of the full video

903
00:39:31,290 --> 00:39:34,034
from the partially masked video.

904
00:39:34,034 --> 00:39:35,400
And that works really well.

905
00:39:35,400 --> 00:39:36,840
It's the first system that we have

906
00:39:36,840 --> 00:39:39,930
that learns good representations of video

907
00:39:39,930 --> 00:39:41,820
so that when you feed those representations

908
00:39:41,820 --> 00:39:44,940
to a supervised classifier head,

909
00:39:44,940 --> 00:39:47,760
it can tell you what action is taking place in the video

910
00:39:47,760 --> 00:39:49,773
with pretty good accuracy.

911
00:39:51,120 --> 00:39:55,950
So it's the first time we get something of that quality.

912
00:39:55,950 --> 00:39:57,030
- So that's a good test

913
00:39:57,030 --> 00:39:58,680
that a good representation is formed.

914
00:39:58,680 --> 00:40:00,270
That means there's something to this.

915
00:40:00,270 --> 00:40:01,480
- Yeah.

916
00:40:01,480 --> 00:40:03,450
We also preliminary result

917
00:40:03,450 --> 00:40:05,160
that seem to indicate

918
00:40:05,160 --> 00:40:09,510
that the representation allows our system to tell

919
00:40:09,510 --> 00:40:12,690
whether the video is physically possible

920
00:40:12,690 --> 00:40:13,920
or completely impossible

921
00:40:13,920 --> 00:40:15,330
because some object disappeared

922
00:40:15,330 --> 00:40:19,530
or an object suddenly jumped from one location to another

923
00:40:19,530 --> 00:40:21,840
or changed shape or something.

924
00:40:21,840 --> 00:40:26,840
- So it's able to capture some physics based constraints

925
00:40:27,420 --> 00:40:29,250
about the reality represented in the video?

926
00:40:29,250 --> 00:40:30,210
- [Yann] Yeah.

927
00:40:30,210 --> 00:40:32,600
- About the appearance and the disappearance of objects?

928
00:40:32,600 --> 00:40:34,230
- Yeah.

929
00:40:34,230 --> 00:40:35,700
That's really new.

930
00:40:35,700 --> 00:40:38,260
- Okay, but can this actually

931
00:40:40,140 --> 00:40:43,050
get us to this kind of world model

932
00:40:43,050 --> 00:40:46,200
that understands enough about the world

933
00:40:46,200 --> 00:40:48,727
to be able to drive a car?

934
00:40:48,727 --> 00:40:50,280
- Possibly.

935
00:40:50,280 --> 00:40:51,540
And this is gonna take a while

936
00:40:51,540 --> 00:40:52,940
before we get to that point.

937
00:40:54,570 --> 00:40:56,880
And there are systems already, robotic systems,

938
00:40:56,880 --> 00:40:58,713
that are based on this idea.

939
00:41:00,624 --> 00:41:02,700
What you need for this

940
00:41:02,700 --> 00:41:04,860
is a slightly modified version of this

941
00:41:04,860 --> 00:41:09,623
where imagine that you have a video,

942
00:41:11,010 --> 00:41:12,600
a complete video,

943
00:41:12,600 --> 00:41:13,980
and what you're doing to this video

944
00:41:13,980 --> 00:41:17,610
is that you are either translating it in time

945
00:41:17,610 --> 00:41:18,443
towards the future.

946
00:41:18,443 --> 00:41:19,767
So you'll only see the beginning of the video,

947
00:41:19,767 --> 00:41:21,750
but you don't see the latter part of it

948
00:41:21,750 --> 00:41:23,373
that is in the original one.

949
00:41:24,300 --> 00:41:27,750
Or you just mask the second half of the video, for example.

950
00:41:27,750 --> 00:41:30,830
And then you train this I-JEPA system

951
00:41:30,830 --> 00:41:32,280
or the type I described,

952
00:41:32,280 --> 00:41:33,990
to predict representation of the full video

953
00:41:33,990 --> 00:41:36,180
from the shifted one.

954
00:41:36,180 --> 00:41:39,720
But you also feed the predictor with an action.

955
00:41:39,720 --> 00:41:42,450
For example, the wheel is turned

956
00:41:42,450 --> 00:41:45,450
10 degrees to the right or something, right?

957
00:41:45,450 --> 00:41:49,920
So if it's a dash cam in a car

958
00:41:49,920 --> 00:41:51,390
and you know the angle of the wheel,

959
00:41:51,390 --> 00:41:53,340
you should be able to predict to some extent

960
00:41:53,340 --> 00:41:56,853
what's going to happen to what you see.

961
00:41:57,779 --> 00:41:59,940
You're not gonna be able to predict all the details

962
00:41:59,940 --> 00:42:02,820
of objects that appear in the view, obviously,

963
00:42:02,820 --> 00:42:05,790
but at an abstract representation level,

964
00:42:05,790 --> 00:42:08,700
you can probably predict what's gonna happen.

965
00:42:08,700 --> 00:42:12,540
So now what you have is an internal model

966
00:42:12,540 --> 00:42:13,457
that says, here is my idea

967
00:42:13,457 --> 00:42:15,270
of the state of the world at time T,

968
00:42:15,270 --> 00:42:17,940
here is an action I'm taking,

969
00:42:17,940 --> 00:42:18,773
here is a prediction

970
00:42:18,773 --> 00:42:20,550
of the state of the world at time T plus one,

971
00:42:20,550 --> 00:42:22,020
T plus delta T,

972
00:42:22,020 --> 00:42:24,330
T plus two seconds, whatever it is.

973
00:42:24,330 --> 00:42:26,220
If you have a model of this type,

974
00:42:26,220 --> 00:42:27,990
you can use it for planning.

975
00:42:27,990 --> 00:42:31,560
So now you can do what LLMs cannot do,

976
00:42:31,560 --> 00:42:34,020
which is planning what you're gonna do

977
00:42:34,020 --> 00:42:37,620
so as you arrive at a particular outcome

978
00:42:37,620 --> 00:42:40,800
or satisfy a particular objective, right?

979
00:42:40,800 --> 00:42:44,865
So you can have a number of objectives, right?

980
00:42:44,865 --> 00:42:49,865
I can predict that if I have an object like this, right?

981
00:42:50,860 --> 00:42:52,650
And I open my hand,

982
00:42:52,650 --> 00:42:54,340
it's gonna fall, right?

983
00:42:54,340 --> 00:42:57,716
And if I push it with a particular force on the table,

984
00:42:57,716 --> 00:42:58,860
it's gonna move.

985
00:42:58,860 --> 00:43:00,090
If I push the table itself,

986
00:43:00,090 --> 00:43:03,331
it's probably not gonna move with the same force.

987
00:43:03,331 --> 00:43:07,860
So we have this internal model of the world in our mind,

988
00:43:09,211 --> 00:43:11,820
which allows us to plan sequences of actions

989
00:43:11,820 --> 00:43:13,941
to arrive at a particular goal.

990
00:43:13,941 --> 00:43:18,570
And so now if you have this world model,

991
00:43:18,570 --> 00:43:21,600
we can imagine a sequence of actions,

992
00:43:21,600 --> 00:43:22,520
predict what the outcome

993
00:43:22,520 --> 00:43:25,260
of the sequence of action is going to be,

994
00:43:25,260 --> 00:43:28,320
measure to what extent the final state

995
00:43:28,320 --> 00:43:30,960
satisfies a particular objective

996
00:43:30,960 --> 00:43:35,013
like moving the bottle to the left of the table.

997
00:43:35,881 --> 00:43:38,430
And then plan a sequence of actions

998
00:43:38,430 --> 00:43:41,490
that will minimize this objective at runtime.

999
00:43:41,490 --> 00:43:43,132
We're not talking about learning,

1000
00:43:43,132 --> 00:43:44,977
we're talking about inference time, right?

1001
00:43:44,977 --> 00:43:46,080
So this is planning, really.

1002
00:43:46,080 --> 00:43:47,220
And in optimal control,

1003
00:43:47,220 --> 00:43:48,053
this is a very classical thing.

1004
00:43:48,053 --> 00:43:50,550
It's called model predictive control.

1005
00:43:50,550 --> 00:43:53,730
You have a model of the system you want to control

1006
00:43:53,730 --> 00:43:55,882
that can predict the sequence of states

1007
00:43:55,882 --> 00:43:58,950
corresponding to a sequence of commands.

1008
00:43:58,950 --> 00:44:02,250
And you are planning a sequence of commands

1009
00:44:02,250 --> 00:44:04,170
so that according to your world model,

1010
00:44:04,170 --> 00:44:06,450
the end state of the system

1011
00:44:06,450 --> 00:44:10,622
will satisfy any objectives that you fix.

1012
00:44:10,622 --> 00:44:15,622
This is the way rocket trajectories have been planned

1013
00:44:16,440 --> 00:44:17,730
since computers have been around.

1014
00:44:17,730 --> 00:44:20,070
So since the early '60s, essentially.

1015
00:44:20,070 --> 00:44:21,840
- So yes, for a model predictive control,

1016
00:44:21,840 --> 00:44:26,010
but you also often talk about hierarchical planning.

1017
00:44:26,010 --> 00:44:26,843
- [Yann] Yeah.

1018
00:44:26,843 --> 00:44:28,950
- Can hierarchical planning emerge from this somehow?

1019
00:44:28,950 --> 00:44:29,783
- Well, so no.

1020
00:44:29,783 --> 00:44:32,229
You will have to build a specific architecture

1021
00:44:32,229 --> 00:44:34,590
to allow for hierarchical planning.

1022
00:44:34,590 --> 00:44:36,840
So hierarchical planning is absolutely necessary

1023
00:44:36,840 --> 00:44:39,513
if you want to plan complex actions.

1024
00:44:40,650 --> 00:44:43,290
If I wanna go from, let's say, from New York to Paris,

1025
00:44:43,290 --> 00:44:45,390
this the example I use all the time.

1026
00:44:45,390 --> 00:44:48,150
And I'm sitting in my office at NYU.

1027
00:44:48,150 --> 00:44:50,490
My objective that I need to minimize

1028
00:44:50,490 --> 00:44:52,080
is my distance to Paris.

1029
00:44:52,080 --> 00:44:52,913
At a high level,

1030
00:44:52,913 --> 00:44:57,390
a very abstract representation of my location,

1031
00:44:57,390 --> 00:44:59,370
I would have to decompose this into two sub-goals.

1032
00:44:59,370 --> 00:45:02,250
First one is go to the airport,

1033
00:45:02,250 --> 00:45:04,680
second one is catch a plane to Paris.

1034
00:45:04,680 --> 00:45:05,513
Okay.

1035
00:45:05,513 --> 00:45:09,150
So my sub-goal is now going to the airport.

1036
00:45:09,150 --> 00:45:11,750
My objective function is my distance to the airport.

1037
00:45:12,660 --> 00:45:14,160
How do I go to the airport?

1038
00:45:14,160 --> 00:45:18,300
Well, I have to go in the street and hail a taxi,

1039
00:45:18,300 --> 00:45:19,750
which you can do in New York.

1040
00:45:21,270 --> 00:45:22,740
Okay, now I have another sub-goal.

1041
00:45:22,740 --> 00:45:24,724
Go down on the street.

1042
00:45:24,724 --> 00:45:27,840
Well, that means going to the elevator,

1043
00:45:27,840 --> 00:45:28,860
going down the elevator,

1044
00:45:28,860 --> 00:45:30,010
walk out to the street.

1045
00:45:30,930 --> 00:45:32,730
How do I go to the elevator?

1046
00:45:32,730 --> 00:45:36,360
I have to stand up from my chair,

1047
00:45:36,360 --> 00:45:38,040
open the door of my office,

1048
00:45:38,040 --> 00:45:40,680
go to the elevator, push the button.

1049
00:45:40,680 --> 00:45:42,330
How do I get up for my chair?

1050
00:45:42,330 --> 00:45:45,660
Like you can imagine going down all the way down

1051
00:45:45,660 --> 00:45:47,400
to basically what amounts

1052
00:45:47,400 --> 00:45:50,400
to millisecond by millisecond muscle control.

1053
00:45:50,400 --> 00:45:51,233
Okay?

1054
00:45:51,233 --> 00:45:55,020
And obviously you're not going to plan your entire trip

1055
00:45:55,020 --> 00:45:56,550
from New York to Paris

1056
00:45:56,550 --> 00:46:00,300
in terms of millisecond by millisecond muscle control.

1057
00:46:00,300 --> 00:46:02,310
First, that would be incredibly expensive,

1058
00:46:02,310 --> 00:46:03,810
but it will also be completely impossible

1059
00:46:03,810 --> 00:46:06,480
because you don't know all the conditions

1060
00:46:06,480 --> 00:46:07,610
of what's gonna happen.

1061
00:46:07,610 --> 00:46:10,450
How long it's gonna take to catch a taxi

1062
00:46:11,761 --> 00:46:13,833
or to go to the airport with traffic.

1063
00:46:14,970 --> 00:46:16,920
I mean, you would have to know exactly

1064
00:46:16,920 --> 00:46:18,030
the condition of everything

1065
00:46:18,030 --> 00:46:19,920
to be able to do this planning,

1066
00:46:19,920 --> 00:46:21,420
and you don't have the information.

1067
00:46:21,420 --> 00:46:23,970
So you have to do this hierarchical planning

1068
00:46:23,970 --> 00:46:25,380
so that you can start acting

1069
00:46:25,380 --> 00:46:27,330
and then sort of re-planning as you go.

1070
00:46:28,230 --> 00:46:32,013
And nobody really knows how to do this in AI.

1071
00:46:33,390 --> 00:46:35,280
Nobody knows how to train a system

1072
00:46:35,280 --> 00:46:38,276
to learn the appropriate multiple levels of representation

1073
00:46:38,276 --> 00:46:41,310
so that hierarchical planning works.

1074
00:46:41,310 --> 00:46:42,990
- Does something like that already emerge?

1075
00:46:42,990 --> 00:46:45,453
So like can you use an LLM,

1076
00:46:46,740 --> 00:46:48,390
state-of-the-art LLM,

1077
00:46:48,390 --> 00:46:50,970
to get you from New York to Paris

1078
00:46:50,970 --> 00:46:54,240
by doing exactly the kind of detailed

1079
00:46:54,240 --> 00:46:56,490
set of questions that you just did?

1080
00:46:56,490 --> 00:47:01,230
Which is can you give me a list of 10 steps I need to do

1081
00:47:01,230 --> 00:47:02,730
to get from New York to Paris?

1082
00:47:02,730 --> 00:47:05,460
And then for each of those steps,

1083
00:47:05,460 --> 00:47:07,200
can you give me a list of 10 steps

1084
00:47:07,200 --> 00:47:09,150
how I make that step happen?

1085
00:47:09,150 --> 00:47:10,380
And for each of those steps,

1086
00:47:10,380 --> 00:47:12,240
can you give me a list of 10 steps

1087
00:47:12,240 --> 00:47:13,200
to make each one of those,

1088
00:47:13,200 --> 00:47:15,948
until you're moving your individual muscles?

1089
00:47:15,948 --> 00:47:17,970
Maybe not.

1090
00:47:17,970 --> 00:47:19,650
Whatever you can actually act upon

1091
00:47:19,650 --> 00:47:20,673
using your own mind.

1092
00:47:21,540 --> 00:47:22,373
- Right.

1093
00:47:22,373 --> 00:47:23,250
So there's a lot of questions

1094
00:47:23,250 --> 00:47:24,540
that are also implied by this, right?

1095
00:47:24,540 --> 00:47:27,750
So the first thing is LLMs will be able to answer

1096
00:47:27,750 --> 00:47:28,770
some of those questions

1097
00:47:28,770 --> 00:47:30,633
down to some level of abstraction.

1098
00:47:32,430 --> 00:47:34,530
Under the condition that they've been trained

1099
00:47:34,530 --> 00:47:37,290
with similar scenarios in their training set.

1100
00:47:37,290 --> 00:47:40,140
- They would be able to answer all of those questions.

1101
00:47:40,140 --> 00:47:43,200
But some of them may be hallucinated,

1102
00:47:43,200 --> 00:47:44,310
meaning non-factual.

1103
00:47:44,310 --> 00:47:45,143
- Yeah, true.

1104
00:47:45,143 --> 00:47:46,320
I mean they'll probably produce some answer.

1105
00:47:46,320 --> 00:47:47,370
Except they're not gonna be able

1106
00:47:47,370 --> 00:47:48,600
to really kind of produce

1107
00:47:48,600 --> 00:47:50,280
millisecond by millisecond muscle control

1108
00:47:50,280 --> 00:47:53,281
of how you stand up from your chair, right?

1109
00:47:53,281 --> 00:47:55,202
But down to some level of abstraction

1110
00:47:55,202 --> 00:47:57,810
where you can describe things by words,

1111
00:47:57,810 --> 00:47:59,550
they might be able to give you a plan,

1112
00:47:59,550 --> 00:48:01,440
but only under the condition that they've been trained

1113
00:48:01,440 --> 00:48:04,110
to produce those kind of plans, right?

1114
00:48:04,110 --> 00:48:06,708
They're not gonna be able to plan for situations

1115
00:48:06,708 --> 00:48:09,360
they never encountered before.

1116
00:48:09,360 --> 00:48:11,820
They basically are going to have to regurgitate the template

1117
00:48:11,820 --> 00:48:12,960
that they've been trained on.

1118
00:48:12,960 --> 00:48:15,631
- But where, just for the example of New York to Paris,

1119
00:48:15,631 --> 00:48:18,405
is it gonna start getting into trouble?

1120
00:48:18,405 --> 00:48:20,144
Like at which layer of abstraction

1121
00:48:20,144 --> 00:48:22,560
do you think you'll start?

1122
00:48:22,560 --> 00:48:23,520
Because like I can imagine

1123
00:48:23,520 --> 00:48:24,737
almost every single part of that,

1124
00:48:24,737 --> 00:48:27,750
an LLM will be able to answer somewhat accurately,

1125
00:48:27,750 --> 00:48:29,730
especially when you're talking about New York and Paris,

1126
00:48:29,730 --> 00:48:31,020
major cities.

1127
00:48:31,020 --> 00:48:33,420
- So I mean certainly an LLM

1128
00:48:33,420 --> 00:48:34,670
would be able to solve that problem

1129
00:48:34,670 --> 00:48:36,297
if you fine tune it for it.

1130
00:48:36,297 --> 00:48:37,130
- [Lex] Sure.

1131
00:48:37,130 --> 00:48:42,130
- And so I can't say that an LLM cannot do this,

1132
00:48:42,420 --> 00:48:44,010
it can't do this if you train it for it,

1133
00:48:44,010 --> 00:48:45,317
there's no question,

1134
00:48:45,317 --> 00:48:47,790
down to a certain level

1135
00:48:47,790 --> 00:48:51,300
where things can be formulated in terms of words.

1136
00:48:51,300 --> 00:48:52,380
But like if you wanna go down

1137
00:48:52,380 --> 00:48:54,444
to like how do you climb down the stairs

1138
00:48:54,444 --> 00:48:57,264
or just stand up from your chair in terms of words,

1139
00:48:57,264 --> 00:48:59,343
like you can't do it.

1140
00:49:00,747 --> 00:49:04,200
That's one of the reasons you need

1141
00:49:04,200 --> 00:49:06,210
experience of the physical world,

1142
00:49:06,210 --> 00:49:07,710
which is much higher bandwidth

1143
00:49:07,710 --> 00:49:10,050
than what you can express in words,

1144
00:49:10,050 --> 00:49:11,040
in human language.

1145
00:49:11,040 --> 00:49:12,480
- So everything we've been talking about

1146
00:49:12,480 --> 00:49:13,804
on the joint embedding space,

1147
00:49:13,804 --> 00:49:16,500
is it possible that that's what we need

1148
00:49:16,500 --> 00:49:18,579
for like the interaction with physical reality

1149
00:49:18,579 --> 00:49:20,730
on the robotics front?

1150
00:49:20,730 --> 00:49:24,290
And then just the LLMs are the thing that sits on top of it

1151
00:49:24,290 --> 00:49:26,490
for the bigger reasoning

1152
00:49:26,490 --> 00:49:30,720
about like the fact that I need to book a plane ticket

1153
00:49:30,720 --> 00:49:33,690
and I need to know know how to go to the websites and so on.

1154
00:49:33,690 --> 00:49:34,523
- Sure.

1155
00:49:34,523 --> 00:49:37,210
And a lot of plans that people know about

1156
00:49:37,210 --> 00:49:41,409
that are relatively high level are actually learned.

1157
00:49:41,409 --> 00:49:46,409
Most people don't invent the plans by themselves.

1158
00:49:50,956 --> 00:49:54,573
We have some ability to do this, of course, obviously,

1159
00:49:54,573 --> 00:49:57,420
but most plans that people use

1160
00:49:57,420 --> 00:49:59,550
are plans that have been trained on.

1161
00:49:59,550 --> 00:50:01,290
Like they've seen other people use those plans

1162
00:50:01,290 --> 00:50:04,170
or they've been told how to do things, right?

1163
00:50:04,170 --> 00:50:07,650
That you can't invent how you like take a person

1164
00:50:07,650 --> 00:50:09,270
who's never heard of airplanes

1165
00:50:09,270 --> 00:50:12,390
and tell them like, how do you go from New York to Paris?

1166
00:50:12,390 --> 00:50:14,040
They're probably not going to be able

1167
00:50:14,040 --> 00:50:16,170
to kind of deconstruct the whole plan

1168
00:50:16,170 --> 00:50:18,780
unless they've seen examples of that before.

1169
00:50:18,780 --> 00:50:21,210
So certainly LLMs are gonna be able to do this.

1170
00:50:21,210 --> 00:50:26,210
But then how you link this from the low level of actions,

1171
00:50:28,242 --> 00:50:30,996
that needs to be done with things like JEPA,

1172
00:50:30,996 --> 00:50:33,810
that basically lift the abstraction level

1173
00:50:33,810 --> 00:50:34,740
of the representation

1174
00:50:34,740 --> 00:50:36,030
without attempting to reconstruct

1175
00:50:36,030 --> 00:50:38,040
every detail of the situation.

1176
00:50:38,040 --> 00:50:39,693
That's why we need JEPAs for.

1177
00:50:40,800 --> 00:50:44,310
- I would love to sort of linger on your skepticism

1178
00:50:44,310 --> 00:50:48,450
around autoregressive LLMs.

1179
00:50:48,450 --> 00:50:53,100
So one way I would like to test that skepticism is

1180
00:50:53,100 --> 00:50:55,100
everything you say makes a lot of sense,

1181
00:50:57,510 --> 00:51:02,370
but if I apply everything you said today and in general

1182
00:51:02,370 --> 00:51:03,450
to like, I don't know,

1183
00:51:03,450 --> 00:51:05,160
10 years ago, maybe a little bit less.

1184
00:51:05,160 --> 00:51:07,950
No, let's say three years ago.

1185
00:51:07,950 --> 00:51:12,660
I wouldn't be able to predict the success of LLMs.

1186
00:51:12,660 --> 00:51:15,660
So does it make sense to you

1187
00:51:15,660 --> 00:51:19,683
that autoregressive LLMs are able to be so damn good?

1188
00:51:20,610 --> 00:51:21,810
- [Yann] Yes.

1189
00:51:21,810 --> 00:51:24,300
- Can you explain your intuition?

1190
00:51:24,300 --> 00:51:29,160
Because if I were to take your wisdom and intuition

1191
00:51:29,160 --> 00:51:30,090
at face value,

1192
00:51:30,090 --> 00:51:32,880
I would say there's no way autoregressive LLMs

1193
00:51:32,880 --> 00:51:34,350
one token at a time,

1194
00:51:34,350 --> 00:51:36,300
would be able to do the kind of things they're doing.

1195
00:51:36,300 --> 00:51:39,300
- No, there's one thing that autoregressive LLMs

1196
00:51:39,300 --> 00:51:42,450
or that LLMs in general, not just the autoregressive ones,

1197
00:51:42,450 --> 00:51:45,093
but including the BERT style bidirectional ones,

1198
00:51:47,130 --> 00:51:49,260
are exploiting and its self supervised running.

1199
00:51:49,260 --> 00:51:51,090
And I've been a very, very strong advocate

1200
00:51:51,090 --> 00:51:53,340
of self supervised running for many years.

1201
00:51:53,340 --> 00:51:58,340
So those things are an incredibly impressive demonstration

1202
00:51:58,620 --> 00:52:01,852
that self supervised learning actually works.

1203
00:52:01,852 --> 00:52:04,920
The idea that started...

1204
00:52:04,920 --> 00:52:07,217
It didn't start with BERT,

1205
00:52:07,217 --> 00:52:09,630
but it was really kind of a good demonstration with this.

1206
00:52:09,630 --> 00:52:14,610
So the idea that you take a piece of text, you corrupt it,

1207
00:52:14,610 --> 00:52:16,140
and then you train some gigantic neural net

1208
00:52:16,140 --> 00:52:18,666
to reconstruct the parts that are missing.

1209
00:52:18,666 --> 00:52:21,050
That has been an enormous...

1210
00:52:23,550 --> 00:52:25,620
Produced an enormous amount of benefits.

1211
00:52:25,620 --> 00:52:30,620
It allowed us to create systems that understand language,

1212
00:52:31,350 --> 00:52:32,650
systems that can translate

1213
00:52:33,960 --> 00:52:36,510
hundreds of languages in any direction,

1214
00:52:36,510 --> 00:52:38,010
systems that are multilingual.

1215
00:52:39,480 --> 00:52:40,313
It's a single system

1216
00:52:40,313 --> 00:52:43,200
that can be trained to understand hundreds of languages

1217
00:52:43,200 --> 00:52:44,700
and translate in any direction

1218
00:52:45,620 --> 00:52:48,390
and produce summaries

1219
00:52:48,390 --> 00:52:51,780
and then answer questions and produce text.

1220
00:52:51,780 --> 00:52:53,663
And then there's a special case of it,

1221
00:52:54,632 --> 00:52:56,610
which is the autoregressive trick

1222
00:52:56,610 --> 00:52:58,560
where you constrain the system

1223
00:52:58,560 --> 00:53:02,010
to not elaborate a representation of the text

1224
00:53:02,010 --> 00:53:03,720
from looking at the entire text,

1225
00:53:03,720 --> 00:53:06,510
but only predicting a word

1226
00:53:06,510 --> 00:53:08,280
from the words that have come before.

1227
00:53:08,280 --> 00:53:09,113
Right?

1228
00:53:09,113 --> 00:53:09,946
And you do this

1229
00:53:09,946 --> 00:53:11,550
by constraining the architecture of the network.

1230
00:53:11,550 --> 00:53:15,120
And that's what you can build an autoregressive LLM from.

1231
00:53:15,120 --> 00:53:17,640
So there was a surprise many years ago

1232
00:53:17,640 --> 00:53:20,910
with what's called decoder only LLM.

1233
00:53:20,910 --> 00:53:22,502
So systems of this type

1234
00:53:22,502 --> 00:53:27,502
that are just trying to produce words from the previous one.

1235
00:53:28,080 --> 00:53:31,230
And the fact that when you scale them up,

1236
00:53:31,230 --> 00:53:36,230
they tend to really kind of understand more about language.

1237
00:53:36,870 --> 00:53:38,100
When you train them on lots of data,

1238
00:53:38,100 --> 00:53:39,330
you make them really big.

1239
00:53:39,330 --> 00:53:40,710
That was kind of a surprise.

1240
00:53:40,710 --> 00:53:42,870
And that surprise occurred quite a while back.

1241
00:53:42,870 --> 00:53:47,870
Like with work from Google, Meta, OpenAI, et cetera,

1242
00:53:50,219 --> 00:53:53,430
going back to the GPT

1243
00:53:53,430 --> 00:53:56,790
kind of general pre-trained transformers.

1244
00:53:56,790 --> 00:53:58,230
- You mean like GPT-2?

1245
00:53:58,230 --> 00:54:00,360
Like there's a certain place

1246
00:54:00,360 --> 00:54:01,380
where you start to realize

1247
00:54:01,380 --> 00:54:06,380
scaling might actually keep giving us an emergent benefit.

1248
00:54:06,690 --> 00:54:09,210
- Yeah, I mean there were work from various places,

1249
00:54:09,210 --> 00:54:14,210
but if you want to kind of place it in the GPT timeline,

1250
00:54:16,380 --> 00:54:18,030
that would be around GPT-2, yeah.

1251
00:54:19,020 --> 00:54:20,820
- Well, 'cause you said it,

1252
00:54:20,820 --> 00:54:23,580
you're so charismatic and you said so many words,

1253
00:54:23,580 --> 00:54:25,830
but self supervised learning, yes.

1254
00:54:25,830 --> 00:54:28,980
But again, the same intuition you're applying

1255
00:54:28,980 --> 00:54:31,530
to saying that autoregressive LLMs

1256
00:54:31,530 --> 00:54:35,190
cannot have a deep understanding of the world,

1257
00:54:35,190 --> 00:54:38,010
if we just apply that same intuition,

1258
00:54:38,010 --> 00:54:39,490
does it make sense to you

1259
00:54:40,500 --> 00:54:42,360
that they're able to form enough

1260
00:54:42,360 --> 00:54:43,770
of a representation in the world

1261
00:54:43,770 --> 00:54:45,780
to be damn convincing,

1262
00:54:45,780 --> 00:54:49,680
essentially passing the original Turing test

1263
00:54:49,680 --> 00:54:50,790
with flying colors.

1264
00:54:50,790 --> 00:54:53,120
- Well, we're fooled by their fluency, right?

1265
00:54:53,120 --> 00:54:56,087
We just assume that if a system is fluent

1266
00:54:56,087 --> 00:54:57,630
in manipulating language,

1267
00:54:57,630 --> 00:55:00,720
then it has all the characteristics of human intelligence.

1268
00:55:00,720 --> 00:55:04,051
But that impression is false.

1269
00:55:04,051 --> 00:55:06,510
We're really fooled by it.

1270
00:55:06,510 --> 00:55:08,880
- Well, what do you think Alan Turing would say?

1271
00:55:08,880 --> 00:55:10,050
Without understanding anything,

1272
00:55:10,050 --> 00:55:11,370
just hanging out with it-

1273
00:55:11,370 --> 00:55:12,330
- Alan Turing would decide

1274
00:55:12,330 --> 00:55:14,746
that a Turing test is a really bad test.

1275
00:55:14,746 --> 00:55:15,579
(Lex chuckles)

1276
00:55:15,579 --> 00:55:16,412
Okay.

1277
00:55:16,412 --> 00:55:18,870
This is what the AI community has decided many years ago

1278
00:55:18,870 --> 00:55:22,020
that the Turing test was a really bad test of intelligence.

1279
00:55:22,020 --> 00:55:23,190
- What would Hans Moravec say

1280
00:55:23,190 --> 00:55:25,566
about the large language models?

1281
00:55:25,566 --> 00:55:26,839
- Hans Moravec would say

1282
00:55:26,839 --> 00:55:30,180
the Moravec's paradox still applies.

1283
00:55:30,180 --> 00:55:31,013
- [Lex] Okay.

1284
00:55:31,013 --> 00:55:31,846
- Okay?

1285
00:55:31,846 --> 00:55:32,679
Okay, we can pass-

1286
00:55:32,679 --> 00:55:34,200
- You don't think he would be really impressed.

1287
00:55:34,200 --> 00:55:35,677
- No, of course everybody would be impressed.

1288
00:55:35,677 --> 00:55:36,682
(laughs)

1289
00:55:36,682 --> 00:55:39,623
But it is not a question of being impressed or not,

1290
00:55:39,623 --> 00:55:41,460
it is a question of knowing

1291
00:55:41,460 --> 00:55:44,160
what the limit of those systems can do.

1292
00:55:44,160 --> 00:55:45,870
Again, they are impressive.

1293
00:55:45,870 --> 00:55:47,490
They can do a lot of useful things.

1294
00:55:47,490 --> 00:55:49,740
There's a whole industry that is being built around them.

1295
00:55:49,740 --> 00:55:51,388
They're gonna make progress,

1296
00:55:51,388 --> 00:55:53,670
but there is a lot of things they cannot do.

1297
00:55:53,670 --> 00:55:55,560
And we have to realize what they cannot do

1298
00:55:55,560 --> 00:55:59,940
and then figure out how we get there.

1299
00:55:59,940 --> 00:56:02,550
And I'm not saying this...

1300
00:56:02,550 --> 00:56:07,370
I'm saying this from basically 10 years of research

1301
00:56:07,370 --> 00:56:11,836
on the idea of self supervised running,

1302
00:56:11,836 --> 00:56:13,770
actually that's going back more than 10 years,

1303
00:56:13,770 --> 00:56:15,270
but the idea of self supervised learning.

1304
00:56:15,270 --> 00:56:17,764
So basically capturing the internal structure

1305
00:56:17,764 --> 00:56:21,180
of a piece of a set of inputs

1306
00:56:21,180 --> 00:56:23,640
without training the system for any particular task, right?

1307
00:56:23,640 --> 00:56:25,636
Learning representations.

1308
00:56:25,636 --> 00:56:28,830
The conference I co-founded 14 years ago

1309
00:56:28,830 --> 00:56:30,810
is called International Conference

1310
00:56:30,810 --> 00:56:31,860
on Learning Representations,

1311
00:56:31,860 --> 00:56:34,548
that's the entire issue that deep learning is dealing with.

1312
00:56:34,548 --> 00:56:35,940
Right?

1313
00:56:35,940 --> 00:56:38,510
And it's been my obsession for almost 40 years now.

1314
00:56:38,510 --> 00:56:42,160
So learning representation is really the thing.

1315
00:56:42,160 --> 00:56:43,530
For the longest time

1316
00:56:43,530 --> 00:56:45,720
we could only do this with supervised learning.

1317
00:56:45,720 --> 00:56:47,425
And then we started working on

1318
00:56:47,425 --> 00:56:50,463
what we used to call unsupervised learning

1319
00:56:50,463 --> 00:56:55,222
and sort of revived the idea of unsupervised learning

1320
00:56:55,222 --> 00:56:59,250
in the early 2000s with Yoshua Bengio and Jeff Hinton.

1321
00:56:59,250 --> 00:57:00,690
Then discovered that supervised learning

1322
00:57:00,690 --> 00:57:02,070
actually works pretty well

1323
00:57:02,070 --> 00:57:03,840
if you can collect enough data.

1324
00:57:03,840 --> 00:57:07,236
And so the whole idea of unsupervised self supervision

1325
00:57:07,236 --> 00:57:10,072
took a backseat for a bit

1326
00:57:10,072 --> 00:57:14,977
and then I kind of tried to revive it in a big way,

1327
00:57:16,890 --> 00:57:20,580
starting in 2014 basically when we started FAIR,

1328
00:57:20,580 --> 00:57:24,036
and really pushing for like finding new methods

1329
00:57:24,036 --> 00:57:26,460
to do self supervised running,

1330
00:57:26,460 --> 00:57:29,910
both for text and for images and for video and audio.

1331
00:57:29,910 --> 00:57:32,451
And some of that work has been incredibly successful.

1332
00:57:32,451 --> 00:57:34,710
I mean, the reason why we have

1333
00:57:34,710 --> 00:57:37,320
multilingual translation system,

1334
00:57:37,320 --> 00:57:38,370
things to do,

1335
00:57:38,370 --> 00:57:41,820
content moderation on Meta, for example, on Facebook

1336
00:57:41,820 --> 00:57:42,653
that are multilingual,

1337
00:57:42,653 --> 00:57:44,460
that understand whether piece of text

1338
00:57:44,460 --> 00:57:46,530
is hate speech or not, or something

1339
00:57:46,530 --> 00:57:47,580
is due to their progress

1340
00:57:47,580 --> 00:57:50,100
using self supervised running for NLP,

1341
00:57:50,100 --> 00:57:52,594
combining this with transformer architectures

1342
00:57:52,594 --> 00:57:53,760
and blah blah blah.

1343
00:57:53,760 --> 00:57:55,800
But that's the big success of self supervised running.

1344
00:57:55,800 --> 00:57:59,070
We had similar success in speech recognition,

1345
00:57:59,070 --> 00:58:00,180
a system called Wav2Vec,

1346
00:58:00,180 --> 00:58:02,520
which is also a joint embedding architecture by the way,

1347
00:58:02,520 --> 00:58:03,720
trained with contrastive learning.

1348
00:58:03,720 --> 00:58:07,449
And that system also can produce

1349
00:58:07,449 --> 00:58:10,590
speech recognition systems that are multilingual

1350
00:58:10,590 --> 00:58:13,050
with mostly unlabeled data

1351
00:58:13,050 --> 00:58:15,090
and only need a few minutes of labeled data

1352
00:58:15,090 --> 00:58:16,950
to actually do speech recognition.

1353
00:58:16,950 --> 00:58:18,063
That's amazing.

1354
00:58:19,620 --> 00:58:22,230
We have systems now based on those combination of ideas

1355
00:58:22,230 --> 00:58:24,079
that can do real time translation

1356
00:58:24,079 --> 00:58:26,852
of hundreds of languages into each other,

1357
00:58:26,852 --> 00:58:28,050
speech to speech.

1358
00:58:28,050 --> 00:58:29,070
- Speech to speech,

1359
00:58:29,070 --> 00:58:31,110
even including, which is fascinating,

1360
00:58:31,110 --> 00:58:33,925
languages that don't have written forms-

1361
00:58:33,925 --> 00:58:35,580
- That's right. - They're spoken only.

1362
00:58:35,580 --> 00:58:36,413
- That's right.

1363
00:58:36,413 --> 00:58:37,246
We don't go through text,

1364
00:58:37,246 --> 00:58:38,820
it goes directly from speech to speech

1365
00:58:38,820 --> 00:58:40,290
using an internal representation

1366
00:58:40,290 --> 00:58:41,970
of kinda speech units that are discrete.

1367
00:58:41,970 --> 00:58:44,048
But it's called Textless NLP.

1368
00:58:44,048 --> 00:58:45,600
We used to call it this way.

1369
00:58:45,600 --> 00:58:47,211
But yeah.

1370
00:58:47,211 --> 00:58:49,260
I mean incredible success there.

1371
00:58:49,260 --> 00:58:53,130
And then for 10 years we tried to apply this idea

1372
00:58:53,130 --> 00:58:55,410
to learning representations of images

1373
00:58:55,410 --> 00:58:57,390
by training a system to predict videos,

1374
00:58:57,390 --> 00:58:58,620
learning intuitive physics

1375
00:58:58,620 --> 00:59:00,360
by training a system to predict

1376
00:59:00,360 --> 00:59:02,340
what's gonna happen in the video.

1377
00:59:02,340 --> 00:59:05,100
And tried and tried and failed and failed

1378
00:59:05,100 --> 00:59:06,780
with generative models,

1379
00:59:06,780 --> 00:59:08,380
with models that predict pixels.

1380
00:59:09,265 --> 00:59:10,836
We could not get them to learn

1381
00:59:10,836 --> 00:59:13,200
good representations of images,

1382
00:59:13,200 --> 00:59:16,410
we could not get them to learn good presentations of videos.

1383
00:59:16,410 --> 00:59:17,250
And we tried many times,

1384
00:59:17,250 --> 00:59:19,063
we published lots of papers on it.

1385
00:59:19,063 --> 00:59:22,293
They kind of sort of worked, but not really great.

1386
00:59:23,220 --> 00:59:24,300
It started working,

1387
00:59:24,300 --> 00:59:27,886
we abandoned this idea of predicting every pixel

1388
00:59:27,886 --> 00:59:30,960
and basically just doing the joint embedding and predicting

1389
00:59:30,960 --> 00:59:32,310
in representation space.

1390
00:59:32,310 --> 00:59:33,303
That works.

1391
00:59:34,140 --> 00:59:36,180
So there's ample evidence

1392
00:59:36,180 --> 00:59:40,590
that we're not gonna be able to learn good representations

1393
00:59:40,590 --> 00:59:42,000
of the real world

1394
00:59:42,000 --> 00:59:43,200
using generative model.

1395
00:59:43,200 --> 00:59:44,580
So I'm telling people,

1396
00:59:44,580 --> 00:59:46,770
everybody's talking about generative AI.

1397
00:59:46,770 --> 00:59:48,810
If you're really interested in human level AI,

1398
00:59:48,810 --> 00:59:50,860
abandon the idea of generative AI.

1399
00:59:50,860 --> 00:59:51,907
(Lex laughs)

1400
00:59:51,907 --> 00:59:52,740
- Okay.

1401
00:59:52,740 --> 00:59:54,840
But you really think it's possible

1402
00:59:54,840 --> 00:59:57,360
to get far with joint embedding representation?

1403
00:59:57,360 --> 01:00:01,350
So like there's common sense reasoning

1404
01:00:01,350 --> 01:00:05,253
and then there's high level reasoning.

1405
01:00:05,253 --> 01:00:08,580
Like I feel like those are two...

1406
01:00:08,580 --> 01:00:11,640
The kind of reasoning that LLMs are able to do.

1407
01:00:11,640 --> 01:00:13,650
Okay, let me not use the word reasoning,

1408
01:00:13,650 --> 01:00:16,050
but the kind of stuff that LLMs are able to do

1409
01:00:16,050 --> 01:00:17,430
seems fundamentally different

1410
01:00:17,430 --> 01:00:19,590
than the common sense reasoning we use

1411
01:00:19,590 --> 01:00:20,792
to navigate the world.

1412
01:00:20,792 --> 01:00:21,625
- [Yann] Yeah.

1413
01:00:21,625 --> 01:00:23,010
- It seems like we're gonna need both-

1414
01:00:23,010 --> 01:00:25,140
- Sure. - Would you be able to get,

1415
01:00:25,140 --> 01:00:27,960
with the joint embedding which is a JEPA type of approach,

1416
01:00:27,960 --> 01:00:30,633
looking at video, would you be able to learn,

1417
01:00:32,370 --> 01:00:33,203
let's see,

1418
01:00:33,203 --> 01:00:35,520
well, how to get from New York to Paris,

1419
01:00:35,520 --> 01:00:40,520
or how to understand the state of politics in the world?

1420
01:00:42,565 --> 01:00:43,830
(both laugh)

1421
01:00:43,830 --> 01:00:44,663
Right?

1422
01:00:44,663 --> 01:00:46,710
These are things where various humans

1423
01:00:46,710 --> 01:00:49,050
generate a lot of language and opinions on,

1424
01:00:49,050 --> 01:00:50,130
in the space of language,

1425
01:00:50,130 --> 01:00:52,670
but don't visually represent that

1426
01:00:52,670 --> 01:00:56,070
in any clearly compressible way.

1427
01:00:56,070 --> 01:00:56,903
- Right.

1428
01:00:56,903 --> 01:00:58,080
Well, there's a lot of situations

1429
01:00:58,080 --> 01:01:00,057
that might be difficult

1430
01:01:00,057 --> 01:01:04,860
for a purely language based system to know.

1431
01:01:04,860 --> 01:01:08,340
Like, okay, you can probably learn from reading texts,

1432
01:01:08,340 --> 01:01:11,460
the entirety of the publicly available text in the world

1433
01:01:11,460 --> 01:01:13,620
that I cannot get from New York to Paris

1434
01:01:13,620 --> 01:01:15,084
by snapping my fingers.

1435
01:01:15,084 --> 01:01:16,320
That's not gonna work, right?

1436
01:01:16,320 --> 01:01:17,153
- [Lex] Yes.

1437
01:01:18,420 --> 01:01:20,346
- But there's probably sort of more complex

1438
01:01:20,346 --> 01:01:22,320
scenarios of this type

1439
01:01:22,320 --> 01:01:25,770
which an LLM may never have encountered

1440
01:01:25,770 --> 01:01:27,720
and may not be able to determine

1441
01:01:27,720 --> 01:01:29,627
whether it's possible or not.

1442
01:01:29,627 --> 01:01:34,627
So that link from the low level to the high level...

1443
01:01:35,490 --> 01:01:38,880
The thing is that the high level that language expresses

1444
01:01:38,880 --> 01:01:43,230
is based on the common experience of the low level,

1445
01:01:43,230 --> 01:01:45,559
which LLMs currently do not have.

1446
01:01:45,559 --> 01:01:47,640
When we talk to each other,

1447
01:01:47,640 --> 01:01:50,106
we know we have a common experience of the world.

1448
01:01:50,106 --> 01:01:54,577
Like a lot of it is similar.

1449
01:01:54,577 --> 01:01:59,040
And LLMs don't have that.

1450
01:01:59,040 --> 01:02:01,110
- But see, there it's present.

1451
01:02:01,110 --> 01:02:02,880
You and I have a common experience of the world

1452
01:02:02,880 --> 01:02:05,880
in terms of the physics of how gravity works

1453
01:02:05,880 --> 01:02:06,713
and stuff like this.

1454
01:02:06,713 --> 01:02:11,713
And that common knowledge of the world,

1455
01:02:11,730 --> 01:02:15,540
I feel like is there in the language.

1456
01:02:15,540 --> 01:02:17,820
We don't explicitly express it,

1457
01:02:17,820 --> 01:02:21,210
but if you have a huge amount of text,

1458
01:02:21,210 --> 01:02:24,808
you're going to get this stuff that's between the lines.

1459
01:02:24,808 --> 01:02:28,650
In order to form a consistent world model,

1460
01:02:28,650 --> 01:02:31,680
you're going to have to understand how gravity works,

1461
01:02:31,680 --> 01:02:34,950
even if you don't have an explicit explanation of gravity.

1462
01:02:34,950 --> 01:02:37,410
So even though, in the case of gravity,

1463
01:02:37,410 --> 01:02:38,580
there is explicit explanation.

1464
01:02:38,580 --> 01:02:40,050
There's gravity in Wikipedia.

1465
01:02:40,050 --> 01:02:44,370
But like the stuff that we think of

1466
01:02:44,370 --> 01:02:46,500
as common sense reasoning,

1467
01:02:46,500 --> 01:02:49,320
I feel like to generate language correctly,

1468
01:02:49,320 --> 01:02:51,603
you're going to have to figure that out.

1469
01:02:51,603 --> 01:02:53,189
Now, you could say as you have,

1470
01:02:53,189 --> 01:02:54,390
there's not enough text- - Well, I agree.

1471
01:02:54,390 --> 01:02:55,881
- Sorry.

1472
01:02:55,881 --> 01:02:56,737
Okay, yeah.

1473
01:02:56,737 --> 01:02:57,570
(laughs)

1474
01:02:57,570 --> 01:02:58,403
You don't think so?

1475
01:02:58,403 --> 01:02:59,236
- No, I agree with what you just said,

1476
01:02:59,236 --> 01:03:03,349
which is that to be able to do high level common sense...

1477
01:03:03,349 --> 01:03:04,800
To have high level common sense,

1478
01:03:04,800 --> 01:03:06,960
you need to have the low level common sense

1479
01:03:06,960 --> 01:03:08,040
to build on top of.

1480
01:03:08,040 --> 01:03:09,030
- [Lex] Yeah.

1481
01:03:09,030 --> 01:03:10,256
But that's not there.

1482
01:03:10,256 --> 01:03:11,463
- That's not there in LLMs.

1483
01:03:11,463 --> 01:03:13,380
LLMs are purely trained from text.

1484
01:03:13,380 --> 01:03:15,010
So then the other statement you made,

1485
01:03:15,010 --> 01:03:16,317
I would not agree

1486
01:03:16,317 --> 01:03:20,370
with the fact that implicit in all languages in the world

1487
01:03:20,370 --> 01:03:22,800
is the underlying reality.

1488
01:03:22,800 --> 01:03:24,480
There's a lot about underlying reality

1489
01:03:24,480 --> 01:03:26,880
which is not expressed in language.

1490
01:03:26,880 --> 01:03:27,990
- Is that obvious to you?

1491
01:03:27,990 --> 01:03:29,013
- Yeah, totally.

1492
01:03:30,330 --> 01:03:34,044
- So like all the conversations we have...

1493
01:03:34,044 --> 01:03:36,300
Okay, there's the dark web,

1494
01:03:36,300 --> 01:03:37,500
meaning whatever,

1495
01:03:37,500 --> 01:03:41,190
the private conversations like DMs and stuff like this,

1496
01:03:41,190 --> 01:03:45,150
which is much, much larger probably than what's available,

1497
01:03:45,150 --> 01:03:46,920
what LLMs are trained on.

1498
01:03:46,920 --> 01:03:48,030
- You don't need to communicate

1499
01:03:48,030 --> 01:03:50,040
the stuff that is common.

1500
01:03:50,040 --> 01:03:51,360
- But the humor, all of it.

1501
01:03:51,360 --> 01:03:52,198
No, you do.

1502
01:03:52,198 --> 01:03:54,647
You don't need to, but it comes through.

1503
01:03:54,647 --> 01:03:58,320
Like if I accidentally knock this over,

1504
01:03:58,320 --> 01:03:59,550
you'll probably make fun of me.

1505
01:03:59,550 --> 01:04:02,520
And in the content of the you making fun of me

1506
01:04:02,520 --> 01:04:07,410
will be explanation of the fact that cups fall

1507
01:04:07,410 --> 01:04:09,420
and then gravity works in this way.

1508
01:04:09,420 --> 01:04:12,750
And then you'll have some very vague information

1509
01:04:12,750 --> 01:04:16,770
about what kind of things explode when they hit the ground.

1510
01:04:16,770 --> 01:04:19,050
And then maybe you'll make a joke about entropy

1511
01:04:19,050 --> 01:04:20,253
or something like this

1512
01:04:20,253 --> 01:04:22,020
and we will never be able to reconstruct this again.

1513
01:04:22,020 --> 01:04:24,769
Like, okay, you'll make a little joke like this

1514
01:04:24,769 --> 01:04:27,090
and there'll be trillion of other jokes.

1515
01:04:27,090 --> 01:04:28,170
And from the jokes,

1516
01:04:28,170 --> 01:04:30,960
you can piece together the fact that gravity works

1517
01:04:30,960 --> 01:04:32,880
and mugs can break and all this kind of stuff,

1518
01:04:32,880 --> 01:04:34,220
you don't need to see...

1519
01:04:35,370 --> 01:04:36,870
It'll be very inefficient.

1520
01:04:36,870 --> 01:04:38,260
It's easier for like

1521
01:04:39,330 --> 01:04:41,700
to not knock the thing over. (laughing)

1522
01:04:41,700 --> 01:04:42,533
- [Yann] Yeah.

1523
01:04:42,533 --> 01:04:44,400
- But I feel like it would be there

1524
01:04:44,400 --> 01:04:46,620
if you have enough of that data.

1525
01:04:46,620 --> 01:04:50,730
- I just think that most of the information of this type

1526
01:04:50,730 --> 01:04:53,497
that we have accumulated when we were babies

1527
01:04:53,497 --> 01:04:58,050
is just not present in text,

1528
01:04:58,050 --> 01:04:59,910
in any description, essentially.

1529
01:04:59,910 --> 01:05:02,875
And the sensory data is a much richer source

1530
01:05:02,875 --> 01:05:04,560
for getting that kind of understanding.

1531
01:05:04,560 --> 01:05:06,093
I mean, that's the 16,000 hours

1532
01:05:06,093 --> 01:05:09,120
of wake time of a 4-year-old.

1533
01:05:09,120 --> 01:05:12,540
And tend to do 15 bytes, going through vision.

1534
01:05:12,540 --> 01:05:13,530
Just vision, right?

1535
01:05:13,530 --> 01:05:17,640
There is a similar bandwidth of touch

1536
01:05:17,640 --> 01:05:20,520
and a little less through audio.

1537
01:05:20,520 --> 01:05:21,510
And then text doesn't...

1538
01:05:21,510 --> 01:05:26,510
Language doesn't come in until like a year in life.

1539
01:05:26,790 --> 01:05:28,620
And by the time you are nine years old,

1540
01:05:28,620 --> 01:05:30,810
you've learned about gravity,

1541
01:05:30,810 --> 01:05:31,920
you know about inertia,

1542
01:05:31,920 --> 01:05:32,753
you know about gravity,

1543
01:05:32,753 --> 01:05:33,990
you know there's stability,

1544
01:05:35,069 --> 01:05:36,330
you know about the distinction

1545
01:05:36,330 --> 01:05:38,850
between animate and inanimate objects.

1546
01:05:38,850 --> 01:05:39,720
By 18 months,

1547
01:05:39,720 --> 01:05:42,906
you know about like why people want to do things

1548
01:05:42,906 --> 01:05:45,510
and you help them if they can't.

1549
01:05:45,510 --> 01:05:47,730
I mean there's a lot of things that you learn

1550
01:05:47,730 --> 01:05:49,740
mostly by observation,

1551
01:05:49,740 --> 01:05:52,010
really not even through interaction.

1552
01:05:52,010 --> 01:05:53,460
In the first few months of life,

1553
01:05:53,460 --> 01:05:55,980
babies don't really have any influence on the world.

1554
01:05:55,980 --> 01:05:58,110
They can only observe, right?

1555
01:05:58,110 --> 01:06:02,100
And you accumulate like a gigantic amount of knowledge

1556
01:06:02,100 --> 01:06:03,000
just from that.

1557
01:06:03,000 --> 01:06:06,423
So that's what we're missing from current AI systems.

1558
01:06:07,500 --> 01:06:10,080
- I think in one of your slides you have this nice plot

1559
01:06:10,080 --> 01:06:13,980
that is one of the ways you show that LLMs are limited.

1560
01:06:13,980 --> 01:06:16,170
I wonder if you could talk about hallucinations

1561
01:06:16,170 --> 01:06:17,320
from your perspectives.

1562
01:06:18,312 --> 01:06:23,312
Why hallucinations happen from large language models,

1563
01:06:23,520 --> 01:06:27,570
and to what degree is that a fundamental flaw

1564
01:06:27,570 --> 01:06:29,400
of large language models.

1565
01:06:29,400 --> 01:06:30,233
- Right.

1566
01:06:30,233 --> 01:06:34,140
So because of the autoregressive prediction,

1567
01:06:34,140 --> 01:06:37,898
every time an LLM produces a token or a word,

1568
01:06:37,898 --> 01:06:40,800
there is some level of probability for that word

1569
01:06:40,800 --> 01:06:44,133
to take you out of the set of reasonable answers.

1570
01:06:45,229 --> 01:06:46,650
And if you assume,

1571
01:06:46,650 --> 01:06:48,030
which is a very strong assumption,

1572
01:06:48,030 --> 01:06:50,650
that the probability of such error

1573
01:06:53,190 --> 01:06:55,200
is those errors are independent

1574
01:06:55,200 --> 01:06:59,520
across a sequence of tokens being produced.

1575
01:06:59,520 --> 01:07:02,430
What that means is that every time you produce a token,

1576
01:07:02,430 --> 01:07:03,263
the probability

1577
01:07:03,263 --> 01:07:06,990
that you stay within the set of correct answer decreases

1578
01:07:06,990 --> 01:07:08,700
and it decreases exponentially.

1579
01:07:08,700 --> 01:07:11,486
- So there's a strong, like you said, assumption there

1580
01:07:11,486 --> 01:07:14,820
that if there's a non-zero probability of making a mistake,

1581
01:07:14,820 --> 01:07:16,320
which there appears to be,

1582
01:07:16,320 --> 01:07:18,720
then there's going to be a kind of drift.

1583
01:07:18,720 --> 01:07:19,920
- Yeah.

1584
01:07:19,920 --> 01:07:21,390
And that drift is exponential.

1585
01:07:21,390 --> 01:07:23,790
It's like errors accumulate, right?

1586
01:07:23,790 --> 01:07:27,377
So the probability that an answer would be nonsensical

1587
01:07:27,377 --> 01:07:31,440
increases exponentially with the number of tokens.

1588
01:07:31,440 --> 01:07:33,840
- Is that obvious to you by the way?

1589
01:07:33,840 --> 01:07:36,810
Well, so mathematically speaking maybe,

1590
01:07:36,810 --> 01:07:39,988
but like isn't there a kind of gravitational pull

1591
01:07:39,988 --> 01:07:41,100
towards the truth?

1592
01:07:41,100 --> 01:07:44,430
Because on average, hopefully,

1593
01:07:44,430 --> 01:07:48,990
the truth is well represented in the training set.

1594
01:07:48,990 --> 01:07:50,970
- No, it's basically a struggle

1595
01:07:50,970 --> 01:07:55,560
against the curse of dimensionality.

1596
01:07:55,560 --> 01:07:57,090
So the way you can correct for this

1597
01:07:57,090 --> 01:07:58,750
is that you fine tune the system

1598
01:07:59,610 --> 01:08:01,590
by having it produce answers

1599
01:08:01,590 --> 01:08:04,890
for all kinds of questions that people might come up with.

1600
01:08:04,890 --> 01:08:06,780
And people are people,

1601
01:08:06,780 --> 01:08:08,760
so a lot of the questions that they have

1602
01:08:08,760 --> 01:08:10,320
are very similar to each other.

1603
01:08:10,320 --> 01:08:11,550
So you can probably cover,

1604
01:08:11,550 --> 01:08:16,550
you know, 80% or whatever of questions that people will ask

1605
01:08:16,822 --> 01:08:20,162
by collecting data.

1606
01:08:22,050 --> 01:08:23,189
And then you fine tune the system

1607
01:08:23,189 --> 01:08:25,680
to produce good answers for all of those things.

1608
01:08:25,680 --> 01:08:27,827
And it's probably gonna be able to learn that

1609
01:08:27,827 --> 01:08:31,622
because it's got a lot of capacity to learn.

1610
01:08:31,622 --> 01:08:36,622
But then there is the enormous set of prompts

1611
01:08:36,990 --> 01:08:39,930
that you have not covered during training.

1612
01:08:39,930 --> 01:08:41,100
And that set is enormous.

1613
01:08:41,100 --> 01:08:43,319
Like within the set of all possible prompts,

1614
01:08:43,319 --> 01:08:47,399
the proportion of prompts that have been used for training

1615
01:08:47,399 --> 01:08:48,693
is absolutely tiny.

1616
01:08:49,667 --> 01:08:53,970
It's a tiny, tiny, tiny subset of all possible prompts.

1617
01:08:53,970 --> 01:08:56,130
And so the system will behave properly

1618
01:08:56,130 --> 01:08:58,050
on the prompts that it's been either trained,

1619
01:08:58,050 --> 01:08:59,403
pre-trained or fine tuned.

1620
01:09:01,529 --> 01:09:04,229
But then there is an entire space of things

1621
01:09:04,229 --> 01:09:06,899
that it cannot possibly have been trained on

1622
01:09:06,899 --> 01:09:09,312
because it's just the number is gigantic.

1623
01:09:09,312 --> 01:09:13,120
So whatever training the system

1624
01:09:14,032 --> 01:09:18,060
has been subject to produce appropriate answers,

1625
01:09:18,060 --> 01:09:20,580
you can break it by finding out a prompt

1626
01:09:20,580 --> 01:09:24,569
that will be outside of the set of prompts

1627
01:09:24,569 --> 01:09:25,590
it's been trained on

1628
01:09:25,590 --> 01:09:27,330
or things that are similar,

1629
01:09:27,330 --> 01:09:29,991
and then it will just spew complete nonsense.

1630
01:09:29,991 --> 01:09:31,067
- When you say prompt,

1631
01:09:31,067 --> 01:09:33,510
do you mean that exact prompt

1632
01:09:33,510 --> 01:09:36,060
or do you mean a prompt that's like,

1633
01:09:36,060 --> 01:09:38,729
in many parts very different than...

1634
01:09:38,729 --> 01:09:42,630
Is it that easy to ask a question

1635
01:09:42,630 --> 01:09:45,540
or to say a thing that hasn't been said before

1636
01:09:45,540 --> 01:09:46,380
on the internet?

1637
01:09:46,380 --> 01:09:48,359
- I mean, people have come up with things

1638
01:09:48,359 --> 01:09:51,210
where like you put essentially

1639
01:09:51,210 --> 01:09:53,220
a random sequence of characters in a prompt

1640
01:09:53,220 --> 01:09:57,690
and that's enough to kind of throw the system into a mode

1641
01:09:57,690 --> 01:10:00,450
where it's gonna answer something completely different

1642
01:10:00,450 --> 01:10:03,420
than it would have answered without this.

1643
01:10:03,420 --> 01:10:05,430
So that's a way to jailbreak the system, basically.

1644
01:10:05,430 --> 01:10:09,380
Go outside of its conditioning, right?

1645
01:10:09,380 --> 01:10:11,547
- So that's a very clear demonstration of it.

1646
01:10:11,547 --> 01:10:16,547
But of course, that goes outside

1647
01:10:16,860 --> 01:10:19,260
of what it's designed to do, right?

1648
01:10:19,260 --> 01:10:20,850
If you actually stitch together

1649
01:10:20,850 --> 01:10:22,800
reasonably grammatical sentences,

1650
01:10:22,800 --> 01:10:26,460
is it that easy to break it?

1651
01:10:26,460 --> 01:10:27,293
- Yeah.

1652
01:10:27,293 --> 01:10:29,010
Some people have done things like

1653
01:10:29,010 --> 01:10:31,090
you write a sentence in English

1654
01:10:32,430 --> 01:10:33,780
or you ask a question in English

1655
01:10:33,780 --> 01:10:36,690
and it produces a perfectly fine answer.

1656
01:10:36,690 --> 01:10:38,710
And then you just substitute a few words

1657
01:10:39,690 --> 01:10:42,480
by the same word in another language,

1658
01:10:42,480 --> 01:10:44,250
and all of a sudden the answer is complete nonsense.

1659
01:10:44,250 --> 01:10:45,083
- Yeah.

1660
01:10:45,083 --> 01:10:46,860
So I guess what I'm saying is like,

1661
01:10:46,860 --> 01:10:51,860
which fraction of prompts that humans are likely to generate

1662
01:10:52,470 --> 01:10:54,360
are going to break the system?

1663
01:10:54,360 --> 01:10:57,600
- So the problem is that there is a long tail.

1664
01:10:57,600 --> 01:10:58,590
- [Lex] Yes.

1665
01:10:58,590 --> 01:11:02,196
- This is an issue that a lot of people have realized

1666
01:11:02,196 --> 01:11:04,080
in social networks and stuff like that,

1667
01:11:04,080 --> 01:11:05,834
which is there's a very, very long tail

1668
01:11:05,834 --> 01:11:07,695
of things that people will ask.

1669
01:11:07,695 --> 01:11:09,839
And you can fine tune the system

1670
01:11:09,839 --> 01:11:12,900
for the 80% or whatever

1671
01:11:12,900 --> 01:11:16,170
of the things that most people will ask.

1672
01:11:16,170 --> 01:11:18,660
And then this long tail is so large

1673
01:11:18,660 --> 01:11:20,730
that you're not gonna be able to fine tune the system

1674
01:11:20,730 --> 01:11:21,900
for all the conditions.

1675
01:11:21,900 --> 01:11:22,733
And in the end,

1676
01:11:22,733 --> 01:11:23,760
the system ends up being

1677
01:11:23,760 --> 01:11:25,680
kind of a giant lookup table, right? (laughing)

1678
01:11:25,680 --> 01:11:26,760
Essentially.

1679
01:11:26,760 --> 01:11:27,810
Which is not really what you want.

1680
01:11:27,810 --> 01:11:29,610
You want systems that can reason,

1681
01:11:29,610 --> 01:11:30,780
certainly that can plan.

1682
01:11:30,780 --> 01:11:33,810
So the type of reasoning that takes place in LLM

1683
01:11:33,810 --> 01:11:35,490
is very, very primitive.

1684
01:11:35,490 --> 01:11:37,050
And the reason you can tell it's primitive

1685
01:11:37,050 --> 01:11:39,113
is because the amount of computation

1686
01:11:39,113 --> 01:11:43,800
that is spent per token produced is constant.

1687
01:11:43,800 --> 01:11:45,840
So if you ask a question

1688
01:11:45,840 --> 01:11:50,310
and that question has an answer in a given number of token,

1689
01:11:50,310 --> 01:11:52,770
the amount of computation devoted to computing that answer

1690
01:11:52,770 --> 01:11:54,933
can be exactly estimated.

1691
01:11:56,959 --> 01:11:59,728
It's the size of the prediction network

1692
01:11:59,728 --> 01:12:03,093
with its 36 layers or 92 layers or whatever it is,

1693
01:12:03,930 --> 01:12:05,460
multiplied by number of tokens.

1694
01:12:05,460 --> 01:12:06,293
That's it.

1695
01:12:06,293 --> 01:12:08,310
And so essentially,

1696
01:12:08,310 --> 01:12:10,560
it doesn't matter if the question being asked

1697
01:12:12,060 --> 01:12:16,620
is simple to answer, complicated to answer,

1698
01:12:16,620 --> 01:12:17,760
impossible to answer

1699
01:12:17,760 --> 01:12:20,060
because it's decided, well, there's something.

1700
01:12:21,180 --> 01:12:22,140
The amount of computation

1701
01:12:22,140 --> 01:12:25,590
the system will be able to devote to the answer is constant

1702
01:12:25,590 --> 01:12:27,870
or is proportional to the number of token produced

1703
01:12:27,870 --> 01:12:29,670
in the answer, right?

1704
01:12:29,670 --> 01:12:30,840
This is not the way we work,

1705
01:12:30,840 --> 01:12:33,990
the way we reason is that

1706
01:12:33,990 --> 01:12:37,050
when we are faced with a complex problem

1707
01:12:37,050 --> 01:12:38,460
or a complex question,

1708
01:12:38,460 --> 01:12:42,720
we spend more time trying to solve it and answer it, right?

1709
01:12:42,720 --> 01:12:43,800
Because it's more difficult.

1710
01:12:43,800 --> 01:12:45,480
- There's a prediction element,

1711
01:12:45,480 --> 01:12:47,250
there's an iterative element

1712
01:12:47,250 --> 01:12:52,250
where you're like adjusting your understanding of a thing

1713
01:12:52,410 --> 01:12:54,810
by going over and over and over.

1714
01:12:54,810 --> 01:12:56,670
There's a hierarchical elements on.

1715
01:12:56,670 --> 01:12:59,457
Does this mean it's a fundamental flaw of LLMs-

1716
01:12:59,457 --> 01:13:00,350
- [Yann] Yeah.

1717
01:13:00,350 --> 01:13:01,823
- Or does it mean that... (laughs)

1718
01:13:01,823 --> 01:13:03,263
There's more part to that question?

1719
01:13:03,263 --> 01:13:04,710
(laughs)

1720
01:13:04,710 --> 01:13:06,734
Now you're just behaving like an LLM.

1721
01:13:06,734 --> 01:13:07,650
(laughs)

1722
01:13:07,650 --> 01:13:08,760
Immediately answering.

1723
01:13:08,760 --> 01:13:13,760
No, that it's just the low level world model

1724
01:13:14,100 --> 01:13:17,190
on top of which we can then build

1725
01:13:17,190 --> 01:13:18,750
some of these kinds of mechanisms,

1726
01:13:18,750 --> 01:13:23,543
like you said, persistent long-term memory or reasoning,

1727
01:13:23,543 --> 01:13:25,320
so on.

1728
01:13:25,320 --> 01:13:29,190
But we need that world model that comes from language.

1729
01:13:29,190 --> 01:13:30,780
Maybe it is not so difficult

1730
01:13:30,780 --> 01:13:33,720
to build this kind of reasoning system

1731
01:13:33,720 --> 01:13:36,780
on top of a well constructed world model.

1732
01:13:36,780 --> 01:13:37,613
- Okay.

1733
01:13:37,613 --> 01:13:38,490
Whether it's difficult or not,

1734
01:13:38,490 --> 01:13:40,950
the near future will say,

1735
01:13:40,950 --> 01:13:43,620
because a lot of people are working on reasoning

1736
01:13:43,620 --> 01:13:46,743
and planning abilities for dialogue systems.

1737
01:13:47,850 --> 01:13:50,703
I mean, even if we restrict ourselves to language,

1738
01:13:52,590 --> 01:13:53,700
just having the ability

1739
01:13:53,700 --> 01:13:55,623
to plan your answer before you answer,

1740
01:13:56,572 --> 01:13:59,460
in terms that are not necessarily linked

1741
01:13:59,460 --> 01:14:02,010
with the language you're gonna use to produce the answer.

1742
01:14:02,010 --> 01:14:02,843
Right?

1743
01:14:02,843 --> 01:14:04,020
So this idea of this mental model

1744
01:14:04,020 --> 01:14:06,000
that allows you to plan what you're gonna say

1745
01:14:06,000 --> 01:14:06,903
before you say it.

1746
01:14:09,600 --> 01:14:11,730
That is very important.

1747
01:14:11,730 --> 01:14:13,890
I think there's going to be a lot of systems

1748
01:14:13,890 --> 01:14:14,850
over the next few years

1749
01:14:14,850 --> 01:14:17,400
that are going to have this capability,

1750
01:14:17,400 --> 01:14:19,680
but the blueprint of those systems

1751
01:14:19,680 --> 01:14:23,190
will be extremely different from autoregressive LLMs.

1752
01:14:23,190 --> 01:14:27,990
So it's the same difference

1753
01:14:27,990 --> 01:14:29,550
as the difference between

1754
01:14:29,550 --> 01:14:31,830
what psychology has called system one and system two

1755
01:14:31,830 --> 01:14:32,663
in humans, right?

1756
01:14:32,663 --> 01:14:35,640
So system one is the type of task that you can accomplish

1757
01:14:35,640 --> 01:14:39,060
without like deliberately consciously think about

1758
01:14:39,060 --> 01:14:40,320
how you do them.

1759
01:14:40,320 --> 01:14:42,120
You just do them.

1760
01:14:42,120 --> 01:14:43,080
You've done them enough

1761
01:14:43,080 --> 01:14:45,091
that you can just do it subconsciously, right?

1762
01:14:45,091 --> 01:14:46,290
Without thinking about them.

1763
01:14:46,290 --> 01:14:48,600
If you're an experienced driver,

1764
01:14:48,600 --> 01:14:51,120
you can drive without really thinking about it

1765
01:14:51,120 --> 01:14:52,740
and you can talk to someone at the same time

1766
01:14:52,740 --> 01:14:54,240
or listen to the radio, right?

1767
01:14:55,650 --> 01:14:58,320
If you are a very experienced chess player,

1768
01:14:58,320 --> 01:15:01,050
you can play against a non-experienced chess player

1769
01:15:01,050 --> 01:15:02,580
without really thinking either,

1770
01:15:02,580 --> 01:15:05,054
you just recognize the pattern and you play, right?

1771
01:15:05,054 --> 01:15:06,633
That's system one.

1772
01:15:08,190 --> 01:15:09,750
So all the things that you do instinctively

1773
01:15:09,750 --> 01:15:12,660
without really having to deliberately plan

1774
01:15:12,660 --> 01:15:13,493
and think about it.

1775
01:15:13,493 --> 01:15:15,210
And then there is other tasks where you need to plan.

1776
01:15:15,210 --> 01:15:19,139
So if you are a not too experienced chess player

1777
01:15:19,139 --> 01:15:20,670
or you are experienced

1778
01:15:20,670 --> 01:15:22,980
but you play against another experienced chess player,

1779
01:15:22,980 --> 01:15:24,780
you think about all kinds of options, right?

1780
01:15:24,780 --> 01:15:27,284
You think about it for a while, right?

1781
01:15:27,284 --> 01:15:30,005
And you're much better if you have time to think about it

1782
01:15:30,005 --> 01:15:35,005
than you are if you play blitz with limited time.

1783
01:15:35,347 --> 01:15:39,303
And so this type of deliberate planning,

1784
01:15:40,590 --> 01:15:43,947
which uses your internal world model, that's system two,

1785
01:15:43,947 --> 01:15:46,510
this is what LLMs currently cannot do.

1786
01:15:46,510 --> 01:15:48,570
How do we get them to do this, right?

1787
01:15:48,570 --> 01:15:50,370
How do we build a system

1788
01:15:50,370 --> 01:15:55,050
that can do this kind of planning or reasoning

1789
01:15:55,050 --> 01:15:57,122
that devotes more resources

1790
01:15:57,122 --> 01:16:00,330
to complex problems than to simple problems.

1791
01:16:00,330 --> 01:16:01,170
And it's not going to be

1792
01:16:01,170 --> 01:16:03,513
autoregressive prediction of tokens,

1793
01:16:03,513 --> 01:16:08,040
it's going to be more something akin to inference

1794
01:16:08,040 --> 01:16:09,480
of latent variables

1795
01:16:09,480 --> 01:16:14,480
in what used to be called probabilistic models

1796
01:16:14,880 --> 01:16:17,700
or graphical models and things of that type.

1797
01:16:17,700 --> 01:16:19,700
So basically the principle is like this.

1798
01:16:21,000 --> 01:16:24,633
The prompt is like observed variables.

1799
01:16:26,010 --> 01:16:29,580
And what the model does

1800
01:16:29,580 --> 01:16:33,360
is that it's basically a measure of...

1801
01:16:33,360 --> 01:16:36,180
It can measure to what extent an answer

1802
01:16:36,180 --> 01:16:37,885
is a good answer for a prompt.

1803
01:16:37,885 --> 01:16:38,970
Okay?

1804
01:16:38,970 --> 01:16:41,190
So think of it as some gigantic neural net,

1805
01:16:41,190 --> 01:16:42,630
but it's got only one output.

1806
01:16:42,630 --> 01:16:45,180
And that output is a scaler number,

1807
01:16:45,180 --> 01:16:47,070
which is let's say zero

1808
01:16:47,070 --> 01:16:49,920
if the answer is a good answer for the question,

1809
01:16:49,920 --> 01:16:51,120
and a large number

1810
01:16:51,120 --> 01:16:53,520
if the answer is not a good answer for the question.

1811
01:16:53,520 --> 01:16:55,500
Imagine you had this model.

1812
01:16:55,500 --> 01:16:56,610
If you had such a model,

1813
01:16:56,610 --> 01:16:58,890
you could use it to produce good answers.

1814
01:16:58,890 --> 01:17:02,490
The way you would do is produce the prompt

1815
01:17:02,490 --> 01:17:05,280
and then search through the space of possible answers

1816
01:17:05,280 --> 01:17:07,473
for one that minimizes that number.

1817
01:17:10,110 --> 01:17:11,580
That's called an energy based model.

1818
01:17:11,580 --> 01:17:14,040
- But that energy based model

1819
01:17:14,040 --> 01:17:18,540
would need the model constructed by the LLM.

1820
01:17:18,540 --> 01:17:20,940
- Well, so really what you need to do

1821
01:17:20,940 --> 01:17:24,466
would be to not search over possible strings of text

1822
01:17:24,466 --> 01:17:27,780
that minimize that energy.

1823
01:17:27,780 --> 01:17:28,613
But what you would do

1824
01:17:28,613 --> 01:17:31,050
is do this in abstract representation space.

1825
01:17:31,050 --> 01:17:34,470
So in sort of the space of abstract thoughts,

1826
01:17:34,470 --> 01:17:37,020
you would elaborate a thought, right?

1827
01:17:37,020 --> 01:17:42,020
Using this process of minimizing the output of your model.

1828
01:17:42,090 --> 01:17:42,923
Okay?

1829
01:17:42,923 --> 01:17:44,673
Which is just a scaler.

1830
01:17:44,673 --> 01:17:46,440
It's an optimization process, right?

1831
01:17:46,440 --> 01:17:48,969
So now the way the system produces its answer

1832
01:17:48,969 --> 01:17:50,800
is through optimization

1833
01:17:51,875 --> 01:17:56,040
by minimizing an objective function basically, right?

1834
01:17:56,040 --> 01:17:57,720
And this is, we're talking about inference,

1835
01:17:57,720 --> 01:17:59,157
we're not talking about training, right?

1836
01:17:59,157 --> 01:18:01,050
The system has been trained already.

1837
01:18:01,050 --> 01:18:03,030
So now we have an abstract representation

1838
01:18:03,030 --> 01:18:04,950
of the thought of the answer,

1839
01:18:04,950 --> 01:18:06,690
representation of the answer.

1840
01:18:06,690 --> 01:18:10,287
We feed that to basically an autoregressive decoder,

1841
01:18:10,287 --> 01:18:11,640
which can be very simple,

1842
01:18:11,640 --> 01:18:15,540
that turns this into a text that expresses this thought.

1843
01:18:15,540 --> 01:18:16,373
Okay?

1844
01:18:16,373 --> 01:18:17,340
So that in my opinion

1845
01:18:17,340 --> 01:18:21,015
is the blueprint of future data systems.

1846
01:18:21,015 --> 01:18:23,430
They will think about their answer,

1847
01:18:23,430 --> 01:18:25,890
plan their answer by optimization

1848
01:18:25,890 --> 01:18:27,971
before turning it into text.

1849
01:18:27,971 --> 01:18:31,260
And that is turning complete.

1850
01:18:31,260 --> 01:18:32,370
- Can you explain exactly

1851
01:18:32,370 --> 01:18:34,470
what the optimization problem there is?

1852
01:18:34,470 --> 01:18:37,212
Like what's the objective function?

1853
01:18:37,212 --> 01:18:38,640
Just linger on it.

1854
01:18:38,640 --> 01:18:40,470
You kind of briefly described it,

1855
01:18:40,470 --> 01:18:43,770
but over what space are you optimizing?

1856
01:18:43,770 --> 01:18:45,810
- The space of representations-

1857
01:18:45,810 --> 01:18:47,162
- Goes abstract representation.

1858
01:18:47,162 --> 01:18:47,995
- That's right.

1859
01:18:47,995 --> 01:18:51,570
So you have an abstract representation inside the system.

1860
01:18:51,570 --> 01:18:52,470
You have a prompt.

1861
01:18:52,470 --> 01:18:53,610
The prompt goes through an encoder,

1862
01:18:53,610 --> 01:18:55,140
produces a representation,

1863
01:18:55,140 --> 01:18:56,340
perhaps goes through a predictor

1864
01:18:56,340 --> 01:18:58,250
that predicts a representation of the answer,

1865
01:18:58,250 --> 01:18:59,790
of the proper answer.

1866
01:18:59,790 --> 01:19:03,840
But that representation may not be a good answer

1867
01:19:03,840 --> 01:19:06,570
because there might be some complicated reasoning

1868
01:19:06,570 --> 01:19:07,560
you need to do, right?

1869
01:19:07,560 --> 01:19:11,220
So then you have another process

1870
01:19:11,220 --> 01:19:15,540
that takes the representation of the answers and modifies it

1871
01:19:15,540 --> 01:19:20,040
so as to minimize a cost function

1872
01:19:20,040 --> 01:19:21,150
that measures to what extent

1873
01:19:21,150 --> 01:19:22,980
the answer is a good answer for the question.

1874
01:19:22,980 --> 01:19:27,563
Now we sort of ignore the fact for...

1875
01:19:27,563 --> 01:19:29,730
I mean, the issue for a moment

1876
01:19:29,730 --> 01:19:30,990
of how you train that system

1877
01:19:30,990 --> 01:19:35,358
to measure whether an answer is a good answer for sure.

1878
01:19:35,358 --> 01:19:38,940
- But suppose such a system could be created,

1879
01:19:38,940 --> 01:19:40,350
what's the process?

1880
01:19:40,350 --> 01:19:42,412
This kind of search like process.

1881
01:19:42,412 --> 01:19:44,010
- It's an optimization process.

1882
01:19:44,010 --> 01:19:47,640
You can do this if the entire system is differentiable,

1883
01:19:47,640 --> 01:19:49,470
that scaler output

1884
01:19:49,470 --> 01:19:52,631
is the result of running through some neural net,

1885
01:19:52,631 --> 01:19:54,420
running the answer,

1886
01:19:54,420 --> 01:19:56,850
the representation of the answer through some neural net.

1887
01:19:56,850 --> 01:19:58,050
Then by gradient descent,

1888
01:19:58,050 --> 01:20:00,600
by back propagating gradients,

1889
01:20:00,600 --> 01:20:01,680
you can figure out

1890
01:20:01,680 --> 01:20:03,900
like how to modify the representation of the answers

1891
01:20:03,900 --> 01:20:05,160
so as to minimize that.

1892
01:20:05,160 --> 01:20:06,660
- So that's still a gradient based.

1893
01:20:06,660 --> 01:20:08,610
- It's gradient based inference.

1894
01:20:08,610 --> 01:20:10,470
So now you have a representation of the answer

1895
01:20:10,470 --> 01:20:12,060
in abstract space.

1896
01:20:12,060 --> 01:20:14,274
Now you can turn it into text, right?

1897
01:20:14,274 --> 01:20:17,540
And the cool thing about this

1898
01:20:17,540 --> 01:20:20,370
is that the representation now

1899
01:20:20,370 --> 01:20:22,500
can be optimized through gradient descent,

1900
01:20:22,500 --> 01:20:24,630
but also is independent of the language

1901
01:20:24,630 --> 01:20:27,570
in which you're going to express the answer.

1902
01:20:27,570 --> 01:20:28,403
- Right.

1903
01:20:28,403 --> 01:20:30,090
So you're operating in the substruct of representation.

1904
01:20:30,090 --> 01:20:32,430
I mean this goes back to the joint embedding.

1905
01:20:32,430 --> 01:20:33,263
- [Yann] Right.

1906
01:20:33,263 --> 01:20:36,108
- That it's better to work in the space of...

1907
01:20:36,108 --> 01:20:37,230
I don't know.

1908
01:20:37,230 --> 01:20:39,180
Or to romanticize the notion

1909
01:20:39,180 --> 01:20:40,620
like space of concepts

1910
01:20:40,620 --> 01:20:45,620
versus the space of concrete sensory information.

1911
01:20:45,720 --> 01:20:47,250
- Right.

1912
01:20:47,250 --> 01:20:48,150
- Okay.

1913
01:20:48,150 --> 01:20:50,670
But can this do something like reasoning,

1914
01:20:50,670 --> 01:20:51,900
which is what we're talking about?

1915
01:20:51,900 --> 01:20:53,543
- Well, not really,

1916
01:20:53,543 --> 01:20:54,376
only in a very simple way.

1917
01:20:54,376 --> 01:20:57,150
I mean basically you can think of those things as doing

1918
01:20:57,150 --> 01:20:59,575
the kind of optimization I was talking about,

1919
01:20:59,575 --> 01:21:01,949
except they're optimizing the discrete space

1920
01:21:01,949 --> 01:21:05,370
which is the space of possible sequences of tokens.

1921
01:21:05,370 --> 01:21:09,240
And they do this optimization in a horribly inefficient way,

1922
01:21:09,240 --> 01:21:11,250
which is generate a lot of hypothesis

1923
01:21:11,250 --> 01:21:13,380
and then select the best ones.

1924
01:21:13,380 --> 01:21:16,560
And that's incredibly wasteful

1925
01:21:16,560 --> 01:21:18,850
in terms of competition,

1926
01:21:18,850 --> 01:21:20,727
'cause you basically have to run your LLM

1927
01:21:20,727 --> 01:21:24,813
for like every possible generative sequence.

1928
01:21:25,680 --> 01:21:27,423
And it's incredibly wasteful.

1929
01:21:28,830 --> 01:21:31,922
So it's much better to do an optimization

1930
01:21:31,922 --> 01:21:33,690
in continuous space

1931
01:21:33,690 --> 01:21:34,950
where you can do gradient descent

1932
01:21:34,950 --> 01:21:36,660
as opposed to like generate tons of things

1933
01:21:36,660 --> 01:21:38,100
and then select the best,

1934
01:21:38,100 --> 01:21:41,010
you just iteratively refine your answer

1935
01:21:41,010 --> 01:21:42,536
to go towards the best, right?

1936
01:21:42,536 --> 01:21:44,190
That's much more efficient.

1937
01:21:44,190 --> 01:21:46,500
But you can only do this in continuous spaces

1938
01:21:46,500 --> 01:21:48,120
with differentiable functions.

1939
01:21:48,120 --> 01:21:50,280
- You're talking about the reasoning,

1940
01:21:50,280 --> 01:21:54,093
like ability to think deeply or to reason deeply.

1941
01:21:55,110 --> 01:21:58,093
How do you know what is an answer

1942
01:21:58,093 --> 01:22:03,093
that's better or worse based on deep reasoning?

1943
01:22:04,650 --> 01:22:05,483
- Right.

1944
01:22:05,483 --> 01:22:06,630
So then we're asking the question,

1945
01:22:06,630 --> 01:22:09,090
of conceptually, how do you train an energy based model?

1946
01:22:09,090 --> 01:22:10,202
Right?

1947
01:22:10,202 --> 01:22:11,035
So energy based model

1948
01:22:11,035 --> 01:22:13,863
is a function with a scaler output, just a number.

1949
01:22:14,940 --> 01:22:17,283
You give it two inputs, X and Y,

1950
01:22:18,210 --> 01:22:20,400
and it tells you whether Y is compatible with X or not.

1951
01:22:20,400 --> 01:22:21,750
X you observe,

1952
01:22:21,750 --> 01:22:24,630
let's say it's a prompt, an image, a video, whatever.

1953
01:22:24,630 --> 01:22:28,033
And Y is a proposal for an answer,

1954
01:22:28,033 --> 01:22:30,650
a continuation of video, whatever.

1955
01:22:30,650 --> 01:22:32,389
And it tells you whether Y is compatible with X.

1956
01:22:32,389 --> 01:22:37,389
And the way it tells you that Y is compatible with X

1957
01:22:37,410 --> 01:22:39,750
is that the output of that function would be zero

1958
01:22:39,750 --> 01:22:40,620
if Y is compatible with X,

1959
01:22:40,620 --> 01:22:44,760
it would be a positive number, non-zero

1960
01:22:44,760 --> 01:22:46,353
if Y is not compatible with X.

1961
01:22:47,280 --> 01:22:48,113
Okay.

1962
01:22:48,113 --> 01:22:49,640
How do you train a system like this?

1963
01:22:49,640 --> 01:22:51,840
At a completely general level,

1964
01:22:51,840 --> 01:22:56,130
is you show it pairs of X and Ys that are compatible,

1965
01:22:56,130 --> 01:22:58,770
a question and the corresponding answer.

1966
01:22:58,770 --> 01:23:02,149
And you train the parameters of the big neural net inside

1967
01:23:02,149 --> 01:23:03,633
to produce zero.

1968
01:23:04,620 --> 01:23:05,453
Okay.

1969
01:23:05,453 --> 01:23:07,230
Now that doesn't completely work

1970
01:23:07,230 --> 01:23:08,880
because the system might decide,

1971
01:23:08,880 --> 01:23:11,610
well, I'm just gonna say zero for everything.

1972
01:23:11,610 --> 01:23:12,930
So now you have to have a process

1973
01:23:12,930 --> 01:23:16,470
to make sure that for a wrong Y,

1974
01:23:16,470 --> 01:23:18,740
the energy will be larger than zero.

1975
01:23:18,740 --> 01:23:20,490
And there you have two options,

1976
01:23:20,490 --> 01:23:21,810
one is contrastive methods.

1977
01:23:21,810 --> 01:23:25,053
So contrastive method is you show an X and a bad Y,

1978
01:23:26,280 --> 01:23:27,113
and you tell the system,

1979
01:23:27,113 --> 01:23:29,640
well, give a high energy to this.

1980
01:23:29,640 --> 01:23:30,840
Like push up the energy, right?

1981
01:23:30,840 --> 01:23:33,060
Change the weights in the neural net that compute the energy

1982
01:23:33,060 --> 01:23:34,353
so that it goes up.

1983
01:23:36,177 --> 01:23:37,746
So that's contrasting methods.

1984
01:23:37,746 --> 01:23:41,250
The problem with this is if the space of Y is large,

1985
01:23:41,250 --> 01:23:43,882
the number of such contrasted samples

1986
01:23:43,882 --> 01:23:47,433
you're gonna have to show is gigantic.

1987
01:23:48,570 --> 01:23:49,830
But people do this.

1988
01:23:49,830 --> 01:23:53,610
They do this when you train a system with RLHF,

1989
01:23:53,610 --> 01:23:55,170
basically what you're training

1990
01:23:55,170 --> 01:23:57,600
is what's called a reward model,

1991
01:23:57,600 --> 01:24:00,120
which is basically an objective function

1992
01:24:00,120 --> 01:24:02,520
that tells you whether an answer is good or bad.

1993
01:24:02,520 --> 01:24:06,960
And that's basically exactly what this is.

1994
01:24:06,960 --> 01:24:08,520
So we already do this to some extent.

1995
01:24:08,520 --> 01:24:09,930
We're just not using it for inference,

1996
01:24:09,930 --> 01:24:11,580
we're just using it for training.

1997
01:24:14,070 --> 01:24:15,930
There is another set of methods

1998
01:24:15,930 --> 01:24:18,878
which are non-contrastive, and I prefer those.

1999
01:24:18,878 --> 01:24:22,817
And those non-contrastive method basically say,

2000
01:24:22,817 --> 01:24:26,040
okay, the energy function

2001
01:24:26,040 --> 01:24:29,618
needs to have low energy on pairs of XYs that are compatible

2002
01:24:29,618 --> 01:24:31,443
that come from your training set.

2003
01:24:32,370 --> 01:24:34,170
How do you make sure that the energy

2004
01:24:34,170 --> 01:24:36,123
is gonna be higher everywhere else?

2005
01:24:37,230 --> 01:24:38,190
And the way you do this

2006
01:24:38,190 --> 01:24:43,190
is by having a regularizer, a criterion,

2007
01:24:43,470 --> 01:24:45,180
a term in your cost function

2008
01:24:45,180 --> 01:24:49,200
that basically minimizes the volume of space

2009
01:24:49,200 --> 01:24:50,450
that can take low energy.

2010
01:24:51,810 --> 01:24:53,130
And the precise way to do this,

2011
01:24:53,130 --> 01:24:55,290
there's all kinds of different specific ways to do this

2012
01:24:55,290 --> 01:24:56,430
depending on the architecture,

2013
01:24:56,430 --> 01:24:58,560
but that's the basic principle.

2014
01:24:58,560 --> 01:25:00,990
So that if you push down the energy function

2015
01:25:00,990 --> 01:25:04,050
for particular regions in the XY space,

2016
01:25:04,050 --> 01:25:06,150
it will automatically go up in other places

2017
01:25:06,150 --> 01:25:09,033
because there's only a limited volume of space

2018
01:25:09,033 --> 01:25:11,130
that can take low energy.

2019
01:25:11,130 --> 01:25:11,963
Okay?

2020
01:25:11,963 --> 01:25:13,560
By the construction of the system

2021
01:25:13,560 --> 01:25:16,770
or by the regularizing function.

2022
01:25:16,770 --> 01:25:18,810
- We've been talking very generally,

2023
01:25:18,810 --> 01:25:21,390
but what is a good X and a good Y?

2024
01:25:21,390 --> 01:25:25,770
What is a good representation of X and Y?

2025
01:25:25,770 --> 01:25:27,240
Because we've been talking about language.

2026
01:25:27,240 --> 01:25:30,450
And if you just take language directly,

2027
01:25:30,450 --> 01:25:32,220
that presumably is not good,

2028
01:25:32,220 --> 01:25:33,053
so there has to be

2029
01:25:33,053 --> 01:25:35,313
some kind of abstract representation of ideas.

2030
01:25:36,150 --> 01:25:37,080
- Yeah.

2031
01:25:37,080 --> 01:25:39,167
I mean you can do this with language directly

2032
01:25:39,167 --> 01:25:42,000
by just, you know, X is a text

2033
01:25:42,000 --> 01:25:43,560
and Y is the continuation of that text.

2034
01:25:43,560 --> 01:25:45,210
- [Lex] Yes.

2035
01:25:45,210 --> 01:25:48,150
- Or X is a question, Y is the answer.

2036
01:25:48,150 --> 01:25:49,680
- But you're saying that's not gonna take it.

2037
01:25:49,680 --> 01:25:52,650
I mean, that's going to do what LLMs are doing.

2038
01:25:52,650 --> 01:25:53,483
- Well, no.

2039
01:25:53,483 --> 01:25:56,610
It depends on how the internal structure of the system

2040
01:25:56,610 --> 01:25:57,443
is built.

2041
01:25:57,443 --> 01:25:59,040
If the internal structure of the system

2042
01:25:59,040 --> 01:26:02,220
is built in such a way that inside of the system

2043
01:26:02,220 --> 01:26:03,540
there is a latent variable,

2044
01:26:03,540 --> 01:26:04,710
let's called it Z,

2045
01:26:04,710 --> 01:26:09,450
that you can manipulate

2046
01:26:09,450 --> 01:26:11,433
so as to minimize the output energy,

2047
01:26:12,900 --> 01:26:16,710
then that Z can be viewed as representation of a good answer

2048
01:26:16,710 --> 01:26:19,410
that you can translate into a Y that is a good answer.

2049
01:26:21,060 --> 01:26:22,680
- So this kind of system could be trained

2050
01:26:22,680 --> 01:26:24,600
in a very similar way?

2051
01:26:24,600 --> 01:26:25,433
- Very similar way.

2052
01:26:25,433 --> 01:26:26,506
But you have to have this way of preventing collapse,

2053
01:26:26,506 --> 01:26:31,304
of ensuring that there is high energy

2054
01:26:31,304 --> 01:26:33,586
for things you don't train it on.

2055
01:26:33,586 --> 01:26:38,450
And currently it's very implicit in LLMs.

2056
01:26:38,450 --> 01:26:39,360
It is done in a way

2057
01:26:39,360 --> 01:26:40,680
that people don't realize it's being done,

2058
01:26:40,680 --> 01:26:42,630
but it is being done.

2059
01:26:42,630 --> 01:26:43,710
It's due to the fact

2060
01:26:43,710 --> 01:26:48,003
that when you give a high probability to a word,

2061
01:26:49,320 --> 01:26:51,360
automatically you give low probability to other words

2062
01:26:51,360 --> 01:26:52,750
because you only have

2063
01:26:53,670 --> 01:26:55,725
a finite amount of probability to go around. (laughing)

2064
01:26:55,725 --> 01:26:56,761
Right?

2065
01:26:56,761 --> 01:26:57,750
They have to sub to one.

2066
01:26:57,750 --> 01:27:00,450
So when you minimize the cross entropy or whatever,

2067
01:27:00,450 --> 01:27:05,197
when you train your LLM to predict the next word,

2068
01:27:05,197 --> 01:27:07,289
you are increasing the probability

2069
01:27:07,289 --> 01:27:09,120
your system will give to the correct word,

2070
01:27:09,120 --> 01:27:10,620
but you're also decreasing the probability

2071
01:27:10,620 --> 01:27:12,300
it will give to the incorrect words.

2072
01:27:12,300 --> 01:27:17,070
Now, indirectly, that gives a low probability to...

2073
01:27:17,070 --> 01:27:19,440
A high probability to sequences of words that are good

2074
01:27:19,440 --> 01:27:21,660
and low probability two sequences of words that are bad,

2075
01:27:21,660 --> 01:27:23,334
but it's very indirect.

2076
01:27:23,334 --> 01:27:26,823
It's not obvious why this actually works at all,

2077
01:27:28,290 --> 01:27:30,331
because you're not doing it on a joint probability

2078
01:27:30,331 --> 01:27:32,355
of all the symbols in a sequence,

2079
01:27:32,355 --> 01:27:34,323
you're just doing it kind of,

2080
01:27:35,940 --> 01:27:37,650
sort of factorized that probability

2081
01:27:37,650 --> 01:27:39,570
in terms of conditional probabilities

2082
01:27:39,570 --> 01:27:41,730
over successive tokens.

2083
01:27:41,730 --> 01:27:43,950
- So how do you do this for visual data?

2084
01:27:43,950 --> 01:27:44,970
- So we've been doing this

2085
01:27:44,970 --> 01:27:47,113
with all JEPA architectures, basically the-

2086
01:27:47,113 --> 01:27:47,946
- [Lex] The joint embedding?

2087
01:27:47,946 --> 01:27:48,779
- I-JEPA.

2088
01:27:50,318 --> 01:27:52,290
So there, the compatibility between two things

2089
01:27:52,290 --> 01:27:56,100
is here's an image or a video,

2090
01:27:56,100 --> 01:27:58,350
here is a corrupted, shifted or transformed version

2091
01:27:58,350 --> 01:28:01,050
of that image or video or masked.

2092
01:28:01,050 --> 01:28:01,883
Okay?

2093
01:28:01,883 --> 01:28:04,200
And then the energy of the system

2094
01:28:04,200 --> 01:28:09,200
is the prediction error of the representation.

2095
01:28:11,899 --> 01:28:14,760
The predicted representation of the good thing

2096
01:28:14,760 --> 01:28:17,550
versus the actual representation of the good thing, right?

2097
01:28:17,550 --> 01:28:20,480
So you run the corrupted image to the system,

2098
01:28:20,480 --> 01:28:24,840
predict the representation of the good input uncorrupted,

2099
01:28:24,840 --> 01:28:26,580
and then compute the prediction error.

2100
01:28:26,580 --> 01:28:28,230
That's the energy of the system.

2101
01:28:28,230 --> 01:28:30,090
So this system will tell you,

2102
01:28:30,090 --> 01:28:35,090
this is a good image and this is a corrupted version.

2103
01:28:36,740 --> 01:28:38,070
It will give you zero energy

2104
01:28:38,070 --> 01:28:41,220
if those two things are effectively,

2105
01:28:41,220 --> 01:28:43,350
one of them is a corrupted version of the other,

2106
01:28:43,350 --> 01:28:44,580
give you a high energy

2107
01:28:44,580 --> 01:28:46,680
if the two images are completely different.

2108
01:28:46,680 --> 01:28:48,570
- And hopefully that whole process

2109
01:28:48,570 --> 01:28:51,390
gives you a really nice compressed representation

2110
01:28:51,390 --> 01:28:54,780
of reality, of visual reality.

2111
01:28:54,780 --> 01:28:55,613
- And we know it does

2112
01:28:55,613 --> 01:28:57,570
because then we use those presentations

2113
01:28:57,570 --> 01:28:59,640
as input to a classification system or something,

2114
01:28:59,640 --> 01:29:00,828
and it works- - And then

2115
01:29:00,828 --> 01:29:01,661
that classification system works really nicely.

2116
01:29:01,661 --> 01:29:03,125
Okay.

2117
01:29:03,125 --> 01:29:04,500
Well, so to summarize,

2118
01:29:04,500 --> 01:29:09,500
you recommend in a spicy way that only Yann LeCun can,

2119
01:29:10,440 --> 01:29:12,750
you recommend that we abandon generative models

2120
01:29:12,750 --> 01:29:14,970
in favor of joint embedding architectures?

2121
01:29:14,970 --> 01:29:16,109
- [Yann] Yes.

2122
01:29:16,109 --> 01:29:17,550
- Abandon autoregressive generation.

2123
01:29:17,550 --> 01:29:18,383
- [Yann] Yes.

2124
01:29:18,383 --> 01:29:19,740
- Abandon... (laughs)

2125
01:29:19,740 --> 01:29:21,780
This feels like court testimony.

2126
01:29:21,780 --> 01:29:23,700
Abandon probabilistic models

2127
01:29:23,700 --> 01:29:26,250
in favor of energy based models, as we talked about.

2128
01:29:26,250 --> 01:29:27,870
Abandon contrastive methods

2129
01:29:27,870 --> 01:29:30,150
in favor of regularized methods.

2130
01:29:30,150 --> 01:29:32,103
And let me ask you about this;

2131
01:29:33,210 --> 01:29:36,300
you've been for a while, a critic of reinforcement learning.

2132
01:29:36,300 --> 01:29:37,133
- [Yann] Yes.

2133
01:29:37,133 --> 01:29:41,310
- So the last recommendation is that we abandon RL

2134
01:29:41,310 --> 01:29:43,650
in favor of model predictive control,

2135
01:29:43,650 --> 01:29:45,090
as you were talking about.

2136
01:29:45,090 --> 01:29:46,560
And only use RL

2137
01:29:46,560 --> 01:29:50,460
when planning doesn't yield the predicted outcome.

2138
01:29:50,460 --> 01:29:52,800
And we use RL in that case

2139
01:29:52,800 --> 01:29:55,710
to adjust the world model or the critic.

2140
01:29:55,710 --> 01:29:57,198
- [Yann] Yes.

2141
01:29:57,198 --> 01:30:00,510
- So you've mentioned RLHF,

2142
01:30:00,510 --> 01:30:02,970
reinforcement learning with human feedback.

2143
01:30:02,970 --> 01:30:05,850
Why do you still hate reinforcement learning?

2144
01:30:05,850 --> 01:30:07,050
- [Yann] I don't hate reinforcement learning,

2145
01:30:07,050 --> 01:30:08,400
and I think it's- - So it's all love?

2146
01:30:08,400 --> 01:30:12,150
- I think it should not be abandoned completely,

2147
01:30:12,150 --> 01:30:14,460
but I think it's use should be minimized

2148
01:30:14,460 --> 01:30:18,390
because it's incredibly inefficient in terms of samples.

2149
01:30:18,390 --> 01:30:21,360
And so the proper way to train a system

2150
01:30:21,360 --> 01:30:24,103
is to first have it learn

2151
01:30:24,103 --> 01:30:27,750
good representations of the world and world models

2152
01:30:27,750 --> 01:30:29,580
from mostly observation,

2153
01:30:29,580 --> 01:30:31,530
maybe a little bit of interactions.

2154
01:30:31,530 --> 01:30:33,060
- And then steer it based on that.

2155
01:30:33,060 --> 01:30:34,260
If the representation is good,

2156
01:30:34,260 --> 01:30:36,810
then the adjustments should be minimal.

2157
01:30:36,810 --> 01:30:37,643
- Yeah.

2158
01:30:37,643 --> 01:30:39,060
Now there's two things.

2159
01:30:39,060 --> 01:30:40,020
If you've learned the world model,

2160
01:30:40,020 --> 01:30:42,690
you can use the world model to plan a sequence of actions

2161
01:30:42,690 --> 01:30:44,490
to arrive at a particular objective.

2162
01:30:45,930 --> 01:30:47,340
You don't need RL,

2163
01:30:47,340 --> 01:30:50,370
unless the way you measure whether you succeed

2164
01:30:50,370 --> 01:30:51,303
might be inexact.

2165
01:30:52,290 --> 01:30:56,260
Your idea of whether you were gonna fall from your bike

2166
01:30:58,470 --> 01:30:59,850
might be wrong,

2167
01:30:59,850 --> 01:31:02,190
or whether the person you're fighting with MMA

2168
01:31:02,190 --> 01:31:03,330
was gonna do something

2169
01:31:03,330 --> 01:31:05,414
and they do something else. (laughing)

2170
01:31:05,414 --> 01:31:09,540
So there's two ways you can be wrong.

2171
01:31:09,540 --> 01:31:12,840
Either your objective function

2172
01:31:12,840 --> 01:31:13,710
does not reflect

2173
01:31:13,710 --> 01:31:16,410
the actual objective function you want to optimize,

2174
01:31:16,410 --> 01:31:19,800
or your world model is inaccurate, right?

2175
01:31:19,800 --> 01:31:22,080
So the prediction you were making

2176
01:31:22,080 --> 01:31:25,320
about what was gonna happen in the world is inaccurate.

2177
01:31:25,320 --> 01:31:27,330
So if you want to adjust your world model

2178
01:31:27,330 --> 01:31:30,930
while you are operating the world

2179
01:31:30,930 --> 01:31:32,730
or your objective function,

2180
01:31:32,730 --> 01:31:35,910
that is basically in the realm of RL.

2181
01:31:35,910 --> 01:31:38,763
This is what RL deals with to some extent, right?

2182
01:31:38,763 --> 01:31:40,820
So adjust your world model.

2183
01:31:40,820 --> 01:31:44,177
And the way to adjust your world model, even in advance,

2184
01:31:44,177 --> 01:31:48,210
is to explore parts of the space with your world model,

2185
01:31:48,210 --> 01:31:50,760
where you know that your world model is inaccurate.

2186
01:31:50,760 --> 01:31:54,120
That's called curiosity basically, or play, right?

2187
01:31:54,120 --> 01:31:55,740
When you play,

2188
01:31:55,740 --> 01:31:58,740
you kind of explore part of the state space

2189
01:31:58,740 --> 01:32:03,740
that you don't want to do for real

2190
01:32:04,179 --> 01:32:05,742
because it might be dangerous,

2191
01:32:05,742 --> 01:32:07,930
but you can adjust your world model

2192
01:32:09,870 --> 01:32:13,050
without killing yourself basically. (laughs)

2193
01:32:13,050 --> 01:32:14,957
So that's what you want to use RL for.

2194
01:32:14,957 --> 01:32:18,750
When it comes time to learning a particular task,

2195
01:32:18,750 --> 01:32:20,580
you already have all the good representations,

2196
01:32:20,580 --> 01:32:21,870
you already have your world model,

2197
01:32:21,870 --> 01:32:25,230
but you need to adjust it for the situation at hand.

2198
01:32:25,230 --> 01:32:26,670
That's when you use RL.

2199
01:32:26,670 --> 01:32:29,640
- Why do you think RLHF works so well?

2200
01:32:29,640 --> 01:32:32,640
This enforcement learning with human feedback,

2201
01:32:32,640 --> 01:32:34,950
why did it have such a transformational effect

2202
01:32:34,950 --> 01:32:38,308
on large language models that came before?

2203
01:32:38,308 --> 01:32:39,960
- So what's had the transformational effect

2204
01:32:39,960 --> 01:32:42,210
is human feedback.

2205
01:32:42,210 --> 01:32:43,590
There is many ways to use it

2206
01:32:43,590 --> 01:32:45,810
and some of it is just purely supervised, actually,

2207
01:32:45,810 --> 01:32:47,490
it's not really reinforcement learning.

2208
01:32:47,490 --> 01:32:49,053
- So it's the HF. (laughing)

2209
01:32:49,053 --> 01:32:50,290
- It's the HF.

2210
01:32:50,290 --> 01:32:53,340
And then there is various ways to use human feedback, right?

2211
01:32:53,340 --> 01:32:56,505
So you can ask humans to rate answers,

2212
01:32:56,505 --> 01:33:00,720
multiple answers that are produced by a world model.

2213
01:33:00,720 --> 01:33:05,580
And then what you do is you train an objective function

2214
01:33:05,580 --> 01:33:07,383
to predict that rating.

2215
01:33:08,280 --> 01:33:11,490
And then you can use that objective function

2216
01:33:11,490 --> 01:33:13,350
to predict whether an answer is good,

2217
01:33:13,350 --> 01:33:15,120
and you can back propagate really through this

2218
01:33:15,120 --> 01:33:16,690
to fine tune your system

2219
01:33:16,690 --> 01:33:19,893
so that it only produces highly rated answers.

2220
01:33:20,730 --> 01:33:22,224
Okay?

2221
01:33:22,224 --> 01:33:23,124
So that's one way.

2222
01:33:24,842 --> 01:33:26,090
So that's like in RL,

2223
01:33:26,090 --> 01:33:29,370
that means training what's called a reward model, right?

2224
01:33:29,370 --> 01:33:30,750
So something that,

2225
01:33:30,750 --> 01:33:31,800
basically your small neural net

2226
01:33:31,800 --> 01:33:35,100
that estimates to what extent an answer is good, right?

2227
01:33:35,100 --> 01:33:36,600
It's very similar to the objective

2228
01:33:36,600 --> 01:33:39,720
I was talking about earlier for planning,

2229
01:33:39,720 --> 01:33:41,310
except now it's not used for planning,

2230
01:33:41,310 --> 01:33:43,210
it's used for fine tuning your system.

2231
01:33:44,490 --> 01:33:45,570
I think it would be much more efficient

2232
01:33:45,570 --> 01:33:46,470
to use it for planning,

2233
01:33:46,470 --> 01:33:49,581
but currently it's used

2234
01:33:49,581 --> 01:33:52,620
to fine tune the parameters of the system.

2235
01:33:52,620 --> 01:33:54,217
Now, there's several ways to do this.

2236
01:33:54,217 --> 01:33:57,054
Some of them are supervised.

2237
01:33:57,054 --> 01:33:59,700
You just ask a human person,

2238
01:33:59,700 --> 01:34:02,370
like what is a good answer for this, right?

2239
01:34:02,370 --> 01:34:04,233
Then you just type the answer.

2240
01:34:05,614 --> 01:34:07,170
I mean, there's lots of ways

2241
01:34:07,170 --> 01:34:09,153
that those systems are being adjusted.

2242
01:34:10,740 --> 01:34:13,590
- Now, a lot of people have been very critical

2243
01:34:13,590 --> 01:34:17,830
of the recently released Google's Gemini 1.5

2244
01:34:19,080 --> 01:34:23,280
for essentially, in my words, I could say super woke.

2245
01:34:23,280 --> 01:34:26,580
Woke in the negative connotation of that word.

2246
01:34:26,580 --> 01:34:30,390
There is some almost hilariously absurd things that it does,

2247
01:34:30,390 --> 01:34:32,760
like it modifies history,

2248
01:34:32,760 --> 01:34:37,560
like generating images of a black George Washington

2249
01:34:37,560 --> 01:34:40,575
or perhaps more seriously

2250
01:34:40,575 --> 01:34:43,260
something that you commented on Twitter,

2251
01:34:43,260 --> 01:34:48,260
which is refusing to comment on or generate images of,

2252
01:34:49,770 --> 01:34:54,770
or even descriptions of Tiananmen Square or the tank men,

2253
01:34:55,590 --> 01:35:00,590
one of the most sort of legendary protest images in history.

2254
01:35:01,200 --> 01:35:04,938
And of course, these images are highly censored

2255
01:35:04,938 --> 01:35:06,780
by the Chinese government.

2256
01:35:06,780 --> 01:35:09,447
And therefore everybody started asking questions

2257
01:35:09,447 --> 01:35:14,447
of what is the process of designing these LLMs?

2258
01:35:14,741 --> 01:35:16,958
What is the role of censorship in these,

2259
01:35:16,958 --> 01:35:19,020
and all that kind of stuff.

2260
01:35:19,020 --> 01:35:22,290
So you commented on Twitter

2261
01:35:22,290 --> 01:35:23,932
saying that open source is the answer.

2262
01:35:23,932 --> 01:35:25,500
(laughs) - Yeah.

2263
01:35:25,500 --> 01:35:26,333
- Essentially.

2264
01:35:26,333 --> 01:35:28,233
So can you explain?

2265
01:35:29,520 --> 01:35:31,200
- I actually made that comment

2266
01:35:31,200 --> 01:35:32,850
on just about every social network I can.

2267
01:35:32,850 --> 01:35:33,933
(Lex laughs)

2268
01:35:33,933 --> 01:35:38,933
And I've made that point multiple times in various forums.

2269
01:35:40,770 --> 01:35:43,110
Here's my point of view on this.

2270
01:35:43,110 --> 01:35:47,310
People can complain that AI systems are biased,

2271
01:35:47,310 --> 01:35:49,740
and they generally are biased

2272
01:35:49,740 --> 01:35:51,642
by the distribution of the training data

2273
01:35:51,642 --> 01:35:53,800
that they've been trained on

2274
01:35:55,110 --> 01:35:57,543
that reflects biases in society.

2275
01:35:59,610 --> 01:36:03,910
And that is potentially offensive to some people

2276
01:36:05,040 --> 01:36:06,870
or potentially not.

2277
01:36:06,870 --> 01:36:10,710
And some techniques to de-bias

2278
01:36:10,710 --> 01:36:13,060
then become offensive to some people

2279
01:36:15,420 --> 01:36:20,420
because of historical incorrectness and things like that.

2280
01:36:23,250 --> 01:36:25,500
And so you can ask the question.

2281
01:36:25,500 --> 01:36:27,203
You can ask two questions.

2282
01:36:27,203 --> 01:36:28,339
The first question is,

2283
01:36:28,339 --> 01:36:30,960
is it possible to produce an AI system that is not biased?

2284
01:36:30,960 --> 01:36:33,390
And the answer is absolutely not.

2285
01:36:33,390 --> 01:36:36,730
And it's not because of technological challenges,

2286
01:36:36,730 --> 01:36:41,370
although there are technological challenges to that.

2287
01:36:41,370 --> 01:36:45,423
It's because bias is in the eye of the beholder.

2288
01:36:46,800 --> 01:36:48,272
Different people may have different ideas

2289
01:36:48,272 --> 01:36:52,737
about what constitutes bias for a lot of things.

2290
01:36:52,737 --> 01:36:57,030
I mean there are facts that are indisputable,

2291
01:36:57,030 --> 01:36:59,730
but there are a lot of opinions or things

2292
01:36:59,730 --> 01:37:01,422
that can be expressed in different ways.

2293
01:37:01,422 --> 01:37:04,340
And so you cannot have an unbiased system,

2294
01:37:04,340 --> 01:37:06,333
that's just an impossibility.

2295
01:37:08,760 --> 01:37:12,627
And so what's the answer to this?

2296
01:37:12,627 --> 01:37:16,530
And the answer is the same answer that we found

2297
01:37:16,530 --> 01:37:20,850
in liberal democracy about the press.

2298
01:37:20,850 --> 01:37:24,213
The press needs to be free and diverse.

2299
01:37:25,440 --> 01:37:28,140
We have free speech for a good reason.

2300
01:37:28,140 --> 01:37:31,937
It's because we don't want all of our information

2301
01:37:31,937 --> 01:37:35,703
to come from a unique source,

2302
01:37:36,660 --> 01:37:40,120
'cause that's opposite to the whole idea of democracy

2303
01:37:41,173 --> 01:37:44,760
and progressive ideas and even science, right?

2304
01:37:44,760 --> 01:37:48,431
In science, people have to argue for different opinions.

2305
01:37:48,431 --> 01:37:51,089
And science makes progress when people disagree

2306
01:37:51,089 --> 01:37:52,560
and they come up with an answer

2307
01:37:52,560 --> 01:37:54,600
and a consensus forms, right?

2308
01:37:54,600 --> 01:37:57,720
And it's true in all democracies around the world.

2309
01:37:57,720 --> 01:38:02,720
So there is a future which is already happening

2310
01:38:03,300 --> 01:38:05,640
where every single one of our interaction

2311
01:38:05,640 --> 01:38:07,230
with the digital world

2312
01:38:07,230 --> 01:38:10,410
will be mediated by AI systems,

2313
01:38:10,410 --> 01:38:11,733
AI assistance, right?

2314
01:38:12,960 --> 01:38:14,790
We're gonna have smart glasses.

2315
01:38:14,790 --> 01:38:16,890
You can already buy them from Meta, (laughing)

2316
01:38:16,890 --> 01:38:18,120
the Ray-Ban Meta.

2317
01:38:18,120 --> 01:38:20,670
Where you can talk to them

2318
01:38:20,670 --> 01:38:21,800
and they are connected with an LLM

2319
01:38:21,800 --> 01:38:25,920
and you can get answers on any question you have.

2320
01:38:25,920 --> 01:38:28,256
Or you can be looking at a monument

2321
01:38:28,256 --> 01:38:31,964
and there is a camera in the system, in the glasses,

2322
01:38:31,964 --> 01:38:34,110
you can ask it like what can you tell me

2323
01:38:34,110 --> 01:38:36,450
about this building or this monument?

2324
01:38:36,450 --> 01:38:39,120
You can be looking at a menu in a foreign language

2325
01:38:39,120 --> 01:38:40,847
and the thing we will translate it for you.

2326
01:38:40,847 --> 01:38:43,620
We can do real time translation

2327
01:38:43,620 --> 01:38:44,760
if we speak different languages.

2328
01:38:44,760 --> 01:38:48,240
So a lot of our interactions with the digital world

2329
01:38:48,240 --> 01:38:49,980
are going to be mediated by those systems

2330
01:38:49,980 --> 01:38:50,930
in the near future.

2331
01:38:53,130 --> 01:38:56,943
Increasingly, the search engines that we're gonna use

2332
01:38:56,943 --> 01:38:58,050
are not gonna be search engines,

2333
01:38:58,050 --> 01:39:01,380
they're gonna be dialogue systems

2334
01:39:01,380 --> 01:39:04,080
that we just ask a question,

2335
01:39:04,080 --> 01:39:05,160
and it will answer

2336
01:39:05,160 --> 01:39:05,993
and then point you

2337
01:39:05,993 --> 01:39:09,690
to the perhaps appropriate reference for it.

2338
01:39:09,690 --> 01:39:10,530
But here is the thing,

2339
01:39:10,530 --> 01:39:11,910
we cannot afford those systems

2340
01:39:11,910 --> 01:39:13,950
to come from a handful of companies

2341
01:39:13,950 --> 01:39:15,300
on the west coast of the US

2342
01:39:17,190 --> 01:39:18,923
because those systems will constitute

2343
01:39:18,923 --> 01:39:21,213
the repository of all human knowledge.

2344
01:39:22,050 --> 01:39:25,238
And we cannot have that be controlled

2345
01:39:25,238 --> 01:39:27,960
by a small number of people, right?

2346
01:39:27,960 --> 01:39:29,130
It has to be diverse

2347
01:39:29,130 --> 01:39:32,190
for the same reason the press has to be diverse.

2348
01:39:32,190 --> 01:39:35,493
So how do we get a diverse set of AI assistance?

2349
01:39:36,489 --> 01:39:38,730
It's very expensive and difficult

2350
01:39:38,730 --> 01:39:40,410
to train a base model, right?

2351
01:39:40,410 --> 01:39:42,047
A base LLM at the moment.

2352
01:39:42,047 --> 01:39:43,890
In the future might be something different,

2353
01:39:43,890 --> 01:39:46,810
but at the moment that's an LLM.

2354
01:39:46,810 --> 01:39:49,443
So only a few companies can do this properly.

2355
01:39:50,520 --> 01:39:55,520
And if some of those subsystems are open source,

2356
01:39:56,100 --> 01:39:57,090
anybody can use them,

2357
01:39:57,090 --> 01:39:58,563
anybody can fine tune them.

2358
01:39:59,397 --> 01:40:01,590
If we put in place some systems

2359
01:40:01,590 --> 01:40:05,523
that allows any group of people,

2360
01:40:06,390 --> 01:40:10,080
whether they are individual citizens,

2361
01:40:10,080 --> 01:40:11,313
groups of citizens,

2362
01:40:12,240 --> 01:40:13,680
government organizations,

2363
01:40:13,680 --> 01:40:18,000
NGOs, companies, whatever,

2364
01:40:18,000 --> 01:40:23,000
to take those open source systems, AI systems,

2365
01:40:23,850 --> 01:40:27,600
and fine tune them for their own purpose on their own data,

2366
01:40:27,600 --> 01:40:29,580
there we're gonna have a very large diversity

2367
01:40:29,580 --> 01:40:31,526
of different AI systems

2368
01:40:31,526 --> 01:40:34,560
that are specialized for all of those things, right?

2369
01:40:34,560 --> 01:40:35,980
So I'll tell you,

2370
01:40:35,980 --> 01:40:38,160
I talked to the French government quite a bit

2371
01:40:38,160 --> 01:40:41,280
and the French government will not accept

2372
01:40:41,280 --> 01:40:44,490
that the digital diet of all their citizens

2373
01:40:44,490 --> 01:40:46,350
be controlled by three companies

2374
01:40:46,350 --> 01:40:48,060
on the west coast of the US.

2375
01:40:48,060 --> 01:40:49,590
That's just not acceptable.

2376
01:40:49,590 --> 01:40:51,120
It's a danger to democracy.

2377
01:40:51,120 --> 01:40:52,537
Regardless of how well intentioned

2378
01:40:52,537 --> 01:40:54,483
those companies are, right?

2379
01:40:56,222 --> 01:41:00,960
And it's also a danger to local culture,

2380
01:41:00,960 --> 01:41:05,310
to values, to language, right?

2381
01:41:05,310 --> 01:41:10,310
I was talking with the founder of Infosys in India.

2382
01:41:13,110 --> 01:41:16,590
He's funding a project to fine tune LLaMA 2,

2383
01:41:16,590 --> 01:41:19,860
the open source model produced by Meta.

2384
01:41:19,860 --> 01:41:23,103
So that LLaMA 2 speaks all 22 official languages in India.

2385
01:41:24,030 --> 01:41:26,400
It's very important for people in India.

2386
01:41:26,400 --> 01:41:28,260
I was talking to a former colleague of mine,

2387
01:41:28,260 --> 01:41:29,093
Moustapha Cisse,

2388
01:41:29,093 --> 01:41:31,200
who used to be a scientist at FAIR,

2389
01:41:31,200 --> 01:41:32,430
and then moved back to Africa

2390
01:41:32,430 --> 01:41:35,160
and created a research lab for Google in Africa

2391
01:41:35,160 --> 01:41:37,950
and now has a new startup Kera.

2392
01:41:37,950 --> 01:41:40,125
And what he's trying to do is basically have LLM

2393
01:41:40,125 --> 01:41:42,840
that speaks the local languages in Senegal

2394
01:41:42,840 --> 01:41:46,170
so that people can have access to medical information,

2395
01:41:46,170 --> 01:41:47,550
'cause they don't have access to doctors,

2396
01:41:47,550 --> 01:41:52,126
it's a very small number of doctors per capita in Senegal.

2397
01:41:52,126 --> 01:41:55,530
I mean, you can't have any of this

2398
01:41:55,530 --> 01:41:58,020
unless you have open source platforms.

2399
01:41:58,020 --> 01:41:59,190
So with open source platforms,

2400
01:41:59,190 --> 01:42:00,660
you can have AI systems

2401
01:42:00,660 --> 01:42:02,958
that are not only diverse in terms of political opinions

2402
01:42:02,958 --> 01:42:05,070
or things of that type,

2403
01:42:05,070 --> 01:42:10,070
but in terms of language, culture, value systems,

2404
01:42:11,910 --> 01:42:16,910
political opinions, technical abilities in various domains.

2405
01:42:18,900 --> 01:42:20,310
And you can have an industry,

2406
01:42:20,310 --> 01:42:22,200
an ecosystem of companies

2407
01:42:22,200 --> 01:42:24,570
that fine tune those open source systems

2408
01:42:24,570 --> 01:42:26,855
for vertical applications in industry, right?

2409
01:42:26,855 --> 01:42:30,270
You have, I don't know, a publisher has thousands of books

2410
01:42:30,270 --> 01:42:31,890
and they want to build a system

2411
01:42:31,890 --> 01:42:33,929
that allows a customer to just ask a question

2412
01:42:33,929 --> 01:42:37,650
about the content of any of their books.

2413
01:42:37,650 --> 01:42:40,419
You need to train on their proprietary data, right?

2414
01:42:40,419 --> 01:42:42,150
You have a company,

2415
01:42:42,150 --> 01:42:44,460
we have one within Meta it's called Meta Mate.

2416
01:42:44,460 --> 01:42:46,390
And it's basically an LLM

2417
01:42:46,390 --> 01:42:47,580
that can answer any question

2418
01:42:47,580 --> 01:42:52,080
about internal stuff about about the company.

2419
01:42:52,080 --> 01:42:53,280
Very useful.

2420
01:42:53,280 --> 01:42:54,720
A lot of companies want this, right?

2421
01:42:54,720 --> 01:42:57,870
A lot of companies want this not just for their employees,

2422
01:42:57,870 --> 01:42:59,010
but also for their customers,

2423
01:42:59,010 --> 01:43:00,750
to take care of their customers.

2424
01:43:00,750 --> 01:43:04,350
So the only way you're gonna have an AI industry,

2425
01:43:04,350 --> 01:43:06,300
the only way you're gonna have AI systems

2426
01:43:06,300 --> 01:43:08,700
that are not uniquely biased,

2427
01:43:08,700 --> 01:43:10,290
is if you have open source platforms

2428
01:43:10,290 --> 01:43:15,290
on top of which any group can build specialized systems.

2429
01:43:16,710 --> 01:43:21,710
So the inevitable direction of history

2430
01:43:22,394 --> 01:43:25,440
is that the vast majority of AI systems

2431
01:43:25,440 --> 01:43:28,440
will be built on top of open source platforms.

2432
01:43:28,440 --> 01:43:30,180
- So that's a beautiful vision.

2433
01:43:30,180 --> 01:43:35,180
So meaning like a company like Meta or Google or so on,

2434
01:43:37,920 --> 01:43:40,590
should take only minimal fine tuning steps

2435
01:43:40,590 --> 01:43:44,910
after the building, the foundation, pre-trained model.

2436
01:43:44,910 --> 01:43:47,310
As few steps as possible.

2437
01:43:47,310 --> 01:43:48,606
- Basically.

2438
01:43:48,606 --> 01:43:49,560
(Lex sighs)

2439
01:43:49,560 --> 01:43:51,540
- Can Meta afford to do that?

2440
01:43:51,540 --> 01:43:52,373
- No.

2441
01:43:52,373 --> 01:43:53,670
- So I don't know if you know this,

2442
01:43:53,670 --> 01:43:56,280
but companies are supposed to make money somehow.

2443
01:43:56,280 --> 01:44:00,750
And open source is like giving away...

2444
01:44:00,750 --> 01:44:02,880
I don't know, Mark made a video,

2445
01:44:02,880 --> 01:44:04,470
Mark Zuckerberg.

2446
01:44:04,470 --> 01:44:08,923
A very sexy video talking about 350,000 Nvidia H100s.

2447
01:44:12,570 --> 01:44:14,640
The math of that is,

2448
01:44:14,640 --> 01:44:17,733
just for the GPUs, that's a hundred billion,

2449
01:44:19,800 --> 01:44:22,380
plus the infrastructure for training everything.

2450
01:44:22,380 --> 01:44:26,070
So I'm no business guy,

2451
01:44:26,070 --> 01:44:27,660
but how do you make money on that?

2452
01:44:27,660 --> 01:44:30,210
So the vision you paint is a really powerful one,

2453
01:44:30,210 --> 01:44:32,610
but how is it possible to make money?

2454
01:44:32,610 --> 01:44:33,443
- Okay.

2455
01:44:33,443 --> 01:44:36,780
So you have several business models, right?

2456
01:44:36,780 --> 01:44:39,640
The business model that Meta is built around

2457
01:44:40,620 --> 01:44:44,193
is you offer a service,

2458
01:44:45,150 --> 01:44:48,720
and the financing of that service

2459
01:44:48,720 --> 01:44:52,710
is either through ads or through business customers.

2460
01:44:52,710 --> 01:44:54,990
So for example, if you have an LLM

2461
01:44:54,990 --> 01:44:58,960
that can help a mom-and-pop pizza place

2462
01:45:00,540 --> 01:45:03,417
by talking to their customers through WhatsApp,

2463
01:45:03,417 --> 01:45:05,971
and so the customers can just order a pizza

2464
01:45:05,971 --> 01:45:08,047
and the system will just ask them,

2465
01:45:08,047 --> 01:45:11,047
like what topping do you want or what size, blah blah, blah.

2466
01:45:12,570 --> 01:45:14,070
The business will pay for that.

2467
01:45:14,070 --> 01:45:14,903
Okay?

2468
01:45:14,903 --> 01:45:15,736
That's a model.

2469
01:45:19,440 --> 01:45:21,309
And otherwise, if it's a system

2470
01:45:21,309 --> 01:45:24,360
that is on the more kind of classical services,

2471
01:45:24,360 --> 01:45:28,140
it can be ad supported or there's several models.

2472
01:45:28,140 --> 01:45:29,283
But the point is,

2473
01:45:30,191 --> 01:45:34,680
if you have a big enough potential customer base

2474
01:45:34,680 --> 01:45:39,003
and you need to build that system anyway for them,

2475
01:45:40,140 --> 01:45:41,070
it doesn't hurt you

2476
01:45:41,070 --> 01:45:43,260
to actually distribute it to open source.

2477
01:45:43,260 --> 01:45:45,420
- Again, I'm no business guy,

2478
01:45:45,420 --> 01:45:48,090
but if you release the open source model,

2479
01:45:48,090 --> 01:45:51,780
then other people can do the same kind of task

2480
01:45:51,780 --> 01:45:52,980
and compete on it.

2481
01:45:52,980 --> 01:45:57,026
Basically provide fine tuned models for businesses,

2482
01:45:57,026 --> 01:45:59,730
is the bet that Meta is making...

2483
01:45:59,730 --> 01:46:01,500
By the way, I'm a huge fan of all this.

2484
01:46:01,500 --> 01:46:03,510
But is the bet that Meta is making

2485
01:46:03,510 --> 01:46:05,610
is like, "we'll do a better job of it?"

2486
01:46:05,610 --> 01:46:06,443
- Well, no.

2487
01:46:06,443 --> 01:46:08,460
The bet is more,

2488
01:46:08,460 --> 01:46:13,110
we already have a huge user base and customer base.

2489
01:46:13,110 --> 01:46:13,943
- [Lex] Ah, right. - Right?

2490
01:46:13,943 --> 01:46:15,360
So it's gonna be useful to them.

2491
01:46:15,360 --> 01:46:17,550
Whatever we offer them is gonna be useful

2492
01:46:17,550 --> 01:46:21,000
and there is a way to derive revenue from this.

2493
01:46:21,000 --> 01:46:22,320
- [Lex] Sure.

2494
01:46:22,320 --> 01:46:23,280
- And it doesn't hurt

2495
01:46:23,280 --> 01:46:28,280
that we provide that system or the base model, right?

2496
01:46:29,400 --> 01:46:32,670
The foundation model in open source

2497
01:46:32,670 --> 01:46:35,820
for others to build applications on top of it too.

2498
01:46:35,820 --> 01:46:36,653
If those applications

2499
01:46:36,653 --> 01:46:38,160
turn out to be useful for our customers,

2500
01:46:38,160 --> 01:46:39,843
we can just buy it for them.

2501
01:46:42,270 --> 01:46:44,280
It could be that they will improve the platform.

2502
01:46:44,280 --> 01:46:46,380
In fact, we see this already.

2503
01:46:46,380 --> 01:46:50,820
I mean there is literally millions of downloads of LLaMA 2

2504
01:46:50,820 --> 01:46:53,269
and thousands of people who have provided ideas

2505
01:46:53,269 --> 01:46:55,590
about how to make it better.

2506
01:46:55,590 --> 01:46:58,815
So this clearly accelerates progress

2507
01:46:58,815 --> 01:47:00,423
to make the system available

2508
01:47:00,423 --> 01:47:05,423
to sort of a wide community of people.

2509
01:47:05,430 --> 01:47:07,800
And there is literally thousands of businesses

2510
01:47:07,800 --> 01:47:09,700
who are building applications with it.

2511
01:47:15,630 --> 01:47:19,032
Meta's ability to derive revenue from this technology

2512
01:47:19,032 --> 01:47:24,032
is not impaired by the distribution

2513
01:47:24,210 --> 01:47:26,580
of base models in open source.

2514
01:47:26,580 --> 01:47:28,620
- The fundamental criticism that Gemini is getting

2515
01:47:28,620 --> 01:47:31,060
is that, as you pointed out on the west coast...

2516
01:47:31,060 --> 01:47:32,435
Just to clarify,

2517
01:47:32,435 --> 01:47:34,620
we're currently in the east coast,

2518
01:47:34,620 --> 01:47:38,511
where I would suppose Meta AI headquarters would be.

2519
01:47:38,511 --> 01:47:39,390
(laughs)

2520
01:47:39,390 --> 01:47:42,480
So strong words about the west coast.

2521
01:47:42,480 --> 01:47:46,950
But I guess the issue that happens is,

2522
01:47:46,950 --> 01:47:50,010
I think it's fair to say that most tech people

2523
01:47:50,010 --> 01:47:53,550
have a political affiliation with the left wing.

2524
01:47:53,550 --> 01:47:55,350
They lean left.

2525
01:47:55,350 --> 01:47:58,470
And so the problem that people are criticizing Gemini with

2526
01:47:58,470 --> 01:48:02,580
is that in that de-biasing process that you mentioned,

2527
01:48:02,580 --> 01:48:07,457
that their ideological lean becomes obvious.

2528
01:48:09,990 --> 01:48:14,580
Is this something that could be escaped?

2529
01:48:14,580 --> 01:48:16,770
You're saying open source is the only way?

2530
01:48:16,770 --> 01:48:17,603
- [Yann] Yeah.

2531
01:48:17,603 --> 01:48:19,680
- Have you witnessed this kind of ideological lean

2532
01:48:19,680 --> 01:48:22,380
that makes engineering difficult?

2533
01:48:22,380 --> 01:48:24,300
- No, I don't think it has to do...

2534
01:48:24,300 --> 01:48:25,500
I don't think the issue has to do

2535
01:48:25,500 --> 01:48:26,760
with the political leaning

2536
01:48:26,760 --> 01:48:29,340
of the people designing those systems.

2537
01:48:29,340 --> 01:48:34,202
It has to do with the acceptability or political leanings

2538
01:48:34,202 --> 01:48:38,340
of their customer base or audience, right?

2539
01:48:38,340 --> 01:48:43,340
So a big company cannot afford to offend too many people.

2540
01:48:43,680 --> 01:48:46,131
So they're going to make sure

2541
01:48:46,131 --> 01:48:49,440
that whatever product they put out is "safe,"

2542
01:48:49,440 --> 01:48:50,463
whatever that means.

2543
01:48:52,744 --> 01:48:55,807
And it's very possible to overdo it.

2544
01:48:55,807 --> 01:48:58,020
And it's also very possible to...

2545
01:48:58,020 --> 01:49:00,360
It's impossible to do it properly for everyone.

2546
01:49:00,360 --> 01:49:02,520
You're not going to satisfy everyone.

2547
01:49:02,520 --> 01:49:03,780
So that's what I said before,

2548
01:49:03,780 --> 01:49:05,269
you cannot have a system that is unbiased

2549
01:49:05,269 --> 01:49:07,743
and is perceived as unbiased by everyone.

2550
01:49:08,700 --> 01:49:09,804
It's gonna be,

2551
01:49:09,804 --> 01:49:11,260
you push it in one way,

2552
01:49:11,260 --> 01:49:14,288
one set of people are gonna see it as biased.

2553
01:49:14,288 --> 01:49:15,750
And then you push it the other way

2554
01:49:15,750 --> 01:49:18,660
and another set of people is gonna see it as biased.

2555
01:49:18,660 --> 01:49:19,493
And then in addition to this,

2556
01:49:19,493 --> 01:49:22,710
there's the issue of if you push the system

2557
01:49:22,710 --> 01:49:24,300
perhaps a little too far in one direction,

2558
01:49:24,300 --> 01:49:25,890
it's gonna be non-factual, right?

2559
01:49:25,890 --> 01:49:30,890
You're gonna have black Nazi soldiers in-

2560
01:49:30,988 --> 01:49:31,821
- Yeah.

2561
01:49:31,821 --> 01:49:34,500
So we should mention image generation

2562
01:49:34,500 --> 01:49:36,281
of black Nazi soldiers,

2563
01:49:36,281 --> 01:49:38,225
which is not factually accurate.

2564
01:49:38,225 --> 01:49:39,058
- Right.

2565
01:49:39,058 --> 01:49:42,363
And can be offensive for some people as well, right?

2566
01:49:44,266 --> 01:49:46,449
So it's gonna be impossible

2567
01:49:46,449 --> 01:49:49,110
to kind of produce systems that are unbiased for everyone.

2568
01:49:49,110 --> 01:49:53,040
So the only solution that I see is diversity.

2569
01:49:53,040 --> 01:49:55,020
- And diversity in full meaning of that word,

2570
01:49:55,020 --> 01:49:57,363
diversity in every possible way.

2571
01:49:57,363 --> 01:49:58,928
- [Yann] Yeah.

2572
01:49:58,928 --> 01:50:02,703
- Marc Andreessen just tweeted today,

2573
01:50:04,200 --> 01:50:06,090
let me do a TL;DR.

2574
01:50:06,090 --> 01:50:08,670
The conclusion is only startups and open source

2575
01:50:08,670 --> 01:50:12,273
can avoid the issue that he's highlighting with big tech.

2576
01:50:13,170 --> 01:50:14,003
He's asking,

2577
01:50:14,003 --> 01:50:17,520
can big tech actually field generative AI products?

2578
01:50:17,520 --> 01:50:20,790
One, ever escalating demands from internal activists,

2579
01:50:20,790 --> 01:50:23,280
employee mobs, crazed executives,

2580
01:50:23,280 --> 01:50:25,200
broken boards, pressure groups,

2581
01:50:25,200 --> 01:50:28,230
extremist regulators, government agencies, the press,

2582
01:50:28,230 --> 01:50:30,450
in quotes "experts,"

2583
01:50:30,450 --> 01:50:34,290
and everything corrupting the output.

2584
01:50:34,290 --> 01:50:36,663
Two, constant risk of generating a bad answer

2585
01:50:36,663 --> 01:50:40,623
or drawing a bad picture or rendering a bad video.

2586
01:50:41,490 --> 01:50:44,490
Who knows what it's going to say or do at any moment?

2587
01:50:44,490 --> 01:50:48,180
Three, legal exposure, product liability, slander,

2588
01:50:48,180 --> 01:50:51,602
election law, many other things and so on.

2589
01:50:51,602 --> 01:50:53,943
Anything that makes Congress mad.

2590
01:50:54,990 --> 01:50:56,580
Four, continuous attempts

2591
01:50:56,580 --> 01:50:58,350
to tighten grip on acceptable output,

2592
01:50:58,350 --> 01:50:59,730
degrade the model,

2593
01:50:59,730 --> 01:51:01,895
like how good it actually is

2594
01:51:01,895 --> 01:51:05,550
in terms of usable and pleasant to use and effective

2595
01:51:05,550 --> 01:51:06,960
and all that kind of stuff.

2596
01:51:06,960 --> 01:51:10,470
And five, publicity of bad text, images, video,

2597
01:51:10,470 --> 01:51:13,110
actual puts those examples into the training data

2598
01:51:13,110 --> 01:51:14,163
for the next version.

2599
01:51:15,044 --> 01:51:15,877
And so on.

2600
01:51:15,877 --> 01:51:18,420
So he just highlights how difficult this is.

2601
01:51:18,420 --> 01:51:21,030
From all kinds of people being unhappy.

2602
01:51:21,030 --> 01:51:23,070
He just said you can't create a system

2603
01:51:23,070 --> 01:51:24,750
that makes everybody happy.

2604
01:51:24,750 --> 01:51:25,583
- [Yann] Yes.

2605
01:51:25,583 --> 01:51:29,100
- So if you're going to do the fine tuning yourself

2606
01:51:29,100 --> 01:51:30,873
and keep a close source,

2607
01:51:31,980 --> 01:51:33,240
essentially the problem there

2608
01:51:33,240 --> 01:51:35,215
is then trying to minimize the number of people

2609
01:51:35,215 --> 01:51:36,780
who are going to be unhappy.

2610
01:51:36,780 --> 01:51:38,160
- [Yann] Yeah.

2611
01:51:38,160 --> 01:51:39,753
- And you're saying like the only...

2612
01:51:39,753 --> 01:51:42,210
That that's almost impossible to do, right?

2613
01:51:42,210 --> 01:51:44,763
And the better way is to do open source.

2614
01:51:45,690 --> 01:51:46,710
- Basically, yeah.

2615
01:51:46,710 --> 01:51:51,710
I mean Marc is right about a number of things that he lists

2616
01:51:51,990 --> 01:51:55,353
that indeed scare large companies.

2617
01:51:57,078 --> 01:52:00,450
Certainly, congressional investigations is one of them.

2618
01:52:00,450 --> 01:52:01,593
Legal liability.

2619
01:52:04,290 --> 01:52:05,123
Making things

2620
01:52:05,123 --> 01:52:09,120
that get people to hurt themselves or hurt others.

2621
01:52:09,120 --> 01:52:12,630
Like big companies are really careful

2622
01:52:12,630 --> 01:52:15,243
about not producing things of this type,

2623
01:52:18,060 --> 01:52:19,860
because they have...

2624
01:52:19,860 --> 01:52:21,330
They don't want to hurt anyone, first of all.

2625
01:52:21,330 --> 01:52:23,220
And then second, they wanna preserve their business.

2626
01:52:23,220 --> 01:52:26,613
So it's essentially impossible for systems like this

2627
01:52:26,613 --> 01:52:30,990
that can inevitably formulate political opinions

2628
01:52:30,990 --> 01:52:32,880
and opinions about various things

2629
01:52:32,880 --> 01:52:34,110
that may be political or not,

2630
01:52:34,110 --> 01:52:36,240
but that people may disagree about.

2631
01:52:36,240 --> 01:52:37,929
About, you know, moral issues

2632
01:52:37,929 --> 01:52:42,929
and things about like questions about religion

2633
01:52:43,470 --> 01:52:44,640
and things like that, right?

2634
01:52:44,640 --> 01:52:46,160
Or cultural issues

2635
01:52:46,160 --> 01:52:48,000
that people from different communities

2636
01:52:48,000 --> 01:52:50,160
would disagree with in the first place.

2637
01:52:50,160 --> 01:52:52,278
So there's only kind of a relatively small number of things

2638
01:52:52,278 --> 01:52:55,963
that people will sort of agree on,

2639
01:52:55,963 --> 01:52:57,377
basic principles.

2640
01:52:57,377 --> 01:52:58,495
But beyond that,

2641
01:52:58,495 --> 01:53:01,890
if you want those systems to be useful,

2642
01:53:01,890 --> 01:53:06,890
they will necessarily have to offend a number of people,

2643
01:53:07,440 --> 01:53:08,273
inevitably.

2644
01:53:09,120 --> 01:53:11,026
- And so open source is just better-

2645
01:53:11,026 --> 01:53:12,856
- [Yann] Diversity is better, right?

2646
01:53:12,856 --> 01:53:15,510
- And open source enables diversity.

2647
01:53:15,510 --> 01:53:16,343
- That's right.

2648
01:53:16,343 --> 01:53:17,800
Open source enables diversity.

2649
01:53:17,800 --> 01:53:19,680
- This can be a fascinating world

2650
01:53:19,680 --> 01:53:22,980
where if it's true that the open source world,

2651
01:53:22,980 --> 01:53:24,030
if Meta leads the way

2652
01:53:24,030 --> 01:53:27,630
and creates this kind of open source foundation model world,

2653
01:53:27,630 --> 01:53:28,590
there's going to be,

2654
01:53:28,590 --> 01:53:31,721
like governments will have a fine tuned model. (laughing)

2655
01:53:31,721 --> 01:53:33,199
- [Yann] Yeah.

2656
01:53:33,199 --> 01:53:34,983
- And then potentially,

2657
01:53:37,290 --> 01:53:39,210
people that vote left and right

2658
01:53:39,210 --> 01:53:40,650
will have their own model and preference

2659
01:53:40,650 --> 01:53:42,030
to be able to choose.

2660
01:53:42,030 --> 01:53:44,124
And it will potentially divide us even more

2661
01:53:44,124 --> 01:53:46,980
but that's on us humans.

2662
01:53:46,980 --> 01:53:48,330
We get to figure out...

2663
01:53:48,330 --> 01:53:50,640
Basically the technology enables humans

2664
01:53:50,640 --> 01:53:53,430
to human more effectively.

2665
01:53:53,430 --> 01:53:57,300
And all the difficult ethical questions that humans raise

2666
01:53:57,300 --> 01:54:02,300
we'll just leave it up to us to figure that out.

2667
01:54:02,640 --> 01:54:04,800
- Yeah, I mean there are some limits to what...

2668
01:54:04,800 --> 01:54:06,510
The same way there are limits to free speech,

2669
01:54:06,510 --> 01:54:08,940
there has to be some limit to the kind of stuff

2670
01:54:08,940 --> 01:54:13,940
that those systems might be authorized to produce,

2671
01:54:15,480 --> 01:54:16,500
some guardrails.

2672
01:54:16,500 --> 01:54:18,360
So I mean, that's one thing I've been interested in,

2673
01:54:18,360 --> 01:54:20,970
which is in the type of architecture

2674
01:54:20,970 --> 01:54:22,740
that we were discussing before,

2675
01:54:22,740 --> 01:54:26,160
where the output of the system

2676
01:54:26,160 --> 01:54:29,910
is a result of an inference to satisfy an objective.

2677
01:54:29,910 --> 01:54:32,013
That objective can include guardrails.

2678
01:54:32,940 --> 01:54:37,440
And we can put guardrails in open source systems.

2679
01:54:37,440 --> 01:54:39,090
I mean, if we eventually have systems

2680
01:54:39,090 --> 01:54:41,580
that are built with this blueprint,

2681
01:54:41,580 --> 01:54:43,906
we can put guardrails in those systems

2682
01:54:43,906 --> 01:54:44,760
that guarantee

2683
01:54:44,760 --> 01:54:47,851
that there is sort of a minimum set of guardrails

2684
01:54:47,851 --> 01:54:50,839
that make the system non-dangerous and non-toxic, et cetera.

2685
01:54:50,839 --> 01:54:53,343
Basic things that everybody would agree on.

2686
01:54:55,260 --> 01:54:57,944
And then the fine tuning that people will add

2687
01:54:57,944 --> 01:54:59,961
or the additional guardrails that people will add

2688
01:54:59,961 --> 01:55:04,961
will kind of cater to their community, whatever it is.

2689
01:55:04,980 --> 01:55:06,690
- And yeah, the fine tuning

2690
01:55:06,690 --> 01:55:09,162
would be more about the gray areas of what is hate speech,

2691
01:55:09,162 --> 01:55:11,490
what is dangerous and all that kind of stuff.

2692
01:55:11,490 --> 01:55:12,323
I mean, you've-

2693
01:55:12,323 --> 01:55:13,156
- [Yann] Or different value systems.

2694
01:55:13,156 --> 01:55:14,340
- Different value systems.

2695
01:55:14,340 --> 01:55:16,800
But still even with the objectives

2696
01:55:16,800 --> 01:55:18,499
of how to build a bio weapon, for example,

2697
01:55:18,499 --> 01:55:20,850
I think something you've commented on,

2698
01:55:20,850 --> 01:55:23,280
or at least there's a paper

2699
01:55:23,280 --> 01:55:24,690
where a collection of researchers

2700
01:55:24,690 --> 01:55:28,113
is trying to understand the social impacts of these LLMs.

2701
01:55:29,370 --> 01:55:31,920
And I guess one threshold that's nice

2702
01:55:31,920 --> 01:55:36,920
is like does the LLM make it any easier than a search would,

2703
01:55:38,010 --> 01:55:39,480
like a Google search would?

2704
01:55:39,480 --> 01:55:40,313
- Right.

2705
01:55:40,313 --> 01:55:44,550
So the increasing number of studies on this

2706
01:55:44,550 --> 01:55:49,202
seems to point to the fact that it doesn't help.

2707
01:55:49,202 --> 01:55:52,043
So having an LLM doesn't help you

2708
01:55:52,043 --> 01:55:57,043
design or build a bio weapon or a chemical weapon

2709
01:55:57,240 --> 01:56:01,643
if you already have access to a search engine and a library.

2710
01:56:01,643 --> 01:56:04,586
And so the sort of increased information you get

2711
01:56:04,586 --> 01:56:07,541
or the ease with which you get it doesn't really help you.

2712
01:56:07,541 --> 01:56:08,880
That's the first thing.

2713
01:56:08,880 --> 01:56:10,290
The second thing is,

2714
01:56:10,290 --> 01:56:12,720
it's one thing to have a list of instructions

2715
01:56:12,720 --> 01:56:17,070
of how to make a chemical weapon, for example, a bio weapon.

2716
01:56:17,070 --> 01:56:19,920
It's another thing to actually build it.

2717
01:56:19,920 --> 01:56:21,776
And it's much harder than you might think,

2718
01:56:21,776 --> 01:56:23,826
and then LLM will not help you with that.

2719
01:56:25,710 --> 01:56:27,000
In fact, nobody in the world,

2720
01:56:27,000 --> 01:56:29,014
not even like countries use bio weapons

2721
01:56:29,014 --> 01:56:31,887
because most of the time they have no idea

2722
01:56:31,887 --> 01:56:34,156
how to protect their own populations against it.

2723
01:56:34,156 --> 01:56:39,156
So it's too dangerous actually to kind of ever use.

2724
01:56:39,270 --> 01:56:43,540
And it's in fact banned by international treaties.

2725
01:56:43,540 --> 01:56:45,230
Chemical weapons is different.

2726
01:56:45,230 --> 01:56:47,640
It's also banned by treaties,

2727
01:56:47,640 --> 01:56:50,730
but it's the same problem.

2728
01:56:50,730 --> 01:56:51,870
It's difficult to use

2729
01:56:51,870 --> 01:56:56,490
in situations that doesn't turn against the perpetrators.

2730
01:56:56,490 --> 01:56:57,870
But we could ask Elon Musk.

2731
01:56:57,870 --> 01:57:01,315
Like I can give you a very precise list of instructions

2732
01:57:01,315 --> 01:57:03,393
of how you build a rocket engine.

2733
01:57:04,350 --> 01:57:06,780
And even if you have a team of 50 engineers

2734
01:57:06,780 --> 01:57:08,220
that are really experienced building it,

2735
01:57:08,220 --> 01:57:10,140
you're still gonna have to blow up a dozen of them

2736
01:57:10,140 --> 01:57:11,640
before you get one that works.

2737
01:57:13,050 --> 01:57:18,050
And it's the same with chemical weapons or bio weapons

2738
01:57:18,710 --> 01:57:19,590
or things like this.

2739
01:57:19,590 --> 01:57:23,100
It requires expertise in the real world

2740
01:57:23,100 --> 01:57:25,260
that the LLM is not gonna help you with.

2741
01:57:25,260 --> 01:57:28,080
- And it requires even the common sense expertise

2742
01:57:28,080 --> 01:57:29,130
that we've been talking about,

2743
01:57:29,130 --> 01:57:34,050
which is how to take language based instructions

2744
01:57:34,050 --> 01:57:36,930
and materialize them in the physical world

2745
01:57:36,930 --> 01:57:41,610
requires a lot of knowledge that's not in the instructions.

2746
01:57:41,610 --> 01:57:42,443
- Yeah, exactly.

2747
01:57:42,443 --> 01:57:44,250
A lot of biologists have posted on this actually

2748
01:57:44,250 --> 01:57:45,720
in response to those things

2749
01:57:45,720 --> 01:57:47,520
saying like do you realize how hard it is

2750
01:57:47,520 --> 01:57:49,320
to actually do the lab work?

2751
01:57:49,320 --> 01:57:50,973
Like this is not trivial.

2752
01:57:51,900 --> 01:57:52,733
- Yeah.

2753
01:57:52,733 --> 01:57:56,932
And that's Hans Moravec comes to light once again.

2754
01:57:56,932 --> 01:57:59,040
Just to linger on LLaMA.

2755
01:57:59,040 --> 01:58:01,860
Mark announced that LLaMA 3 is coming out eventually,

2756
01:58:01,860 --> 01:58:03,510
I don't think there's a release date,

2757
01:58:03,510 --> 01:58:06,930
but what are you most excited about?

2758
01:58:06,930 --> 01:58:09,150
First of all, LLaMA 2 that's already out there,

2759
01:58:09,150 --> 01:58:12,810
and maybe the future LLaMA 3, 4, 5, 6, 10,

2760
01:58:12,810 --> 01:58:15,633
just the future of the open source under Meta?

2761
01:58:17,310 --> 01:58:18,143
- Well, a number of things.

2762
01:58:18,143 --> 01:58:22,020
So there's gonna be like various versions of LLaMA

2763
01:58:22,020 --> 01:58:26,940
that are improvements of previous LLaMAs.

2764
01:58:26,940 --> 01:58:30,447
Bigger, better, multimodal, things like that.

2765
01:58:30,447 --> 01:58:32,070
And then in future generations,

2766
01:58:32,070 --> 01:58:34,140
systems that are capable of planning,

2767
01:58:34,140 --> 01:58:36,334
that really understand how the world works,

2768
01:58:36,334 --> 01:58:39,600
maybe are trained from video so they have some world model.

2769
01:58:39,600 --> 01:58:42,570
Maybe capable of the type of reasoning and planning

2770
01:58:42,570 --> 01:58:44,130
I was talking about earlier.

2771
01:58:44,130 --> 01:58:45,360
Like how long is that gonna take?

2772
01:58:45,360 --> 01:58:48,184
Like when is the research that is going in that direction

2773
01:58:48,184 --> 01:58:52,590
going to sort of feed into the product line, if you want,

2774
01:58:52,590 --> 01:58:53,520
of LLaMA?

2775
01:58:53,520 --> 01:58:54,717
I don't know, I can't tell you.

2776
01:58:54,717 --> 01:58:56,310
And there's a few breakthroughs

2777
01:58:56,310 --> 01:58:59,700
that we have to basically go through

2778
01:58:59,700 --> 01:59:01,449
before we can get there.

2779
01:59:01,449 --> 01:59:03,916
But you'll be able to monitor our progress

2780
01:59:03,916 --> 01:59:07,050
because we publish our research, right?

2781
01:59:07,050 --> 01:59:11,473
So last week we published the V-JEPA work,

2782
01:59:11,473 --> 01:59:13,260
which is sort of a first step

2783
01:59:13,260 --> 01:59:15,060
towards training systems from video.

2784
01:59:16,200 --> 01:59:18,990
And then the next step is gonna be world models

2785
01:59:18,990 --> 01:59:21,778
based on kind of this type of idea,

2786
01:59:21,778 --> 01:59:23,432
training from video.

2787
01:59:23,432 --> 01:59:28,149
There's similar work at DeepMind also taking place,

2788
01:59:28,149 --> 01:59:33,149
and also at UC Berkeley on world models and video.

2789
01:59:33,840 --> 01:59:35,220
A lot of people are working on this.

2790
01:59:35,220 --> 01:59:38,520
I think a lot of good ideas are appearing.

2791
01:59:38,520 --> 01:59:41,790
My bet is that those systems are gonna be JEPA-like,

2792
01:59:41,790 --> 01:59:43,983
they're not gonna be generative models.

2793
01:59:45,360 --> 01:59:49,636
And we'll see what the future will tell.

2794
01:59:49,636 --> 01:59:52,040
There's really good work at...

2795
01:59:54,157 --> 01:59:56,520
A gentleman called Danijar Hafner who is now DeepMind,

2796
01:59:56,520 --> 01:59:58,770
who's worked on kind of models of this type

2797
01:59:58,770 --> 02:00:00,090
that learn representations

2798
02:00:00,090 --> 02:00:02,772
and then use them for planning or learning tasks

2799
02:00:02,772 --> 02:00:04,233
by reinforcement training.

2800
02:00:05,820 --> 02:00:07,410
And a lot of work at Berkeley

2801
02:00:07,410 --> 02:00:11,145
by Pieter Abbeel, Sergey Levine,

2802
02:00:11,145 --> 02:00:13,590
a bunch of other people of that type.

2803
02:00:13,590 --> 02:00:14,760
I'm collaborating with actually

2804
02:00:14,760 --> 02:00:18,190
in the context of some grants with my NYU hat.

2805
02:00:20,010 --> 02:00:22,050
And then collaborations also through Meta,

2806
02:00:22,050 --> 02:00:24,630
'cause the lab at Berkeley

2807
02:00:24,630 --> 02:00:28,320
is associated with Meta in some way, with FAIR.

2808
02:00:28,320 --> 02:00:29,940
So I think it's very exciting.

2809
02:00:29,940 --> 02:00:34,230
I think I'm super excited about...

2810
02:00:34,230 --> 02:00:35,670
I haven't been that excited

2811
02:00:35,670 --> 02:00:38,549
about like the direction of machine learning and AI

2812
02:00:38,549 --> 02:00:41,998
since 10 years ago when FAIR was started,

2813
02:00:41,998 --> 02:00:44,154
and before that, 30 years ago,

2814
02:00:44,154 --> 02:00:45,670
when we were working on,

2815
02:00:45,670 --> 02:00:46,862
sorry 35,

2816
02:00:46,862 --> 02:00:51,341
on combination nets and the early days of neural net.

2817
02:00:51,341 --> 02:00:54,162
So I'm super excited

2818
02:00:54,162 --> 02:00:57,600
because I see a path towards

2819
02:00:57,600 --> 02:00:59,814
potentially human level intelligence

2820
02:00:59,814 --> 02:01:04,140
with systems that can understand the world,

2821
02:01:04,140 --> 02:01:06,359
remember, plan, reason.

2822
02:01:06,359 --> 02:01:09,716
There is some set of ideas to make progress there

2823
02:01:09,716 --> 02:01:12,420
that might have a chance of working.

2824
02:01:12,420 --> 02:01:14,474
And I'm really excited about this.

2825
02:01:14,474 --> 02:01:15,610
What I like is that

2826
02:01:18,399 --> 02:01:20,856
somewhat we get onto like a good direction

2827
02:01:20,856 --> 02:01:24,900
and perhaps succeed before my brain turns to a white sauce

2828
02:01:24,900 --> 02:01:26,391
or before I need to retire.

2829
02:01:26,391 --> 02:01:28,350
(laughs)

2830
02:01:28,350 --> 02:01:29,400
- Yeah.

2831
02:01:29,400 --> 02:01:30,233
Yeah.

2832
02:01:30,233 --> 02:01:32,060
Are you also excited by...

2833
02:01:34,380 --> 02:01:38,040
Is it beautiful to you just the amount of GPUs involved,

2834
02:01:38,040 --> 02:01:42,767
sort of the whole training process on this much compute?

2835
02:01:42,767 --> 02:01:43,931
Just zooming out,

2836
02:01:43,931 --> 02:01:47,070
just looking at earth and humans together

2837
02:01:47,070 --> 02:01:49,740
have built these computing devices

2838
02:01:49,740 --> 02:01:52,713
and are able to train this one brain,

2839
02:01:53,646 --> 02:01:56,431
we then open source.

2840
02:01:56,431 --> 02:01:57,630
(laughs)

2841
02:01:57,630 --> 02:02:01,080
Like giving birth to this open source brain

2842
02:02:01,080 --> 02:02:04,350
trained on this gigantic compute system.

2843
02:02:04,350 --> 02:02:07,710
There's just the details of how to train on that,

2844
02:02:07,710 --> 02:02:10,080
how to build the infrastructure and the hardware,

2845
02:02:10,080 --> 02:02:12,300
the cooling, all of this kind of stuff.

2846
02:02:12,300 --> 02:02:14,400
Are you just still the most of your excitement

2847
02:02:14,400 --> 02:02:16,900
is in the theory aspect of it?

2848
02:02:16,900 --> 02:02:19,620
Meaning like the software.

2849
02:02:19,620 --> 02:02:21,590
- Well, I used to be a hardware guy many years ago.

2850
02:02:21,590 --> 02:02:22,440
(laughs) - Yes, yes, that's right.

2851
02:02:22,440 --> 02:02:23,273
- Decades ago.

2852
02:02:23,273 --> 02:02:25,770
- Hardware has improved a little bit.

2853
02:02:25,770 --> 02:02:27,810
Changed a little bit, yeah.

2854
02:02:27,810 --> 02:02:32,400
- I mean, certainly scale is necessary but not sufficient.

2855
02:02:32,400 --> 02:02:33,233
- [Lex] Absolutely.

2856
02:02:33,233 --> 02:02:34,650
- So we certainly need computation.

2857
02:02:34,650 --> 02:02:37,776
I mean, we're still far in terms of compute power

2858
02:02:37,776 --> 02:02:39,630
from what we would need

2859
02:02:39,630 --> 02:02:42,930
to match the compute power of the human brain.

2860
02:02:42,930 --> 02:02:45,448
This may occur in the next couple decades,

2861
02:02:45,448 --> 02:02:47,610
but we're still some ways away.

2862
02:02:47,610 --> 02:02:49,920
And certainly in terms of power efficiency,

2863
02:02:49,920 --> 02:02:50,793
we're really far.

2864
02:02:51,960 --> 02:02:56,010
So a lot of progress to make in hardware.

2865
02:02:56,010 --> 02:03:00,270
And right now a lot of the progress is not...

2866
02:03:00,270 --> 02:03:03,030
I mean, there's a bit coming from Silicon technology,

2867
02:03:03,030 --> 02:03:06,480
but a lot of it coming from architectural innovation

2868
02:03:06,480 --> 02:03:10,230
and quite a bit coming from like more efficient ways

2869
02:03:10,230 --> 02:03:13,620
of implementing the architectures that have become popular.

2870
02:03:13,620 --> 02:03:17,550
Basically combination of transformers and com net, right?

2871
02:03:17,550 --> 02:03:22,260
And so there's still some ways to go

2872
02:03:22,260 --> 02:03:27,161
until we are going to saturate.

2873
02:03:27,161 --> 02:03:28,380
We're gonna have to come up

2874
02:03:28,380 --> 02:03:31,950
with like new principles, new fabrication technology,

2875
02:03:31,950 --> 02:03:34,503
new basic components,

2876
02:03:35,713 --> 02:03:38,419
perhaps based on sort of different principles

2877
02:03:38,419 --> 02:03:41,970
than those classical digital CMOS.

2878
02:03:41,970 --> 02:03:42,803
- Interesting.

2879
02:03:42,803 --> 02:03:46,353
So you think in order to build AmI, ami,

2880
02:03:48,270 --> 02:03:52,860
we potentially might need some hardware innovation too?

2881
02:03:52,860 --> 02:03:55,680
- Well, if we wanna make it ubiquitous,

2882
02:03:55,680 --> 02:03:56,580
yeah, certainly.

2883
02:03:56,580 --> 02:04:01,580
Because we're gonna have to reduce the power consumption.

2884
02:04:02,190 --> 02:04:03,390
A GPU today, right?

2885
02:04:03,390 --> 02:04:05,553
Is half a kilowatt to a kilowatt.

2886
02:04:06,900 --> 02:04:08,703
Human brain is about 25 watts.

2887
02:04:09,904 --> 02:04:13,050
And the GPU is way below the power of human brain.

2888
02:04:13,050 --> 02:04:14,820
You need something like a hundred thousand

2889
02:04:14,820 --> 02:04:16,369
or a million to match it.

2890
02:04:16,369 --> 02:04:19,773
So we are off by a huge factor.

2891
02:04:21,450 --> 02:04:26,340
- You often say that AGI is not coming soon.

2892
02:04:26,340 --> 02:04:30,270
Meaning like not this year, not the next few years,

2893
02:04:30,270 --> 02:04:32,820
potentially farther away.

2894
02:04:32,820 --> 02:04:35,760
What's your basic intuition behind that?

2895
02:04:35,760 --> 02:04:39,120
- So first of all, it's not to be an event, right?

2896
02:04:39,120 --> 02:04:40,140
The idea somehow

2897
02:04:40,140 --> 02:04:42,819
which is popularized by science fiction in Hollywood

2898
02:04:42,819 --> 02:04:47,040
that somehow somebody is gonna discover the secret,

2899
02:04:47,040 --> 02:04:50,504
the secret to AGI or human level AI or AmI,

2900
02:04:50,504 --> 02:04:52,410
whatever you wanna call it,

2901
02:04:52,410 --> 02:04:55,200
and then turn on a machine and then we have AGI.

2902
02:04:55,200 --> 02:04:57,120
That's just not going to happen.

2903
02:04:57,120 --> 02:04:58,620
It's not going to be an event.

2904
02:04:59,790 --> 02:05:02,643
It's gonna be gradual progress.

2905
02:05:03,630 --> 02:05:04,590
Are we gonna have systems

2906
02:05:04,590 --> 02:05:07,740
that can learn from video how the world works

2907
02:05:07,740 --> 02:05:09,450
and learn good representations?

2908
02:05:09,450 --> 02:05:10,350
Yeah.

2909
02:05:10,350 --> 02:05:13,050
Before we get them to the scale and performance

2910
02:05:13,050 --> 02:05:14,040
that we observe in humans,

2911
02:05:14,040 --> 02:05:15,630
it's gonna take quite a while.

2912
02:05:15,630 --> 02:05:17,280
It's not gonna happen in one day.

2913
02:05:18,980 --> 02:05:20,490
Are we gonna get systems

2914
02:05:20,490 --> 02:05:24,215
that can have large amount of associated memories

2915
02:05:24,215 --> 02:05:26,700
so they can remember stuff?

2916
02:05:26,700 --> 02:05:27,533
Yeah.

2917
02:05:27,533 --> 02:05:28,740
But same, it's not gonna happen tomorrow.

2918
02:05:28,740 --> 02:05:30,510
I mean, there is some basic techniques

2919
02:05:30,510 --> 02:05:31,500
that need to be developed.

2920
02:05:31,500 --> 02:05:32,333
We have a lot of them,

2921
02:05:32,333 --> 02:05:36,120
but like to get this to work together with a full system

2922
02:05:36,120 --> 02:05:37,110
is another story.

2923
02:05:37,110 --> 02:05:39,240
Are we gonna have systems that can reason and plan,

2924
02:05:39,240 --> 02:05:43,110
perhaps along the lines of objective driven AI architectures

2925
02:05:43,110 --> 02:05:45,000
that I described before?

2926
02:05:45,000 --> 02:05:47,223
Yeah, but like before we get this to work properly,

2927
02:05:47,223 --> 02:05:48,483
it's gonna take a while.

2928
02:05:49,320 --> 02:05:51,330
And before we get all those things to work together.

2929
02:05:51,330 --> 02:05:52,800
And then on top of this,

2930
02:05:52,800 --> 02:05:55,140
have systems that can learn like hierarchical planning,

2931
02:05:55,140 --> 02:05:56,790
hierarchical representations,

2932
02:05:56,790 --> 02:05:58,368
systems that can be configured

2933
02:05:58,368 --> 02:06:00,620
for a lot of different situation at hands

2934
02:06:00,620 --> 02:06:02,847
the way the human brain can.

2935
02:06:02,847 --> 02:06:07,523
All of this is gonna take at least a decade,

2936
02:06:07,523 --> 02:06:08,589
probably much more,

2937
02:06:08,589 --> 02:06:11,070
because there are a lot of problems

2938
02:06:11,070 --> 02:06:12,663
that we're not seeing right now

2939
02:06:12,663 --> 02:06:15,270
that we have not encountered.

2940
02:06:15,270 --> 02:06:17,220
And so we don't know if there is an easy solution

2941
02:06:17,220 --> 02:06:18,573
within this framework.

2942
02:06:21,600 --> 02:06:23,340
It's not just around the corner.

2943
02:06:23,340 --> 02:06:27,540
I mean, I've been hearing people for the last 12, 15 years

2944
02:06:27,540 --> 02:06:29,699
claiming that AGI is just around the corner

2945
02:06:29,699 --> 02:06:32,580
and being systematically wrong.

2946
02:06:32,580 --> 02:06:34,470
And I knew they were wrong when they were saying it.

2947
02:06:34,470 --> 02:06:35,494
I called it bullshit.

2948
02:06:35,494 --> 02:06:36,540
(laughs)

2949
02:06:36,540 --> 02:06:38,160
- Why do you think people have been calling...

2950
02:06:38,160 --> 02:06:39,690
First of all, I mean, from the beginning of,

2951
02:06:39,690 --> 02:06:41,694
from the birth of the term artificial intelligence,

2952
02:06:41,694 --> 02:06:45,280
there has been an eternal optimism

2953
02:06:46,200 --> 02:06:49,050
that's perhaps unlike other technologies.

2954
02:06:49,050 --> 02:06:51,780
Is it Moravec's paradox?

2955
02:06:51,780 --> 02:06:53,399
Is it the explanation

2956
02:06:53,399 --> 02:06:56,970
for why people are so optimistic about AGI?

2957
02:06:56,970 --> 02:06:58,702
- I don't think it's just Moravec's paradox.

2958
02:06:58,702 --> 02:07:00,120
Moravec's paradox is a consequence

2959
02:07:00,120 --> 02:07:03,720
of realizing that the world is not as easy as we think.

2960
02:07:03,720 --> 02:07:08,236
So first of all, intelligence is not a linear thing

2961
02:07:08,236 --> 02:07:10,530
that you can measure with a scaler,

2962
02:07:10,530 --> 02:07:11,580
with a single number.

2963
02:07:12,736 --> 02:07:17,736
Can you say that humans are smarter than orangutans?

2964
02:07:18,330 --> 02:07:20,190
In some ways, yes,

2965
02:07:20,190 --> 02:07:22,110
but in some ways orangutans are smarter than humans

2966
02:07:22,110 --> 02:07:23,790
in a lot of domains

2967
02:07:23,790 --> 02:07:26,010
that allows them to survive in the forest, (laughing)

2968
02:07:26,010 --> 02:07:26,843
for example.

2969
02:07:26,843 --> 02:07:30,360
- So IQ is a very limited measure of intelligence.

2970
02:07:30,360 --> 02:07:31,193
True intelligence

2971
02:07:31,193 --> 02:07:33,840
is bigger than what IQ, for example, measures.

2972
02:07:33,840 --> 02:07:38,763
- Well, IQ can measure approximately something for humans,

2973
02:07:39,660 --> 02:07:43,680
but because humans kind of come

2974
02:07:43,680 --> 02:07:48,410
in relatively kind of uniform form, right?

2975
02:07:48,410 --> 02:07:49,677
- [Lex] Yeah.

2976
02:07:49,677 --> 02:07:53,283
- But it only measures one type of ability

2977
02:07:53,283 --> 02:07:56,613
that may be relevant for some tasks, but not others.

2978
02:07:58,920 --> 02:08:02,199
But then if you are talking about other intelligent entities

2979
02:08:02,199 --> 02:08:07,140
for which the basic things that are easy to them

2980
02:08:07,140 --> 02:08:08,790
is very different,

2981
02:08:08,790 --> 02:08:11,400
then it doesn't mean anything.

2982
02:08:11,400 --> 02:08:15,623
So intelligence is a collection of skills

2983
02:08:18,090 --> 02:08:21,123
and an ability to acquire new skills efficiently.

2984
02:08:22,410 --> 02:08:23,910
Right?

2985
02:08:23,910 --> 02:08:25,650
And the collection of skills

2986
02:08:25,650 --> 02:08:29,500
that a particular intelligent entity possess

2987
02:08:29,500 --> 02:08:31,680
or is capable of learning quickly

2988
02:08:31,680 --> 02:08:35,310
is different from the collection of skills of another one.

2989
02:08:35,310 --> 02:08:37,410
And because it's a multidimensional thing,

2990
02:08:37,410 --> 02:08:39,450
the set of skills is a high dimensional space,

2991
02:08:39,450 --> 02:08:40,738
you can't measure.

2992
02:08:40,738 --> 02:08:42,321
You cannot compare two things

2993
02:08:42,321 --> 02:08:45,720
as to whether one is more intelligent than the other.

2994
02:08:45,720 --> 02:08:46,863
It's multidimensional.

2995
02:08:48,720 --> 02:08:53,720
- So you push back against what are called AI doomers a lot.

2996
02:08:55,419 --> 02:08:57,750
Can you explain their perspective

2997
02:08:57,750 --> 02:08:59,760
and why you think they're wrong?

2998
02:08:59,760 --> 02:09:00,593
- Okay.

2999
02:09:00,593 --> 02:09:03,600
So AI doomers imagine all kinds of catastrophe scenarios

3000
02:09:03,600 --> 02:09:07,440
of how AI could escape our control

3001
02:09:07,440 --> 02:09:10,657
and basically kill us all. (laughs)

3002
02:09:10,657 --> 02:09:14,430
And that relies on a whole bunch of assumptions

3003
02:09:14,430 --> 02:09:15,530
that are mostly false.

3004
02:09:16,560 --> 02:09:18,120
So the first assumption

3005
02:09:18,120 --> 02:09:20,190
is that the emergence of super intelligence

3006
02:09:20,190 --> 02:09:21,780
could be an event.

3007
02:09:21,780 --> 02:09:25,050
That at some point we're going to figure out the secret

3008
02:09:25,050 --> 02:09:28,290
and we'll turn on a machine that is super intelligent.

3009
02:09:28,290 --> 02:09:30,420
And because we'd never done it before,

3010
02:09:30,420 --> 02:09:33,030
it's gonna take over the world and kill us all.

3011
02:09:33,030 --> 02:09:33,870
That is false.

3012
02:09:33,870 --> 02:09:35,880
It's not gonna be an event.

3013
02:09:35,880 --> 02:09:39,581
We're gonna have systems that are like as smart as a cat,

3014
02:09:39,581 --> 02:09:44,581
have all the characteristics of human level intelligence,

3015
02:09:44,940 --> 02:09:46,020
but their level of intelligence

3016
02:09:46,020 --> 02:09:49,833
would be like a cat or a parrot maybe or something.

3017
02:09:50,740 --> 02:09:53,850
And then we're gonna walk our way up

3018
02:09:53,850 --> 02:09:55,410
to kind of make those things more intelligent.

3019
02:09:55,410 --> 02:09:56,760
And as we make them more intelligent,

3020
02:09:56,760 --> 02:09:58,347
we're also gonna put some guardrails in them

3021
02:09:58,347 --> 02:10:00,420
and learn how to kind of put some guardrails

3022
02:10:00,420 --> 02:10:01,740
so they behave properly.

3023
02:10:01,740 --> 02:10:03,810
And we're not gonna do this with just one...

3024
02:10:03,810 --> 02:10:04,770
It's not gonna be one effort,

3025
02:10:04,770 --> 02:10:07,590
but it's gonna be lots of different people doing this.

3026
02:10:07,590 --> 02:10:09,270
And some of them are gonna succeed

3027
02:10:09,270 --> 02:10:12,477
at making intelligent systems that are controllable and safe

3028
02:10:12,477 --> 02:10:14,370
and have the right guardrails.

3029
02:10:14,370 --> 02:10:15,930
And if some other goes rogue,

3030
02:10:15,930 --> 02:10:19,080
then we can use the good ones to go against the rogue ones.

3031
02:10:19,080 --> 02:10:20,370
(laughs)

3032
02:10:20,370 --> 02:10:24,940
So it's gonna be smart AI police against your rogue AI.

3033
02:10:24,940 --> 02:10:27,660
So it's not gonna be like we're gonna be exposed

3034
02:10:27,660 --> 02:10:29,450
to like a single rogue AI that's gonna kill us all.

3035
02:10:29,450 --> 02:10:31,800
That's just not happening.

3036
02:10:31,800 --> 02:10:33,270
Now, there is another fallacy,

3037
02:10:33,270 --> 02:10:36,300
which is the fact that because the system is intelligent,

3038
02:10:36,300 --> 02:10:38,013
it necessarily wants to take over.

3039
02:10:40,650 --> 02:10:43,350
And there is several arguments

3040
02:10:43,350 --> 02:10:44,760
that make people scared of this,

3041
02:10:44,760 --> 02:10:48,128
which I think are completely false as well.

3042
02:10:48,128 --> 02:10:53,128
So one of them is in nature,

3043
02:10:53,400 --> 02:10:54,867
it seems to be that the more intelligent species

3044
02:10:54,867 --> 02:10:58,140
are the ones that end up dominating the other.

3045
02:10:58,140 --> 02:11:03,107
And even extinguishing the others

3046
02:11:03,107 --> 02:11:06,783
sometimes by design, sometimes just by mistake.

3047
02:11:09,649 --> 02:11:12,734
And so there is sort of a thinking

3048
02:11:12,734 --> 02:11:15,990
by which you say, well, if AI systems

3049
02:11:15,990 --> 02:11:17,400
are more intelligent than us,

3050
02:11:17,400 --> 02:11:19,620
surely they're going to eliminate us,

3051
02:11:19,620 --> 02:11:21,480
if not by design,

3052
02:11:21,480 --> 02:11:23,480
simply because they don't care about us.

3053
02:11:24,360 --> 02:11:27,398
And that's just preposterous for a number of reasons.

3054
02:11:27,398 --> 02:11:30,308
First reason is they're not going to be a species.

3055
02:11:30,308 --> 02:11:33,240
They're not gonna be a species that competes with us.

3056
02:11:33,240 --> 02:11:35,430
They're not gonna have the desire to dominate

3057
02:11:35,430 --> 02:11:36,720
because the desire to dominate

3058
02:11:36,720 --> 02:11:38,580
is something that has to be hardwired

3059
02:11:38,580 --> 02:11:41,013
into an intelligent system.

3060
02:11:42,180 --> 02:11:43,623
It is hardwired in humans,

3061
02:11:44,619 --> 02:11:46,170
it is hardwired in baboons,

3062
02:11:46,170 --> 02:11:47,883
in chimpanzees, in wolves,

3063
02:11:48,840 --> 02:11:49,983
not in orangutans.

3064
02:11:51,300 --> 02:11:56,300
The species in which this desire to dominate or submit

3065
02:11:56,460 --> 02:11:59,020
or attain status in other ways

3066
02:12:00,335 --> 02:12:03,240
is specific to social species.

3067
02:12:03,240 --> 02:12:06,420
Non-social species like orangutans don't have it.

3068
02:12:06,420 --> 02:12:07,253
Right?

3069
02:12:07,253 --> 02:12:09,060
And they are as smart as we are, almost.

3070
02:12:09,060 --> 02:12:10,726
Right?

3071
02:12:10,726 --> 02:12:12,090
- And to you, there's not significant incentive

3072
02:12:12,090 --> 02:12:15,120
for humans to encode that into the AI systems.

3073
02:12:15,120 --> 02:12:17,550
And to the degree they do,

3074
02:12:17,550 --> 02:12:22,080
there'll be other AIs that sort of punish them for it.

3075
02:12:22,080 --> 02:12:22,980
Out-compete them over-

3076
02:12:22,980 --> 02:12:24,270
- Well, there's all kinds of incentive

3077
02:12:24,270 --> 02:12:26,790
to make AI systems submissive to humans.

3078
02:12:26,790 --> 02:12:27,700
Right? - [Lex] Right.

3079
02:12:27,700 --> 02:12:29,783
- I mean, this is the way we're gonna build them, right?

3080
02:12:29,783 --> 02:12:32,733
And so then people say, oh, but look at LLMs.

3081
02:12:32,733 --> 02:12:33,900
LLMs are not controllable.

3082
02:12:33,900 --> 02:12:35,042
And they're right,

3083
02:12:35,042 --> 02:12:36,690
LLMs are not controllable.

3084
02:12:36,690 --> 02:12:37,920
But objective driven AI,

3085
02:12:37,920 --> 02:12:41,430
so systems that derive their answers

3086
02:12:41,430 --> 02:12:43,710
by optimization of an objective

3087
02:12:43,710 --> 02:12:45,374
means they have to optimize this objective,

3088
02:12:45,374 --> 02:12:48,270
and that objective can include guardrails.

3089
02:12:48,270 --> 02:12:52,770
One guardrail is obey humans.

3090
02:12:52,770 --> 02:12:54,570
Another guardrail is don't obey humans

3091
02:12:54,570 --> 02:12:56,913
if it's hurting other humans-

3092
02:12:56,913 --> 02:12:59,520
- I've heard that before somewhere, I don't remember-

3093
02:12:59,520 --> 02:13:00,535
- [Yann] Yes. (Lex laughs)

3094
02:13:00,535 --> 02:13:01,890
Maybe in a book. (laughs)

3095
02:13:01,890 --> 02:13:03,270
- Yeah.

3096
02:13:03,270 --> 02:13:04,485
But speaking of that book,

3097
02:13:04,485 --> 02:13:08,010
could there be unintended consequences also

3098
02:13:08,010 --> 02:13:09,150
from all of this?

3099
02:13:09,150 --> 02:13:09,983
- No, of course.

3100
02:13:09,983 --> 02:13:12,570
So this is not a simple problem, right?

3101
02:13:12,570 --> 02:13:14,176
I mean designing those guardrails

3102
02:13:14,176 --> 02:13:16,200
so that the system behaves properly

3103
02:13:16,200 --> 02:13:20,790
is not gonna be a simple issue

3104
02:13:20,790 --> 02:13:22,440
for which there is a silver bullet,

3105
02:13:22,440 --> 02:13:23,910
for which you have a mathematical proof

3106
02:13:23,910 --> 02:13:25,560
that the system can be safe.

3107
02:13:25,560 --> 02:13:27,360
It's gonna be very progressive,

3108
02:13:27,360 --> 02:13:28,650
iterative design system

3109
02:13:28,650 --> 02:13:31,140
where we put those guardrails

3110
02:13:31,140 --> 02:13:32,940
in such a way that the system behave properly.

3111
02:13:32,940 --> 02:13:35,070
And sometimes they're going to do something

3112
02:13:35,070 --> 02:13:38,097
that was unexpected because the guardrail wasn't right,

3113
02:13:38,097 --> 02:13:41,070
and we're gonna correct them so that they do it right.

3114
02:13:41,070 --> 02:13:44,070
The idea somehow that we can't get it slightly wrong,

3115
02:13:44,070 --> 02:13:46,440
because if we get it slightly wrong we all die,

3116
02:13:46,440 --> 02:13:47,913
is ridiculous.

3117
02:13:49,260 --> 02:13:50,960
We're just gonna go progressively.

3118
02:13:53,538 --> 02:13:56,847
The analogy I've used many times is turbojet design.

3119
02:14:00,490 --> 02:14:02,160
How did we figure out

3120
02:14:02,160 --> 02:14:06,693
how to make turbojets so unbelievably reliable, right?

3121
02:14:06,693 --> 02:14:10,684
I mean, those are like incredibly complex pieces of hardware

3122
02:14:10,684 --> 02:14:12,810
that run at really high temperatures

3123
02:14:12,810 --> 02:14:17,580
for 20 hours at a time sometimes.

3124
02:14:17,580 --> 02:14:20,790
And we can fly halfway around the world

3125
02:14:20,790 --> 02:14:25,790
on a two engine jet liner at near the speed of sound.

3126
02:14:27,240 --> 02:14:28,340
Like how incredible is this?

3127
02:14:28,340 --> 02:14:30,093
It is just unbelievable.

3128
02:14:31,110 --> 02:14:33,510
And did we do this

3129
02:14:33,510 --> 02:14:35,640
because we invented like a general principle

3130
02:14:35,640 --> 02:14:37,110
of how to make turbojets safe?

3131
02:14:37,110 --> 02:14:39,030
No, it took decades

3132
02:14:39,030 --> 02:14:40,980
to kind of fine tune the design of those systems

3133
02:14:40,980 --> 02:14:43,320
so that they were safe.

3134
02:14:43,320 --> 02:14:46,231
Is there a separate group

3135
02:14:46,231 --> 02:14:50,100
within General Electric or Snecma or whatever

3136
02:14:50,100 --> 02:14:54,600
that is specialized in turbojet safety?

3137
02:14:54,600 --> 02:14:56,130
No.

3138
02:14:56,130 --> 02:14:58,950
The design is all about safety.

3139
02:14:58,950 --> 02:15:01,200
Because a better turbojet is also a safer turbojet,

3140
02:15:01,200 --> 02:15:03,630
a more reliable one.

3141
02:15:03,630 --> 02:15:04,740
It's the same for AI.

3142
02:15:04,740 --> 02:15:08,550
Like do you need specific provisions to make AI safe?

3143
02:15:08,550 --> 02:15:10,500
No, you need to make better AI systems

3144
02:15:10,500 --> 02:15:11,340
and they will be safe

3145
02:15:11,340 --> 02:15:14,880
because they are designed to be more useful

3146
02:15:14,880 --> 02:15:16,440
and more controllable.

3147
02:15:16,440 --> 02:15:17,819
- So let's imagine a system,

3148
02:15:17,819 --> 02:15:22,819
AI system that's able to be incredibly convincing

3149
02:15:23,310 --> 02:15:24,963
and can convince you of anything.

3150
02:15:25,890 --> 02:15:28,113
I can at least imagine such a system.

3151
02:15:29,160 --> 02:15:33,990
And I can see such a system be weapon-like,

3152
02:15:33,990 --> 02:15:35,490
because it can control people's minds,

3153
02:15:35,490 --> 02:15:37,080
we're pretty gullible.

3154
02:15:37,080 --> 02:15:38,337
We want to believe a thing.

3155
02:15:38,337 --> 02:15:40,860
And you can have an AI system that controls it

3156
02:15:40,860 --> 02:15:43,623
and you could see governments using that as a weapon.

3157
02:15:44,790 --> 02:15:47,613
So do you think if you imagine such a system,

3158
02:15:48,600 --> 02:15:53,340
there's any parallel to something like nuclear weapons?

3159
02:15:53,340 --> 02:15:54,450
- [Yann] No.

3160
02:15:54,450 --> 02:15:58,800
- So why is that technology different?

3161
02:15:58,800 --> 02:16:01,440
So you're saying there's going to be gradual development?

3162
02:16:01,440 --> 02:16:02,273
- [Yann] Yeah.

3163
02:16:02,273 --> 02:16:03,962
- I mean it might be rapid,

3164
02:16:03,962 --> 02:16:05,940
but they'll be iterative.

3165
02:16:05,940 --> 02:16:09,060
And then we'll be able to kind of respond and so on.

3166
02:16:09,060 --> 02:16:12,751
- So that AI system designed by Vladimir Putin or whatever,

3167
02:16:12,751 --> 02:16:16,765
or his minions (laughing)

3168
02:16:16,765 --> 02:16:21,765
is gonna be like trying to talk to every American

3169
02:16:21,780 --> 02:16:24,003
to convince them to vote for-

3170
02:16:25,050 --> 02:16:25,920
- [Lex] Whoever.

3171
02:16:25,920 --> 02:16:30,920
- Whoever pleases Putin or whatever.

3172
02:16:31,200 --> 02:16:36,200
Or rile people up against each other

3173
02:16:36,465 --> 02:16:37,983
as they've been trying to do.

3174
02:16:39,270 --> 02:16:40,950
They're not gonna be talking to you,

3175
02:16:40,950 --> 02:16:43,200
they're gonna be talking to your AI assistant

3176
02:16:44,400 --> 02:16:47,764
which is going to be as smart as theirs, right?

3177
02:16:47,764 --> 02:16:51,120
Because as I said, in the future,

3178
02:16:51,120 --> 02:16:53,370
every single one of your interaction with the digital world

3179
02:16:53,370 --> 02:16:55,799
will be mediated by your AI assistant.

3180
02:16:55,799 --> 02:16:58,388
So the first thing you're gonna ask is, is this a scam?

3181
02:16:58,388 --> 02:17:00,719
Like is this thing like telling me the truth?

3182
02:17:00,719 --> 02:17:03,240
Like it's not even going to be able to get to you

3183
02:17:03,240 --> 02:17:05,340
because it's only going to talk to your AI assistant,

3184
02:17:05,340 --> 02:17:07,374
and your AI is not even going to...

3185
02:17:07,374 --> 02:17:10,740
It's gonna be like a spam filter, right?

3186
02:17:10,740 --> 02:17:13,920
You're not even seeing the email, the spam email, right?

3187
02:17:13,920 --> 02:17:16,743
It's automatically put in a folder that you never see.

3188
02:17:16,743 --> 02:17:18,330
It's gonna be the same thing.

3189
02:17:18,330 --> 02:17:21,272
That AI system that tries to convince you of something,

3190
02:17:21,272 --> 02:17:22,920
it's gonna be talking to an AI system

3191
02:17:22,920 --> 02:17:25,202
which is gonna be at least as smart as it.

3192
02:17:26,285 --> 02:17:29,508
And is gonna say, this is spam. (laughs)

3193
02:17:29,508 --> 02:17:32,219
It's not even going to bring it to your attention.

3194
02:17:32,219 --> 02:17:34,721
- So to you it's very difficult for any one AI system

3195
02:17:34,721 --> 02:17:37,440
to take such a big leap ahead

3196
02:17:37,440 --> 02:17:40,049
to where it can convince even the other AI systems?

3197
02:17:40,049 --> 02:17:43,740
So like there's always going to be this kind of race

3198
02:17:43,740 --> 02:17:46,590
where nobody's way ahead?

3199
02:17:46,590 --> 02:17:48,809
- That's the history of the world.

3200
02:17:48,809 --> 02:17:49,643
History of the world

3201
02:17:49,643 --> 02:17:51,637
is whenever there is a progress someplace,

3202
02:17:51,637 --> 02:17:54,030
there is a countermeasure.

3203
02:17:54,030 --> 02:17:57,290
And it's a cat and mouse game.

3204
02:17:57,290 --> 02:17:58,498
- Mostly yes,

3205
02:17:58,498 --> 02:18:01,650
but this is why nuclear weapons are so interesting

3206
02:18:01,650 --> 02:18:05,370
because that was such a powerful weapon

3207
02:18:05,370 --> 02:18:07,413
that it mattered who got it first.

3208
02:18:08,430 --> 02:18:13,416
That you could imagine Hitler, Stalin, Mao

3209
02:18:16,440 --> 02:18:17,727
getting the weapon first

3210
02:18:17,727 --> 02:18:21,476
and that having a different kind of impact on the world

3211
02:18:21,476 --> 02:18:24,299
than the United States getting the weapon first.

3212
02:18:24,299 --> 02:18:27,510
To you, nuclear weapons is like...

3213
02:18:27,510 --> 02:18:32,250
You don't imagine a breakthrough discovery

3214
02:18:32,250 --> 02:18:35,820
and then Manhattan project like effort for AI?

3215
02:18:35,820 --> 02:18:36,653
- No.

3216
02:18:36,653 --> 02:18:39,209
As I said, it's not going to be an event.

3217
02:18:39,209 --> 02:18:41,956
It's gonna be continuous progress.

3218
02:18:41,956 --> 02:18:45,691
And whenever one breakthrough occurs,

3219
02:18:45,691 --> 02:18:48,990
it's gonna be widely disseminated really quickly.

3220
02:18:48,990 --> 02:18:51,090
Probably first within industry.

3221
02:18:51,090 --> 02:18:52,170
I mean, this is not a domain

3222
02:18:52,170 --> 02:18:55,156
where government or military organizations

3223
02:18:55,156 --> 02:18:57,075
are particularly innovative,

3224
02:18:57,075 --> 02:18:59,594
and they're in fact way behind.

3225
02:18:59,594 --> 02:19:02,340
And so this is gonna come from industry.

3226
02:19:02,340 --> 02:19:04,846
And this kind of information disseminates extremely quickly.

3227
02:19:04,846 --> 02:19:08,129
We've seen this over the last few years, right?

3228
02:19:08,129 --> 02:19:10,290
Where you have a new...

3229
02:19:10,290 --> 02:19:12,030
Like even take AlphaGo.

3230
02:19:12,030 --> 02:19:13,980
This was reproduced within three months

3231
02:19:14,910 --> 02:19:18,000
even without like particularly detailed information, right?

3232
02:19:18,000 --> 02:19:18,833
- Yeah.

3233
02:19:18,833 --> 02:19:20,915
This is an industry that's not good at secrecy.

3234
02:19:20,915 --> 02:19:21,749
(laughs)

3235
02:19:21,749 --> 02:19:22,980
- But even if there is,

3236
02:19:22,980 --> 02:19:26,263
just the fact that you know that something is possible

3237
02:19:26,263 --> 02:19:28,536
makes you like realize

3238
02:19:28,536 --> 02:19:31,080
that it's worth investing the time to actually do it.

3239
02:19:31,080 --> 02:19:35,223
You may be the second person to do it but you'll do it.

3240
02:19:36,821 --> 02:19:40,520
Say for all the innovations

3241
02:19:40,520 --> 02:19:43,360
of self supervised running transformers,

3242
02:19:43,360 --> 02:19:46,320
decoder only architectures, LLMs.

3243
02:19:46,320 --> 02:19:47,520
I mean those things,

3244
02:19:47,520 --> 02:19:49,860
you don't need to know exactly the details of how they work

3245
02:19:49,860 --> 02:19:52,108
to know that it's possible

3246
02:19:52,108 --> 02:19:54,690
because it's deployed and then it's getting reproduced.

3247
02:19:54,690 --> 02:19:59,690
And then people who work for those companies move.

3248
02:20:00,420 --> 02:20:02,550
They go from one company to another.

3249
02:20:02,550 --> 02:20:05,070
And the information disseminates.

3250
02:20:05,070 --> 02:20:09,750
What makes the success of the US tech industry

3251
02:20:09,750 --> 02:20:11,730
and Silicon Valley in particular, is exactly that,

3252
02:20:11,730 --> 02:20:14,400
is because information circulates really, really quickly

3253
02:20:14,400 --> 02:20:17,460
and disseminates very quickly.

3254
02:20:17,460 --> 02:20:21,750
And so the whole region sort of is ahead

3255
02:20:21,750 --> 02:20:24,570
because of that circulation of information.

3256
02:20:24,570 --> 02:20:28,560
- Maybe just to linger on the psychology of AI doomers.

3257
02:20:28,560 --> 02:20:31,903
You give in the classic Yann LeCun way,

3258
02:20:31,903 --> 02:20:33,480
a pretty good example

3259
02:20:33,480 --> 02:20:36,840
of just when a new technology comes to be,

3260
02:20:36,840 --> 02:20:38,724
you say engineer says,

3261
02:20:38,724 --> 02:20:43,724
"I invented this new thing, I call it a ballpen."

3262
02:20:44,220 --> 02:20:46,297
And then the TwitterSphere responds,

3263
02:20:46,297 --> 02:20:48,509
"OMG people could write horrible things with it

3264
02:20:48,509 --> 02:20:51,030
like misinformation, propaganda, hate speech.

3265
02:20:51,030 --> 02:20:52,740
Ban it now!"

3266
02:20:52,740 --> 02:20:54,813
Then writing doomers come in,

3267
02:20:55,920 --> 02:20:57,547
akin to the AI doomers,

3268
02:20:57,547 --> 02:21:01,020
"imagine if everyone can get a ballpen.

3269
02:21:01,020 --> 02:21:01,991
This could destroy society.

3270
02:21:01,991 --> 02:21:03,120
There should be a law

3271
02:21:03,120 --> 02:21:05,610
against using ballpen to write hate speech,

3272
02:21:05,610 --> 02:21:07,260
regulate ballpens now."

3273
02:21:07,260 --> 02:21:09,787
And then the pencil industry mogul says,

3274
02:21:09,787 --> 02:21:12,690
"yeah, ballpens are very dangerous,

3275
02:21:12,690 --> 02:21:15,810
unlike pencil writing which is erasable,

3276
02:21:15,810 --> 02:21:18,510
ballpen writing stays forever.

3277
02:21:18,510 --> 02:21:21,807
Government should require a license for a pen manufacturer."

3278
02:21:22,680 --> 02:21:27,680
I mean, this does seem to be part of human psychology

3279
02:21:28,114 --> 02:21:31,593
when it comes up against new technology.

3280
02:21:33,146 --> 02:21:36,783
What deep insights can you speak to about this?

3281
02:21:37,770 --> 02:21:42,300
- Well, there is a natural fear of new technology

3282
02:21:43,500 --> 02:21:45,360
and the impact it can have on society.

3283
02:21:45,360 --> 02:21:48,960
And people have kind of instinctive reaction

3284
02:21:48,960 --> 02:21:52,980
to the world they know

3285
02:21:52,980 --> 02:21:55,987
being threatened by major transformations

3286
02:21:55,987 --> 02:21:57,850
that are either cultural phenomena

3287
02:21:57,850 --> 02:22:01,053
or technological revolutions.

3288
02:22:02,190 --> 02:22:04,193
And they fear for their culture,

3289
02:22:04,193 --> 02:22:05,467
they fear for their job,

3290
02:22:05,467 --> 02:22:10,207
they fear for the future of their children

3291
02:22:10,207 --> 02:22:13,860
and their way of life, right?

3292
02:22:13,860 --> 02:22:17,043
So any change is feared.

3293
02:22:17,043 --> 02:22:20,039
And you see this along history,

3294
02:22:20,039 --> 02:22:24,060
like any technological revolution or cultural phenomenon

3295
02:22:24,060 --> 02:22:29,060
was always accompanied by groups or reaction in the media

3296
02:22:31,095 --> 02:22:36,095
that basically attributed all the problems,

3297
02:22:36,360 --> 02:22:37,770
the current problems of society

3298
02:22:37,770 --> 02:22:40,320
to that particular change, right?

3299
02:22:40,320 --> 02:22:44,493
Electricity was going to kill everyone at some point.

3300
02:22:45,397 --> 02:22:47,880
The train was going to be a horrible thing

3301
02:22:47,880 --> 02:22:50,883
because you can't breathe past 50 kilometers an hour.

3302
02:22:52,230 --> 02:22:54,000
And so there's a wonderful website

3303
02:22:54,000 --> 02:22:56,873
called a Pessimists Archive, right?

3304
02:22:56,873 --> 02:22:59,430
Which has all those newspaper clips (laughing)

3305
02:22:59,430 --> 02:23:02,245
of all the horrible things people imagined would arrive

3306
02:23:02,245 --> 02:23:06,878
because of either technological innovation

3307
02:23:06,878 --> 02:23:09,933
or a cultural phenomenon.

3308
02:23:13,505 --> 02:23:18,505
Wonderful examples of jazz or comic books

3309
02:23:18,670 --> 02:23:23,010
being blamed for unemployment

3310
02:23:23,010 --> 02:23:25,847
or young people not wanting to work anymore

3311
02:23:25,847 --> 02:23:27,360
and things like that, right?

3312
02:23:27,360 --> 02:23:30,693
And that has existed for centuries.

3313
02:23:34,323 --> 02:23:36,723
And it's knee jerk reactions.

3314
02:23:38,520 --> 02:23:43,520
The question is do we embrace change or do we resist it?

3315
02:23:45,137 --> 02:23:47,190
And what are the real dangers

3316
02:23:47,190 --> 02:23:50,523
as opposed to the imagined imagined ones?

3317
02:23:51,810 --> 02:23:53,820
- So people worry about...

3318
02:23:53,820 --> 02:23:55,860
I think one thing they worry about with big tech,

3319
02:23:55,860 --> 02:23:58,650
something we've been talking about over and over

3320
02:23:58,650 --> 02:24:02,310
but I think worth mentioning again,

3321
02:24:02,310 --> 02:24:05,050
they worry about how powerful AI will be

3322
02:24:05,970 --> 02:24:07,380
and they worry about it

3323
02:24:07,380 --> 02:24:09,482
being in the hands of one centralized power

3324
02:24:09,482 --> 02:24:13,740
of just a handful of central control.

3325
02:24:13,740 --> 02:24:16,033
And so that's the skepticism with big tech.

3326
02:24:16,033 --> 02:24:18,780
These companies can make a huge amount of money

3327
02:24:18,780 --> 02:24:21,780
and control this technology.

3328
02:24:21,780 --> 02:24:24,003
And by so doing,

3329
02:24:24,900 --> 02:24:29,100
take advantage, abuse the little guy in society.

3330
02:24:29,100 --> 02:24:31,920
- Well, that's exactly why we need open source platforms.

3331
02:24:31,920 --> 02:24:32,753
- Yeah.

3332
02:24:32,753 --> 02:24:34,722
I just wanted to... (laughs)

3333
02:24:34,722 --> 02:24:36,712
Nail the point home more and more.

3334
02:24:36,712 --> 02:24:37,545
- [Yann] Yes.

3335
02:24:38,520 --> 02:24:40,620
- So let me ask you on your...

3336
02:24:40,620 --> 02:24:42,420
Like I said, you do get a little bit

3337
02:24:44,490 --> 02:24:46,800
flavorful on the internet.

3338
02:24:46,800 --> 02:24:50,850
Joscha Bach tweeted something that you LOL'd at

3339
02:24:50,850 --> 02:24:53,340
in reference to HAL 9,000.

3340
02:24:53,340 --> 02:24:54,173
Quote,

3341
02:24:54,173 --> 02:24:55,680
"I appreciate your argument

3342
02:24:55,680 --> 02:24:57,540
and I fully understand your frustration,

3343
02:24:57,540 --> 02:25:01,110
but whether the pod bay doors should be opened or closed

3344
02:25:01,110 --> 02:25:03,900
is a complex and nuanced issue."

3345
02:25:03,900 --> 02:25:06,993
So you're at the head of Meta AI.

3346
02:25:08,866 --> 02:25:12,030
This is something that really worries me,

3347
02:25:12,030 --> 02:25:15,540
that our AI overlords

3348
02:25:15,540 --> 02:25:20,460
will speak down to us with corporate speak of this nature

3349
02:25:20,460 --> 02:25:23,433
and you sort of resist that with your way of being.

3350
02:25:24,540 --> 02:25:27,150
Is this something you can just comment on

3351
02:25:27,150 --> 02:25:29,550
sort of working at a big company,

3352
02:25:29,550 --> 02:25:34,550
how you can avoid the over fearing, I suppose,

3353
02:25:37,830 --> 02:25:41,370
the through caution create harm?

3354
02:25:41,370 --> 02:25:42,300
- Yeah.

3355
02:25:42,300 --> 02:25:45,822
Again, I think the answer to this is open source platforms

3356
02:25:45,822 --> 02:25:49,770
and then enabling a widely diverse set of people

3357
02:25:49,770 --> 02:25:53,700
to build AI assistants

3358
02:25:53,700 --> 02:25:55,155
that represent the diversity

3359
02:25:55,155 --> 02:25:57,930
of cultures, opinions, languages,

3360
02:25:57,930 --> 02:25:59,606
and value systems across the world.

3361
02:25:59,606 --> 02:26:04,606
So that you're not bound to just be brainwashed

3362
02:26:04,950 --> 02:26:07,140
by a particular way of thinking

3363
02:26:07,140 --> 02:26:10,440
because of a single AI entity.

3364
02:26:10,440 --> 02:26:13,980
So I mean, I think it's a really, really important question

3365
02:26:13,980 --> 02:26:14,910
for society.

3366
02:26:14,910 --> 02:26:16,503
And the problem I'm seeing,

3367
02:26:20,430 --> 02:26:21,870
which is why I've been so vocal

3368
02:26:21,870 --> 02:26:25,170
and sometimes a little sardonic about it-

3369
02:26:25,170 --> 02:26:26,370
- Never stop.

3370
02:26:26,370 --> 02:26:27,285
Never stop, Yann.

3371
02:26:27,285 --> 02:26:28,620
(both laugh)

3372
02:26:28,620 --> 02:26:31,500
We love it. - Is because I see the danger

3373
02:26:31,500 --> 02:26:32,970
of this concentration of power

3374
02:26:32,970 --> 02:26:36,360
through proprietary AI systems

3375
02:26:36,360 --> 02:26:39,870
as a much bigger danger than everything else.

3376
02:26:39,870 --> 02:26:44,870
That if we really want diversity of opinion AI systems

3377
02:26:46,290 --> 02:26:48,690
that in the future

3378
02:26:48,690 --> 02:26:52,980
that we'll all be interacting through AI systems,

3379
02:26:52,980 --> 02:26:54,270
we need those to be diverse

3380
02:26:54,270 --> 02:26:58,023
for the preservation of a diversity of ideas

3381
02:26:58,023 --> 02:27:03,023
and creeds and political opinions and whatever,

3382
02:27:06,150 --> 02:27:07,800
and the preservation of democracy.

3383
02:27:07,800 --> 02:27:12,150
And what works against this

3384
02:27:12,150 --> 02:27:15,870
is people who think that for reasons of security,

3385
02:27:15,870 --> 02:27:19,320
we should keep AI systems under lock and key

3386
02:27:19,320 --> 02:27:20,250
because it's too dangerous

3387
02:27:20,250 --> 02:27:22,660
to put it in the hands of everybody

3388
02:27:23,641 --> 02:27:26,241
because it could be used by terrorists or something.

3389
02:27:28,800 --> 02:27:33,800
That would lead to potentially a very bad future

3390
02:27:36,240 --> 02:27:38,550
in which all of our information diet

3391
02:27:38,550 --> 02:27:41,132
is controlled by a small number of companies

3392
02:27:41,132 --> 02:27:43,113
through proprietary systems.

3393
02:27:44,294 --> 02:27:47,042
- So you trust humans with this technology

3394
02:27:47,042 --> 02:27:52,042
to build systems that are on the whole good for humanity?

3395
02:27:53,250 --> 02:27:56,153
- Isn't that what democracy and free speech is all about?

3396
02:27:56,153 --> 02:27:57,450
- I think so.

3397
02:27:57,450 --> 02:28:00,360
- Do you trust institutions to do the right thing?

3398
02:28:00,360 --> 02:28:03,179
Do you trust people to do the right thing?

3399
02:28:03,179 --> 02:28:05,370
And yeah, there's bad people who are gonna do bad things,

3400
02:28:05,370 --> 02:28:07,740
but they're not going to have superior technology

3401
02:28:07,740 --> 02:28:08,573
to the good people.

3402
02:28:08,573 --> 02:28:12,540
So then it's gonna be my good AI against your bad AI, right?

3403
02:28:12,540 --> 02:28:15,809
I mean it's the examples that we were just talking about

3404
02:28:15,809 --> 02:28:20,809
of maybe some rogue country will build some AI system

3405
02:28:21,827 --> 02:28:23,850
that's gonna try to convince everybody

3406
02:28:23,850 --> 02:28:27,055
to go into a civil war or something

3407
02:28:27,055 --> 02:28:31,803
or elect a favorable ruler.

3408
02:28:32,892 --> 02:28:35,687
But then they will have to go past our AI systems, right?

3409
02:28:35,687 --> 02:28:36,520
(laughs)

3410
02:28:36,520 --> 02:28:38,700
- An AI system with a strong Russian accent

3411
02:28:38,700 --> 02:28:40,320
will be trying to convince our-

3412
02:28:40,320 --> 02:28:42,600
- And doesn't put any articles in their sentences.

3413
02:28:42,600 --> 02:28:45,750
(both laugh)

3414
02:28:45,750 --> 02:28:48,630
- Well, it'll be at the very least, absurdly comedic.

3415
02:28:48,630 --> 02:28:50,083
Okay.

3416
02:28:50,083 --> 02:28:55,083
So since we talked about sort of the physical reality,

3417
02:28:55,380 --> 02:28:58,730
I'd love to ask your vision of the future with robots

3418
02:28:58,730 --> 02:29:00,570
in this physical reality.

3419
02:29:00,570 --> 02:29:02,791
So many of the kinds of intelligence

3420
02:29:02,791 --> 02:29:05,370
you've been speaking about

3421
02:29:05,370 --> 02:29:06,720
would empower robots

3422
02:29:06,720 --> 02:29:10,470
to be more effective collaborators with us humans.

3423
02:29:10,470 --> 02:29:14,490
So since Tesla's Optimus team

3424
02:29:14,490 --> 02:29:17,160
has been showing us some progress in humanoid robots,

3425
02:29:17,160 --> 02:29:20,610
I think it really reinvigorated the whole industry

3426
02:29:20,610 --> 02:29:22,860
that I think Boston Dynamics has been leading

3427
02:29:22,860 --> 02:29:24,270
for a very, very long time.

3428
02:29:24,270 --> 02:29:25,680
So now there's all kinds of companies,

3429
02:29:25,680 --> 02:29:28,636
Figure AI, obviously Boston Dynamics-

3430
02:29:28,636 --> 02:29:29,989
- [Yann] Unitree.

3431
02:29:29,989 --> 02:29:31,533
- Unitree.

3432
02:29:31,533 --> 02:29:33,480
But there's like a lot of them.

3433
02:29:33,480 --> 02:29:34,313
It's great.

3434
02:29:34,313 --> 02:29:35,400
It's great.

3435
02:29:35,400 --> 02:29:36,333
I mean I love it.

3436
02:29:37,740 --> 02:29:42,390
So do you think there'll be millions of humanoid robots

3437
02:29:42,390 --> 02:29:44,040
walking around soon?

3438
02:29:44,040 --> 02:29:46,260
- Not soon, but it's gonna happen.

3439
02:29:46,260 --> 02:29:47,130
Like the next decade

3440
02:29:47,130 --> 02:29:49,500
I think is gonna be really interesting in robots.

3441
02:29:49,500 --> 02:29:53,670
Like the emergence of the robotics industry

3442
02:29:53,670 --> 02:29:57,720
has been in the waiting for 10, 20 years,

3443
02:29:57,720 --> 02:29:58,710
without really emerging

3444
02:29:58,710 --> 02:30:01,253
other than for like kind of pre-program behavior

3445
02:30:01,253 --> 02:30:03,096
and stuff like that.

3446
02:30:03,096 --> 02:30:08,096
And the main issue is again, the Moravec's paradox.

3447
02:30:08,700 --> 02:30:09,900
Like how do we get the systems

3448
02:30:09,900 --> 02:30:11,160
to understand how the world works

3449
02:30:11,160 --> 02:30:13,200
and kind of plan actions?

3450
02:30:13,200 --> 02:30:15,723
And so we can do it for really specialized tasks.

3451
02:30:17,820 --> 02:30:21,090
And the way Boston Dynamics goes about it

3452
02:30:21,090 --> 02:30:25,265
is basically with a lot of handcrafted dynamical models

3453
02:30:25,265 --> 02:30:28,707
and careful planning in advance,

3454
02:30:28,707 --> 02:30:32,010
which is very classical robotics with a lot of innovation,

3455
02:30:32,010 --> 02:30:33,742
a little bit of perception,

3456
02:30:33,742 --> 02:30:35,760
but it's still not...

3457
02:30:35,760 --> 02:30:38,733
Like they can't build a domestic robot, right?

3458
02:30:40,260 --> 02:30:43,463
And we're still some distance away

3459
02:30:43,463 --> 02:30:46,173
from completely autonomous level five driving.

3460
02:30:47,610 --> 02:30:49,470
And we're certainly very far away

3461
02:30:49,470 --> 02:30:53,403
from having level five autonomous driving

3462
02:30:53,403 --> 02:30:55,740
by a system that can train itself

3463
02:30:55,740 --> 02:30:59,463
by driving 20 hours, like any 17-year-old.

3464
02:31:00,636 --> 02:31:05,636
So until we have, again, world models,

3465
02:31:08,070 --> 02:31:09,270
systems that can train themselves

3466
02:31:09,270 --> 02:31:11,133
to understand how the world works,

3467
02:31:12,188 --> 02:31:16,920
we're not gonna have significant progress in robotics.

3468
02:31:16,920 --> 02:31:18,532
So a lot of the people

3469
02:31:18,532 --> 02:31:21,571
working on robotic hardware at the moment

3470
02:31:21,571 --> 02:31:23,520
are betting or banking

3471
02:31:23,520 --> 02:31:25,283
on the fact that AI

3472
02:31:25,283 --> 02:31:28,020
is gonna make sufficient progress towards that.

3473
02:31:28,020 --> 02:31:31,381
- And they're hoping to discover a product in it too-

3474
02:31:31,381 --> 02:31:32,430
- [Yann] Yeah.

3475
02:31:32,430 --> 02:31:34,650
- Before you have a really strong world model,

3476
02:31:34,650 --> 02:31:38,010
there'll be an almost strong world model.

3477
02:31:38,010 --> 02:31:40,830
And people are trying to find a product

3478
02:31:40,830 --> 02:31:43,740
in a clumsy robot, I suppose.

3479
02:31:43,740 --> 02:31:45,780
Like not a perfectly efficient robot.

3480
02:31:45,780 --> 02:31:46,950
So there's the factory setting

3481
02:31:46,950 --> 02:31:48,360
where humanoid robots

3482
02:31:48,360 --> 02:31:51,270
can help automate some aspects of the factory.

3483
02:31:51,270 --> 02:31:53,370
I think that's a crazy difficult task

3484
02:31:53,370 --> 02:31:54,990
'cause of all the safety required

3485
02:31:54,990 --> 02:31:56,040
and all this kind of stuff,

3486
02:31:56,040 --> 02:31:57,700
I think in the home is more interesting.

3487
02:31:57,700 --> 02:32:00,450
But then you start to think...

3488
02:32:00,450 --> 02:32:03,240
I think you mentioned loading the dishwasher, right?

3489
02:32:03,240 --> 02:32:04,073
- [Yann] Yeah.

3490
02:32:04,073 --> 02:32:06,690
- Like I suppose that's one of the main problems

3491
02:32:06,690 --> 02:32:07,650
you're working on.

3492
02:32:07,650 --> 02:32:10,696
- I mean there's cleaning up. (laughs)

3493
02:32:10,696 --> 02:32:11,529
- [Lex] Yeah.

3494
02:32:11,529 --> 02:32:13,317
- Cleaning the house,

3495
02:32:13,317 --> 02:32:17,190
clearing up the table after a meal,

3496
02:32:17,190 --> 02:32:21,600
washing the dishes, all those tasks, cooking.

3497
02:32:21,600 --> 02:32:24,030
I mean all the tasks that in principle could be automated

3498
02:32:24,030 --> 02:32:26,730
but are actually incredibly sophisticated,

3499
02:32:26,730 --> 02:32:28,320
really complicated.

3500
02:32:28,320 --> 02:32:29,730
- But even just basic navigation

3501
02:32:29,730 --> 02:32:32,100
around a space full of uncertainty.

3502
02:32:32,100 --> 02:32:33,150
- That sort of works.

3503
02:32:33,150 --> 02:32:35,580
Like you can sort of do this now.

3504
02:32:35,580 --> 02:32:37,260
Navigation is fine.

3505
02:32:37,260 --> 02:32:40,708
- Well, navigation in a way that's compelling to us humans

3506
02:32:40,708 --> 02:32:42,840
is a different thing.

3507
02:32:42,840 --> 02:32:43,673
- Yeah.

3508
02:32:43,673 --> 02:32:45,330
It's not gonna be necessarily...

3509
02:32:45,330 --> 02:32:46,560
I mean we have demos actually

3510
02:32:46,560 --> 02:32:51,560
'cause there is a so-called embodied AI group at FAIR

3511
02:32:52,080 --> 02:32:55,140
and they've been not building their own robots

3512
02:32:55,140 --> 02:32:57,153
but using commercial robots.

3513
02:32:57,990 --> 02:33:02,310
And you can tell the robot dog like go to the fridge

3514
02:33:02,310 --> 02:33:03,630
and they can actually open the fridge

3515
02:33:03,630 --> 02:33:05,534
and they can probably pick up a can in the fridge

3516
02:33:05,534 --> 02:33:08,963
and stuff like that and bring it to you.

3517
02:33:08,963 --> 02:33:10,310
So it can navigate,

3518
02:33:10,310 --> 02:33:12,600
it can grab objects

3519
02:33:12,600 --> 02:33:14,337
as long as it's been trained to recognize them,

3520
02:33:14,337 --> 02:33:17,163
which vision systems work pretty well nowadays.

3521
02:33:18,360 --> 02:33:23,188
But it's not like a completely general robot

3522
02:33:23,188 --> 02:33:24,980
that would be sophisticated enough

3523
02:33:24,980 --> 02:33:28,785
to do things like clearing up the dinner table.

3524
02:33:28,785 --> 02:33:30,990
(laughs)

3525
02:33:30,990 --> 02:33:33,240
- Yeah, to me that's an exciting future

3526
02:33:33,240 --> 02:33:35,040
of getting humanoid robots.

3527
02:33:35,040 --> 02:33:36,690
Robots in general in the home more and more

3528
02:33:36,690 --> 02:33:38,019
because it gets humans

3529
02:33:38,019 --> 02:33:40,818
to really directly interact with AI systems

3530
02:33:40,818 --> 02:33:42,090
in the physical space.

3531
02:33:42,090 --> 02:33:44,340
And in so doing it allows us

3532
02:33:44,340 --> 02:33:46,620
to philosophically, psychologically explore

3533
02:33:46,620 --> 02:33:47,840
our relationships with robots.

3534
02:33:47,840 --> 02:33:50,700
It can be really, really interesting.

3535
02:33:50,700 --> 02:33:54,228
So I hope you make progress on the whole JEPA thing soon.

3536
02:33:54,228 --> 02:33:55,068
(laughs)

3537
02:33:55,068 --> 02:33:58,593
- Well, I mean, I hope things can work as planned.

3538
02:34:00,540 --> 02:34:03,120
I mean, again, we've been like kinda working on this idea

3539
02:34:03,120 --> 02:34:07,113
of self supervised learning from video for 10 years.

3540
02:34:07,950 --> 02:34:12,000
And only made significant progress in the last two or three.

3541
02:34:12,000 --> 02:34:13,590
- And actually you've mentioned

3542
02:34:13,590 --> 02:34:15,120
that there's a lot of interesting breakthroughs

3543
02:34:15,120 --> 02:34:18,037
that can happen without having access to a lot of compute.

3544
02:34:18,037 --> 02:34:20,310
So if you're interested in doing a PhD

3545
02:34:20,310 --> 02:34:21,690
in this kind of stuff,

3546
02:34:21,690 --> 02:34:23,680
there's a lot of possibilities still

3547
02:34:23,680 --> 02:34:25,590
to do innovative work.

3548
02:34:25,590 --> 02:34:26,880
So like what advice would you give

3549
02:34:26,880 --> 02:34:30,300
to a undergrad that's looking to go to grad school

3550
02:34:30,300 --> 02:34:32,340
and do a PhD?

3551
02:34:32,340 --> 02:34:35,580
- So basically, I've listed them already.

3552
02:34:35,580 --> 02:34:38,733
This idea of how do you train a world model by observation?

3553
02:34:39,810 --> 02:34:41,400
And you don't have to train necessarily

3554
02:34:41,400 --> 02:34:43,833
on gigantic data sets.

3555
02:34:45,690 --> 02:34:47,070
I mean, it could turn out to be necessary

3556
02:34:47,070 --> 02:34:48,780
to actually train on large data sets

3557
02:34:48,780 --> 02:34:51,780
to have emergent properties like we have with LLMs.

3558
02:34:51,780 --> 02:34:53,640
But I think there is a lot of good ideas that can be done

3559
02:34:53,640 --> 02:34:56,760
without necessarily scaling up.

3560
02:34:56,760 --> 02:34:58,470
Then there is how do you do planning

3561
02:34:58,470 --> 02:35:00,570
with a learn world model?

3562
02:35:00,570 --> 02:35:02,610
If the world the system evolves in

3563
02:35:02,610 --> 02:35:03,720
is not the physical world,

3564
02:35:03,720 --> 02:35:06,372
but is the world of let's say the internet

3565
02:35:06,372 --> 02:35:09,558
or some sort of world

3566
02:35:09,558 --> 02:35:11,513
of where an action consists

3567
02:35:11,513 --> 02:35:13,342
in doing a search in a search engine

3568
02:35:13,342 --> 02:35:14,943
or interrogating a database,

3569
02:35:14,943 --> 02:35:18,120
or running a simulation

3570
02:35:18,120 --> 02:35:19,435
or calling a calculator

3571
02:35:19,435 --> 02:35:21,303
or solving a differential equation,

3572
02:35:22,590 --> 02:35:23,460
how do you get a system

3573
02:35:23,460 --> 02:35:25,320
to actually plan a sequence of actions

3574
02:35:25,320 --> 02:35:28,921
to give the solution to a problem?

3575
02:35:28,921 --> 02:35:32,150
And so the question of planning

3576
02:35:32,150 --> 02:35:35,161
is not just a question of planning physical actions,

3577
02:35:35,161 --> 02:35:39,000
it could be planning actions to use tools

3578
02:35:39,000 --> 02:35:40,040
for a dialogue system

3579
02:35:40,040 --> 02:35:42,333
or for any kind of intelligence system.

3580
02:35:43,230 --> 02:35:47,130
And there's some work on this but not a huge amount.

3581
02:35:47,130 --> 02:35:48,524
Some work at FAIR,

3582
02:35:48,524 --> 02:35:52,268
one called Toolformer, which was a couple years ago

3583
02:35:52,268 --> 02:35:55,191
and some more recent work on planning,

3584
02:35:55,191 --> 02:35:59,730
but I don't think we have like a good solution

3585
02:35:59,730 --> 02:36:00,810
for any of that.

3586
02:36:00,810 --> 02:36:03,600
Then there is the question of hierarchical planning.

3587
02:36:03,600 --> 02:36:05,995
So the example I mentioned

3588
02:36:05,995 --> 02:36:10,620
of planning a trip from New York to Paris,

3589
02:36:10,620 --> 02:36:11,453
that's hierarchical,

3590
02:36:11,453 --> 02:36:13,830
but almost every action that we take

3591
02:36:13,830 --> 02:36:17,520
involves hierarchical planning in some sense.

3592
02:36:17,520 --> 02:36:20,700
And we really have absolutely no idea how to do this.

3593
02:36:20,700 --> 02:36:22,680
Like there's zero demonstration

3594
02:36:22,680 --> 02:36:26,463
of hierarchical planning in AI,

3595
02:36:28,050 --> 02:36:32,677
where the various levels of representations

3596
02:36:32,677 --> 02:36:36,480
that are necessary have been learned.

3597
02:36:36,480 --> 02:36:39,510
We can do like two level hierarchical planning

3598
02:36:39,510 --> 02:36:41,160
when we design the two levels.

3599
02:36:41,160 --> 02:36:44,880
So for example, you have like a dog legged robot, right?

3600
02:36:44,880 --> 02:36:48,330
You want it to go from the living room to the kitchen.

3601
02:36:48,330 --> 02:36:51,300
You can plan a path that avoids the obstacle.

3602
02:36:51,300 --> 02:36:54,521
And then you can send this to a lower level planner

3603
02:36:54,521 --> 02:36:56,622
that figures out how to move the legs

3604
02:36:56,622 --> 02:36:59,580
to kind of follow that trajectories, right?

3605
02:36:59,580 --> 02:37:00,413
So that works,

3606
02:37:00,413 --> 02:37:03,933
but that two level planning is designed by hand, right?

3607
02:37:05,310 --> 02:37:09,034
We specify what the proper levels of abstraction,

3608
02:37:09,034 --> 02:37:13,170
the representation at each level of abstraction have to be.

3609
02:37:13,170 --> 02:37:14,130
How do you learn this?

3610
02:37:14,130 --> 02:37:16,620
How do you learn that hierarchical representation

3611
02:37:16,620 --> 02:37:19,923
of action plans, right?

3612
02:37:20,880 --> 02:37:22,320
With com nets and deep learning,

3613
02:37:22,320 --> 02:37:23,580
we can train the system

3614
02:37:23,580 --> 02:37:26,313
to learn hierarchical representations of percepts.

3615
02:37:27,420 --> 02:37:28,260
What is the equivalent

3616
02:37:28,260 --> 02:37:30,810
when what you're trying to represent are action plans?

3617
02:37:30,810 --> 02:37:31,860
- For action plans.

3618
02:37:31,860 --> 02:37:32,693
Yeah.

3619
02:37:32,693 --> 02:37:35,550
So you want basically a robot dog or humanoid robot

3620
02:37:35,550 --> 02:37:38,944
that turns on and travels from New York to Paris

3621
02:37:38,944 --> 02:37:40,263
all by itself.

3622
02:37:41,160 --> 02:37:41,993
- [Yann] For example.

3623
02:37:41,993 --> 02:37:43,264
- All right.

3624
02:37:43,264 --> 02:37:47,063
It might have some trouble at the TSA but-

3625
02:37:47,063 --> 02:37:49,140
- No, but even doing something fairly simple

3626
02:37:49,140 --> 02:37:50,580
like a household task.

3627
02:37:50,580 --> 02:37:51,413
- [Lex] Sure.

3628
02:37:51,413 --> 02:37:53,850
- Like cooking or something.

3629
02:37:53,850 --> 02:37:54,683
- Yeah.

3630
02:37:54,683 --> 02:37:55,516
There's a lot involved.

3631
02:37:55,516 --> 02:37:56,865
It's a super complex task.

3632
02:37:56,865 --> 02:37:59,343
Once again, we take it for granted.

3633
02:38:00,630 --> 02:38:05,630
What hope do you have for the future of humanity?

3634
02:38:05,745 --> 02:38:07,830
We're talking about so many exciting technologies,

3635
02:38:07,830 --> 02:38:09,540
so many exciting possibilities.

3636
02:38:09,540 --> 02:38:12,150
What gives you hope when you look out

3637
02:38:12,150 --> 02:38:15,090
over the next 10, 20, 50, 100 years?

3638
02:38:15,090 --> 02:38:16,320
If you look at social media,

3639
02:38:16,320 --> 02:38:21,320
there's wars going on, there's division, there's hatred,

3640
02:38:21,660 --> 02:38:24,600
all this kind of stuff that's also part of humanity.

3641
02:38:24,600 --> 02:38:27,003
But amidst all that, what gives you hope?

3642
02:38:29,130 --> 02:38:30,280
- I love that question.

3643
02:38:34,500 --> 02:38:37,923
We can make humanity smarter with AI.

3644
02:38:39,300 --> 02:38:40,380
Okay?

3645
02:38:40,380 --> 02:38:44,193
I mean AI basically will amplify human intelligence.

3646
02:38:45,300 --> 02:38:47,820
It's as if every one of us

3647
02:38:47,820 --> 02:38:52,200
will have a staff of smart AI assistants.

3648
02:38:52,200 --> 02:38:53,733
They might be smarter than us.

3649
02:38:54,660 --> 02:38:55,810
They'll do our bidding,

3650
02:38:58,860 --> 02:39:01,800
perhaps execute a task

3651
02:39:01,800 --> 02:39:05,340
in ways that are much better than we could do ourselves

3652
02:39:05,340 --> 02:39:07,830
because they'd be smarter than us.

3653
02:39:07,830 --> 02:39:10,650
And so it's like everyone would be the boss

3654
02:39:10,650 --> 02:39:14,673
of a staff of super smart virtual people.

3655
02:39:15,960 --> 02:39:18,480
So we shouldn't feel threatened by this

3656
02:39:18,480 --> 02:39:19,980
any more than we should feel threatened

3657
02:39:19,980 --> 02:39:22,632
by being the manager of a group of people,

3658
02:39:22,632 --> 02:39:24,933
some of whom are more intelligent than us.

3659
02:39:27,300 --> 02:39:29,213
I certainly have a lot of experience with this.

3660
02:39:29,213 --> 02:39:30,046
(laughs)

3661
02:39:30,046 --> 02:39:34,964
Of having people working with me who are smarter than me.

3662
02:39:34,964 --> 02:39:36,720
That's actually a wonderful thing.

3663
02:39:36,720 --> 02:39:40,290
So having machines that are smarter than us,

3664
02:39:40,290 --> 02:39:44,190
that assist us in all of our tasks, our daily lives,

3665
02:39:44,190 --> 02:39:45,810
whether it's professional or personal,

3666
02:39:45,810 --> 02:39:48,300
I think would be an absolutely wonderful thing.

3667
02:39:48,300 --> 02:39:50,403
Because intelligence is the commodity

3668
02:39:52,585 --> 02:39:54,003
that is most in demand.

3669
02:39:55,620 --> 02:39:57,300
I mean, all the mistakes that humanity makes

3670
02:39:57,300 --> 02:39:59,310
is because of lack of intelligence, really,

3671
02:39:59,310 --> 02:40:01,923
or lack of knowledge, which is related.

3672
02:40:02,970 --> 02:40:07,410
So making people smarter which can only be better.

3673
02:40:07,410 --> 02:40:08,790
I mean, for the same reason

3674
02:40:08,790 --> 02:40:10,640
that public education is a good thing

3675
02:40:12,600 --> 02:40:15,090
and books are a good thing,

3676
02:40:15,090 --> 02:40:17,215
and the internet is also a good thing, intrinsically.

3677
02:40:17,215 --> 02:40:19,800
And even social networks are a good thing

3678
02:40:19,800 --> 02:40:21,062
if you run them properly.

3679
02:40:21,062 --> 02:40:21,895
(laughs)

3680
02:40:21,895 --> 02:40:23,240
It's difficult, but you can.

3681
02:40:25,620 --> 02:40:30,620
Because it helps the communication

3682
02:40:31,410 --> 02:40:32,610
of information and knowledge

3683
02:40:32,610 --> 02:40:34,200
and the transmission of knowledge.

3684
02:40:34,200 --> 02:40:36,753
So AI is gonna make humanity smarter.

3685
02:40:37,920 --> 02:40:39,670
And the analogy I've been using

3686
02:40:40,740 --> 02:40:44,887
is the fact that perhaps an equivalent event

3687
02:40:44,887 --> 02:40:47,640
in the history of humanity

3688
02:40:47,640 --> 02:40:52,640
to what might be provided by generalization of AI assistant

3689
02:40:53,130 --> 02:40:55,163
is the invention of the printing press.

3690
02:40:55,163 --> 02:40:57,270
It made everybody smarter.

3691
02:40:57,270 --> 02:41:02,123
The fact that people could have access to books.

3692
02:41:03,720 --> 02:41:06,720
Books were a lot cheaper than they were before.

3693
02:41:06,720 --> 02:41:10,830
And so a lot more people had an incentive to learn to read,

3694
02:41:10,830 --> 02:41:12,280
which wasn't the case before.

3695
02:41:14,250 --> 02:41:17,730
And people became smarter.

3696
02:41:17,730 --> 02:41:21,450
It enabled the enlightenment, right?

3697
02:41:21,450 --> 02:41:22,560
There wouldn't be an enlightenment

3698
02:41:22,560 --> 02:41:24,690
without the printing press.

3699
02:41:24,690 --> 02:41:29,690
It enabled philosophy, rationalism,

3700
02:41:30,817 --> 02:41:33,273
escape from religious doctrine,

3701
02:41:34,851 --> 02:41:38,523
democracy, science.

3702
02:41:41,550 --> 02:41:43,680
And certainly without this

3703
02:41:43,680 --> 02:41:46,200
there wouldn't have been the American Revolution

3704
02:41:46,200 --> 02:41:47,520
or the French Revolution.

3705
02:41:47,520 --> 02:41:52,173
And so we'll still be under feudal regimes perhaps.

3706
02:41:53,910 --> 02:41:57,960
And so it completely transformed the world

3707
02:41:57,960 --> 02:41:59,430
because people became smarter

3708
02:41:59,430 --> 02:42:01,890
and kinda learned about things.

3709
02:42:01,890 --> 02:42:05,040
Now, it also created 200 years

3710
02:42:05,040 --> 02:42:08,820
of essentially religious conflicts in Europe, right?

3711
02:42:08,820 --> 02:42:12,450
Because the first thing that people read was the Bible

3712
02:42:12,450 --> 02:42:15,120
and realized that

3713
02:42:15,120 --> 02:42:17,190
perhaps there was a different interpretation of the Bible

3714
02:42:17,190 --> 02:42:19,920
than what the priests were telling them.

3715
02:42:19,920 --> 02:42:22,740
And so that created the Protestant movement

3716
02:42:22,740 --> 02:42:23,820
and created a rift.

3717
02:42:23,820 --> 02:42:25,512
And in fact, the Catholic church

3718
02:42:25,512 --> 02:42:28,470
didn't like the idea of the printing press

3719
02:42:28,470 --> 02:42:29,970
but they had no choice.

3720
02:42:29,970 --> 02:42:32,274
And so it had some bad effects and some good effects.

3721
02:42:32,274 --> 02:42:33,810
I don't think anyone today

3722
02:42:33,810 --> 02:42:35,880
would say that the invention of the printing press

3723
02:42:35,880 --> 02:42:38,250
had an overall negative effect

3724
02:42:38,250 --> 02:42:40,659
despite the fact that it created 200 years

3725
02:42:40,659 --> 02:42:44,370
of religious conflicts in Europe.

3726
02:42:44,370 --> 02:42:45,813
Now compare this,

3727
02:42:46,770 --> 02:42:48,653
and I was very proud of myself

3728
02:42:48,653 --> 02:42:51,630
to come up with this analogy,

3729
02:42:51,630 --> 02:42:54,693
but realized someone else came with the same idea before me.

3730
02:42:55,530 --> 02:42:58,890
Compare this with what happened in the Ottoman Empire.

3731
02:42:58,890 --> 02:43:03,890
The Ottoman Empire banned the printing press for 200 years.

3732
02:43:07,410 --> 02:43:10,230
And it didn't ban it for all languages,

3733
02:43:10,230 --> 02:43:11,730
only for Arabic.

3734
02:43:11,730 --> 02:43:13,230
You could actually print books

3735
02:43:13,230 --> 02:43:18,210
in Latin or Hebrew or whatever in the Ottoman Empire,

3736
02:43:18,210 --> 02:43:19,293
just not in Arabic.

3737
02:43:20,670 --> 02:43:25,110
And I thought it was because

3738
02:43:25,110 --> 02:43:27,720
the rulers just wanted to preserve

3739
02:43:27,720 --> 02:43:30,136
the control over the population and the dogma,

3740
02:43:30,136 --> 02:43:32,223
religious dogma and everything.

3741
02:43:33,060 --> 02:43:37,353
But after talking with the UAE Minister of AI,

3742
02:43:38,580 --> 02:43:40,203
Omar Al Olama,

3743
02:43:41,701 --> 02:43:44,163
he told me no, there was another reason.

3744
02:43:45,114 --> 02:43:47,951
And the other reason was that

3745
02:43:47,951 --> 02:43:52,877
it was to preserve the corporation of calligraphers, right?

3746
02:43:53,820 --> 02:43:56,040
There's like an art form

3747
02:43:56,040 --> 02:44:01,040
which is writing those beautiful Arabic poems

3748
02:44:02,220 --> 02:44:04,797
or whatever religious text in this thing.

3749
02:44:04,797 --> 02:44:07,980
And it was very powerful corporation of scribes basically

3750
02:44:07,980 --> 02:44:12,270
that kinda run a big chunk of the empire.

3751
02:44:12,270 --> 02:44:14,190
And we couldn't put them out of business.

3752
02:44:14,190 --> 02:44:16,080
So they banned the bridging press

3753
02:44:16,080 --> 02:44:18,603
in part to protect that business.

3754
02:44:21,330 --> 02:44:23,130
Now, what's the analogy for AI today?

3755
02:44:23,130 --> 02:44:25,260
Like who are we protecting by banning AI?

3756
02:44:25,260 --> 02:44:28,920
Like who are the people who are asking that AI be regulated

3757
02:44:28,920 --> 02:44:31,800
to protect their jobs?

3758
02:44:31,800 --> 02:44:35,250
And of course, it's a real question

3759
02:44:35,250 --> 02:44:37,560
of what is gonna be the effect

3760
02:44:37,560 --> 02:44:41,490
of technological transformation like AI

3761
02:44:41,490 --> 02:44:45,330
on the job market and the labor market?

3762
02:44:45,330 --> 02:44:46,920
And there are economists

3763
02:44:46,920 --> 02:44:49,170
who are much more expert at this than I am,

3764
02:44:49,170 --> 02:44:50,310
but when I talk to them,

3765
02:44:50,310 --> 02:44:54,237
they tell us we're not gonna run out of job.

3766
02:44:54,237 --> 02:44:56,757
This is not gonna cause mass unemployment.

3767
02:44:56,757 --> 02:45:01,020
This is just gonna be gradual shift

3768
02:45:01,020 --> 02:45:02,280
of different professions.

3769
02:45:02,280 --> 02:45:04,410
The professions that are gonna be hot

3770
02:45:04,410 --> 02:45:05,883
10 or 15 years from now,

3771
02:45:06,870 --> 02:45:09,390
we have no idea today what they're gonna be.

3772
02:45:09,390 --> 02:45:12,150
The same way if we go back 20 years in the past,

3773
02:45:12,150 --> 02:45:15,000
like who could have thought 20 years ago

3774
02:45:15,000 --> 02:45:17,580
that like the hottest job,

3775
02:45:17,580 --> 02:45:21,180
even like 5, 10 years ago was mobile app developer?

3776
02:45:21,180 --> 02:45:23,400
Like smartphones weren't invented.

3777
02:45:23,400 --> 02:45:24,690
- Most of the jobs of the future

3778
02:45:24,690 --> 02:45:27,090
might be in the Metaverse. (laughs)

3779
02:45:27,090 --> 02:45:28,020
- Well, it could be.

3780
02:45:28,020 --> 02:45:29,070
Yeah.

3781
02:45:29,070 --> 02:45:31,920
- But the point is you can't possibly predict.

3782
02:45:31,920 --> 02:45:33,150
But you're right.

3783
02:45:33,150 --> 02:45:35,300
I mean, you've made a lot of strong points.

3784
02:45:36,213 --> 02:45:38,490
And I believe that people are fundamentally good,

3785
02:45:38,490 --> 02:45:42,237
and so if AI, especially open source AI

3786
02:45:42,237 --> 02:45:45,810
can make them smarter,

3787
02:45:45,810 --> 02:45:48,360
it just empowers the goodness in humans.

3788
02:45:48,360 --> 02:45:49,907
- So I share that feeling.

3789
02:45:49,907 --> 02:45:50,880
Okay?

3790
02:45:50,880 --> 02:45:54,390
I think people are fundamentally good. (laughing)

3791
02:45:54,390 --> 02:45:56,640
And in fact a lot of doomers are doomers

3792
02:45:56,640 --> 02:46:00,361
because they don't think that people are fundamentally good.

3793
02:46:00,361 --> 02:46:04,410
And they either don't trust people

3794
02:46:04,410 --> 02:46:07,860
or they don't trust the institution to do the right thing

3795
02:46:07,860 --> 02:46:09,410
so that people behave properly.

3796
02:46:10,620 --> 02:46:13,500
- Well, I think both you and I believe in humanity,

3797
02:46:13,500 --> 02:46:16,440
and I think I speak for a lot of people

3798
02:46:16,440 --> 02:46:20,040
in saying thank you for pushing the open source movement,

3799
02:46:20,040 --> 02:46:24,270
pushing to making both research and AI open source,

3800
02:46:24,270 --> 02:46:25,620
making it available to people,

3801
02:46:25,620 --> 02:46:27,750
and also the models themselves,

3802
02:46:27,750 --> 02:46:28,650
making that open source also.

3803
02:46:28,650 --> 02:46:30,360
So thank you for that.

3804
02:46:30,360 --> 02:46:32,250
And thank you for speaking your mind

3805
02:46:32,250 --> 02:46:34,320
in such colorful and beautiful ways on the internet.

3806
02:46:34,320 --> 02:46:35,730
I hope you never stop.

3807
02:46:35,730 --> 02:46:37,890
You're one of the most fun people I know

3808
02:46:37,890 --> 02:46:39,060
and get to be a fan of.

3809
02:46:39,060 --> 02:46:42,057
So Yann, thank you for speaking to me once again,

3810
02:46:42,057 --> 02:46:43,980
and thank you for being you.

3811
02:46:43,980 --> 02:46:44,813
- Thank you Lex.

3812
02:46:45,660 --> 02:46:48,360
- Thanks for listening to this conversation with Yann LeCun.

3813
02:46:48,360 --> 02:46:49,620
To support this podcast,

3814
02:46:49,620 --> 02:46:52,230
please check out our sponsors in the description.

3815
02:46:52,230 --> 02:46:54,180
And now let me leave you with some words

3816
02:46:54,180 --> 02:46:55,653
from Arthur C. Clarke,

3817
02:46:56,917 --> 02:46:59,850
"the only way to discover the limits of the possible

3818
02:46:59,850 --> 02:47:03,537
is to go beyond them into the impossible."

3819
02:47:04,620 --> 02:47:07,773
Thank you for listening and hope to see you next time.

