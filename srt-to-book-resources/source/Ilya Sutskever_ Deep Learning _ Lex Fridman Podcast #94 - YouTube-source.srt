1
00:00:00,100 --> 00:00:03,260
The following is a conversation with Ilya Sutskever

2
00:00:03,260 --> 00:00:11,180
co-founder and chief scientist of OpenAI, one of the most cited computer scientists in history with over 165

3
00:00:11,180 --> 00:00:20,080
000 citations, and to me, one of the most brilliant and insightful minds ever in the field of deep learning.

4
00:00:20,080 --> 00:00:25,240
There are very few people in this world who I would rather talk to and brainstorm with about deep learning

5
00:00:25,240 --> 00:00:28,580
intelligence, and life in general than Ilya

6
00:00:28,580 --> 00:00:30,720
on and off the mic.

7
00:00:30,720 --> 00:00:33,780
This was an honor and a pleasure.

8
00:00:33,780 --> 00:00:37,280
This conversation was recorded before the outbreak of the pandemic.

9
00:00:37,280 --> 00:00:41,500
For everyone feeling the medical, psychological, and financial burden of this crisis

10
00:00:41,500 --> 00:00:43,220
I'm sending love your way.

11
00:00:43,220 --> 00:00:44,260
Stay strong.

12
00:00:44,260 --> 00:00:45,460
We're in this together.

13
00:00:45,460 --> 00:00:46,360
We'll beat this thing.

14
00:00:46,360 --> 00:00:49,700
This is the Artificial Intelligence podcast.

15
00:00:49,700 --> 00:00:51,820
If you enjoy it, subscribe on YouTube

16
00:00:51,820 --> 00:00:54,120
review it with five stars on Apple podcast

17
00:00:54,120 --> 00:00:57,949
support on Patreon, or simply connect with me on Twitter @lexfridman

18
00:00:57,949 --> 00:01:00,600
spelled F-R-I-D-M-A-N.

19
00:01:00,600 --> 00:01:03,089
As usual, I'll do a few minutes of ads now

20
00:01:03,089 --> 00:01:06,670
and never any ads in the middle that can break the flow of the conversation.

21
00:01:06,670 --> 00:01:11,030
I hope that works for you and doesn't hurt the listening experience.

22
00:01:11,030 --> 00:01:15,780
This show is presented by Cash App, the number one finance app in the App Store.

23
00:01:15,780 --> 00:01:18,880
When you get it, use code LEXPODCAST.

24
00:01:18,880 --> 00:01:21,020
Cash App lets you send money to friends

25
00:01:21,020 --> 00:01:25,440
buy Bitcoin, invest in the stock market with as little as $1.

26
00:01:25,440 --> 00:01:27,480
Since Cash App allows you to buy Bitcoin

27
00:01:27,480 --> 00:01:33,100
let me mention that cryptocurrency in the context of the history of money is fascinating.

28
00:01:33,100 --> 00:01:36,840
I recommend A Cent of Money as a great book on this history.

29
00:01:36,840 --> 00:01:39,620
Both the book and audiobook are great.

30
00:01:39,620 --> 00:01:42,460
Debits and credits on ledgers started around 30

31
00:01:42,460 --> 00:01:43,900
000 years ago.

32
00:01:43,900 --> 00:01:47,200
The US dollar created over 200 years ago

33
00:01:47,200 --> 00:01:51,680
and Bitcoin, the first decentralized cryptocurrency released just over 10 years ago.

34
00:01:51,680 --> 00:01:57,000
So given that history, cryptocurrency is still very much in its early days of development

35
00:01:57,000 --> 00:01:59,360
but it's still aiming to, and just might

36
00:01:59,360 --> 00:02:01,840
redefine the nature of money.

37
00:02:01,840 --> 00:02:08,120
So again, if you get Cash App from the App Store or Google Play and use the code LEXPODCAST

38
00:02:08,120 --> 00:02:12,500
you get $10, and Cash App will also donate $10 to FIRST

39
00:02:12,500 --> 00:02:18,600
an organization that is helping advance robotics and STEM education for young people around the world.

40
00:02:18,600 --> 00:02:22,859
And now here's my conversation with Ilya Sutskever.

41
00:02:22,859 --> 00:02:26,780
You were one of the three authors, with Alex Krizhevsky

42
00:02:26,780 --> 00:02:31,680
Geoff Hinton, of the famed AlexNet paper that is arguably

43
00:02:31,680 --> 00:02:37,870
uh, the paper that marked the big catalytic moment that launched the deep learning revolution.

44
00:02:37,870 --> 00:02:38,475
At that time.

45
00:02:38,475 --> 00:02:38,570
..

46
00:02:38,570 --> 00:02:39,609
Take us back to that time.

47
00:02:39,609 --> 00:02:46,040
What was your intuition about neural networks, about the representational power of neural networks?

48
00:02:46,040 --> 00:02:51,810
And maybe you could mention, how did that evolve over the next few years up to today

49
00:02:51,810 --> 00:02:53,520
over the 10 years?

50
00:02:53,520 --> 00:02:55,260
Yeah, I can answer that question.

51
00:02:55,260 --> 00:03:00,060
At some point, in about 2010 or 2011

52
00:03:00,060 --> 00:03:02,800
I connected two facts in my mind.

53
00:03:02,800 --> 00:03:06,500
Basically, the realization was this.

54
00:03:06,500 --> 00:03:11,012
At some point, we realized that we can train very large.

55
00:03:11,012 --> 00:03:11,310
..

56
00:03:11,310 --> 00:03:11,800
I shouldn't say very.

57
00:03:11,800 --> 00:03:13,400
You know, they were tiny by today's standards

58
00:03:13,400 --> 00:03:18,590
but large and deep neural networks end-to-end with backpropagation.

59
00:03:18,590 --> 00:03:22,420
At some point, different people obtained this result.

60
00:03:22,420 --> 00:03:23,890
I obtained this result.

61
00:03:23,890 --> 00:03:33,680
The first th- the first moment in which I realized that deep neural networks are powerful was when James Martens invented the Hessian-free optimizer in 2010

62
00:03:33,680 --> 00:03:41,680
and he trained a 10-layer neural network end-to-end without pre-training from scratch.

63
00:03:41,680 --> 00:03:43,825
And when that happened, I thought, "This is it.

64
00:03:43,825 --> 00:03:45,609
" Because if you can train a big neural network

65
00:03:45,609 --> 00:03:49,560
a big neural network can represent very complicated function.

66
00:03:49,560 --> 00:03:52,740
Because if you have a neural network with 10 layers

67
00:03:52,740 --> 00:03:58,400
it's as though you allow the human brain to run for some number of milliseconds.

68
00:03:58,400 --> 00:04:03,210
Neuron firings are slow, and so in maybe 100 milliseconds

69
00:04:03,210 --> 00:04:04,700
uh, your neurons only fire 10 times.

70
00:04:04,700 --> 00:04:06,840
So it's also kind of like 10 layers

71
00:04:06,840 --> 00:04:10,540
and in 100 milliseconds, you can perfectly recognize any object.

72
00:04:10,540 --> 00:04:11,540
So I thought.

73
00:04:11,540 --> 00:04:11,730
..

74
00:04:11,730 --> 00:04:17,680
So I already had the idea then that we need to train a very big neural network on lots of supervised data

75
00:04:17,680 --> 00:04:21,440
and then it must succeed because we can find the best neural network.

76
00:04:21,440 --> 00:04:24,540
And then there's also theory that if you have more data than parameters

77
00:04:24,540 --> 00:04:25,700
you won't overfit.

78
00:04:25,700 --> 00:04:30,420
Today, we know that actually this theory is very incomplete and you won't overfit even if you have less data than parameters.

79
00:04:30,420 --> 00:04:32,500
But definitely, if you have more data than parameters

80
00:04:32,500 --> 00:04:33,390
you won't overfit.

81
00:04:33,390 --> 00:04:39,160
So the fact that neural networks were heavily over-parameterized wasn't discouraging to you?

82
00:04:39,160 --> 00:04:43,100
So y- y- you were thinking about the theory that the number of parameters

83
00:04:43,100 --> 00:04:45,260
the fact there's a huge number of parameters is okay?

84
00:04:45,260 --> 00:04:46,000
It's gonna be okay?

85
00:04:46,000 --> 00:04:48,280
I mean, there was some evidence before that it was okay-ish

86
00:04:48,280 --> 00:04:52,130
but the theory was most, the theory was that if you had a big data set and a big neural net

87
00:04:52,130 --> 00:04:53,140
it was going to work.

88
00:04:53,140 --> 00:04:56,540
The over-parameterization just didn't really, um, figure much as

89
00:04:56,540 --> 00:04:57,060
as a problem.

90
00:04:57,060 --> 00:05:00,460
I thought, well, with images, you're just gonna add some data augmentation and it's gonna be okay.

91
00:05:00,460 --> 00:05:02,460
So where was any doubt coming from?

92
00:05:02,460 --> 00:05:06,440
The, the main doubt was, can we train a big ne- will we have enough computer to train a big enough neural net?

93
00:05:06,440 --> 00:05:07,580
With backpropagation.

94
00:05:07,580 --> 00:05:09,480
Backpropagation I thought was, would work.

95
00:05:09,480 --> 00:05:14,140
The thing which wasn't clear would, was whether there would be enough compute to get a very convincing result.

96
00:05:14,140 --> 00:05:19,200
And then at some point, Alex Krizhevsky wrote these insanely fast CUDA kernels for training convolutional neural nets

97
00:05:19,200 --> 00:05:20,920
and that was, bam, let's do this.

98
00:05:20,920 --> 00:05:22,840
Let's get ImageNet, and it's gonna be the greatest thing.

99
00:05:22,840 --> 00:05:29,640
Was your intuition, most of your intuition from empirical results by you and by others?

100
00:05:29,640 --> 00:05:34,740
So, like, just actually demonstrating that a piece of program can train a 10-layer neural network?

101
00:05:34,740 --> 00:05:40,540
Or was there some pen and paper or marker and whiteboard thinking intuition?

102
00:05:40,540 --> 00:05:45,570
Like, 'cause you just connected a 10-layer large neural network to the brain.

103
00:05:45,570 --> 00:05:46,630
So you just mentioned the brain.

104
00:05:46,630 --> 00:05:53,880
So in your intuition about neural networks, does the human brain come into play as a intuition builder?

105
00:05:53,880 --> 00:05:55,020
Definitely.

106
00:05:55,020 --> 00:06:00,330
I mean, you, you know, you gotta be precise with these analogies between neural art- artificial neural networks and the brain.

107
00:06:00,330 --> 00:06:00,776
But.

108
00:06:00,776 --> 00:06:00,965
..

109
00:06:00,965 --> 00:06:10,856
There is no question that the brain is a huge source of intuition and inspiration for deep learning researchers since all the way from Rosenblatt in the '60s.

110
00:06:10,856 --> 00:06:15,786
Like, if you look at- the wh- the whole idea of a neural network is directly inspired by the brain.

111
00:06:15,786 --> 00:06:18,076
You had people like McCollum and Pitts who were saying

112
00:06:18,076 --> 00:06:22,096
"Hey, you got these n- n- neurons in the brain.

113
00:06:22,096 --> 00:06:24,456
And hey, we recently learned about the computer and automata.

114
00:06:24,456 --> 00:06:30,996
Can we use some ideas from the computer and automata to design some kind of computational object that's going to be simple

115
00:06:30,996 --> 00:06:32,761
computational, and kind of like the brain?

116
00:06:32,761 --> 00:06:33,796
" And they invented the neuron.

117
00:06:33,796 --> 00:06:36,016
So, they were inspired by it back then.

118
00:06:36,016 --> 00:06:38,616
Then you had the convolutional neural network from Fukushima

119
00:06:38,616 --> 00:06:40,196
and then later Yann LeCun, who said

120
00:06:40,196 --> 00:06:42,686
"Hey, if you limit the receptive fields of a neural network

121
00:06:42,686 --> 00:06:45,363
it's going to be especially suitable for images

122
00:06:45,363 --> 00:06:47,036
" as it turned out to be true.

123
00:06:47,036 --> 00:06:52,436
So, there was- there was a very small number of examples where analogies to the brain were successful

124
00:06:52,436 --> 00:06:57,686
and I thought, "Well, probably an artificial neuron is not that different from the brain if you squint hard enough.

125
00:06:57,686 --> 00:07:00,828
So, let's just assume it is and r- and roll with it.

126
00:07:00,828 --> 00:07:03,876
" So, now, we're now at a time where deep learning is very successful.

127
00:07:03,876 --> 00:07:07,736
So, let us squint less, and say

128
00:07:07,736 --> 00:07:09,196
let's, uh, open our eyes and say

129
00:07:09,196 --> 00:07:13,866
w- what to you is an interesting difference between the human brain .

130
00:07:13,866 --> 00:07:13,886
..

131
00:07:13,886 --> 00:07:16,076
Now, I know you're probably not an expert

132
00:07:16,076 --> 00:07:19,816
uh, neither a neuroscientist and neurobiologist, but loosely speaking

133
00:07:19,816 --> 00:07:26,336
what's the difference between the human brain and artificial neural networks that's interesting to you for the next decade or two?

134
00:07:26,336 --> 00:07:27,396
That's a good question to ask.

135
00:07:27,396 --> 00:07:32,436
What is in- what is an interesting difference between the neurons betwe- between the brain and our artificial neural networks?

136
00:07:32,436 --> 00:07:37,059
So, I feel like today, artificial neural networks .

137
00:07:37,059 --> 00:07:37,186
..

138
00:07:37,186 --> 00:07:43,056
So, we all agree that there are certain dimensions in which the human brain vastly outperforms our models.

139
00:07:43,056 --> 00:07:50,176
But I also think that there are some ways in which our artificial neural networks have a number of very important advantages over the brain.

140
00:07:50,176 --> 00:07:55,676
Look- looking at the- the advantages versus disadvantages is a good way to figure out what is the important difference.

141
00:07:55,676 --> 00:07:59,746
So, the brain uses spikes which may or may not be important.

142
00:07:59,746 --> 00:08:01,436
Yeah, that's a really interesting question.

143
00:08:01,436 --> 00:08:02,896
Do- do you think it's important or not?

144
00:08:02,896 --> 00:08:06,766
My- That- that's one big architectural difference between- Yeah.

145
00:08:06,766 --> 00:08:06,766
.

146
00:08:06,766 --> 00:08:06,766
.

147
00:08:06,766 --> 00:08:08,222
artificial neural networks and .

148
00:08:08,222 --> 00:08:08,396
..

149
00:08:08,396 --> 00:08:11,705
It's hard to tell, but my prior is not very high

150
00:08:11,705 --> 00:08:13,346
and I can te- I can say why.

151
00:08:13,346 --> 00:08:15,396
You know, there are people who are interested in spike in neural networks

152
00:08:15,396 --> 00:08:22,436
and basically what they figured out is that they need to simulate the non-spiking neural networks in spikes.

153
00:08:22,436 --> 00:08:22,776
Hmm.

154
00:08:22,776 --> 00:08:24,316
And that's how they're gonna make- make them work.

155
00:08:24,316 --> 00:08:26,996
If you don't simulate the non-spiking neural networks in spikes

156
00:08:26,996 --> 00:08:28,616
it's not going to work, because the question is

157
00:08:28,616 --> 00:08:29,456
why should it work?

158
00:08:29,456 --> 00:08:31,856
And that connects to questions around back propagation

159
00:08:31,856 --> 00:08:34,876
and questions around deep learning.

160
00:08:34,876 --> 00:08:36,956
You've got this giant neural network.

161
00:08:36,956 --> 00:08:38,456
Why should it work at all?

162
00:08:38,456 --> 00:08:41,956
Why should the learning rule work at all?

163
00:08:41,956 --> 00:08:45,896
It's not a self-evident question, especially if you

164
00:08:45,896 --> 00:08:49,396
let's say, if you were just starting in the field and you read the very early papers.

165
00:08:49,396 --> 00:08:51,616
You can say, "Hey, people are saying

166
00:08:51,616 --> 00:08:53,598
'Let's build neural networks.

167
00:08:53,598 --> 00:08:55,936
'" That's a great idea, because the brain is a neural network

168
00:08:55,936 --> 00:08:57,685
so it would be useful to build neural networks.

169
00:08:57,685 --> 00:08:58,076
Yep.

170
00:08:58,076 --> 00:09:00,476
Now, let's figure out how to train them.

171
00:09:00,476 --> 00:09:02,216
It should be possible to train them, probably

172
00:09:02,216 --> 00:09:03,516
but how?

173
00:09:03,516 --> 00:09:07,296
And so the big idea is the cost function.

174
00:09:07,296 --> 00:09:08,816
That's the big idea.

175
00:09:08,816 --> 00:09:14,976
The cost function is a way of measuring the performance of the system according to some measure.

176
00:09:14,976 --> 00:09:16,146
By the way, that is a big .

177
00:09:16,146 --> 00:09:16,286
..

178
00:09:16,286 --> 00:09:17,196
Well, l- actually, let me think.

179
00:09:17,196 --> 00:09:21,216
Is that- is that, uh, one, a difficult idea to arrive at?

180
00:09:21,216 --> 00:09:22,776
And how big of an idea is that

181
00:09:22,776 --> 00:09:27,019
that there's a single cost function .

182
00:09:27,019 --> 00:09:27,025
..

183
00:09:27,025 --> 00:09:28,976
Let me- sorry, let me take a pause.

184
00:09:28,976 --> 00:09:33,356
Is supervised learning a difficult concept to come to?

185
00:09:33,356 --> 00:09:34,676
I don't know.

186
00:09:34,676 --> 00:09:36,496
All concepts are very easy in retrospect.

187
00:09:36,496 --> 00:09:38,116
Yeah, that's why it seems trivial now

188
00:09:38,116 --> 00:09:38,796
but I .

189
00:09:38,796 --> 00:09:38,956
..

190
00:09:38,956 --> 00:09:41,446
'Cause- 'cause the reason I ask that, and we'll talk about it

191
00:09:41,446 --> 00:09:43,476
is there other things?

192
00:09:43,476 --> 00:09:47,196
Is there things that don't necessarily have a cost function

193
00:09:47,196 --> 00:09:50,876
or maybe have many cost functions, or maybe have dynamic cost functions

194
00:09:50,876 --> 00:09:54,136
or maybe a totally different kind of architectures?

195
00:09:54,136 --> 00:09:54,166
Yeah.

196
00:09:54,166 --> 00:09:56,956
'Cause we have to think like that in order to arrive at something new

197
00:09:56,956 --> 00:09:57,276
right?

198
00:09:57,276 --> 00:10:03,976
So, the only- so the good examples of things which don't have clear cost functions are GANs.

199
00:10:03,976 --> 00:10:04,176
Right.

200
00:10:04,176 --> 00:10:05,816
In a GAN, you have a game.

201
00:10:05,816 --> 00:10:08,276
So, instead of thinking of a cost function

202
00:10:08,276 --> 00:10:14,036
where you wanna optimize, but you know that you have an algorithm gradient descent which will optimize the cost function

203
00:10:14,036 --> 00:10:18,196
and then you can reason about the behavior of your system in terms of what it optimizes.

204
00:10:18,196 --> 00:10:20,196
With a GAN, you say, "I have a game

205
00:10:20,196 --> 00:10:24,379
and I'll reason abo- about the behavior of the system in terms of the equilibria of the game.

206
00:10:24,379 --> 00:10:30,176
" But it's all about coming up with these mathematical objects that help us reason about the behavior of our system.

207
00:10:30,176 --> 00:10:31,236
Right, that's really interesting.

208
00:10:31,236 --> 00:10:32,296
Yeah, so GAN is the only one.

209
00:10:32,296 --> 00:10:36,506
It's kind of a com- the cost function is emergent from the comparison.

210
00:10:36,506 --> 00:10:39,016
It's- I don't- I don't know if it has a cost function.

211
00:10:39,016 --> 00:10:41,396
I don't know if it's meaningful to talk about the cost function of a GAN.

212
00:10:41,396 --> 00:10:44,046
It's kind of like the cost function of biological evolution

213
00:10:44,046 --> 00:10:45,776
or the cost function of the economy.

214
00:10:45,776 --> 00:10:50,996
It's- you can talk about regions to which it could be- go towards

215
00:10:50,996 --> 00:10:54,099
but I don't think .

216
00:10:54,099 --> 00:10:55,306
..

217
00:10:55,306 --> 00:10:57,546
I don't think the cost function analogy is the most useful for GAN.

218
00:10:57,546 --> 00:10:59,379
So, if evolution doesn't .

219
00:10:59,379 --> 00:10:59,386
..

220
00:10:59,386 --> 00:11:00,176
That's really interesting.

221
00:11:00,176 --> 00:11:02,716
So, if evolution doesn't really have a cost function

222
00:11:02,716 --> 00:11:06,739
like, a cost function based on its .

223
00:11:06,739 --> 00:11:06,746
..

224
00:11:06,746 --> 00:11:11,306
Something akin to our mathematical conception of a cost function

225
00:11:11,306 --> 00:11:15,176
then do you think cost functions in deep learning are holding us back?

226
00:11:15,176 --> 00:11:15,866
Yeah, I .

227
00:11:15,866 --> 00:11:15,886
..

228
00:11:15,886 --> 00:11:21,436
So, you- you just kinda mentioned that cost function is a- is a nice first profound idea.

229
00:11:21,436 --> 00:11:23,396
Do you think that's a good idea?

230
00:11:23,396 --> 00:11:26,776
Do you think it's an idea we'll go past?

231
00:11:26,776 --> 00:11:29,416
So, self-play starts to touch on that a little bit

232
00:11:29,416 --> 00:11:31,766
uh, in reinforcement learning, uh, systems.

233
00:11:31,766 --> 00:11:32,456
That's right.

234
00:11:32,456 --> 00:11:39,136
Self-play, and also ideas around exploration where you're trying to take action that sur- that surprise a predictor.

235
00:11:39,136 --> 00:11:40,526
I'm a big fan of cost functions.

236
00:11:40,526 --> 00:11:42,756
I think cost functions are great and they serve us really well

237
00:11:42,756 --> 00:11:45,316
and I think that whenever we can do things we c- with cost functions

238
00:11:45,316 --> 00:11:47,356
we should.

239
00:11:47,356 --> 00:11:55,616
And, you know, maybe there is a chance that we will come up with some yet another profound way of looking at things that will involve cost functions in a less central way.

240
00:11:55,616 --> 00:11:56,116
But I don't know.

241
00:11:56,116 --> 00:11:58,476
I think cost functions are, I mean

242
00:11:58,476 --> 00:12:02,982
I would not bet against cost functions.

243
00:12:02,982 --> 00:12:13,084
Is there other things about the brain that pop into your mind that might be different and interesting for us to consider in designing artificial neural networks?

244
00:12:13,084 --> 00:12:16,244
So we talked about spiking a little bit.

245
00:12:16,244 --> 00:12:18,584
I mean, one- one thing which may potentially be useful

246
00:12:18,584 --> 00:12:22,244
I think people, neuroscientists have figured out something about the learning rule of the brain

247
00:12:22,244 --> 00:12:28,384
or s- I'm talking about spike-timing-dependent plasticity, and it would be nice if some people were to study that in simulation.

248
00:12:28,384 --> 00:12:30,924
Wait, sorry, spike-time-independent plasticity?

249
00:12:30,924 --> 00:12:31,214
Yeah, that's right.

250
00:12:31,214 --> 00:12:31,854
What's that?

251
00:12:31,854 --> 00:12:39,614
STDP, it's- it's a particular learning rule that uses spike timing to figure out how to- to determine how to update the synapses.

252
00:12:39,614 --> 00:12:44,344
So it's kind of like, if a synapse fires into the neuron before the neuron fires

253
00:12:44,344 --> 00:12:49,544
then it strengthen the synapse, and if the synapse fires into the neuron shortly after the neuron fired

254
00:12:49,544 --> 00:12:50,804
then it weakens the synapse.

255
00:12:50,804 --> 00:12:52,274
Something along this line.

256
00:12:52,274 --> 00:12:56,324
I am 90% sure it's right, so if I said something wrong here

257
00:12:56,324 --> 00:12:57,254
don't-  .

258
00:12:57,254 --> 00:12:57,254
..

259
00:12:57,254 --> 00:12:59,484
don't get too angry.

260
00:12:59,484 --> 00:13:01,084
But you sounded brilliant while saying it.

261
00:13:01,084 --> 00:13:04,264
But the timing, d- that's one thing that's missing

262
00:13:04,264 --> 00:13:07,464
the- the temporal dynamics is not captured.

263
00:13:07,464 --> 00:13:10,144
I think that's like a fundamental property of the brain

264
00:13:10,144 --> 00:13:13,344
is the timing of the s- of the signals.

265
00:13:13,344 --> 00:13:15,484
Well, we have recurrent neural networks.

266
00:13:15,484 --> 00:13:18,314
But, you- you think of that as the s- I mean

267
00:13:18,314 --> 00:13:22,194
that's a very crude simplified, uh, what's that called?

268
00:13:22,194 --> 00:13:25,444
The- the- there's a clock, I guess

269
00:13:25,444 --> 00:13:27,624
to, uh, recurrent neural networks.

270
00:13:27,624 --> 00:13:27,754
Mm-hmm.

271
00:13:27,754 --> 00:13:30,144
It's this- this seems like the brain is the general

272
00:13:30,144 --> 00:13:33,404
the continuous version of that, the- the generalization

273
00:13:33,404 --> 00:13:39,944
where all possible timings are possible, and then within those timings is contained some information.

274
00:13:39,944 --> 00:13:53,564
You think recurrent neural networks, the recurrence in recurrent neural networks can capture the same kind of phenomena as the timing that seems to be important for the brain

275
00:13:53,564 --> 00:13:55,884
i- i- in the- in the firing of neurons in the brain?

276
00:13:55,884 --> 00:14:00,744
I- I mean, I think, I think recurrent neural netwo- r- recurrent neural networks are amazing

277
00:14:00,744 --> 00:14:07,734
and they can do, I think they can do anything we- we'd want them to- we'd want a system to do.

278
00:14:07,734 --> 00:14:10,473
Right now, recurrent neural networks have been superseded by transformers

279
00:14:10,473 --> 00:14:12,704
but maybe one day they'll make a comeback

280
00:14:12,704 --> 00:14:13,704
maybe they'll be back.

281
00:14:13,704 --> 00:14:14,304
We'll see.

282
00:14:14,304 --> 00:14:17,744
 Let me, uh, on a small tangent say

283
00:14:17,744 --> 00:14:19,104
do you think they'll be back?

284
00:14:19,104 --> 00:14:22,864
So, so much of, uh, the breakthroughs recently that we'll talk about on

285
00:14:22,864 --> 00:14:30,964
uh, natural language processing and language modeling has been with transformers that don't emphasize recurrence.

286
00:14:30,964 --> 00:14:33,364
Do you think recurrence will make a comeback?

287
00:14:33,364 --> 00:14:37,024
Well, some kind of recurrence I think very likely.

288
00:14:37,024 --> 00:14:42,624
Recurrent neural networks for proc- as they're typically thought of for processing sequences

289
00:14:42,624 --> 00:14:43,824
I think it's also possible.

290
00:14:43,824 --> 00:14:46,244
W- what- w- what is, to you

291
00:14:46,244 --> 00:14:47,964
a recurrent neural network?

292
00:14:47,964 --> 00:14:50,924
In generally speaking, I guess, what is a recurrent neural network?

293
00:14:50,924 --> 00:14:54,964
You have a neural network which maintains a high dimensional hidden state.

294
00:14:54,964 --> 00:15:02,723
And then when an observation arrives, it updates its high dimensional hidden state through its connections in some way.

295
00:15:02,723 --> 00:15:05,984
So do you think, you know, that's what

296
00:15:05,984 --> 00:15:08,184
like, expert systems did, right?

297
00:15:08,184 --> 00:15:17,223
Symbolic AI, uh, the knowledge-based, growing a knowledge base is- is maintaining a hidden state

298
00:15:17,223 --> 00:15:20,284
which is its knowledge base and is growing it by sequential processing.

299
00:15:20,284 --> 00:15:22,704
Do you think of it more generally in that way

300
00:15:22,704 --> 00:15:34,464
or is it simply, is it the more constrained form th- of- of a hidden state with certain kind of gating units that we think of as today with LSTMs and that?

301
00:15:34,464 --> 00:15:37,294
I mean, the hidden state is technically what you described there

302
00:15:37,294 --> 00:15:41,364
the hidden state that goes inside the LSTM or the RNN or something like this.

303
00:15:41,364 --> 00:15:43,233
But then what should be contained, you know

304
00:15:43,233 --> 00:15:44,784
if you want to make the expert system

305
00:15:44,784 --> 00:15:47,324
um, analogy, I'm not.

306
00:15:47,324 --> 00:15:47,754
..

307
00:15:47,754 --> 00:15:51,114
I mean, you could say that the knowledge is stored in the- in the connections

308
00:15:51,114 --> 00:15:56,284
and then the short-term processing is- is done in the- in the hidden state.

309
00:15:56,284 --> 00:15:57,344
Yes.

310
00:15:57,344 --> 00:15:58,304
Could you say that?

311
00:15:58,304 --> 00:15:58,504
Yes.

312
00:15:58,504 --> 00:16:05,604
So, uh, sort of, do you think there's a future of building large-scale knowledge bases within the neural networks?

313
00:16:05,604 --> 00:16:07,964
Definitely.

314
00:16:07,964 --> 00:16:12,723
 So, we're gonna pause on that confidence 'cause I wanna explore that

315
00:16:12,723 --> 00:16:14,784
but let me zoom back out and ask

316
00:16:14,784 --> 00:16:19,303
um, back to the history of ImageNet

317
00:16:19,303 --> 00:16:21,324
neural networks have been around for many decades

318
00:16:21,324 --> 00:16:22,863
as you mentioned.

319
00:16:22,863 --> 00:16:25,844
What do you think were the key ideas that led to their success

320
00:16:25,844 --> 00:16:32,504
that ImageNet moment and beyond, the- the success in the past 10 years?

321
00:16:32,504 --> 00:16:35,544
Okay, so the question is, to make sure I didn't miss anything

322
00:16:35,544 --> 00:16:39,373
the key ideas that led to the success of deep learning over the past 10 years.

323
00:16:39,373 --> 00:16:45,424
Exactly, even though the fundamental thing behind deep learning has been around for much longer.

324
00:16:45,424 --> 00:16:51,344
So, the key idea about deep learning

325
00:16:51,344 --> 00:16:54,014
or rather, the s- the key fact about deep learning

326
00:16:54,014 --> 00:17:01,304
before deep learning started to be successful, is that it was underestimated.

327
00:17:01,304 --> 00:17:06,304
People who worked in machine learning simply didn't think that n- neural networks could do much.

328
00:17:06,304 --> 00:17:10,522
People didn't believe that large neural networks could be trained.

329
00:17:10,522 --> 00:17:13,384
People thought that, well, there was lots of

330
00:17:13,384 --> 00:17:17,242
there was a lot of debate going on in machine learning about what are the right methods and so on

331
00:17:17,242 --> 00:17:20,292
and people were arguing because there were no

332
00:17:20,292 --> 00:17:23,424
there- there were- there were no- there was no way to get hard facts.

333
00:17:23,424 --> 00:17:26,904
And by that I mean there were no benchmarks which were truly hard

334
00:17:26,904 --> 00:17:28,434
that if you s- do really well on them

335
00:17:28,434 --> 00:17:32,380
then you can say, "Look, here is my system.

336
00:17:32,380 --> 00:17:34,116
" That's when you switch from.

337
00:17:34,116 --> 00:17:34,434
..

338
00:17:34,434 --> 00:17:38,624
Th- that's when this field becomes a little bit more of an engineering field.

339
00:17:38,624 --> 00:17:42,354
So in terms of deep learning, to answer the question directly

340
00:17:42,354 --> 00:17:43,543
the ideas were all there.

341
00:17:43,543 --> 00:17:49,744
The thing that was missing was a lot of supervised data and a lot of compute.

342
00:17:49,744 --> 00:17:52,644
Once you have a lot of supervised data and a lot of compute

343
00:17:52,644 --> 00:17:54,724
then there is a third thing which is needed as well

344
00:17:54,724 --> 00:17:56,384
and that is conviction.

345
00:17:56,384 --> 00:18:03,584
Conviction that if you take the right stuff which already exists and apply and mix with a lot of data and a lot of compute

346
00:18:03,584 --> 00:18:05,494
that it will in fact work.

347
00:18:05,494 --> 00:18:07,768
And so that was the missing piece.

348
00:18:07,768 --> 00:18:08,727
It was, you had the.

349
00:18:08,727 --> 00:18:08,817
..

350
00:18:08,817 --> 00:18:13,728
You needed the, the data, you needed the compute which showed up in terms of GPUs

351
00:18:13,728 --> 00:18:17,268
and you needed the conviction to realize that you need to mix them together.

352
00:18:17,268 --> 00:18:19,468
So that's really interesting.

353
00:18:19,468 --> 00:18:32,048
So, uh, I- I guess the presence of compute and the presence of supervised data allowed the empirical evidence to do the convincing of the majority of the computer science community.

354
00:18:32,048 --> 00:18:34,078
So, I guess there's a key moment with

355
00:18:34,078 --> 00:18:37,778
uh, uh, Jitendra Malik and, uh

356
00:18:37,778 --> 00:18:41,768
Alex, uh, Alyosha Efros who were very skeptical

357
00:18:41,768 --> 00:18:42,608
right?

358
00:18:42,608 --> 00:18:46,668
And then there's a Geoffrey Hinton that was the opposite of skeptical

359
00:18:46,668 --> 00:18:50,248
and there was a convincing moment, and I think ImageNet served as that moment.

360
00:18:50,248 --> 00:18:50,848
That's right.

361
00:18:50,848 --> 00:18:52,738
And it represented this kind of.

362
00:18:52,738 --> 00:18:52,998
..

363
00:18:52,998 --> 00:18:56,217
Or the big pillars of computer vision community kind of.

364
00:18:56,217 --> 00:18:56,277
..

365
00:18:56,277 --> 00:19:00,548
 The- the wizards got together, and then all of a sudden

366
00:19:00,548 --> 00:19:01,528
there was a shift.

367
00:19:01,528 --> 00:19:06,277
And it's not enough for the ideas to all be there and the compute to be there.

368
00:19:06,277 --> 00:19:11,203
It's for it to convince the cynicism that existed that- that.

369
00:19:11,203 --> 00:19:11,418
..

370
00:19:11,418 --> 00:19:15,947
It's interesting that people just didn't believe for a couple of decades.

371
00:19:15,947 --> 00:19:18,588
Yeah, well, but it was more than that.

372
00:19:18,588 --> 00:19:19,194
It's kind of.

373
00:19:19,194 --> 00:19:19,318
..

374
00:19:19,318 --> 00:19:21,288
When- when- when- when put this way, it sounds like

375
00:19:21,288 --> 00:19:24,928
"Well, you know, those silly people who didn't believe what were

376
00:19:24,928 --> 00:19:25,550
what were they missing.

377
00:19:25,550 --> 00:19:30,108
" But in reality, things were confusing because neural networks really did not work on anything.

378
00:19:30,108 --> 00:19:30,268
Right.

379
00:19:30,268 --> 00:19:33,608
And they were not the best method on pretty much anything as well.

380
00:19:33,608 --> 00:19:36,328
And it was pretty rational to say, "Yeah

381
00:19:36,328 --> 00:19:39,401
this stuff doesn't have any traction.

382
00:19:39,401 --> 00:19:42,778
" And that's why you need to have these very hard tasks which are

383
00:19:42,778 --> 00:19:46,988
which produce undeniable evidence, and that's how we make progress.

384
00:19:46,988 --> 00:19:48,628
And that's why the field is making progress today

385
00:19:48,628 --> 00:19:52,848
because we have these hard benchmarks which represent true progress.

386
00:19:52,848 --> 00:19:53,422
And so.

387
00:19:53,422 --> 00:19:53,658
..

388
00:19:53,658 --> 00:19:57,488
And this is why we are able to avoid endless debate.

389
00:19:57,488 --> 00:20:03,068
So, incredibly, you've contributed some of the biggest recent ideas in AI in

390
00:20:03,068 --> 00:20:07,048
in computer vision, language, natural language processing

391
00:20:07,048 --> 00:20:11,328
reinforcement learning, sort of everything in between.

392
00:20:11,328 --> 00:20:12,608
Maybe not GANs.

393
00:20:12,608 --> 00:20:14,394
 Is there.

394
00:20:14,394 --> 00:20:14,657
..

395
00:20:14,657 --> 00:20:16,228
 There may not be a topic you haven't touched

396
00:20:16,228 --> 00:20:19,688
and of course the, the fundamental science of deep learning.

397
00:20:19,688 --> 00:20:23,108
What is the difference to you between vision

398
00:20:23,108 --> 00:20:28,348
language, and as in reinforcement learning, action as learning problems

399
00:20:28,348 --> 00:20:29,588
and what are the commonalities?

400
00:20:29,588 --> 00:20:31,568
Do you see them as all interconnected?

401
00:20:31,568 --> 00:20:37,088
Are they fundamentally different domains that require different approaches?

402
00:20:37,088 --> 00:20:39,688
Okay, that's a good question.

403
00:20:39,688 --> 00:20:41,908
Machine learning is a field with a lot of unity

404
00:20:41,908 --> 00:20:44,128
a huge amount of unity.

405
00:20:44,128 --> 00:20:45,368
In fact- What do you mean by unity?

406
00:20:45,368 --> 00:20:48,398
Like, overlap of ideas?

407
00:20:48,398 --> 00:20:50,178
Overlap of ideas, overlap of principles.

408
00:20:50,178 --> 00:20:53,278
In fact, there is only one or two or three principles which are very

409
00:20:53,278 --> 00:20:54,458
very simple.

410
00:20:54,458 --> 00:20:57,408
And then they, then they apply in almost the same way

411
00:20:57,408 --> 00:20:59,947
in almost the same way to the different modalities

412
00:20:59,947 --> 00:21:01,428
to the different problems.

413
00:21:01,428 --> 00:21:07,208
And that's why today, when someone writes a paper on improving optimization of deep learning and vision

414
00:21:07,208 --> 00:21:12,368
it improves the different NLP applications and it improves the different reinforcement learning applications.

415
00:21:12,368 --> 00:21:18,648
Reinforcement learn- So, I would say that computer vision and NLP are very similar to each other.

416
00:21:18,648 --> 00:21:22,208
Today, they differ in that they have slightly different architectures.

417
00:21:22,208 --> 00:21:26,568
We use transformers in NLP, and we use convolutional neural networks in vision.

418
00:21:26,568 --> 00:21:31,868
But it's also possible that one day, this will change and everything will be unified with a single architecture.

419
00:21:31,868 --> 00:21:35,408
Because if you go back a few years ago in natural language processing

420
00:21:35,408 --> 00:21:39,368
there were a hu- a huge number of architectures

421
00:21:39,368 --> 00:21:43,388
for every different tiny problem had its own architecture.

422
00:21:43,388 --> 00:21:47,608
Today, there's just one transformer for all those different tasks.

423
00:21:47,608 --> 00:21:49,728
And if you go back in time even more

424
00:21:49,728 --> 00:21:51,358
you had even more and more fragmentation.

425
00:21:51,358 --> 00:21:56,488
And every little problem in AI had its own little sub-specialization and sub-

426
00:21:56,488 --> 00:21:58,548
you know, little set of collection of skills

427
00:21:58,548 --> 00:22:01,008
people who would know how to engineer the features.

428
00:22:01,008 --> 00:22:02,888
Now, it's all been subsumed by deep learning.

429
00:22:02,888 --> 00:22:04,308
We have this unification.

430
00:22:04,308 --> 00:22:08,518
And so I expect vision to become unified with natural language as well.

431
00:22:08,518 --> 00:22:10,508
Or rather, I shouldn't say "expect," I think it's possible.

432
00:22:10,508 --> 00:22:15,568
I don't want to be too sure, because I think on the conventional neural net is very computationally efficient.

433
00:22:15,568 --> 00:22:16,868
RL is different.

434
00:22:16,868 --> 00:22:20,848
RL does require slightly different techniques, because you really do need to take action.

435
00:22:20,848 --> 00:22:23,898
You really do need to do something about exploration.

436
00:22:23,898 --> 00:22:25,188
Your variance is much higher.

437
00:22:25,188 --> 00:22:27,688
But I think there is a lot of unity even there.

438
00:22:27,688 --> 00:22:29,928
And I would expect, for example, that at some point

439
00:22:29,928 --> 00:22:35,228
there will be some broader unification between RL and supervised learning

440
00:22:35,228 --> 00:22:38,568
where somehow the RL will be making decisions to make the supervised learning go better.

441
00:22:38,568 --> 00:22:43,268
And it will be, I imagine, one big black box and you just throw ev- you know

442
00:22:43,268 --> 00:22:48,048
you shovel, shovel things into it and it just figures out what to do with whatever you shovel at it.

443
00:22:48,048 --> 00:22:55,148
I mean, reinforcement learning has some aspects of language and vision combined almost.

444
00:22:55,148 --> 00:22:58,868
There's elements of a long-term memory that you should be utilizing

445
00:22:58,868 --> 00:23:03,088
and there's elements of a really rich sensory space.

446
00:23:03,088 --> 00:23:04,621
So it seems like the.

447
00:23:04,621 --> 00:23:05,028
..

448
00:23:05,028 --> 00:23:08,408
It's- it's like the union of the two or something like that.

449
00:23:08,408 --> 00:23:09,988
I'd- I'd say something slightly differently.

450
00:23:09,988 --> 00:23:17,388
I'd say that reinforcement learning is neither, but it naturally interfaces and integrates with the two of them.

451
00:23:17,388 --> 00:23:19,328
Do you think action is fundamentally different?

452
00:23:19,328 --> 00:23:21,185
So yeah, what is interesting about.

453
00:23:21,185 --> 00:23:21,378
..

454
00:23:21,378 --> 00:23:26,048
What is unique about policy of learning to act?

455
00:23:26,048 --> 00:23:29,828
Well, so one example, for instance, is that when you learn to act

456
00:23:29,828 --> 00:23:35,788
you are fundamentally in a non-stationary world, because as your actions change

457
00:23:35,788 --> 00:23:38,188
the things you see s- start changing.

458
00:23:38,188 --> 00:23:41,308
You, you experience the world in a different way

459
00:23:41,308 --> 00:23:48,008
and this is not the case for the more traditional static problem where you have a dist- some distribution and you just apply a model to that distribution.

460
00:23:48,008 --> 00:23:57,018
You think it's a fundamentally different problem, or is it just a more difficult general- it's a generalization of the problem of understanding?

461
00:23:57,018 --> 00:23:59,838
I mean, it's- it's- it's a question of definitions almost.

462
00:23:59,838 --> 00:24:02,038
There is a huge- I mean, there is a huge amount of commonality for sure.

463
00:24:02,038 --> 00:24:04,098
You take gradients, you try- you take gradients

464
00:24:04,098 --> 00:24:06,108
we try to approximate gradients in both cases.

465
00:24:06,108 --> 00:24:07,938
In some ca- in the case of reinforcement learning

466
00:24:07,938 --> 00:24:11,148
you have some tools to reduce the variance of the gradients

467
00:24:11,148 --> 00:24:12,389
you do that.

468
00:24:12,389 --> 00:24:12,773
..

469
00:24:12,773 --> 00:24:15,816
. there's lots of commonality, use the same neural net in both cases

470
00:24:15,816 --> 00:24:20,076
you compute the gradient, you apply Adam in both cases.

471
00:24:20,076 --> 00:24:24,196
So, I mean, there's lots in common for sure

472
00:24:24,196 --> 00:24:28,896
but there are some small differences which are not completely insignificant.

473
00:24:28,896 --> 00:24:32,406
It's really just a matter of your po- of your point of view what frame of reference you

474
00:24:32,406 --> 00:24:33,706
what, h- how much do you zoom

475
00:24:33,706 --> 00:24:37,286
wanna zoom in or out as you look at these problems.

476
00:24:37,286 --> 00:24:39,816
Which problem do you think is harder?

477
00:24:39,816 --> 00:24:43,496
So, people like Noam Chomsky believe that language is fundamental to everything

478
00:24:43,496 --> 00:24:45,656
so it underlies everything.

479
00:24:45,656 --> 00:24:50,616
Do you think language understanding is harder than visual scene understanding

480
00:24:50,616 --> 00:24:51,496
or vice versa?

481
00:24:51,496 --> 00:24:56,196
I think that asking if a problem is hard is slightly wrong.

482
00:24:56,196 --> 00:24:57,486
I think the question is a little bit wrong

483
00:24:57,486 --> 00:24:59,516
and I wanna explain why.

484
00:24:59,516 --> 00:25:03,116
So, what does it mean for a problem to be hard?

485
00:25:03,116 --> 00:25:04,796
Okay.

486
00:25:04,796 --> 00:25:10,716
The non-interesting dumb answer to that is there's this s- s- is a benchmark

487
00:25:10,716 --> 00:25:13,696
and there's a human level of performance on that benchmark.

488
00:25:13,696 --> 00:25:18,426
And how is the effort required to reach the human level- Okay.

489
00:25:18,426 --> 00:25:18,432
.

490
00:25:18,432 --> 00:25:18,446
..

491
00:25:18,446 --> 00:25:19,096
benchmark.

492
00:25:19,096 --> 00:25:23,328
So, from the perspective of how much until we get to human level.

493
00:25:23,328 --> 00:25:23,726
..

494
00:25:23,726 --> 00:25:26,256
On a very good benchmark.

495
00:25:26,256 --> 00:25:26,780
Yeah, like some.

496
00:25:26,780 --> 00:25:26,846
..

497
00:25:26,846 --> 00:25:28,936
I- I- I under- I understand what you mean by that.

498
00:25:28,936 --> 00:25:32,076
So, what I was going, going to say that a lot of it depends on

499
00:25:32,076 --> 00:25:33,146
you know, once you solve a problem

500
00:25:33,146 --> 00:25:34,006
it stops being hard.

501
00:25:34,006 --> 00:25:34,836
And that's- Right.

502
00:25:34,836 --> 00:25:35,066
 .

503
00:25:35,066 --> 00:25:35,066
..

504
00:25:35,066 --> 00:25:36,686
all, all, that's always true, and so- That's a.

505
00:25:36,686 --> 00:25:36,686
..

506
00:25:36,686 --> 00:25:36,686
..

507
00:25:36,686 --> 00:25:36,686
..

508
00:25:36,686 --> 00:25:39,776
but if something is hard or not depends on what our tools can do today.

509
00:25:39,776 --> 00:25:51,546
So, you know, we say today, true human level language understanding and visual perception are hard in the sense that there is no way of solving the problem completely in the next three months.

510
00:25:51,546 --> 00:25:52,066
Right.

511
00:25:52,066 --> 00:25:53,996
So, I agree with that statement.

512
00:25:53,996 --> 00:25:56,616
Beyond that, I'm just, I'd be, my s- my guess would be as good as yours

513
00:25:56,616 --> 00:25:57,476
I don't know.

514
00:25:57,476 --> 00:25:58,056
Oh, okay.

515
00:25:58,056 --> 00:26:02,766
So, you don't have a fundamental intuition about how hard language understanding is?

516
00:26:02,766 --> 00:26:03,506
Well, I think I, I, I

517
00:26:03,506 --> 00:26:04,296
now I changed my mind.

518
00:26:04,296 --> 00:26:06,717
I'd say language is probably going to be harder.

519
00:26:06,717 --> 00:26:06,826
..

520
00:26:06,826 --> 00:26:09,196
I mean, it depends on how you define it.

521
00:26:09,196 --> 00:26:12,986
Like, if you mean absolute top-notch 100% language understanding

522
00:26:12,986 --> 00:26:15,176
I'll go with language.

523
00:26:15,176 --> 00:26:18,936
And so, uh, th- But then if I show you a piece of paper with letters on it

524
00:26:18,936 --> 00:26:19,901
is that.

525
00:26:19,901 --> 00:26:20,216
..

526
00:26:20,216 --> 00:26:21,396
You, you see what I mean?

527
00:26:21,396 --> 00:26:22,626
And so, you have a vision system

528
00:26:22,626 --> 00:26:25,136
you say it's the best f- human level vision system.

529
00:26:25,136 --> 00:26:25,709
I show you.

530
00:26:25,709 --> 00:26:25,786
..

531
00:26:25,786 --> 00:26:28,816
I open a book and I show you letters.

532
00:26:28,816 --> 00:26:32,226
Will it understand how these letters form into word and sentences and meaning?

533
00:26:32,226 --> 00:26:33,756
Is this part of the vision problem?

534
00:26:33,756 --> 00:26:36,136
Where does vision end and language begin?

535
00:26:36,136 --> 00:26:38,256
Yeah, so Chomsky would say it starts at language.

536
00:26:38,256 --> 00:26:41,886
So, vision is just a little example of the kind of

537
00:26:41,886 --> 00:26:42,801
uh, structuring we.

538
00:26:42,801 --> 00:26:42,805
..

539
00:26:42,805 --> 00:26:49,216
And, you know, fundamental hierarchy of ideas that's already represented in our brain somehow

540
00:26:49,216 --> 00:26:51,425
that's represented through language.

541
00:26:51,425 --> 00:26:56,636
But where does vision stop and language begin?

542
00:26:56,636 --> 00:27:04,336
That's a really interesting, uh, question.

543
00:27:04,336 --> 00:27:07,654
 It.

544
00:27:07,654 --> 00:27:07,746
..

545
00:27:07,746 --> 00:27:18,436
So, one possibility is that it's impossible to achieve really deep understanding in either images or language without basically using the same kind of system.

546
00:27:18,436 --> 00:27:20,496
So, you're going to get the other for free.

547
00:27:20,496 --> 00:27:22,776
 I think, I think it's pretty likely that

548
00:27:22,776 --> 00:27:27,356
yes, if we can get one, we prob- our machine learning is probably that good that we can get the other.

549
00:27:27,356 --> 00:27:28,609
But it's not 100.

550
00:27:28,609 --> 00:27:28,706
..

551
00:27:28,706 --> 00:27:30,236
I'm not 100% sure.

552
00:27:30,236 --> 00:27:36,776
And also, uh, I think a lot r- a lo- a lot of it really does depend on your definitions.

553
00:27:36,776 --> 00:27:37,816
Definitions of?

554
00:27:37,816 --> 00:27:40,036
Of, like, perfect vision.

555
00:27:40,036 --> 00:27:42,056
Because ra- you know, reading is vision

556
00:27:42,056 --> 00:27:44,676
but should it count?

557
00:27:44,676 --> 00:27:44,956
Yeah.

558
00:27:44,956 --> 00:27:48,316
Uh, to me, sort of my definition is if a system looked at an image

559
00:27:48,316 --> 00:27:56,036
and then a system looked at a piece of text and then told me something about that

560
00:27:56,036 --> 00:27:58,416
and I was really impressed.

561
00:27:58,416 --> 00:27:59,496
That's relative.

562
00:27:59,496 --> 00:28:01,876
You'll be impressed for half an hour, and then you're gonna say

563
00:28:01,876 --> 00:28:03,436
"Well, I mean, all the systems do that

564
00:28:03,436 --> 00:28:05,043
but here's the thing they don't do.

565
00:28:05,043 --> 00:28:05,365
" Yeah.

566
00:28:05,365 --> 00:28:07,136
But I, I don't have that with humans.

567
00:28:07,136 --> 00:28:08,896
Humans continue to impress me.

568
00:28:08,896 --> 00:28:10,616
Is that true?

569
00:28:10,616 --> 00:28:11,420
Well, th- the ones.

570
00:28:11,420 --> 00:28:11,526
..

571
00:28:11,526 --> 00:28:11,856
Okay.

572
00:28:11,856 --> 00:28:13,976
So, uh, I'm a fan of monogamy

573
00:28:13,976 --> 00:28:15,976
so I, I like the idea of marrying somebody

574
00:28:15,976 --> 00:28:18,176
being with them for several decades.

575
00:28:18,176 --> 00:28:19,666
 So, I s- I believe in the fact that

576
00:28:19,666 --> 00:28:23,526
yes, it's possible to have somebody continuously giving you

577
00:28:23,526 --> 00:28:28,036
uh, ple- pleasurable, interesting, witty, new ideas

578
00:28:28,036 --> 00:28:28,636
friends.

579
00:28:28,636 --> 00:28:29,156
Yeah.

580
00:28:29,156 --> 00:28:29,976
I think, I think so.

581
00:28:29,976 --> 00:28:32,076
They continue to surprise you.

582
00:28:32,076 --> 00:28:35,236
The surprise, it's, um, you know

583
00:28:35,236 --> 00:28:39,876
that injection of randomness that seems to be a

584
00:28:39,876 --> 00:28:43,556
th- it seems to be a, a nice source of

585
00:28:43,556 --> 00:28:47,276
yeah, continued, uh, inspiration.

586
00:28:47,276 --> 00:28:48,856
Like, the, the wit, the humor

587
00:28:48,856 --> 00:28:49,313
I think.

588
00:28:49,313 --> 00:28:49,426
..

589
00:28:49,426 --> 00:28:51,156
Yeah.

590
00:28:51,156 --> 00:28:53,521
That, that, the, that would be a.

591
00:28:53,521 --> 00:28:53,746
..

592
00:28:53,746 --> 00:28:57,716
It's a very subjective test, but I think if you have enough humans in the room.

593
00:28:57,716 --> 00:28:59,026
Yeah.

594
00:28:59,026 --> 00:29:00,596
I, I, I understand what you mean.

595
00:29:00,596 --> 00:29:00,746
Yeah.

596
00:29:00,746 --> 00:29:03,096
I feel like I, I, I misunderstood what you meant by impressing you.

597
00:29:03,096 --> 00:29:06,576
I thought you meant to impress you with its intelligence

598
00:29:06,576 --> 00:29:09,276
with how, how, with how good, well it understands

599
00:29:09,276 --> 00:29:09,956
um, an image.

600
00:29:09,956 --> 00:29:11,736
I wo- I, I thought you meant something like

601
00:29:11,736 --> 00:29:13,236
"I'm gonna show it a really complicated image

602
00:29:13,236 --> 00:29:14,056
and it's gonna get it right.

603
00:29:14,056 --> 00:29:14,980
And you're gonna say, 'Wow.

604
00:29:14,980 --> 00:29:15,166
'" Right.

605
00:29:15,166 --> 00:29:15,756
"That's really cool.

606
00:29:15,756 --> 00:29:19,944
Our systems of, you know, January 2020 have not been doing that.

607
00:29:19,944 --> 00:29:20,366
" Yeah.

608
00:29:20,366 --> 00:29:22,285
No, I, I, I think it all boils down to

609
00:29:22,285 --> 00:29:26,116
like, the reason people click like on stuff on the internet

610
00:29:26,116 --> 00:29:28,356
which is, like, it makes them laugh.

611
00:29:28,356 --> 00:29:31,116
So, it's, like, humor or wit- Yeah.

612
00:29:31,116 --> 00:29:31,126
.

613
00:29:31,126 --> 00:29:31,126
..

614
00:29:31,126 --> 00:29:32,236
or insight.

615
00:29:32,236 --> 00:29:32,798
We'll, we'll.

616
00:29:32,798 --> 00:29:32,906
..

617
00:29:32,906 --> 00:29:34,056
I'm, I'm sure we'll get it as

618
00:29:34,056 --> 00:29:35,456
get that as well.

619
00:29:35,456 --> 00:29:40,016
So, forgive the romanticized question, but looking back

620
00:29:40,016 --> 00:29:46,836
to you, what is the most beautiful or surprising idea in deep learning or AI in general you've come across?

621
00:29:46,836 --> 00:29:50,666
So, I think the most beautiful thing about deep learning is that it actually works.

622
00:29:50,666 --> 00:29:53,196
 And I mean it, because you got these ideas

623
00:29:53,196 --> 00:29:56,156
you got the little neural network, you got the back propagation algorithm

624
00:29:56,156 --> 00:30:00,386
and then you got some theories as to

625
00:30:00,386 --> 00:30:02,136
you know, this is kinda like the brain.

626
00:30:02,136 --> 00:30:03,656
So, maybe if you make it large

627
00:30:03,656 --> 00:30:05,996
if you make the neural network large, and you train it on a lot of data

628
00:30:05,996 --> 00:30:09,756
then it will do the same function that the brain does.

629
00:30:09,756 --> 00:30:10,746
And it turns out to be true.

630
00:30:10,746 --> 00:30:12,556
That's crazy.

631
00:30:12,556 --> 00:30:16,474
 And now we just train these neural networks and you make them larger and they keep getting better.

632
00:30:16,474 --> 00:30:16,679
..

633
00:30:16,679 --> 00:30:17,952
. and I find it unbelievable.

634
00:30:17,952 --> 00:30:22,552
I find it unbelievable that this whole AI stuff with neural networks works.

635
00:30:22,552 --> 00:30:25,032
Have you built up an intuition of why?

636
00:30:25,032 --> 00:30:27,962
Are there little bits and pieces of intuitions

637
00:30:27,962 --> 00:30:31,411
of insights of why this whole thing works?

638
00:30:31,411 --> 00:30:33,302
I mean, some, definitely.

639
00:30:33,302 --> 00:30:34,748
While we know that optimization .

640
00:30:34,748 --> 00:30:35,101
..

641
00:30:35,101 --> 00:30:36,441
we, we now have good re- you know

642
00:30:36,441 --> 00:30:40,631
we've take, we- we've had lots of empirical

643
00:30:40,631 --> 00:30:44,841
you know, huge amounts of empirical reasons to believe that optimization should work on all

644
00:30:44,841 --> 00:30:47,191
most problems we care about.

645
00:30:47,191 --> 00:30:48,727
Do you have insights of what .

646
00:30:48,727 --> 00:30:48,801
..

647
00:30:48,801 --> 00:30:50,831
so you just said empirical evidence.

648
00:30:50,831 --> 00:30:54,185
Is most of your .

649
00:30:54,185 --> 00:30:54,611
..

650
00:30:54,611 --> 00:30:58,472
s- sort of empirical evidence kind of convinces you.

651
00:30:58,472 --> 00:31:00,371
It's like evolution is empirical.

652
00:31:00,371 --> 00:31:05,651
It shows you that, look, this evolutionary process seems to be a good way to design

653
00:31:05,651 --> 00:31:08,272
uh, organisms that survive in their environment.

654
00:31:08,272 --> 00:31:13,992
But it doesn't really get you to the insights of how the whole thing works.

655
00:31:13,992 --> 00:31:14,668
Well, I think it's, it's .

656
00:31:14,668 --> 00:31:14,782
..

657
00:31:14,782 --> 00:31:16,552
a, a good analogy is physics.

658
00:31:16,552 --> 00:31:21,766
You know how you say, "Hey, let's do some physics calculation and come up with some new physics theory and make some prediction.

659
00:31:21,766 --> 00:31:24,002
" But then you've gotta run the experiment.

660
00:31:24,002 --> 00:31:25,121
You know, you've gotta run the experiment.

661
00:31:25,121 --> 00:31:26,161
It's important.

662
00:31:26,161 --> 00:31:27,472
So it's a bit the- the same here

663
00:31:27,472 --> 00:31:31,111
except that maybe some- sometimes the experiment came before the theory.

664
00:31:31,111 --> 00:31:32,131
But it still is the case.

665
00:31:32,131 --> 00:31:35,042
You know, you have some data, and you come up with some prediction.

666
00:31:35,042 --> 00:31:36,611
You say, "Yeah, let's make a big neural network.

667
00:31:36,611 --> 00:31:37,171
Let's train it.

668
00:31:37,171 --> 00:31:39,871
And it's going to work much better than anything before it

669
00:31:39,871 --> 00:31:42,681
and it will, in fact, continue to get better as we make it larger.

670
00:31:42,681 --> 00:31:43,651
" And it turns out to be true

671
00:31:43,651 --> 00:31:47,012
that's- that's amazing when a theory is validated like this

672
00:31:47,012 --> 00:31:47,431
you know.

673
00:31:47,431 --> 00:31:48,792
It's not a mathematical theory.

674
00:31:48,792 --> 00:31:51,752
It's more of a biological theory almost.

675
00:31:51,752 --> 00:31:55,512
So I think there are not-terrible analogies between deep learning and biology.

676
00:31:55,512 --> 00:31:58,802
I would say it's like the geometric mean of biology and physics.

677
00:31:58,802 --> 00:32:00,272
 That's deep learning.

678
00:32:00,272 --> 00:32:03,442
The geometric mean of biology and physics.

679
00:32:03,442 --> 00:32:06,371
I think I'm gonna need a few hours to wrap my head around that.

680
00:32:06,371 --> 00:32:10,515
Uh, 'cause just to find the geometric .

681
00:32:10,515 --> 00:32:10,522
..

682
00:32:10,522 --> 00:32:16,451
just to find, uh, the set of what biology represents.

683
00:32:16,451 --> 00:32:18,052
Well, biology, p- in, in biology

684
00:32:18,052 --> 00:32:19,532
things are really complicated.

685
00:32:19,532 --> 00:32:19,542
Yeah.

686
00:32:19,542 --> 00:32:21,035
And theories are really, really .

687
00:32:21,035 --> 00:32:21,042
..

688
00:32:21,042 --> 00:32:22,851
it's really hard to have good predictive theory.

689
00:32:22,851 --> 00:32:23,161
And if .

690
00:32:23,161 --> 00:32:23,181
..

691
00:32:23,181 --> 00:32:24,982
in physics, the theories are too good.

692
00:32:24,982 --> 00:32:26,312
F- i- in, in theory, in physics

693
00:32:26,312 --> 00:32:29,391
people make these super precise theories which make these amazing predictions.

694
00:32:29,391 --> 00:32:31,472
And in machine learning, we're kinda in between.

695
00:32:31,472 --> 00:32:37,772
Kind of in between, but it would be nice if machine learning somehow helped us discover the unification of the two

696
00:32:37,772 --> 00:32:40,992
as opposed to sort of the in between.

697
00:32:40,992 --> 00:32:41,532
But you're right.

698
00:32:41,532 --> 00:32:42,282
That's .

699
00:32:42,282 --> 00:32:42,282
..

700
00:32:42,282 --> 00:32:45,032
you're, you're kinda trying to juggle both.

701
00:32:45,032 --> 00:32:50,292
So do you think there's still beautiful and mysterious properties in neural networks that are yet to be discovered?

702
00:32:50,292 --> 00:32:51,421
Definitely.

703
00:32:51,421 --> 00:32:54,992
I think that we are still massively underestimating deep learning.

704
00:32:54,992 --> 00:32:56,692
What do you think it will look like?

705
00:32:56,692 --> 00:32:58,034
Like, what .

706
00:32:58,034 --> 00:32:58,322
..

707
00:32:58,322 --> 00:32:58,322
?

708
00:32:58,322 --> 00:32:59,272
If I knew, I would've done it

709
00:32:59,272 --> 00:33:00,272
you know?

710
00:33:00,272 --> 00:33:05,252
 So, uh, but if you look at all the progress from the past 10 years

711
00:33:05,252 --> 00:33:10,015
I would say most of it, I would say there have been a few cases where some .

712
00:33:10,015 --> 00:33:10,022
..

713
00:33:10,022 --> 00:33:12,951
where things that felt like really new ideas showed up.

714
00:33:12,951 --> 00:33:15,391
But by and large, it was every year we thought

715
00:33:15,391 --> 00:33:17,272
"Okay, deep learning goes this far.

716
00:33:17,272 --> 00:33:19,021
Nope, it actually goes further.

717
00:33:19,021 --> 00:33:20,042
"  And then the next year, "Okay

718
00:33:20,042 --> 00:33:20,517
now it .

719
00:33:20,517 --> 00:33:20,651
..

720
00:33:20,651 --> 00:33:21,075
you .

721
00:33:21,075 --> 00:33:21,081
..

722
00:33:21,081 --> 00:33:22,512
now this is, this is peak deep learning.

723
00:33:22,512 --> 00:33:23,151
We are really done.

724
00:33:23,151 --> 00:33:24,420
Nope, goes further.

725
00:33:24,420 --> 00:33:26,111
" It just keeps going further each year.

726
00:33:26,111 --> 00:33:26,141
Mm-hmm.

727
00:33:26,141 --> 00:33:29,212
So that means that we keep underestimating, we keep not understanding it.

728
00:33:29,212 --> 00:33:31,451
Has surprising properties all the time.

729
00:33:31,451 --> 00:33:33,631
Do you think it's getting harder and harder?

730
00:33:33,631 --> 00:33:34,371
To make progress?

731
00:33:34,371 --> 00:33:36,052
Yeah, to make progress.

732
00:33:36,052 --> 00:33:36,841
It depends on what you mean.

733
00:33:36,841 --> 00:33:41,242
I think the field will continue to make very robust progress for quite a while.

734
00:33:41,242 --> 00:33:45,171
I think for individual researchers, especially people who are doing r- um

735
00:33:45,171 --> 00:33:50,180
research, it can be harder, because there is a very large number of researchers right now.

736
00:33:50,180 --> 00:33:51,871
I think that if you have a lot of compute

737
00:33:51,871 --> 00:33:54,782
then you can make a lot of very interesting discoveries.

738
00:33:54,782 --> 00:34:02,451
But then you have to deal with the challenge of managing a huge computes- a huge clus- a huge compute cluster to run your experiments.

739
00:34:02,451 --> 00:34:03,272
It's a little bit harder.

740
00:34:03,272 --> 00:34:06,532
So I'm asking you all these questions that nobody knows the answer to.

741
00:34:06,532 --> 00:34:08,331
But you're one of the smartest people I know

742
00:34:08,331 --> 00:34:09,951
so I'm gonna keep asking.

743
00:34:09,951 --> 00:34:10,501
The .

744
00:34:10,501 --> 00:34:10,522
..

745
00:34:10,522 --> 00:34:14,672
so let's imagine all the breakthroughs that happen in the next 30 years in deep learning.

746
00:34:14,672 --> 00:34:22,112
Do you think most of those breakthroughs can be done by one person with one computer?

747
00:34:22,112 --> 00:34:26,132
Sort of in the space of breakthroughs, do you think compute will be

748
00:34:26,132 --> 00:34:32,371
uh, compute and large efforts will be necessary?

749
00:34:32,371 --> 00:34:33,911
I mean, I can't be sure.

750
00:34:33,911 --> 00:34:36,491
A- when you say one computer, you mean how- how large?

751
00:34:36,491 --> 00:34:40,652
 Ah, you're, uh, you're clever.

752
00:34:40,652 --> 00:34:42,652
I mean, one com- one GPU.

753
00:34:42,652 --> 00:34:43,991
I see.

754
00:34:43,991 --> 00:34:47,551
I think it's pretty unlikely.

755
00:34:47,551 --> 00:34:48,721
I think it's pretty unlikely.

756
00:34:48,721 --> 00:34:50,931
I think that there are many .

757
00:34:50,931 --> 00:34:51,091
..

758
00:34:51,091 --> 00:34:54,712
the, the stack of deep learning is starting to be quite deep.

759
00:34:54,712 --> 00:34:59,832
If you look at it, you've got all the way from the ideas

760
00:34:59,832 --> 00:35:04,172
the systems to build the datasets, the distributed programming

761
00:35:04,172 --> 00:35:08,192
the building the actual cluster, the GPU programming

762
00:35:08,192 --> 00:35:09,031
putting it all together.

763
00:35:09,031 --> 00:35:10,572
So now the stack is getting really deep

764
00:35:10,572 --> 00:35:11,921
and I think it becomes .

765
00:35:11,921 --> 00:35:12,301
..

766
00:35:12,301 --> 00:35:14,712
it can be quite hard for a single person to become

767
00:35:14,712 --> 00:35:17,971
to be world-class in every single layer of the stack.

768
00:35:17,971 --> 00:35:26,011
What about the, what, like, Vladimir Vapnik really insists on is taking MNIST and trying to learn from very few examples.

769
00:35:26,011 --> 00:35:29,192
So being able to learn e- more efficiently.

770
00:35:29,192 --> 00:35:30,541
Do you think that's .

771
00:35:30,541 --> 00:35:30,562
..

772
00:35:30,562 --> 00:35:34,892
there will be breakthroughs in that space that would may not need the huge compute?

773
00:35:34,892 --> 00:35:40,692
I think there will be hu- a ver- I think there will be a large number of breakthroughs in general that will not need a huge amount of compute.

774
00:35:40,692 --> 00:35:42,192
So maybe I should clarify that.

775
00:35:42,192 --> 00:35:44,772
I think that some breakthroughs will require a lot of compute.

776
00:35:44,772 --> 00:35:45,462
Sure.

777
00:35:45,462 --> 00:35:50,252
And I think building systems which actually do things will require a huge amount of computes.

778
00:35:50,252 --> 00:35:51,372
That one is pretty obvious.

779
00:35:51,372 --> 00:35:53,572
If you want to do X- Right.

780
00:35:53,572 --> 00:35:53,582
.

781
00:35:53,582 --> 00:35:53,582
..

782
00:35:53,582 --> 00:35:56,632
and X requires a huge neural net, you gotta get a huge neural net.

783
00:35:56,632 --> 00:35:59,375
But I think there will be lots of .

784
00:35:59,375 --> 00:35:59,382
..

785
00:35:59,382 --> 00:36:05,132
I think there is lots of room for very important work being done by small groups and individuals.

786
00:36:05,132 --> 00:36:07,952
Can you maybe, sort of o- on the topic of the

787
00:36:07,952 --> 00:36:13,531
the science of deep learning, talk about one of the recent papers that you released?

788
00:36:13,531 --> 00:36:13,721
Sure.

789
00:36:13,721 --> 00:36:14,971
The deep double descent?

790
00:36:14,971 --> 00:36:15,692
Mm-hmm.

791
00:36:15,692 --> 00:36:18,192
Where bigger models and more data hurt.

792
00:36:18,192 --> 00:36:19,692
Uh, I think it's a really interesting paper.

793
00:36:19,692 --> 00:36:22,415
Can you, can you describe the main idea and .

794
00:36:22,415 --> 00:36:22,422
..

795
00:36:22,422 --> 00:36:23,612
Yeah, definitely.

796
00:36:23,612 --> 00:36:25,162
So what happened is that some .

797
00:36:25,162 --> 00:36:25,505
..

798
00:36:25,505 --> 00:36:31,582
Over, over the years, some small number of researchers noticed that it is kind of weird that when you make the neural network larger

799
00:36:31,582 --> 00:36:32,142
it works better.

800
00:36:32,142 --> 00:36:34,832
And it seems to go in contradiction with statistical ideas.

801
00:36:34,832 --> 00:36:38,972
And then some people made an analysis showing that actually you got this double descent bump.

802
00:36:38,972 --> 00:36:43,452
And what we've done was to show that double descent occurs for all

803
00:36:43,452 --> 00:36:46,472
for pretty much all practical deep learning systems.

804
00:36:46,472 --> 00:36:50,572
And that it will be also- So can you step back?

805
00:36:50,572 --> 00:36:55,482
Um, what's the X axis and the Y axis of a double descent plot?

806
00:36:55,482 --> 00:36:57,112
Okay, great.

807
00:36:57,112 --> 00:37:02,112
So you can, you can look, you can do things like

808
00:37:02,112 --> 00:37:10,072
you can take a neural network and you can start increasing its size slowly while keeping your data set fixed.

809
00:37:10,072 --> 00:37:14,252
So if you increase the size of the neural network slowly

810
00:37:14,252 --> 00:37:20,352
and if you don't do early stopping, that's a pretty important detail

811
00:37:20,352 --> 00:37:22,502
then when the neural network is really small

812
00:37:22,502 --> 00:37:26,132
you make it larger, you get a very rapid increase in performance.

813
00:37:26,132 --> 00:37:30,192
Then you continue to make it larger, and at some point performance will get worse.

814
00:37:30,192 --> 00:37:36,292
And it gets, and- and it gets the worst exactly at the point at which it achieves zero training error

815
00:37:36,292 --> 00:37:38,712
precisely zero training loss.

816
00:37:38,712 --> 00:37:41,572
And then as you make it large, it starts to get better again.

817
00:37:41,572 --> 00:37:46,912
And it's kind of counterintuitive because you'd expect deep learning phenomena to be monotonic.

818
00:37:46,912 --> 00:37:50,032
And it's hard to be sure what it means

819
00:37:50,032 --> 00:37:53,152
but it also occurs in, in the case of linear classifiers.

820
00:37:53,152 --> 00:37:57,092
And the intuition basically boils down to the following.

821
00:37:57,092 --> 00:37:58,872
When you, when you have a lot

822
00:37:58,872 --> 00:38:02,952
when you have a large data set and a small model

823
00:38:02,952 --> 00:38:04,942
then small, tiny, random.

824
00:38:04,942 --> 00:38:05,082
..

825
00:38:05,082 --> 00:38:07,172
So, so basically what is overfitting?

826
00:38:07,172 --> 00:38:16,112
Overfitting is when your model is somehow very sensitive to the small random unimportant stuff in your data set.

827
00:38:16,112 --> 00:38:16,992
In the training data.

828
00:38:16,992 --> 00:38:19,072
In the training data set, precisely.

829
00:38:19,072 --> 00:38:22,592
So if you have a small model and you have a big data set

830
00:38:22,592 --> 00:38:24,792
and there may be some random thing, you know

831
00:38:24,792 --> 00:38:29,172
some training cases are randomly in the data set and others may not be there.

832
00:38:29,172 --> 00:38:34,312
But the small model, but the small model is kind of insensitive to this randomness because it's the sa- you

833
00:38:34,312 --> 00:38:38,412
there is pretty much no uncertainty about the model when the data set is large.

834
00:38:38,412 --> 00:38:38,932
So, okay.

835
00:38:38,932 --> 00:38:41,272
So at the very basic level, to me

836
00:38:41,272 --> 00:38:50,312
it is the most surprising thing that neural networks don't overfit every time very quickly

837
00:38:50,312 --> 00:38:54,092
uh,  before ever being able to learn anything.

838
00:38:54,092 --> 00:38:56,372
The huge number of parameters.

839
00:38:56,372 --> 00:38:57,445
So here is, so there is one way.

840
00:38:57,445 --> 00:38:57,482
..

841
00:38:57,482 --> 00:39:00,212
Okay, so maybe the, so let, let me try to give the explanation

842
00:39:00,212 --> 00:39:02,082
maybe that will be, that will work.

843
00:39:02,082 --> 00:39:03,632
So you got a huge neural network.

844
00:39:03,632 --> 00:39:05,187
Let's suppose you've got a.

845
00:39:05,187 --> 00:39:06,042
..

846
00:39:06,042 --> 00:39:07,612
You are, you have a huge neural network

847
00:39:07,612 --> 00:39:09,772
you have a huge number of parameters.

848
00:39:09,772 --> 00:39:11,812
And now let's pretend everything is linear, which is not

849
00:39:11,812 --> 00:39:13,172
but let's just pretend.

850
00:39:13,172 --> 00:39:17,612
Then there is this big subspace where your neural network achieves zero error.

851
00:39:17,612 --> 00:39:18,112
Mm-hmm.

852
00:39:18,112 --> 00:39:21,912
And SGT is going to find approximately the point- Stochastic gradient

853
00:39:21,912 --> 00:39:21,922
yeah.

854
00:39:21,922 --> 00:39:26,632
That's right, yeah, approximately the point with the smallest norm in that subspace.

855
00:39:26,632 --> 00:39:27,192
Okay.

856
00:39:27,192 --> 00:39:35,412
And that can also be proven to be insensitive to the small randomness in the data when the dimensionality is high.

857
00:39:35,412 --> 00:39:39,372
But when the dimensionality of the data is equal to the dimensionality of the model

858
00:39:39,372 --> 00:39:44,432
then there is a one-to-one correspondence between all the data sets and the models.

859
00:39:44,432 --> 00:39:47,332
So small changes in the data set actually lead to large changes in the model

860
00:39:47,332 --> 00:39:48,852
and that's why performance gets worse.

861
00:39:48,852 --> 00:39:51,552
So this is the best explanation more or less.

862
00:39:51,552 --> 00:39:56,152
So then it would be good for the model to have more parameters

863
00:39:56,152 --> 00:39:58,672
sort of, um, to be bigger than the data?

864
00:39:58,672 --> 00:39:59,082
That's right.

865
00:39:59,082 --> 00:40:00,882
But o- only if you don't early stop.

866
00:40:00,882 --> 00:40:02,841
If you introduce early stop in your regularization

867
00:40:02,841 --> 00:40:06,182
you can make the double s- descent bump almost completely disappear.

868
00:40:06,182 --> 00:40:07,121
What is early stop?

869
00:40:07,121 --> 00:40:11,472
Early stopping is when you train your model and you monitor your test

870
00:40:11,472 --> 00:40:13,632
your validation performance.

871
00:40:13,632 --> 00:40:15,912
And then if at some point validation performance starts to get worse

872
00:40:15,912 --> 00:40:17,642
you say, "Okay, let's stop training.

873
00:40:17,642 --> 00:40:18,032
We are good.

874
00:40:18,032 --> 00:40:18,572
We are good.

875
00:40:18,572 --> 00:40:19,920
We are good enough.

876
00:40:19,920 --> 00:40:23,132
" So the, the magic happens after, after that moment.

877
00:40:23,132 --> 00:40:25,072
So you don't want to do the early stopping?

878
00:40:25,072 --> 00:40:26,621
Well, if you don't do the early stopping

879
00:40:26,621 --> 00:40:29,662
you get these very, you get a very pronounced double descent.

880
00:40:29,662 --> 00:40:33,492
Do you have any intuition why this happens?

881
00:40:33,492 --> 00:40:34,312
Double descent?

882
00:40:34,312 --> 00:40:35,572
Oh, sorry, early stopping?

883
00:40:35,572 --> 00:40:36,822
No, the double descent.

884
00:40:36,822 --> 00:40:37,432
So the- Oh, yeah.

885
00:40:37,432 --> 00:40:38,892
So I try, let's see.

886
00:40:38,892 --> 00:40:47,692
The intuition is basically is this, that when the data set has as many degrees of freedom as the model

887
00:40:47,692 --> 00:40:49,822
then there is a one-to-one correspondence between them.

888
00:40:49,822 --> 00:40:55,152
And so small changes to the data set lead to noticeable changes in the model.

889
00:40:55,152 --> 00:40:57,312
So your model is very sensitive to all the randomness.

890
00:40:57,312 --> 00:40:59,612
It is unable to discard it.

891
00:40:59,612 --> 00:41:06,672
Whereas it turns out that when you have a lot more data than parameters or a lot more parameters than data

892
00:41:06,672 --> 00:41:10,532
the resulting solution will be insensitive to small changes in the data set.

893
00:41:10,532 --> 00:41:13,512
Oh, so it's able to, that's nicely put

894
00:41:13,512 --> 00:41:16,346
discard the small changes, the, the randomnessaa.

895
00:41:16,346 --> 00:41:16,422
..

896
00:41:16,422 --> 00:41:17,032
Randomness, exactly.

897
00:41:17,032 --> 00:41:20,612
The, the, the, the spurious correlations which you don't want.

898
00:41:20,612 --> 00:41:23,502
Geoff Hinton suggested we need to throw backpropagation.

899
00:41:23,502 --> 00:41:25,232
We already kind of talked about this a little bit

900
00:41:25,232 --> 00:41:29,832
but he suggested we need to throw away backpropagation and start over.

901
00:41:29,832 --> 00:41:32,132
I mean, o- of course some of that is a little bit

902
00:41:32,132 --> 00:41:35,282
um, wit and humor.

903
00:41:35,282 --> 00:41:36,742
But what do you think?

904
00:41:36,742 --> 00:41:39,672
What could be an alternative method of training neural networks?

905
00:41:39,672 --> 00:41:45,272
Well, the thing that he said precisely is that to the extent that you can't find backpropagation in the brain

906
00:41:45,272 --> 00:41:49,112
it's worth seeing if we can learn something from how the brain learns.

907
00:41:49,112 --> 00:41:52,392
But backpropagation is very useful and we should keep using it.

908
00:41:52,392 --> 00:41:58,112
Oh, you're saying that once we discover the mechanism of learning in the brain or any aspects of that mechanism

909
00:41:58,112 --> 00:42:00,652
we should also try to implement that in neural networks?

910
00:42:00,652 --> 00:42:03,722
If it turns out that we can't find backpropagation in the brain.

911
00:42:03,722 --> 00:42:06,072
If we can't find backpropagation in the brain.

912
00:42:06,072 --> 00:42:13,952
Well, so I guess your answer to that is backpropagation is pretty damn useful.

913
00:42:13,952 --> 00:42:16,032
So w- why are we complaining?

914
00:42:16,032 --> 00:42:18,432
I mean, I, I personally am a big fan of backpropagation.

915
00:42:18,432 --> 00:42:21,992
I think it's a great algorithm because it solves an extremely fundamental problem

916
00:42:21,992 --> 00:42:28,516
which is finding a neural circuit subject to some constraints.

917
00:42:28,516 --> 00:42:28,746
..

918
00:42:28,746 --> 00:42:30,492
. and I don't see that problem going away.

919
00:42:30,492 --> 00:42:38,712
So, that's why I- I really, I think it's pretty unlikely that we'll have anything which is going to be dramatically different.

920
00:42:38,712 --> 00:42:42,632
It could happen, but I wouldn't bet on it right now.

921
00:42:42,632 --> 00:42:46,882
So, let me ask a sort of big picture question.

922
00:42:46,882 --> 00:42:51,732
Do you think can- do you think neural networks can be made to reason?

923
00:42:51,732 --> 00:42:52,652
Why not?

924
00:42:52,652 --> 00:42:58,292
 Well, if you look for example at AlphaGo or AlphaZero

925
00:42:58,292 --> 00:43:05,252
the neural network of AlphaZero plays Go, which a g- which we all agree is a game that requires reasoning

926
00:43:05,252 --> 00:43:07,012
better than 99.

927
00:43:07,012 --> 00:43:08,602
9% of all humans.

928
00:43:08,602 --> 00:43:10,312
Just the neural network, without the search

929
00:43:10,312 --> 00:43:12,332
just the neural network itself.

930
00:43:12,332 --> 00:43:17,852
Doesn't that give us an existence proof that neural networks can reason?

931
00:43:17,852 --> 00:43:19,612
To push back and disagree a little bit

932
00:43:19,612 --> 00:43:23,312
we all agree that Go is reasoning.

933
00:43:23,312 --> 00:43:26,080
Uh, I think I- I agree, I don't think that's a trivial.

934
00:43:26,080 --> 00:43:26,202
..

935
00:43:26,202 --> 00:43:28,781
So obviously, reasoning, like intelligence, is

936
00:43:28,781 --> 00:43:32,132
um, is a loose, gray area term

937
00:43:32,132 --> 00:43:32,632
a little bit.

938
00:43:32,632 --> 00:43:34,092
Maybe you disagree with that.

939
00:43:34,092 --> 00:43:39,412
But yes, I think it has some of the same elements of reasoning.

940
00:43:39,412 --> 00:43:42,872
Reasoning is almost, like, akin to search

941
00:43:42,872 --> 00:43:43,202
right?

942
00:43:43,202 --> 00:43:51,492
There's a sequential element of stepwise consideration of possibilities

943
00:43:51,492 --> 00:43:57,652
and sort of building on top of those possibilities in a sequential manner until you arrive at some insight.

944
00:43:57,652 --> 00:44:00,652
Sort of, yeah, I guess playing Go is kind of like that

945
00:44:00,652 --> 00:44:04,132
and when you have a single neural network doing that without search

946
00:44:04,132 --> 00:44:04,922
it's kind of like that.

947
00:44:04,922 --> 00:44:13,912
So, there's an existent proof in a particular constrained environment that a- a process akin to what many people call reasoning exists

948
00:44:13,912 --> 00:44:17,192
but more general kind of reasoning.

949
00:44:17,192 --> 00:44:18,872
So, off the board.

950
00:44:18,872 --> 00:44:20,592
There is one other existence proof.

951
00:44:20,592 --> 00:44:21,072
Oh, boy.

952
00:44:21,072 --> 00:44:22,172
Which one?

953
00:44:22,172 --> 00:44:22,852
 Us humans?

954
00:44:22,852 --> 00:44:23,402
Yes.

955
00:44:23,402 --> 00:44:24,072
Okay.

956
00:44:24,072 --> 00:44:24,492
All right.

957
00:44:24,492 --> 00:44:38,872
So, do you think the architecture that will allow neural networks to reason will look similar to the neural network architectures we have today?

958
00:44:38,872 --> 00:44:39,592
I think it will.

959
00:44:39,592 --> 00:44:39,908
I think.

960
00:44:39,908 --> 00:44:39,961
..

961
00:44:39,961 --> 00:44:44,112
Well, I don't want to make too o- overly definitive statements.

962
00:44:44,112 --> 00:44:53,722
I think it's definitely possible that the neural networks that will produce the reasoning breakthroughs of the future will be very similar to the architectures that exist today.

963
00:44:53,722 --> 00:44:57,212
Maybe a little bit more recurrent, maybe a little bit deeper.

964
00:44:57,212 --> 00:45:02,992
But, but these- these neural nets are so insanely powerful.

965
00:45:02,992 --> 00:45:05,572
Why wouldn't they be able to learn to reason?

966
00:45:05,572 --> 00:45:07,312
Humans can reason.

967
00:45:07,312 --> 00:45:09,332
So, why can't neural networks?

968
00:45:09,332 --> 00:45:14,702
So, do you think the kind of stuff we've seen neural networks do is a kind of just weak reasoning?

969
00:45:14,702 --> 00:45:16,652
So, it's not a fundamentally different process?

970
00:45:16,652 --> 00:45:19,752
Again, this is stuff we don't- nobody knows the answer to.

971
00:45:19,752 --> 00:45:22,632
So, when it comes to our neural networks

972
00:45:22,632 --> 00:45:28,332
I would- the thing which I would say is that neural networks are capable of reasoning.

973
00:45:28,332 --> 00:45:31,992
But if you train a neural network on a task which doesn't require reasoning

974
00:45:31,992 --> 00:45:34,052
it's not going to reason.

975
00:45:34,052 --> 00:45:44,492
This is a well-known effect, where the neural network will solve exactly the- it will solve the problem that you pose in front of it in the easiest way possible.

976
00:45:44,492 --> 00:45:45,112
Right.

977
00:45:45,112 --> 00:45:52,852
That takes us to the- to o- o- one of the brilliant sort of ways you've described neural networks

978
00:45:52,852 --> 00:45:57,152
which is, uh, you've referred to neural networks as the search for small circuits

979
00:45:57,152 --> 00:46:04,552
and maybe general intelligence as the search for small programs

980
00:46:04,552 --> 00:46:06,772
which I found as a metaphor very compelling.

981
00:46:06,772 --> 00:46:09,012
Can you elaborate on that difference?

982
00:46:09,012 --> 00:46:10,052
Yeah.

983
00:46:10,052 --> 00:46:20,992
So, the thing which I said precisely was that if you can find the shortest program that outputs the data in your- at your disposal

984
00:46:20,992 --> 00:46:24,232
then you will be able to use it to make the best prediction possible.

985
00:46:24,232 --> 00:46:24,442
Mm-hmm.

986
00:46:24,442 --> 00:46:29,212
And that's a theoretical statement which can be proven mathematically.

987
00:46:29,212 --> 00:46:39,032
Now, you can also prove mathematically that it is- that finding the shortest program regenerates some data is not com- is not a computable operation.

988
00:46:39,032 --> 00:46:42,852
No, uh, finite amount of compute can do this.

989
00:46:42,852 --> 00:46:50,192
So then, with- with neural networks, neural networks are the next best thing that actually works in practice.

990
00:46:50,192 --> 00:46:54,812
We are not able to find the best- the shortest program which generates our data

991
00:46:54,812 --> 00:46:58,452
but we are able to find, you know

992
00:46:58,452 --> 00:47:01,072
a small, but now- now that statement should be amended

993
00:47:01,072 --> 00:47:05,352
even a large circuit which fits our data in some way.

994
00:47:05,352 --> 00:47:10,072
Well, I think what you meant by the small circuit is the smallest needed circuit.

995
00:47:10,072 --> 00:47:12,352
Well, I- s- the thing- the thing which I would change now

996
00:47:12,352 --> 00:47:17,132
back- back then, I really have- I haven't fully internalized the over-parametr- the over-parameterized results.

997
00:47:17,132 --> 00:47:20,532
The re- the things we know about over-parameterized neural nets

998
00:47:20,532 --> 00:47:27,656
now I would phrase it as, "A large circuit that can- wh- whose weights contain a small amount of information

999
00:47:27,656 --> 00:47:29,232
" which I think is what's going on.

1000
00:47:29,232 --> 00:47:37,072
If you imagine the training process of a neural network as you slowly transmit entropy from the data set to the parameters

1001
00:47:37,072 --> 00:47:42,932
then somehow, the amount of information in the weights ends up being not very large

1002
00:47:42,932 --> 00:47:45,292
which would explain why they generalize so well.

1003
00:47:45,292 --> 00:47:51,932
So that's- there the large circuit might be one that's helpful for the regulariz- for the generalization?

1004
00:47:51,932 --> 00:47:53,732
Yeah, something like this.

1005
00:47:53,732 --> 00:48:02,471
But do you see their- do you see it important to be able to try to learn something like programs?

1006
00:48:02,471 --> 00:48:04,892
I mean, if we can, definitely.

1007
00:48:04,892 --> 00:48:07,592
I think it's kind of- the answer is kind of yes

1008
00:48:07,592 --> 00:48:09,152
if we can do it.

1009
00:48:09,152 --> 00:48:10,992
We should do things that we can do it.

1010
00:48:10,992 --> 00:48:14,032
 It's- it's- the reason we are pushing on deep learning

1011
00:48:14,032 --> 00:48:21,552
the fundamental reason, the cau- the- the- the- the root cause is that we are able to train them.

1012
00:48:21,552 --> 00:48:23,892
So, in other words, training comes first.

1013
00:48:23,892 --> 00:48:26,762
We've got our pillar, which is the training pillar

1014
00:48:26,762 --> 00:48:30,912
and now we are trying to contort our neural networks around the training pillar.

1015
00:48:30,912 --> 00:48:31,971
We gotta stay trainable.

1016
00:48:31,971 --> 00:48:36,471
This is an invo- this is an invariant we cannot violate.

1017
00:48:36,471 --> 00:48:37,341
And so.

1018
00:48:37,341 --> 00:48:37,748
..

1019
00:48:37,748 --> 00:48:38,154
..

1020
00:48:38,154 --> 00:48:41,248
. being trainable means starting from scratch, knowing nothing

1021
00:48:41,248 --> 00:48:44,608
you can actually pretty quickly converge towards knowing a lot.

1022
00:48:44,608 --> 00:48:45,948
Or even slowly.

1023
00:48:45,948 --> 00:48:50,728
But it means that given the resources at your disposal

1024
00:48:50,728 --> 00:48:55,448
you can train the neural net and get it to achieve useful performance.

1025
00:48:55,448 --> 00:48:57,468
Yeah, that's a pillar we can't move away from.

1026
00:48:57,468 --> 00:48:57,818
That's right.

1027
00:48:57,818 --> 00:48:59,468
Because if you c- and whereas, if you say

1028
00:48:59,468 --> 00:49:01,568
"Hey, let's find the shortest program," well

1029
00:49:01,568 --> 00:49:02,848
we can't do that.

1030
00:49:02,848 --> 00:49:06,088
So it doesn't matter how useful that would be

1031
00:49:06,088 --> 00:49:07,308
we can't do it.

1032
00:49:07,308 --> 00:49:08,488
So we won't.

1033
00:49:08,488 --> 00:49:09,108
So do you think.

1034
00:49:09,108 --> 00:49:09,218
..

1035
00:49:09,218 --> 00:49:14,508
You kind of mentioned that neural networks are good at finding small circuits or large circuits.

1036
00:49:14,508 --> 00:49:19,308
Do you think then the matter of finding small programs is just the data?

1037
00:49:19,308 --> 00:49:20,128
No.

1038
00:49:20,128 --> 00:49:20,558
So do.

1039
00:49:20,558 --> 00:49:20,738
..

1040
00:49:20,738 --> 00:49:23,848
the ki- sorry, not, not the size or character

1041
00:49:23,848 --> 00:49:25,928
the qual- the, the, the type of data.

1042
00:49:25,928 --> 00:49:28,568
Sort of ask, giving it programs.

1043
00:49:28,568 --> 00:49:32,008
Well, I think the thing is that right now

1044
00:49:32,008 --> 00:49:33,000
finding.

1045
00:49:33,000 --> 00:49:33,218
..

1046
00:49:33,218 --> 00:49:38,988
there are no good precedents of people successfully finding programs really well.

1047
00:49:38,988 --> 00:49:44,368
And so the way you'd find programs is you'd train a deep neural network to do it basically.

1048
00:49:44,368 --> 00:49:44,778
Right.

1049
00:49:44,778 --> 00:49:48,128
Which is, which is the right way to go about it.

1050
00:49:48,128 --> 00:49:50,818
But there's not good, uh, illustrations of that yet.

1051
00:49:50,818 --> 00:49:51,888
It has- hasn't been yet.

1052
00:49:51,888 --> 00:49:55,788
But I- in, in principle, it should be possible.

1053
00:49:55,788 --> 00:49:58,208
Can you elaborate another bit?

1054
00:49:58,208 --> 00:49:58,278
Do you.

1055
00:49:58,278 --> 00:49:58,278
..

1056
00:49:58,278 --> 00:49:59,888
what's your insight in principle?

1057
00:49:59,888 --> 00:50:04,228
Well- Put another way, you don't see why it's not possible.

1058
00:50:04,228 --> 00:50:08,675
Well, it's kind of like more, it's more a statement of.

1059
00:50:08,675 --> 00:50:09,467
..

1060
00:50:09,467 --> 00:50:15,071
I think that it's, I thi- I think that it's unwise to bet against deep learning and-  .

1061
00:50:15,071 --> 00:50:15,078
..

1062
00:50:15,078 --> 00:50:18,698
if it's a fun- if it's a cognitive function that humans seem to be able to do

1063
00:50:18,698 --> 00:50:25,808
then it doesn't take too long for some deep neural net to pop up that can do it too.

1064
00:50:25,808 --> 00:50:26,258
 Yeah.

1065
00:50:26,258 --> 00:50:27,838
I'm, I'm, I'm, I'm there with you.

1066
00:50:27,838 --> 00:50:28,174
I can.

1067
00:50:28,174 --> 00:50:28,278
..

1068
00:50:28,278 --> 00:50:33,088
I've, I've stopped betting against neural networks at this point

1069
00:50:33,088 --> 00:50:35,748
because they continue to surprise us.

1070
00:50:35,748 --> 00:50:37,288
What about long-term memory?

1071
00:50:37,288 --> 00:50:42,208
Can neural networks have long-term memory or something like knowledge bases?

1072
00:50:42,208 --> 00:50:55,838
So being able to aggregate important information over long periods of time that would then serve as useful sort of representations of state that

1073
00:50:55,838 --> 00:51:01,648
uh, you could make decisions by, so have a long-term context based on which you're making the decision.

1074
00:51:01,648 --> 00:51:05,988
So in some sense, the parameters already do that.

1075
00:51:05,988 --> 00:51:08,998
The parameters are an aggregation of the da- of the neural.

1076
00:51:08,998 --> 00:51:09,098
..

1077
00:51:09,098 --> 00:51:10,868
of the entirety of the neural net's experience

1078
00:51:10,868 --> 00:51:13,008
and so they count as the long- as long-form

1079
00:51:13,008 --> 00:51:15,628
long-term knowledge.

1080
00:51:15,628 --> 00:51:20,008
And people have trained various neural nets to act as knowledge bases and

1081
00:51:20,008 --> 00:51:21,463
you know, investigated th- invest.

1082
00:51:21,463 --> 00:51:21,537
..

1083
00:51:21,537 --> 00:51:23,708
people have investigated language models as knowledge bases.

1084
00:51:23,708 --> 00:51:27,328
So there is work, there is work there.

1085
00:51:27,328 --> 00:51:28,678
Yeah, but in some sense.

1086
00:51:28,678 --> 00:51:29,908
Do you think in every sense?

1087
00:51:29,908 --> 00:51:31,568
Do you think there's a.

1088
00:51:31,568 --> 00:51:32,548
..

1089
00:51:32,548 --> 00:51:40,248
it's- it's all just a, a matter of coming up with a better mechanism of forgetting the useless stuff and remembering the useful stuff?

1090
00:51:40,248 --> 00:51:46,888
'Cause right now, I mean, there's not been mechanisms that do remember really long-term information.

1091
00:51:46,888 --> 00:51:48,888
What do you mean by that precisely?

1092
00:51:48,888 --> 00:51:49,791
Precisely.

1093
00:51:49,791 --> 00:51:49,958
..

1094
00:51:49,958 --> 00:51:51,868
I like, I like the word precisely.

1095
00:51:51,868 --> 00:52:00,508
So I'm thinking of the kind of compression of information the knowledge bases represent

1096
00:52:00,508 --> 00:52:03,073
sort of creating a.

1097
00:52:03,073 --> 00:52:03,838
..

1098
00:52:03,838 --> 00:52:08,688
Now, I apologize for my sort of human-centric thinking about what knowledge is

1099
00:52:08,688 --> 00:52:15,838
'cause n- neural networks aren't in-interpretable necessarily with the kind of knowledge they have discovered.

1100
00:52:15,838 --> 00:52:18,757
But a good example for me is knowledge bases

1101
00:52:18,757 --> 00:52:23,428
being able to build up over time something like the knowledge that Wikipedia represents.

1102
00:52:23,428 --> 00:52:29,788
It's a really compressed, structured, f- uh

1103
00:52:29,788 --> 00:52:30,888
knowledge base.

1104
00:52:30,888 --> 00:52:34,388
O- obviously not the actual Wikipedia or the language

1105
00:52:34,388 --> 00:52:37,948
but like a semantic web, the dream that a semantic web represented.

1106
00:52:37,948 --> 00:52:40,408
So it's a really nice compressed knowledge base

1107
00:52:40,408 --> 00:52:44,898
or something akin to that in a non-interpretable sense as

1108
00:52:44,898 --> 00:52:47,018
um, neural networks would have.

1109
00:52:47,018 --> 00:52:49,488
Well, the neural networks would be non-interpretable if you look at their weights

1110
00:52:49,488 --> 00:52:52,228
but their outputs should be very interpretable.

1111
00:52:52,228 --> 00:52:53,248
Okay, so yeah, how do you.

1112
00:52:53,248 --> 00:52:53,298
..

1113
00:52:53,298 --> 00:52:55,848
how do you make very smart neural networks

1114
00:52:55,848 --> 00:52:58,128
like language models, interpretable?

1115
00:52:58,128 --> 00:53:02,158
Well, you ask them to generate some text and the text will generally be interpretable.

1116
00:53:02,158 --> 00:53:04,778
Do you find that the epitome of interpretability?

1117
00:53:04,778 --> 00:53:06,038
Like, uh, can you do better?

1118
00:53:06,038 --> 00:53:08,365
Like c- can you a- 'cause you can't.

1119
00:53:08,365 --> 00:53:08,438
..

1120
00:53:08,438 --> 00:53:12,268
Okay, I would like to know what does it know and what doesn't it know.

1121
00:53:12,268 --> 00:53:20,408
I would like the neural network to come up with examples where it's completely dumb and examples where it's completely brilliant.

1122
00:53:20,408 --> 00:53:26,488
And the only way I know how to do that now is to generate a lot of examples and use my human judgment.

1123
00:53:26,488 --> 00:53:31,560
But it would be nice if a neural network had some aware- self-awareness about.

1124
00:53:31,560 --> 00:53:31,778
..

1125
00:53:31,778 --> 00:53:33,188
 Yeah, 100%.

1126
00:53:33,188 --> 00:53:35,406
I'm, I'm a big believer in self-awareness and I think that.

1127
00:53:35,406 --> 00:53:35,478
..

1128
00:53:35,478 --> 00:53:43,708
 I think, I think new- neural net self-awareness will allow for things like the capabilities like the ones you described

1129
00:53:43,708 --> 00:53:51,018
like for them to know what they know and what they don't know and for them to know where to invest to increase their skills most optimally.

1130
00:53:51,018 --> 00:53:54,408
And to your question of interpretability, there are actually two answers to that question.

1131
00:53:54,408 --> 00:53:56,488
One answer is, you know, we have the neural net

1132
00:53:56,488 --> 00:54:05,948
so we can analyze the neurons and we can try to understand what the different neurons and different layers mean and you can actually do that and OpenAI has done some work on that.

1133
00:54:05,948 --> 00:54:08,888
But there is a different answer which is that

1134
00:54:08,888 --> 00:54:13,208
I would say this is the human-centric answer where you say

1135
00:54:13,208 --> 00:54:15,068
you know, you look at a human being

1136
00:54:15,068 --> 00:54:16,009
you can't read.

1137
00:54:16,009 --> 00:54:16,098
..

1138
00:54:16,098 --> 00:54:18,708
you know, h- h- how do you know what a human being is thinking?

1139
00:54:18,708 --> 00:54:20,608
You ask them, you say, "Hey, what do you think about this?

1140
00:54:20,608 --> 00:54:21,444
What do you think about that?

1141
00:54:21,444 --> 00:54:23,968
" And you get some answers.

1142
00:54:23,968 --> 00:54:28,068
The answers you get are sticky in the sense you already have a mental model.

1143
00:54:28,068 --> 00:54:29,726
You already have an.

1144
00:54:29,726 --> 00:54:30,418
..

1145
00:54:30,418 --> 00:54:32,768
uh, yeah, a mental model of that human being.

1146
00:54:32,768 --> 00:54:37,730
You already have an understanding of like a b- a big conception of what it.

1147
00:54:37,730 --> 00:54:37,878
..

1148
00:54:37,878 --> 00:54:39,388
of that human being, how they think

1149
00:54:39,388 --> 00:54:42,908
what they know, how they see the world and then everything you ask

1150
00:54:42,908 --> 00:54:45,194
you're s- adding on to that.

1151
00:54:45,194 --> 00:54:45,476
..

1152
00:54:45,476 --> 00:54:52,848
. and that stickiness seems to be- that's one of the really interesting qualities of the- the human being

1153
00:54:52,848 --> 00:54:55,048
is that information is sticky.

1154
00:54:55,048 --> 00:54:57,548
You don't- you seem to remember the useful stuff

1155
00:54:57,548 --> 00:55:01,808
aggregate it well, and forget most of the information that's not useful.

1156
00:55:01,808 --> 00:55:06,788
Th- that process, but that's also pretty similar to the process that neural networks do.

1157
00:55:06,788 --> 00:55:09,868
It's just that neural networks are much crappier at it at this time.

1158
00:55:09,868 --> 00:55:13,308
It's n- it doesn't seem to be fundamentally that different.

1159
00:55:13,308 --> 00:55:17,067
But just to stick on reasoning for a little longer

1160
00:55:17,067 --> 00:55:18,748
 you said, "Why not?

1161
00:55:18,748 --> 00:55:19,669
Why can't I reason?

1162
00:55:19,669 --> 00:55:23,607
" What- what's a good impressive feat, benchmark

1163
00:55:23,607 --> 00:55:30,627
to you, of reasoning that you would be impressed by if neural networks were able to do?

1164
00:55:30,627 --> 00:55:32,868
Is that something you already have in mind?

1165
00:55:32,868 --> 00:55:35,008
Well, I think writing, writing really good code.

1166
00:55:35,008 --> 00:55:44,627
I think proving really hard theorems, solving open-ended problems with out-of-the-box solutions.

1167
00:55:44,627 --> 00:55:49,528
And, uh, sort of theorem type mathematical problems?

1168
00:55:49,528 --> 00:55:52,688
Yeah, I think tho- tho- those ones are a very natural example as well.

1169
00:55:52,688 --> 00:55:54,488
You know, if you can prove an unproven theorem

1170
00:55:54,488 --> 00:55:56,418
then it's hard to argue you don't reason.

1171
00:55:56,418 --> 00:56:00,468
And so, by the way, and this comes back to the point about the hard results

1172
00:56:00,468 --> 00:56:04,408
you know, if you got a har- if you have b- machine learning

1173
00:56:04,408 --> 00:56:06,087
deep learning as a field is very fortunate

1174
00:56:06,087 --> 00:56:12,127
because we have the ability to sometimes produce these unambiguous results and when they happen

1175
00:56:12,127 --> 00:56:14,188
uh, the debate changes, the conversation changes.

1176
00:56:14,188 --> 00:56:19,538
It's a convers- y- we, we have the ability to produce conversation-changing results.

1177
00:56:19,538 --> 00:56:21,658
Conversation, and then, of course, just like you said

1178
00:56:21,658 --> 00:56:23,258
people kind of take that for granted and say

1179
00:56:23,258 --> 00:56:25,063
"That wasn't actually a hard problem.

1180
00:56:25,063 --> 00:56:26,368
" Well, I mean, at some point

1181
00:56:26,368 --> 00:56:28,288
we'll probably run out of hard problems.

1182
00:56:28,288 --> 00:56:35,168
Yeah, that whole mortality thing is kind of s- kind of a sticky problem that we haven't s- quite figured out.

1183
00:56:35,168 --> 00:56:37,248
Maybe we'll solve that one.

1184
00:56:37,248 --> 00:56:40,908
I think one of the fascinating things in- in your entire body of work

1185
00:56:40,908 --> 00:56:46,648
but also the work at OpenAI recently, one of the conversation-changers has been in the world of language models.

1186
00:56:46,648 --> 00:56:54,627
Can you briefly kinda try to describe the recent history of using neural networks in the domain of language and text?

1187
00:56:54,627 --> 00:56:56,587
Well, there's been lots of history.

1188
00:56:56,587 --> 00:56:58,728
I think, I think the Elman network was

1189
00:56:58,728 --> 00:57:00,268
was a s- was a s- was a small

1190
00:57:00,268 --> 00:57:03,928
tiny recurrent neural network applied to language back in the '80s.

1191
00:57:03,928 --> 00:57:07,448
So the history's really, you know, fairly long

1192
00:57:07,448 --> 00:57:08,788
at least.

1193
00:57:08,788 --> 00:57:17,908
And the thing that started- th- the thing that changed the trajectory of neural networks and language is the thing that changed the trajectory of all deep learning

1194
00:57:17,908 --> 00:57:19,748
and that's data and compute.

1195
00:57:19,748 --> 00:57:24,468
So suddenly, you move from small language models which learn a little bit

1196
00:57:24,468 --> 00:57:31,228
and with language models in particular, you can- there's a very clear explanation for why they need to be large to be good

1197
00:57:31,228 --> 00:57:33,288
because they're trying to predict the next word.

1198
00:57:33,288 --> 00:57:34,668
Mm-hmm.

1199
00:57:34,668 --> 00:57:36,928
So we don't- when you don't know anything

1200
00:57:36,928 --> 00:57:41,488
you'll notice very, very broad strokes, surface level patterns

1201
00:57:41,488 --> 00:57:46,528
like sometimes there are characters and there is space between those characters.

1202
00:57:46,528 --> 00:57:51,948
You'll notice this pattern, and you'll notice that sometimes there is a comma and then the next character is a capital letter.

1203
00:57:51,948 --> 00:57:53,668
You'll notice that pattern.

1204
00:57:53,668 --> 00:57:57,188
Eventually, you may start to notice that there are certain words th- occur often.

1205
00:57:57,188 --> 00:57:59,428
You may notice that spellings are a thing.

1206
00:57:59,428 --> 00:58:01,168
You may notice syntax.

1207
00:58:01,168 --> 00:58:03,708
And when you get really good at all these

1208
00:58:03,708 --> 00:58:05,048
you start to notice the semantics.

1209
00:58:05,048 --> 00:58:07,908
You start to notice the facts.

1210
00:58:07,908 --> 00:58:11,468
But for that to happen, the language model needs to be larger.

1211
00:58:11,468 --> 00:58:16,468
So that's- let's linger on that, 'cause that's where you and Noam Chomsky disagree.

1212
00:58:16,468 --> 00:58:21,928
 So you think we're actually taking, uh

1213
00:58:21,928 --> 00:58:25,008
incremental steps, a sort of larger network

1214
00:58:25,008 --> 00:58:29,528
larger compute will be able to get to the semantics

1215
00:58:29,528 --> 00:58:34,028
be able to understand language without what, uh

1216
00:58:34,028 --> 00:58:40,448
Noam likes to sort of think of as a fundamental understandings of the structure of language

1217
00:58:40,448 --> 00:58:45,908
like, uh, imposing your theory of language onto the learning mechanism?

1218
00:58:45,908 --> 00:58:52,627
So you're saying the learning, you can learn from raw data the mechanism that underlies language?

1219
00:58:52,627 --> 00:58:56,788
Well, I thi- I think it's pretty likely

1220
00:58:56,788 --> 00:59:02,217
but I also want to say that I don't really k- know precisely what is- what

1221
00:59:02,217 --> 00:59:08,857
uh, Chomsky means b- when he talks about im- y- you said something about imposing your structure on language.

1222
00:59:08,857 --> 00:59:11,857
I'm not 100% sure what he means, but empirically

1223
00:59:11,857 --> 00:59:14,688
it seems that when you inspect those larger language models

1224
00:59:14,688 --> 00:59:18,548
they exhibit signs of understanding the semantics, whereas the smaller language models do not.

1225
00:59:18,548 --> 00:59:21,948
We've seen that a few years ago when we did work on the sentiment neuron.

1226
00:59:21,948 --> 00:59:28,788
We trained a small, you know, smallish LSTM to predict the next character in Amazon reviews

1227
00:59:28,788 --> 00:59:33,944
and we noticed that when you increase the size of the LSTM from 500 s- LSTM cells to 4

1228
00:59:33,944 --> 00:59:39,948
000 LSTM cells, then one of the neurons starts to represent the sentiment of the article

1229
00:59:39,948 --> 00:59:42,028
oh, sorry, of the review.

1230
00:59:42,028 --> 00:59:42,788
Now, why is that?

1231
00:59:42,788 --> 00:59:45,268
S- sentiment is a pretty semantic attribute.

1232
00:59:45,268 --> 00:59:46,928
It's not a syntactic attribute.

1233
00:59:46,928 --> 00:59:49,458
And for people who might not know, I don't know if that's a standard term

1234
00:59:49,458 --> 00:59:52,038
but sentiment is whether it's a positive or a negative review.

1235
00:59:52,038 --> 00:59:54,308
That's right, like this, is the person happy with something

1236
00:59:54,308 --> 00:59:56,188
or is the person unhappy with something?

1237
00:59:56,188 --> 01:00:01,948
And so here we had very clear evidence that a small neural net does not capture sentiment

1238
01:00:01,948 --> 01:00:03,688
while a large neural net does.

1239
01:00:03,688 --> 01:00:04,837
And why is that?

1240
01:00:04,837 --> 01:00:08,788
Well, our theory is that at some point you run out of syntax to model

1241
01:00:08,788 --> 01:00:11,108
so you start, gotta focus on something else.

1242
01:00:11,108 --> 01:00:15,888
And with size, you quickly run out of syntax to model

1243
01:00:15,888 --> 01:00:18,148
and then you really start to focus on the semantics

1244
01:00:18,148 --> 01:00:19,448
is- would be the idea.

1245
01:00:19,448 --> 01:00:19,908
That's right.

1246
01:00:19,908 --> 01:00:23,868
And so I don't wa- I don't want to imply that our models have complete semantic understanding

1247
01:00:23,868 --> 01:00:29,428
because that's not true, but they definitely are showing signs of semantic understanding

1248
01:00:29,428 --> 01:00:32,768
partial semantic understanding, but the smaller models do not show that

1249
01:00:32,768 --> 01:00:34,548
those signs.

1250
01:00:34,548 --> 01:00:38,148
Can you take a step back and say what is GPT-2

1251
01:00:38,148 --> 01:00:43,828
which is one of the big language models that was the conversation-changer in the past couple of years?

1252
01:00:43,828 --> 01:00:47,970
Yeah, so it's, so, so GPT-2 is a transformer-.

1253
01:00:47,970 --> 01:00:48,202
..

1254
01:00:48,202 --> 01:00:51,692
with one and a half billion parameters that was trained on a

1255
01:00:51,692 --> 01:01:00,252
on about 40 billion tokens of text, which were obtained from web pages that were linked to

1256
01:01:00,252 --> 01:01:02,372
from Reddit articles with more than three up votes.

1257
01:01:02,372 --> 01:01:03,932
And what's a transformer?

1258
01:01:03,932 --> 01:01:09,832
The transformer is the most important advance in neural network architectures in recent history.

1259
01:01:09,832 --> 01:01:11,512
What is attention maybe too?

1260
01:01:11,512 --> 01:01:15,012
'Cause I think that's an interesting idea, not necessarily sort of technically speaking

1261
01:01:15,012 --> 01:01:21,152
but the idea of attention versus maybe what recr- rec- recurrent neural networks represent.

1262
01:01:21,152 --> 01:01:21,462
Yeah.

1263
01:01:21,462 --> 01:01:27,912
So the thing is, the transformer is a combination of multiple ideas simultaneously of which attention is one.

1264
01:01:27,912 --> 01:01:29,412
Do you think attention is the key?

1265
01:01:29,412 --> 01:01:29,832
No.

1266
01:01:29,832 --> 01:01:32,492
It's a key, but it's not the key.

1267
01:01:32,492 --> 01:01:37,702
The transformer is successful because it is the simultaneous combination of multiple ideas.

1268
01:01:37,702 --> 01:01:39,062
And if you were to remove either idea

1269
01:01:39,062 --> 01:01:41,572
it would be much less successful.

1270
01:01:41,572 --> 01:01:43,912
So, the transformer uses a lot of attention

1271
01:01:43,912 --> 01:01:47,422
but attention existed for a few years, so that can't be the main innovation.

1272
01:01:47,422 --> 01:01:55,132
The transformer is designed in such a way that it runs really fast on the GPU

1273
01:01:55,132 --> 01:01:58,152
and that makes a huge amount of difference.

1274
01:01:58,152 --> 01:01:59,412
This is one thing.

1275
01:01:59,412 --> 01:02:02,872
The second thing is a transformer is not recurrent

1276
01:02:02,872 --> 01:02:08,052
and that is really important too because it is more shallow and therefore much easier to optimize.

1277
01:02:08,052 --> 01:02:10,472
So, in other words, it uses attention.

1278
01:02:10,472 --> 01:02:14,352
It is, it is a, a really great fit to the GPU.

1279
01:02:14,352 --> 01:02:17,952
And it is not recurrent, so therefore less deep and easier to optimize.

1280
01:02:17,952 --> 01:02:20,772
And the combination of those factors make it successful.

1281
01:02:20,772 --> 01:02:24,212
So, now it makes, it makes great use of your GPU.

1282
01:02:24,212 --> 01:02:27,552
It allows you to achieve better results for the same amount of compute

1283
01:02:27,552 --> 01:02:31,112
and that's why it's successful.

1284
01:02:31,112 --> 01:02:36,172
Were you surprised how well transformers worked and GPT-2 worked?

1285
01:02:36,172 --> 01:02:37,717
So, you worked on language.

1286
01:02:37,717 --> 01:02:37,892
..

1287
01:02:37,892 --> 01:02:42,292
You've had a lot of great ideas before transformers came about in language

1288
01:02:42,292 --> 01:02:46,192
so you got to see the whole set of revolutions before and after.

1289
01:02:46,192 --> 01:02:47,592
Were you surprised?

1290
01:02:47,592 --> 01:02:48,162
Yeah, a little.

1291
01:02:48,162 --> 01:02:49,252
A little?

1292
01:02:49,252 --> 01:02:50,122
 Yeah.

1293
01:02:50,122 --> 01:02:54,532
I mean, it's hard, it's hard to remember because you adapt really quickly

1294
01:02:54,532 --> 01:02:56,012
but it definitely was surprising.

1295
01:02:56,012 --> 01:02:56,932
It definitely was.

1296
01:02:56,932 --> 01:02:57,713
In fact, I'll.

1297
01:02:57,713 --> 01:02:57,842
..

1298
01:02:57,842 --> 01:02:58,281
You know what?

1299
01:02:58,281 --> 01:03:00,532
I'll, I'll retract my statement.

1300
01:03:00,532 --> 01:03:02,552
It was, it was pretty amazing.

1301
01:03:02,552 --> 01:03:05,212
It was just amazing to see it generate this text

1302
01:03:05,212 --> 01:03:08,792
all this- and you know, you gotta keep in mind that we've seen- at that time

1303
01:03:08,792 --> 01:03:11,992
we've seen all this progress in GANs, in improving th- you know

1304
01:03:11,992 --> 01:03:14,272
the samples produced by GANs were just amazing.

1305
01:03:14,272 --> 01:03:14,772
Mm-hmm.

1306
01:03:14,772 --> 01:03:15,992
You have these realistic faces.

1307
01:03:15,992 --> 01:03:18,052
But text hasn't really moved that much.

1308
01:03:18,052 --> 01:03:20,601
And suddenly, we moved from, you know

1309
01:03:20,601 --> 01:03:23,922
whatever GANs were in 2015 to the best

1310
01:03:23,922 --> 01:03:26,012
most amazing GANs in one step.

1311
01:03:26,012 --> 01:03:26,282
Right.

1312
01:03:26,282 --> 01:03:27,592
And that was really stunning.

1313
01:03:27,592 --> 01:03:30,412
Even though theory predicted, yeah, you train a big language model

1314
01:03:30,412 --> 01:03:31,912
of course you should get this.

1315
01:03:31,912 --> 01:03:33,192
But then to see it with your own eyes

1316
01:03:33,192 --> 01:03:34,952
it's something else.

1317
01:03:34,952 --> 01:03:38,132
And yet, we adapt really quickly, and now there's

1318
01:03:38,132 --> 01:03:40,926
uh, sort of.

1319
01:03:40,926 --> 01:03:41,642
..

1320
01:03:41,642 --> 01:03:49,382
Some cognitive scientists write articles saying that GPT-2 models don't truly understand language.

1321
01:03:49,382 --> 01:03:55,752
So, we adapt quickly to how amazing the fact that they're able to model the language so well is.

1322
01:03:55,752 --> 01:03:58,812
So, what do you think is the bar?

1323
01:03:58,812 --> 01:03:59,452
For what?

1324
01:03:59,452 --> 01:04:02,076
For impressing us that it.

1325
01:04:02,076 --> 01:04:02,492
..

1326
01:04:02,492 --> 01:04:03,752
I don't know.

1327
01:04:03,752 --> 01:04:06,172
Do you think that bar will continuously be moved?

1328
01:04:06,172 --> 01:04:06,932
Definitely.

1329
01:04:06,932 --> 01:04:11,232
 I, I, I think when you start to see really dramatic economic impact

1330
01:04:11,232 --> 01:04:12,012
that's when.

1331
01:04:12,012 --> 01:04:13,041
I think that's in some sense- I see.

1332
01:04:13,041 --> 01:04:13,041
.

1333
01:04:13,041 --> 01:04:13,041
.

1334
01:04:13,041 --> 01:04:16,972
the next barrier, because right now, if you think about the work in AI

1335
01:04:16,972 --> 01:04:18,952
it's really confusing.

1336
01:04:18,952 --> 01:04:22,592
It's really hard to know what to make of all these advances.

1337
01:04:22,592 --> 01:04:25,532
It's kind of like, okay, you got an advance

1338
01:04:25,532 --> 01:04:26,862
and now you can do, uh, more things.

1339
01:04:26,862 --> 01:04:30,452
And you got another improvement, and you got another cool demo.

1340
01:04:30,452 --> 01:04:36,192
At some point, I think people who are outside of AI

1341
01:04:36,192 --> 01:04:38,761
they can no longer distinguish this progress anymore.

1342
01:04:38,761 --> 01:04:45,362
So, we were talking offline about translating Russian to English and how there's a lot of brilliant work in Russian that the

1343
01:04:45,362 --> 01:04:46,462
the rest of the world doesn't know about.

1344
01:04:46,462 --> 01:04:47,582
That's true for Chinese.

1345
01:04:47,582 --> 01:04:49,052
That's true for a lot of, uh

1346
01:04:49,052 --> 01:04:52,252
for, for a lot of scientists and just artistic work in general.

1347
01:04:52,252 --> 01:04:57,122
Do you think translation is the place where we're going to see sort of economic big impact?

1348
01:04:57,122 --> 01:04:57,922
I, I don't know.

1349
01:04:57,922 --> 01:05:00,452
I, I think, I think there is a huge number of applic- I mean

1350
01:05:00,452 --> 01:05:04,951
so first of all, I would wanna s- I wanna point out that translation already today- That's true.

1351
01:05:04,951 --> 01:05:04,961
.

1352
01:05:04,961 --> 01:05:04,961
..

1353
01:05:04,961 --> 01:05:05,571
is huge.

1354
01:05:05,571 --> 01:05:08,972
I think billions of people interact with, uh

1355
01:05:08,972 --> 01:05:11,132
big chunks of the internet primarily through translation.

1356
01:05:11,132 --> 01:05:14,412
So, translation is already huge, and it's hu- hugely

1357
01:05:14,412 --> 01:05:16,012
hugely positive too.

1358
01:05:16,012 --> 01:05:20,352
I think self-driving is going to be tr- hugely impactful.

1359
01:05:20,352 --> 01:05:24,492
And that's, you know, it's, it's unknown exactly when it happens.

1360
01:05:24,492 --> 01:05:27,072
But again, I would, I would not bet against deep learning

1361
01:05:27,072 --> 01:05:29,272
so I- So, that's deep learning in general.

1362
01:05:29,272 --> 01:05:31,952
But you, you th- you think- Deep learning for self-driving.

1363
01:05:31,952 --> 01:05:32,192
Yes.

1364
01:05:32,192 --> 01:05:35,342
Deep learning for self-driving, but I, I was talking about sort of language models.

1365
01:05:35,342 --> 01:05:35,732
I see.

1366
01:05:35,732 --> 01:05:36,892
Ju- just to ch- just to check- I veered

1367
01:05:36,892 --> 01:05:38,192
I veered off a little bit.

1368
01:05:38,192 --> 01:05:41,172
Just to check, you're not seeing a connection between driving and language?

1369
01:05:41,172 --> 01:05:41,772
No, no.

1370
01:05:41,772 --> 01:05:42,372
Okay.

1371
01:05:42,372 --> 01:05:44,012
Or rather, they both use neural nets.

1372
01:05:44,012 --> 01:05:45,572
That would be a poetic connection.

1373
01:05:45,572 --> 01:05:47,172
I think there might be some.

1374
01:05:47,172 --> 01:05:49,672
And like you said, there might be some kind of unification towards

1375
01:05:49,672 --> 01:05:58,212
uh, a kind of multitask transformers that can take on both language and vision tasks

1376
01:05:58,212 --> 01:06:01,452
and be an interesting unification.

1377
01:06:01,452 --> 01:06:02,002
Uh, let's see.

1378
01:06:02,002 --> 01:06:04,092
What can I ask about GPT-2 more?

1379
01:06:04,092 --> 01:06:04,434
Um.

1380
01:06:04,434 --> 01:06:04,582
..

1381
01:06:04,582 --> 01:06:05,612
It's simple.

1382
01:06:05,612 --> 01:06:06,972
There's not much to ask.

1383
01:06:06,972 --> 01:06:08,542
 It's- So- You take a, you take a

1384
01:06:08,542 --> 01:06:09,902
you take a transformer, you make it bigger

1385
01:06:09,902 --> 01:06:12,722
you give it more data, and suddenly it does all those amazing things.

1386
01:06:12,722 --> 01:06:14,912
Yeah, one of the beautiful things is that GPT

1387
01:06:14,912 --> 01:06:19,172
the transformers are fundamentally simple to explain, to train.

1388
01:06:19,172 --> 01:06:27,052
Do you think bigger will continue to show better results in language?

1389
01:06:27,052 --> 01:06:27,692
Probably.

1390
01:06:27,692 --> 01:06:30,532
Sort of like, what are the next steps with GPT-2

1391
01:06:30,532 --> 01:06:31,452
do you think?

1392
01:06:31,452 --> 01:06:33,202
I mean, for, I think for, for

1393
01:06:33,202 --> 01:06:37,652
for sure seeing what, uh, larger versions can do is one direction.

1394
01:06:37,652 --> 01:06:41,232
Also, I mean, there are, there are many questions.

1395
01:06:41,232 --> 01:06:44,032
There's one question which I'm curious about, and that's the following.

1396
01:06:44,032 --> 01:06:46,952
So, right now, GPT-2 So we feed it all this data from the internet

1397
01:06:46,952 --> 01:06:51,932
which means that it needs to memorize all those random facts about everything in the internet.

1398
01:06:51,932 --> 01:06:54,357
And it would be nice if.

1399
01:06:54,357 --> 01:06:54,800
..

1400
01:06:54,800 --> 01:06:55,244
..

1401
01:06:55,244 --> 01:07:01,806
. the model could somehow use its own intelligence to decide what data it wants to sta- a- a- accept

1402
01:07:01,806 --> 01:07:03,596
and what data it wants to reject.

1403
01:07:03,596 --> 01:07:04,356
Just like people.

1404
01:07:04,356 --> 01:07:07,185
People don't learn all data indiscriminately.

1405
01:07:07,185 --> 01:07:09,915
We are super selective about what we learn.

1406
01:07:09,915 --> 01:07:11,576
And I think this kind of active learning

1407
01:07:11,576 --> 01:07:14,296
I think, would be very nice to have.

1408
01:07:14,296 --> 01:07:14,756
Yeah.

1409
01:07:14,756 --> 01:07:16,776
T- listen, I love active learning.

1410
01:07:16,776 --> 01:07:20,999
So, let,  let me ask, does the selection of data.

1411
01:07:20,999 --> 01:07:21,185
..

1412
01:07:21,185 --> 01:07:23,036
Can you just elaborate that a little bit more?

1413
01:07:23,036 --> 01:07:26,236
Do you think the selection of data is

1414
01:07:26,236 --> 01:07:27,477
um.

1415
01:07:27,477 --> 01:07:28,225
..

1416
01:07:28,225 --> 01:07:33,856
Like, I- I have this kind of sense that the optimization of how you select data

1417
01:07:33,856 --> 01:07:39,756
so the active learning process, is going to be a place for a lot of breakthroughs

1418
01:07:39,756 --> 01:07:42,176
even in the near future.

1419
01:07:42,176 --> 01:07:45,116
Because there just hasn't been many breakthroughs there that are public.

1420
01:07:45,116 --> 01:07:49,296
I feel like there might be private breakthroughs that companies keep to themselves

1421
01:07:49,296 --> 01:07:52,935
'cause it's a fundamental problem that has to be solved if you wanna solve self-driving

1422
01:07:52,935 --> 01:07:54,915
if you wanna solve a particular task.

1423
01:07:54,915 --> 01:07:55,916
But d- do you.

1424
01:07:55,916 --> 01:07:55,966
..

1425
01:07:55,966 --> 01:07:57,856
Wha- what do you think about this space in general?

1426
01:07:57,856 --> 01:07:58,176
Yeah.

1427
01:07:58,176 --> 01:08:00,216
So, I think that for something like active learning

1428
01:08:00,216 --> 01:08:02,475
or in fact, for any kind of capability

1429
01:08:02,475 --> 01:08:05,836
like active learning, the thing that it really needs is a problem.

1430
01:08:05,836 --> 01:08:07,875
It needs a problem that requires it.

1431
01:08:07,875 --> 01:08:12,996
It's very hard to do research about a capability if you don't have a task

1432
01:08:12,996 --> 01:08:14,516
because then what's going to happen is it

1433
01:08:14,516 --> 01:08:16,776
you will come up with an artificial task

1434
01:08:16,776 --> 01:08:20,675
get good results, but not really convince anyone.

1435
01:08:20,675 --> 01:08:21,106
Right.

1436
01:08:21,106 --> 01:08:27,555
Like, we're- we're now past the stage where getting a result on MNIST

1437
01:08:27,555 --> 01:08:30,876
some clever formulation of MNIST, will- will convince people.

1438
01:08:30,876 --> 01:08:31,256
That's right.

1439
01:08:31,256 --> 01:08:37,675
In fact, you could quite easily come up with a simple active learning scheme on MNIST and get a 10X speedup.

1440
01:08:37,675 --> 01:08:39,696
But then, so what?

1441
01:08:39,696 --> 01:08:42,296
And I think that with active learning, there needs

1442
01:08:42,296 --> 01:08:42,857
there need.

1443
01:08:42,857 --> 01:08:43,026
..

1444
01:08:43,026 --> 01:08:46,049
Uh, active learning will naturally arise as there are.

1445
01:08:46,049 --> 01:08:46,106
..

1446
01:08:46,106 --> 01:08:48,456
as problems that require it pop up.

1447
01:08:48,456 --> 01:08:50,171
That's how I would.

1448
01:08:50,171 --> 01:08:50,256
..

1449
01:08:50,256 --> 01:08:52,876
That's my- my take on it.

1450
01:08:52,876 --> 01:08:56,135
There's another interesting thing that OpenAI has brought up with GPT-2

1451
01:08:56,135 --> 01:09:01,368
which is when you create a- a powerful artificial intelligence system.

1452
01:09:01,368 --> 01:09:01,446
..

1453
01:09:01,446 --> 01:09:04,553
And it was unclear what kind of detrimental.

1454
01:09:04,553 --> 01:09:04,666
..

1455
01:09:04,666 --> 01:09:09,595
once you release GPT-2, what kind of detrimental effect it'll have.

1456
01:09:09,595 --> 01:09:14,095
Because if you have an- a model that can generate pretty realistic text

1457
01:09:14,095 --> 01:09:16,456
you can start to imagine that, you know

1458
01:09:16,456 --> 01:09:16,975
on the.

1459
01:09:16,975 --> 01:09:17,126
..

1460
01:09:17,126 --> 01:09:21,746
it would be used by bots in some- some way that we can't even imagine.

1461
01:09:21,746 --> 01:09:24,106
So, like, there's this nervousness about what it's possible to do.

1462
01:09:24,106 --> 01:09:28,156
So, you- you did a really kind of brave and I think profound thing

1463
01:09:28,156 --> 01:09:29,916
which is start a conversation about this.

1464
01:09:29,916 --> 01:09:36,156
Like, how do we release powerful artificial intelligence models to the public

1465
01:09:36,156 --> 01:09:37,576
if we do at all?

1466
01:09:37,576 --> 01:09:46,095
How do we privately discuss with other even competitors about how we manage the use of the systems and so on?

1467
01:09:46,095 --> 01:09:49,576
So, from that, this whole experience, you released a report on it.

1468
01:09:49,576 --> 01:09:55,296
But, in general, are there any insights that you've gathered from just thinking about this

1469
01:09:55,296 --> 01:09:57,715
about how you release models like this?

1470
01:09:57,715 --> 01:09:59,075
I mean, I think that.

1471
01:09:59,075 --> 01:09:59,236
..

1472
01:09:59,236 --> 01:10:04,476
My take on this is that the field of AI has been in a state of childhood

1473
01:10:04,476 --> 01:10:09,656
and now it's exiting that state, and it's entering a state of maturity.

1474
01:10:09,656 --> 01:10:14,156
What that means is that AI is very successful and also very impactful.

1475
01:10:14,156 --> 01:10:18,156
And its impact is not only large, but it's also growing.

1476
01:10:18,156 --> 01:10:27,516
And so, for that reason, it seems wise to start thinking about the impact of our systems before releasing them maybe a little bit too soon

1477
01:10:27,516 --> 01:10:29,715
rather than a little bit too late.

1478
01:10:29,715 --> 01:10:32,916
And with the case of GPT-2, like I mentioned earlier

1479
01:10:32,916 --> 01:10:35,196
the results really were stunning.

1480
01:10:35,196 --> 01:10:37,236
And it seemed plausible.

1481
01:10:37,236 --> 01:10:38,715
It didn't seem certain.

1482
01:10:38,715 --> 01:10:45,276
It seemed plausible that something like GPT-2 could easily be used to reduce the cost of disinformation.

1483
01:10:45,276 --> 01:10:48,706
And so, there was a question of

1484
01:10:48,706 --> 01:10:50,065
what's the best way to release it?

1485
01:10:50,065 --> 01:10:51,776
And a staged release seemed logical.

1486
01:10:51,776 --> 01:10:56,955
A small model was released, and there was time to see the.

1487
01:10:56,955 --> 01:10:57,505
..

1488
01:10:57,505 --> 01:10:59,736
Many people used these models in lots of cool ways.

1489
01:10:59,736 --> 01:11:02,076
There've been lots of really cool applications.

1490
01:11:02,076 --> 01:11:06,196
There haven't been any negative application that we know of

1491
01:11:06,196 --> 01:11:07,616
and so eventually it was released.

1492
01:11:07,616 --> 01:11:09,996
But also other people replicated similar models.

1493
01:11:09,996 --> 01:11:12,578
That's an interesting question though, "that we know of.

1494
01:11:12,578 --> 01:11:16,396
" So, in your view, staged release is

1495
01:11:16,396 --> 01:11:21,584
um, at least part of the answer to the question of how do we.

1496
01:11:21,584 --> 01:11:22,496
..

1497
01:11:22,496 --> 01:11:23,515
Uh, how.

1498
01:11:23,515 --> 01:11:23,705
..

1499
01:11:23,705 --> 01:11:25,856
What do we do once we create a system like this?

1500
01:11:25,856 --> 01:11:27,255
It's part of the answer, yes.

1501
01:11:27,255 --> 01:11:30,016
Uh, uh, is there any other insights?

1502
01:11:30,016 --> 01:11:32,436
Like, say you don't want to release the model at all

1503
01:11:32,436 --> 01:11:35,866
because it's useful to you for whatever the business is.

1504
01:11:35,866 --> 01:11:39,156
Well, there are plen- plen- ple- plenty of people don't release models already.

1505
01:11:39,156 --> 01:11:39,516
Right.

1506
01:11:39,516 --> 01:11:39,996
Of course.

1507
01:11:39,996 --> 01:11:47,736
But is there some moral, ethical responsibility when you have a very powerful model to sort of communicate?

1508
01:11:47,736 --> 01:11:51,466
Like, just as you said, when you had GPT-2

1509
01:11:51,466 --> 01:11:54,196
it was unclear how much it could be used for misinformation.

1510
01:11:54,196 --> 01:11:56,276
It's an open question.

1511
01:11:56,276 --> 01:12:01,635
And getting an answer to that might require that you talk to other really smart people that are outside of

1512
01:12:01,635 --> 01:12:04,836
uh, uh, outside of your particular group.

1513
01:12:04,836 --> 01:12:06,005
I- uh, have you.

1514
01:12:06,005 --> 01:12:06,255
..

1515
01:12:06,255 --> 01:12:13,456
Please tell me there is some optimistic pathway for people across the world to collaborate on these kinds of cases.

1516
01:12:13,456 --> 01:12:19,616
Or, is it still really difficult from- from one company to talk to another company?

1517
01:12:19,616 --> 01:12:21,316
So, it's definitely possible.

1518
01:12:21,316 --> 01:12:28,316
It's definitely possible to discuss these kind of models with colleagues elsewhere

1519
01:12:28,316 --> 01:12:30,736
and to get the- get their take on what's

1520
01:12:30,736 --> 01:12:32,236
uh, on what to do.

1521
01:12:32,236 --> 01:12:33,716
How hard is it though?

1522
01:12:33,716 --> 01:12:35,765
I mean.

1523
01:12:35,765 --> 01:12:36,505
..

1524
01:12:36,505 --> 01:12:38,076
Do you see that happening?

1525
01:12:38,076 --> 01:12:43,296
I think that's- that's a place where it's important to gradually build trust between companies.

1526
01:12:43,296 --> 01:12:50,856
Because ultimately, all the AI developers are building technology which is be- going to be increasingly more powerful.

1527
01:12:50,856 --> 01:12:53,783
And so, it's.

1528
01:12:53,783 --> 01:12:54,755
..

1529
01:12:54,755 --> 01:12:58,131
The way to think about it is that ultimately we're all in it together.

1530
01:12:58,131 --> 01:13:02,599
Yeah, it's, uh, I tend to believe in the

1531
01:13:02,599 --> 01:13:04,440
uh, th- the better angels of our nature.

1532
01:13:04,440 --> 01:13:12,879
But I do hope that, um, that when you build a really powerful AI system in a particular domain

1533
01:13:12,879 --> 01:13:17,030
that you also think about the potential negative consequences of

1534
01:13:17,030 --> 01:13:17,642
um.

1535
01:13:17,642 --> 01:13:17,990
..

1536
01:13:17,990 --> 01:13:19,320
Yeah.

1537
01:13:19,320 --> 01:13:30,459
 It's an interesting and scary possibility that there will be a race for AI dev- AI development that would push people to close that development

1538
01:13:30,459 --> 01:13:33,559
and not share ideas with others.

1539
01:13:33,559 --> 01:13:34,639
I don't love this.

1540
01:13:34,639 --> 01:13:36,679
I've been in acad- a pure academic for 10 years.

1541
01:13:36,679 --> 01:13:38,879
I really like sharing ideas, and it's fun.

1542
01:13:38,879 --> 01:13:40,780
It's exciting.

1543
01:13:40,780 --> 01:13:42,698
What do you think it takes to.

1544
01:13:42,698 --> 01:13:42,830
..

1545
01:13:42,830 --> 01:13:44,540
Let's talk about AGI a little bit.

1546
01:13:44,540 --> 01:13:48,000
Wh- what do you think it takes to build a system of human-level intelligence?

1547
01:13:48,000 --> 01:13:49,610
We talked about reasoning.

1548
01:13:49,610 --> 01:13:51,360
We talked about long-term memory.

1549
01:13:51,360 --> 01:13:52,980
But in general, what does it take

1550
01:13:52,980 --> 01:13:53,799
do you think?

1551
01:13:53,799 --> 01:14:02,919
Well, I can't be sure, but I think that deep learning plus maybe another small idea.

1552
01:14:02,919 --> 01:14:05,679
Do you think self-play will be involved?

1553
01:14:05,679 --> 01:14:09,099
Sort of like, you've spoken about the powerful mechanism of self-play

1554
01:14:09,099 --> 01:14:13,360
where systems learn by sort of, uh

1555
01:14:13,360 --> 01:14:20,280
exploring the world in a competitive setting against other entities that are similarly skilled as them

1556
01:14:20,280 --> 01:14:23,099
and so incrementally improve in this way.

1557
01:14:23,099 --> 01:14:26,679
Do you think self-play will be a component of building an AGI system?

1558
01:14:26,679 --> 01:14:28,200
Yeah.

1559
01:14:28,200 --> 01:14:30,379
So, what I would say, to build AGI

1560
01:14:30,379 --> 01:14:35,099
I think is going to be deep learning plus some ideas.

1561
01:14:35,099 --> 01:14:37,639
And I think self-play will be one of those ideas.

1562
01:14:37,639 --> 01:14:40,597
I think that that is a very.

1563
01:14:40,597 --> 01:14:40,940
..

1564
01:14:40,940 --> 01:14:49,700
Self-play has this amazing property that it can surprise us in truly novel ways.

1565
01:14:49,700 --> 01:14:52,693
For example, like, we.

1566
01:14:52,693 --> 01:14:52,849
..

1567
01:14:52,849 --> 01:14:56,599
I mean, pretty much every self-play system

1568
01:14:56,599 --> 01:14:58,199
both our Dota bot.

1569
01:14:58,199 --> 01:14:58,509
..

1570
01:14:58,509 --> 01:15:02,700
I don't know if em- eh- OpenAI had a release about multi-agent

1571
01:15:02,700 --> 01:15:05,639
where you had two little agents who were playing hide and seek.

1572
01:15:05,639 --> 01:15:06,190
Mm-hmm.

1573
01:15:06,190 --> 01:15:08,299
And, of course, also AlphaZero.

1574
01:15:08,299 --> 01:15:11,080
They were all produced surprising behaviors.

1575
01:15:11,080 --> 01:15:13,308
They all produced behaviors that we didn't expect.

1576
01:15:13,308 --> 01:15:15,980
They are creative solutions to problems.

1577
01:15:15,980 --> 01:15:22,259
And that seems like an important part of AGI that our systems don't exhibit routinely right now.

1578
01:15:22,259 --> 01:15:24,980
And so, that's why I like this area

1579
01:15:24,980 --> 01:15:27,620
I like this direction, because of its ability to surprise us.

1580
01:15:27,620 --> 01:15:28,468
To surprise us.

1581
01:15:28,468 --> 01:15:31,259
And an AG- And what- AGI system would surprise us fundamentally.

1582
01:15:31,259 --> 01:15:31,709
Yes.

1583
01:15:31,709 --> 01:15:34,580
But- and to be precise, not just a r- not just a random surprise

1584
01:15:34,580 --> 01:15:39,219
but to find a surprising solution to a problem that's also useful.

1585
01:15:39,219 --> 01:15:39,509
Right.

1586
01:15:39,509 --> 01:15:45,700
Now, a lot of the self-play mechanisms have been used in the game context

1587
01:15:45,700 --> 01:15:49,049
or at least in the simulation context.

1588
01:15:49,049 --> 01:15:56,759
W- how much, how much do- how far along the path to AGI do you think will be done in simulation?

1589
01:15:56,759 --> 01:16:01,360
How much faith, promise, do you have in simulation

1590
01:16:01,360 --> 01:16:05,379
versus having to have a system that operates in the real world

1591
01:16:05,379 --> 01:16:13,299
wheth- whether it's the real world of digital real world data or real world like actual physical world with robotics?

1592
01:16:13,299 --> 01:16:15,089
I don't think it's an either/or.

1593
01:16:15,089 --> 01:16:17,559
I think simulation is a tool, and it helps.

1594
01:16:17,559 --> 01:16:21,160
It has certain strengths and certain weaknesses, and we should use it.

1595
01:16:21,160 --> 01:16:22,599
Yeah, but- Okay.

1596
01:16:22,599 --> 01:16:22,749
.

1597
01:16:22,749 --> 01:16:22,769
..

1598
01:16:22,769 --> 01:16:24,559
uh, I understand that.

1599
01:16:24,559 --> 01:16:29,879
That's, um, that's true.

1600
01:16:29,879 --> 01:16:36,339
But one of the criticisms of self-play, one of the criticisms in reinforcement learning is one of the

1601
01:16:36,339 --> 01:16:37,192
the.

1602
01:16:37,192 --> 01:16:37,589
..

1603
01:16:37,589 --> 01:16:41,959
Its current power, its current results, while amazing

1604
01:16:41,959 --> 01:16:46,400
have been demonstrated in a simulated environments or very constrained physical environments.

1605
01:16:46,400 --> 01:16:48,240
Do you think it's possible to escape them

1606
01:16:48,240 --> 01:16:53,440
escape the simulated environments and be able to learn in non-simulated environments?

1607
01:16:53,440 --> 01:17:06,820
Or do you think it's possible to also just simulate in the photorealistic and physics realistic way the real world in a way that we can solve real problems with self-play in simulation?

1608
01:17:06,820 --> 01:17:11,759
So, I think that sim- transfer from simulation to the real world is definitely possible

1609
01:17:11,759 --> 01:17:14,169
and is- and has been exhibited many times in

1610
01:17:14,169 --> 01:17:15,400
by many different groups.

1611
01:17:15,400 --> 01:17:18,740
It's been especially successful in vision.

1612
01:17:18,740 --> 01:17:29,910
Also, OpenAI in the summer has demonstrated a robot hand which was trained entirely in simulation in a certain way that allowed for sim-to-real transfer to occur.

1613
01:17:29,910 --> 01:17:31,419
Is this, uh, for the Rubik's Cube?

1614
01:17:31,419 --> 01:17:32,719
Yeah, that's right.

1615
01:17:32,719 --> 01:17:34,700
And so that- I wasn't aware that was trained in simulation.

1616
01:17:34,700 --> 01:17:37,080
It was trained in simulation entirely.

1617
01:17:37,080 --> 01:17:37,620
R- really?

1618
01:17:37,620 --> 01:17:39,459
So, it wasn't in the phys- the

1619
01:17:39,459 --> 01:17:41,059
the hand wasn't trained?

1620
01:17:41,059 --> 01:17:41,839
No.

1621
01:17:41,839 --> 01:17:44,900
100% of the training was done in simulation.

1622
01:17:44,900 --> 01:17:49,059
And the policy that was learned in simulation was trained to be very adaptive

1623
01:17:49,059 --> 01:17:52,940
so adaptive that when you transfer it, it could very quickly adapt to the physical

1624
01:17:52,940 --> 01:17:54,000
to the physical world.

1625
01:17:54,000 --> 01:17:58,919
So, the kind of perturbations with the giraffe or whatever the heck it was

1626
01:17:58,919 --> 01:17:59,624
those weren't.

1627
01:17:59,624 --> 01:17:59,730
..

1628
01:17:59,730 --> 01:18:01,919
Were those part of the sim- simulation?

1629
01:18:01,919 --> 01:18:04,031
Well, the simulation was generally.

1630
01:18:04,031 --> 01:18:04,230
..

1631
01:18:04,230 --> 01:18:08,139
So, the s- the simulation was trained to be robust to many different things

1632
01:18:08,139 --> 01:18:10,660
but not the kind of perturbations we've had in the video.

1633
01:18:10,660 --> 01:18:11,429
So- I see.

1634
01:18:11,429 --> 01:18:11,429
.

1635
01:18:11,429 --> 01:18:11,429
.

1636
01:18:11,429 --> 01:18:12,679
it's never been trained with a glove.

1637
01:18:12,679 --> 01:18:14,730
It's never been trained with a, uh

1638
01:18:14,730 --> 01:18:17,099
uh, stuffed giraffe.

1639
01:18:17,099 --> 01:18:19,339
So, in theory, these are novel perturbations?

1640
01:18:19,339 --> 01:18:19,740
Correct.

1641
01:18:19,740 --> 01:18:21,809
It's not in theory, in practice, in pr- Uh

1642
01:18:21,809 --> 01:18:23,759
that those are novel perturbations?

1643
01:18:23,759 --> 01:18:24,548
Well, that's.

1644
01:18:24,548 --> 01:18:24,660
..

1645
01:18:24,660 --> 01:18:25,860
Okay.

1646
01:18:25,860 --> 01:18:26,389
Yeah.

1647
01:18:26,389 --> 01:18:31,320
That's a clean, small scale, but clean example of a transfer from the simulated world to the

1648
01:18:31,320 --> 01:18:32,160
to the physical world.

1649
01:18:32,160 --> 01:18:38,240
Yeah, and I will also say that I expect the transfer capabilities of deep learning to increase in general.

1650
01:18:38,240 --> 01:18:42,839
And the better the transfer capabilities are, the more useful simulation will become.

1651
01:18:42,839 --> 01:18:45,138
Because then you could take.

1652
01:18:45,138 --> 01:18:45,290
..

1653
01:18:45,290 --> 01:18:50,339
You could experience something in simulation, and then learn a moral of the story

1654
01:18:50,339 --> 01:18:52,580
which you could then carry with you to the real world.

1655
01:18:52,580 --> 01:18:53,549
Right.

1656
01:18:53,549 --> 01:18:56,349
As humans do all the time when they play computer games.

1657
01:18:56,349 --> 01:19:01,339
So, let me ask sort of a embodied question

1658
01:19:01,339 --> 01:19:03,440
staying on AGI for a sec.

1659
01:19:03,440 --> 01:19:07,384
Uh, do you think, uh, AGI system would need to have a body?

1660
01:19:07,384 --> 01:19:07,634
..

1661
01:19:07,634 --> 01:19:12,160
. we need to have some of those human elements of self-awareness

1662
01:19:12,160 --> 01:19:15,120
consciousness, sort of f- fear of mortality

1663
01:19:15,120 --> 01:19:20,360
sort of self-preservation in the physical space, which comes with having a body?

1664
01:19:20,360 --> 01:19:22,470
I think having a body will be useful.

1665
01:19:22,470 --> 01:19:24,390
I don't think it's necessary.

1666
01:19:24,390 --> 01:19:25,890
But I think it's very useful to have a body

1667
01:19:25,890 --> 01:19:28,840
for sure, because you can learn a whole new.

1668
01:19:28,840 --> 01:19:28,990
..

1669
01:19:28,990 --> 01:19:32,559
You- you can learn things which cannot be learned without a body.

1670
01:19:32,559 --> 01:19:35,440
But, at the same time, I think that you can co- if you don't have a body

1671
01:19:35,440 --> 01:19:38,620
you could compensate for it and still succeed.

1672
01:19:38,620 --> 01:19:39,400
You think so?

1673
01:19:39,400 --> 01:19:39,920
Yes.

1674
01:19:39,920 --> 01:19:40,880
Well, there is evidence for this.

1675
01:19:40,880 --> 01:19:44,290
For example, there are many people who were born deaf and blind

1676
01:19:44,290 --> 01:19:48,240
and they were able to compensate for the lack of modalities.

1677
01:19:48,240 --> 01:19:51,600
I'm thinking about Hel- Helen Keller specifically.

1678
01:19:51,600 --> 01:19:54,820
So, even if you're not able to physically interact with the world

1679
01:19:54,820 --> 01:19:56,488
and if you're not able to.

1680
01:19:56,488 --> 01:19:56,880
..

1681
01:19:56,880 --> 01:19:59,112
I mean, I actually was getting at.

1682
01:19:59,112 --> 01:19:59,680
..

1683
01:19:59,680 --> 01:20:02,587
Maybe let me ask on the more particular.

1684
01:20:02,587 --> 01:20:02,669
..

1685
01:20:02,669 --> 01:20:05,370
I'm not sure if it's connected to having a body or not

1686
01:20:05,370 --> 01:20:11,280
but, uh, the idea of consciousness, and a more constrained version of that is self-awareness.

1687
01:20:11,280 --> 01:20:15,640
Do you think an AGI system should have consciousness?

1688
01:20:15,640 --> 01:20:16,312
It's what.

1689
01:20:16,312 --> 01:20:16,390
..

1690
01:20:16,390 --> 01:20:19,519
We can't define con- whatever the heck you think consciousness is.

1691
01:20:19,519 --> 01:20:20,130
Yeah.

1692
01:20:20,130 --> 01:20:24,400
Hard question to answer, given how hard it is to define it.

1693
01:20:24,400 --> 01:20:26,510
Do you think it's useful to think about?

1694
01:20:26,510 --> 01:20:28,420
I mean, it's- it's- it's definitely interesting.

1695
01:20:28,420 --> 01:20:29,900
It's fascinating.

1696
01:20:29,900 --> 01:20:33,960
I think it's definitely possible that our systems will be conscious.

1697
01:20:33,960 --> 01:20:36,250
Do you think that's an emergent thing that just comes from.

1698
01:20:36,250 --> 01:20:36,470
..

1699
01:20:36,470 --> 01:20:40,880
Do you think consciousness could emerge from the representation that's stored within your networks?

1700
01:20:40,880 --> 01:20:44,910
So, like, that it naturally just emerges when you become more and more.

1701
01:20:44,910 --> 01:20:45,170
..

1702
01:20:45,170 --> 01:20:47,180
You're able to represent more and more of the world?

1703
01:20:47,180 --> 01:20:48,780
Well, let's say, I'd make the following argument

1704
01:20:48,780 --> 01:20:53,860
which is, humans are conscious.

1705
01:20:53,860 --> 01:20:59,580
And if you believe that artificial neural nets are sufficiently similar to the brain

1706
01:20:59,580 --> 01:21:03,309
then there- there should at least exist artificial neural nets who should be conscious

1707
01:21:03,309 --> 01:21:04,320
too.

1708
01:21:04,320 --> 01:21:06,640
 You're leaning on that existence proof pretty heavily.

1709
01:21:06,640 --> 01:21:07,019
Okay.

1710
01:21:07,019 --> 01:21:09,886
It- it- so- But it's- it's- So- so- .

1711
01:21:09,886 --> 01:21:10,120
..

1712
01:21:10,120 --> 01:21:12,160
that- that- that's the best answer I can give.

1713
01:21:12,160 --> 01:21:13,380
No, I- I know.

1714
01:21:13,380 --> 01:21:14,580
I know.

1715
01:21:14,580 --> 01:21:15,480
I know.

1716
01:21:15,480 --> 01:21:20,452
Um, there's still an open question if there's not some magic in the brain that we're not.

1717
01:21:20,452 --> 01:21:20,849
..

1718
01:21:20,849 --> 01:21:23,660
I mean, I don't mean a non-materialistic magic

1719
01:21:23,660 --> 01:21:29,960
but that, um- that the brain might be a lot more complicated and interesting than we give it credit for.

1720
01:21:29,960 --> 01:21:32,540
If that's the case, then it should show up

1721
01:21:32,540 --> 01:21:34,490
and at some point- At some point.

1722
01:21:34,490 --> 01:21:34,490
.

1723
01:21:34,490 --> 01:21:34,490
.

1724
01:21:34,490 --> 01:21:36,600
we will- we will find out if we can't continue to make progress.

1725
01:21:36,600 --> 01:21:38,820
It just- But I think- I think it's unlikely.

1726
01:21:38,820 --> 01:21:44,620
So, we talk about consciousness, but let me talk about another poorly defined concept of intelligence.

1727
01:21:44,620 --> 01:21:46,860
Again, we've talked about reasoning.

1728
01:21:46,860 --> 01:21:48,140
We've talked about memory.

1729
01:21:48,140 --> 01:21:51,700
What do you think is a good test of intelligence for you?

1730
01:21:51,700 --> 01:21:58,620
Are you impressed by the test that Alan Turing formulated with the imitation game of na- with natural language?

1731
01:21:58,620 --> 01:22:06,400
Is there something in your mind that you would be deeply impressed by if a system was able to do?

1732
01:22:06,400 --> 01:22:08,019
I mean, lots of things.

1733
01:22:08,019 --> 01:22:12,540
There's certain- there's a certain frontier- there is a certain frontier of capabilities today.

1734
01:22:12,540 --> 01:22:13,309
Yeah.

1735
01:22:13,309 --> 01:22:16,940
And there exists things outside of that frontier

1736
01:22:16,940 --> 01:22:18,860
and I would be impressed by any such thing.

1737
01:22:18,860 --> 01:22:26,140
Like, for example, I would be impressed by a deep learning system which solves a very pedestrian

1738
01:22:26,140 --> 01:22:30,720
you know, pedestrian task, like machine translation or computer vision task or something

1739
01:22:30,720 --> 01:22:37,300
which never makes mistake a human would make under any circumstances.

1740
01:22:37,300 --> 01:22:42,780
I think that is something which have not yet been demonstrated and I would find it very impressive.

1741
01:22:42,780 --> 01:22:42,960
Yeah.

1742
01:22:42,960 --> 01:22:46,620
So right now, they make mistakes in diff- they might be more accurate than human beings

1743
01:22:46,620 --> 01:22:48,980
but they still- they make a different set of mistakes.

1744
01:22:48,980 --> 01:22:57,409
So my- my- I would guess that a lot of the skepticism that some people have about deep learning is when they look at their mistakes and they say

1745
01:22:57,409 --> 01:23:00,260
"Well, those mistakes, they make no sense.

1746
01:23:00,260 --> 01:23:03,895
Like, if you understood the concept, you wouldn't make that mistake.

1747
01:23:03,895 --> 01:23:04,110
" Yes.

1748
01:23:04,110 --> 01:23:09,360
And I think that changing that would be- would- would- that would- that would inspire me.

1749
01:23:09,360 --> 01:23:12,600
That would be, yes, this is- this- this is- this is progress.

1750
01:23:12,600 --> 01:23:12,890
Yeah.

1751
01:23:12,890 --> 01:23:15,480
That's re- that's a really nice way to put it.

1752
01:23:15,480 --> 01:23:21,540
But I also just don't like that human instinct to criticize a model as not intelligent.

1753
01:23:21,540 --> 01:23:28,620
That's the same instinct as we do when we criticize any group of creatures as the other

1754
01:23:28,620 --> 01:23:36,460
because it's very possible that, uh, GPT-2 is much smarter than human beings at many things.

1755
01:23:36,460 --> 01:23:37,620
And- That's definitely true.

1756
01:23:37,620 --> 01:23:39,400
It has a lot more breadth of knowledge.

1757
01:23:39,400 --> 01:23:40,280
Yes.

1758
01:23:40,280 --> 01:23:46,080
Breadth of knowledge, and even- and even perhaps depth on certain topics.

1759
01:23:46,080 --> 01:23:48,340
It's kind of hard to judge what depth means

1760
01:23:48,340 --> 01:23:54,500
but there's definitely a sense in which humans don't make mistakes that these models do.

1761
01:23:54,500 --> 01:23:54,880
Yes.

1762
01:23:54,880 --> 01:23:57,820
The same is applied to autonomous vehicles.

1763
01:23:57,820 --> 01:24:01,800
The same is probably gonna continue being applied to a lot of artificial intelligence systems.

1764
01:24:01,800 --> 01:24:02,754
We find.

1765
01:24:02,754 --> 01:24:03,049
..

1766
01:24:03,049 --> 01:24:04,140
This is the annoying thing.

1767
01:24:04,140 --> 01:24:05,469
This is the process of.

1768
01:24:05,469 --> 01:24:05,809
..

1769
01:24:05,809 --> 01:24:16,480
In the 21st century, the process of analyzing the progress of AI is the search for one case where the system fails in a big way where humans would not

1770
01:24:16,480 --> 01:24:20,740
and then many people writing articles about it

1771
01:24:20,740 --> 01:24:26,670
and then broadly as a com- as a- the public generally gets convinced that the system is not intelligent.

1772
01:24:26,670 --> 01:24:32,000
And we, like, pacify ourselves by thinking it's not intelligent because of this one anecdotal case.

1773
01:24:32,000 --> 01:24:34,610
And this can- seems to continue happening.

1774
01:24:34,610 --> 01:24:35,070
Yeah.

1775
01:24:35,070 --> 01:24:36,170
I mean, there is truth to that.

1776
01:24:36,170 --> 01:24:40,930
Though there is peop- although I'm sure that plenty of people are also extremely impressed by the systems that exist today.

1777
01:24:40,930 --> 01:24:43,150
But I think this connects to the earlier point we discussed

1778
01:24:43,150 --> 01:24:47,140
that it's just confusing to judge progress in AI.

1779
01:24:47,140 --> 01:24:47,780
Yeah.

1780
01:24:47,780 --> 01:24:50,840
And, you know, you have a new robot demonstrating something.

1781
01:24:50,840 --> 01:24:52,820
How impressed should you be?

1782
01:24:52,820 --> 01:24:59,320
And I th- and I think that people will start to be impressed once AI starts to really move the needle on the GDP.

1783
01:24:59,320 --> 01:25:03,820
So, you're one of the people that might be able to create an AGI system here.

1784
01:25:03,820 --> 01:25:07,030
Not you, but you and OpenAI.

1785
01:25:07,030 --> 01:25:09,120
If- if you do create an AGI system

1786
01:25:09,120 --> 01:25:13,280
and you get to spend sort of the evening with it

1787
01:25:13,280 --> 01:25:16,130
him, her, what would you talk about

1788
01:25:16,130 --> 01:25:17,416
do you think?

1789
01:25:17,416 --> 01:25:17,778
..

1790
01:25:17,778 --> 01:25:19,170
. the very first time?

1791
01:25:19,170 --> 01:25:19,790
First time.

1792
01:25:19,790 --> 01:25:28,150
Well, the first time, I would just- I would just ask all kinds of questions and try to make it- to get it to make a mistake and I would be amazed that it doesn't make mistakes.

1793
01:25:28,150 --> 01:25:34,470
And I'd just keep- keep asking broad que- What- what- what kind of questions do you think

1794
01:25:34,470 --> 01:25:39,190
uh, would they be factual or would they be personal

1795
01:25:39,190 --> 01:25:40,990
emotional, psychological?

1796
01:25:40,990 --> 01:25:42,570
What do you think?

1797
01:25:42,570 --> 01:25:44,030
All of the above.

1798
01:25:44,030 --> 01:25:47,270
 Would you ask for advice?

1799
01:25:47,270 --> 01:25:48,010
Definitely.

1800
01:25:48,010 --> 01:25:51,640
 I mean, why- why would I limit myself- Yeah.

1801
01:25:51,640 --> 01:25:51,646
.

1802
01:25:51,646 --> 01:25:51,660
..

1803
01:25:51,660 --> 01:25:53,210
talking to a system like this?

1804
01:25:53,210 --> 01:26:00,310
Now, again, let me emphasize the fact that you truly are one of the people that might be in the room where this happens.

1805
01:26:00,310 --> 01:26:05,730
So let me ask a sort of a profound question about

1806
01:26:05,730 --> 01:26:06,296
um.

1807
01:26:06,296 --> 01:26:06,620
..

1808
01:26:06,620 --> 01:26:13,050
I've just talked to a Stalin historian,  I've been talking to a lot of people who are studying power.

1809
01:26:13,050 --> 01:26:17,790
Uh, Abraham Lincoln said, "Nearly all men can stand adversity

1810
01:26:17,790 --> 01:26:19,810
but if you want to test a man's character

1811
01:26:19,810 --> 01:26:21,290
give him power.

1812
01:26:21,290 --> 01:26:24,710
" I would say the power of the 21st century

1813
01:26:24,710 --> 01:26:28,450
maybe the 22nd, but hopefully the 21st

1814
01:26:28,450 --> 01:26:33,510
would be the creation of an AGI system and the people who have control

1815
01:26:33,510 --> 01:26:36,150
direct possession and control of the AGI system.

1816
01:26:36,150 --> 01:26:44,210
So what do you think, after spending that evening  having a discussion with the AGI system

1817
01:26:44,210 --> 01:26:46,750
what do you think you would do?

1818
01:26:46,750 --> 01:27:01,890
Well, the ideal world I'd like to imagine is one where humanity are like the board- the board members of a company where the AGI is the CEO.

1819
01:27:01,890 --> 01:27:04,916
So it would be.

1820
01:27:04,916 --> 01:27:05,680
..

1821
01:27:05,680 --> 01:27:12,390
I would like the- the picture which I would imagine is you have some kinda different entities

1822
01:27:12,390 --> 01:27:19,060
different countries or cities, and the people that live there vote for what the AGI that represents them should do.

1823
01:27:19,060 --> 01:27:21,410
And then the AGI that represents them goes and does it.

1824
01:27:21,410 --> 01:27:26,350
I think a picture like that, I find very appealing.

1825
01:27:26,350 --> 01:27:28,710
And you could have multiple A- you would have an AGI for a city

1826
01:27:28,710 --> 01:27:32,880
for a country, and it would be- it would be trying to

1827
01:27:32,880 --> 01:27:36,100
in effect, take the democratic process to the next level.

1828
01:27:36,100 --> 01:27:38,670
And the board can always fire the CEO?

1829
01:27:38,670 --> 01:27:39,360
Essentially.

1830
01:27:39,360 --> 01:27:40,690
Press the reset button, say.

1831
01:27:40,690 --> 01:27:42,979
Press the reset- Re- re-randomize the parameters, yeah.

1832
01:27:42,979 --> 01:27:44,520
Well, let me sort of.

1833
01:27:44,520 --> 01:27:44,860
..

1834
01:27:44,860 --> 01:27:45,870
That- that's actually.

1835
01:27:45,870 --> 01:27:46,039
..

1836
01:27:46,039 --> 01:27:49,090
Okay, that- that's a beautiful vision, I think

1837
01:27:49,090 --> 01:27:53,770
as long as it's possible to con- to press the reset button.

1838
01:27:53,770 --> 01:27:56,430
Do you think it will always be possible to press the reset button?

1839
01:27:56,430 --> 01:28:00,650
So I think that it def- it definitely will be possible to build.

1840
01:28:00,650 --> 01:28:02,901
So you're talking.

1841
01:28:02,901 --> 01:28:02,979
..

1842
01:28:02,979 --> 01:28:09,080
So the question that I really understand from you is will- will- will humans or

1843
01:28:09,080 --> 01:28:14,330
you know, humans people have control over the AI systems that they build?

1844
01:28:14,330 --> 01:28:14,919
Yes.

1845
01:28:14,919 --> 01:28:21,870
And my answer is, it's definitely possible to build AI systems which will want to be controlled by other humans.

1846
01:28:21,870 --> 01:28:23,787
Wow, that's part of their.

1847
01:28:23,787 --> 01:28:24,100
..

1848
01:28:24,100 --> 01:28:26,180
So it's not that just they can't help but be controlled

1849
01:28:26,180 --> 01:28:34,539
but that's- that's, um, the- they exist- th- one of the objectives of their existence is to be controlled?

1850
01:28:34,539 --> 01:28:42,450
In the same way that human parents generally want to help their children

1851
01:28:42,450 --> 01:28:44,470
they want their children to succeed.

1852
01:28:44,470 --> 01:28:46,030
It's not a burden for them.

1853
01:28:46,030 --> 01:28:51,850
They are excited to help the children and to feed them and to dress them and to take care of them.

1854
01:28:51,850 --> 01:28:56,310
And I believe, with high c- conviction

1855
01:28:56,310 --> 01:28:58,950
that the same will be possible for an AGI.

1856
01:28:58,950 --> 01:29:00,510
It will be possible to program an AGI

1857
01:29:00,510 --> 01:29:07,090
to design it in such a way that it will have a similar deep drive that it will be delighted to fulfill.

1858
01:29:07,090 --> 01:29:11,210
And the drive will be to help humans flourish.

1859
01:29:11,210 --> 01:29:15,479
But let me take a step back to that moment where you create the AGI system.

1860
01:29:15,479 --> 01:29:17,590
I think this is a really crucial moment.

1861
01:29:17,590 --> 01:29:28,910
And between that moment and the- the democratic board members with the AGI at the head

1862
01:29:28,910 --> 01:29:31,850
there has to be a relinquishing of power.

1863
01:29:31,850 --> 01:29:36,470
So as George Washington, despite all the bad things he did

1864
01:29:36,470 --> 01:29:39,370
one of the big things he did is he relinquished power.

1865
01:29:39,370 --> 01:29:42,200
He, first of all, didn't want to be president 

1866
01:29:42,200 --> 01:29:48,650
and even when he became president, he gave- he didn't keep just serving as most dictators do for indefinitely.

1867
01:29:48,650 --> 01:29:59,320
Do you see yourself being able to relinquish control over an AGI system given how much power you can have over the world?

1868
01:29:59,320 --> 01:30:02,530
At first, financial, just make a lot of money

1869
01:30:02,530 --> 01:30:02,780
right?

1870
01:30:02,780 --> 01:30:07,030
And then control by having possession as AGI system.

1871
01:30:07,030 --> 01:30:09,050
I- I- I'd find it trivial to do that.

1872
01:30:09,050 --> 01:30:12,690
I'd find it trivial to, like, relinquish this- this kind of po- I mean

1873
01:30:12,690 --> 01:30:17,410
you know, the- the kind of scenario you are describing sounds terrifying to me.

1874
01:30:17,410 --> 01:30:19,020
That's all.

1875
01:30:19,020 --> 01:30:22,490
I would absolutely not want to be in that position.

1876
01:30:22,490 --> 01:30:29,390
 Do you think you represent the majority or the minority of people in the AI community?

1877
01:30:29,390 --> 01:30:31,870
Well, I- I mean, I- It's a open question

1878
01:30:31,870 --> 01:30:32,810
an important one.

1879
01:30:32,810 --> 01:30:34,631
"Are most people good?

1880
01:30:34,631 --> 01:30:36,600
" is another way to ask it .

1881
01:30:36,600 --> 01:30:39,410
So I don't know if most people are good

1882
01:30:39,410 --> 01:30:44,410
but I think that when it really counts

1883
01:30:44,410 --> 01:30:47,070
people can be better than we think.

1884
01:30:47,070 --> 01:30:49,290
That is beautifully put, yeah.

1885
01:30:49,290 --> 01:30:54,610
Are there specific mechanism you can think of of aligning AI gene values to human values?

1886
01:30:54,610 --> 01:30:55,291
Is that.

1887
01:30:55,291 --> 01:30:55,500
..

1888
01:30:55,500 --> 01:31:00,390
Do you think about these problems of continued alignment as we develop the AI systems?

1889
01:31:00,390 --> 01:31:02,810
Yeah, definitely.

1890
01:31:02,810 --> 01:31:07,236
In some sense, the kinda question which you are asking is.

1891
01:31:07,236 --> 01:31:07,440
..

1892
01:31:07,440 --> 01:31:10,730
So if I were to translate that question to today's terms- Yes.

1893
01:31:10,730 --> 01:31:10,740
.

1894
01:31:10,740 --> 01:31:10,740
..

1895
01:31:10,740 --> 01:31:20,970
it would be a question about how to get an RL agent that's optimizing a value function which itself has learned.

1896
01:31:20,970 --> 01:31:23,198
And if you look at humans, humans are like that

1897
01:31:23,198 --> 01:31:27,538
because the reward function, the value function of humans is not external

1898
01:31:27,538 --> 01:31:28,618
it is internal.

1899
01:31:28,618 --> 01:31:30,238
That's right.

1900
01:31:30,238 --> 01:31:31,724
And.

1901
01:31:31,724 --> 01:31:32,418
..

1902
01:31:32,418 --> 01:31:36,858
there are definite ideas of how to train a value function.

1903
01:31:36,858 --> 01:31:47,558
Basically, an objective, you know, a- as objective as possible perception system that will be trained separately to recognize

1904
01:31:47,558 --> 01:31:55,278
to internalize human judgments on different situations, and then that component would then be integrated as the value

1905
01:31:55,278 --> 01:31:57,538
as the base value function for some more

1906
01:31:57,538 --> 01:31:59,078
more capable RL system.

1907
01:31:59,078 --> 01:32:00,638
You could imagine a process like this.

1908
01:32:00,638 --> 01:32:02,438
I'm not saying this is the process.

1909
01:32:02,438 --> 01:32:07,498
I'm saying this is an example of the kind of thing you could do.

1910
01:32:07,498 --> 01:32:12,158
So, o- on that topic of the objective functions of human existence

1911
01:32:12,158 --> 01:32:17,498
what do you, what do you think is the objective function that's implicit in human existence?

1912
01:32:17,498 --> 01:32:18,938
What's the meaning of life?

1913
01:32:18,938 --> 01:32:24,940
Oh.

1914
01:32:24,940 --> 01:32:28,888
..

1915
01:32:28,888 --> 01:32:31,498
I think the question is w- is, is wrong in some way.

1916
01:32:31,498 --> 01:32:35,638
I think that the question implies that there is an ex- there is an objective answer

1917
01:32:35,638 --> 01:32:36,848
which is an external answer, you know.

1918
01:32:36,848 --> 01:32:38,228
Your meaning of life is X.

1919
01:32:38,228 --> 01:32:38,668
Right.

1920
01:32:38,668 --> 01:32:40,738
I think what's going on is that we exist

1921
01:32:40,738 --> 01:32:45,658
and that's amazing, and we should try to make the most of it

1922
01:32:45,658 --> 01:32:53,238
and try to maximize our own value and enjoyment of a very short time while we do exist.

1923
01:32:53,238 --> 01:32:56,198
It's, it's funny, because action does require an objective function.

1924
01:32:56,198 --> 01:33:01,118
It's definitely there s- in some form, but it's difficult to make it explicit

1925
01:33:01,118 --> 01:33:03,938
and maybe impossible to make it explicit, I guess is what you're getting at.

1926
01:33:03,938 --> 01:33:08,188
And that's an interesting fact of an RL environment .

1927
01:33:08,188 --> 01:33:08,882
Well, what.

1928
01:33:08,882 --> 01:33:09,068
..

1929
01:33:09,068 --> 01:33:10,538
N- I was making a slightly different point

1930
01:33:10,538 --> 01:33:16,884
is that humans want things, and their wants create the drives that cause them to.

1931
01:33:16,884 --> 01:33:17,008
..

1932
01:33:17,008 --> 01:33:19,898
You know, our wants are our objective functions

1933
01:33:19,898 --> 01:33:21,878
our individual objective functions.

1934
01:33:21,878 --> 01:33:24,338
We can later decide that we want to change

1935
01:33:24,338 --> 01:33:27,278
that what we wanted before is no longer good and we want something else.

1936
01:33:27,278 --> 01:33:28,698
Yeah, but they're so dynamic.

1937
01:33:28,698 --> 01:33:31,975
The- there's got to be some underlying, sort of Freud.

1938
01:33:31,975 --> 01:33:32,188
..

1939
01:33:32,188 --> 01:33:33,978
There's, uh, there's like sexual stuff.

1940
01:33:33,978 --> 01:33:36,348
There's people who think it's the fear of

1941
01:33:36,348 --> 01:33:39,018
fear of death, and there's also, uh

1942
01:33:39,018 --> 01:33:40,678
the desire for knowledge and, you know

1943
01:33:40,678 --> 01:33:41,958
all these kinds of things.

1944
01:33:41,958 --> 01:33:43,478
Uh, procreation.

1945
01:33:43,478 --> 01:33:46,218
Th- the sort of all the evolutionary arguments.

1946
01:33:46,218 --> 01:33:46,968
There seems to be.

1947
01:33:46,968 --> 01:33:47,148
..

1948
01:33:47,148 --> 01:33:49,958
There might be some kind of fundamental objective function from

1949
01:33:49,958 --> 01:33:54,118
uh, from which everything else, um, emerges.

1950
01:33:54,118 --> 01:33:56,007
But it seems like it's, that's very difficult to make explicit.

1951
01:33:56,007 --> 01:33:58,878
I mean, I think, I think, I think that probably is an evolutionary objective function

1952
01:33:58,878 --> 01:34:02,618
which is to survive and procreate and make sh- make your children succeed.

1953
01:34:02,618 --> 01:34:04,358
That would be my guess.

1954
01:34:04,358 --> 01:34:08,218
But it doesn't give an answer to the question of what's the meaning of life.

1955
01:34:08,218 --> 01:34:13,298
I think you can see how humans are part of this big process

1956
01:34:13,298 --> 01:34:14,378
this ancient process.

1957
01:34:14,378 --> 01:34:16,404
We are o- we are.

1958
01:34:16,404 --> 01:34:16,727
..

1959
01:34:16,727 --> 01:34:20,878
We exist on a small planet, and that's it.

1960
01:34:20,878 --> 01:34:24,198
So given that we exist, try to make the most of it

1961
01:34:24,198 --> 01:34:29,068
and try to enjoy more and suffer less as much as we can.

1962
01:34:29,068 --> 01:34:31,898
Let me ask two silly questions about life.

1963
01:34:31,898 --> 01:34:34,538
One, do you have regrets?

1964
01:34:34,538 --> 01:34:37,798
Moments that if you, uh, went back

1965
01:34:37,798 --> 01:34:39,058
you would do differently?

1966
01:34:39,058 --> 01:34:42,318
And two, are there moments that you're especially proud of

1967
01:34:42,318 --> 01:34:44,798
that made you truly happy?

1968
01:34:44,798 --> 01:34:45,718
So I can answer that.

1969
01:34:45,718 --> 01:34:47,078
I can answer both questions.

1970
01:34:47,078 --> 01:34:52,798
 Of c- of course, there are m- there's a huge number of choices and decisions that I've made that

1971
01:34:52,798 --> 01:34:55,538
with the benefit of hindsight, I wouldn't have made them

1972
01:34:55,538 --> 01:34:56,978
and I do experience some regret.

1973
01:34:56,978 --> 01:35:00,338
But, you know, I try to take solace in the knowledge that

1974
01:35:00,338 --> 01:35:02,978
at the time, I did the best I could.

1975
01:35:02,978 --> 01:35:04,358
And in terms of things that I'm proud of

1976
01:35:04,358 --> 01:35:04,624
there are.

1977
01:35:04,624 --> 01:35:04,648
..

1978
01:35:04,648 --> 01:35:08,278
I'm very fortunate to have things I'm pro- to have done things I'm proud of

1979
01:35:08,278 --> 01:35:10,918
and they made me happy for m- for some time

1980
01:35:10,918 --> 01:35:14,678
but I don't think that that is the source of happiness.

1981
01:35:14,678 --> 01:35:17,378
So your academic accomplishments, all the papers

1982
01:35:17,378 --> 01:35:19,478
you're one of the most cited people in the world

1983
01:35:19,478 --> 01:35:24,698
all of the breakthroughs I mentioned in computer vision and language and so on.

1984
01:35:24,698 --> 01:35:24,904
Is.

1985
01:35:24,904 --> 01:35:25,028
..

1986
01:35:25,028 --> 01:35:29,578
What is the source of happiness and pride for you?

1987
01:35:29,578 --> 01:35:31,098
I mean, all those things are a source of pride

1988
01:35:31,098 --> 01:35:31,447
for sure.

1989
01:35:31,447 --> 01:35:35,238
I'm very grateful for having done all those things

1990
01:35:35,238 --> 01:35:37,487
and it was very fun to do them.

1991
01:35:37,487 --> 01:35:38,340
But happiness comes.

1992
01:35:38,340 --> 01:35:38,448
..

1993
01:35:38,448 --> 01:35:39,197
But, you know, you can.

1994
01:35:39,197 --> 01:35:39,307
..

1995
01:35:39,307 --> 01:35:40,175
Ha- happiness.

1996
01:35:40,175 --> 01:35:40,307
..

1997
01:35:40,307 --> 01:35:43,798
Well, my current view is that happiness comes from our.

1998
01:35:43,798 --> 01:35:44,168
..

1999
01:35:44,168 --> 01:35:45,298
To a large, to a very large degree

2000
01:35:45,298 --> 01:35:47,158
from the way we look at things.

2001
01:35:47,158 --> 01:35:51,328
You know, you can have a simple meal and be quite happy as a result

2002
01:35:51,328 --> 01:35:54,938
or you can talk to someone and be happy as a result as well.

2003
01:35:54,938 --> 01:35:59,598
Or, con- conversely, you can have a meal and be disappointed that the meal wasn't a better meal.

2004
01:35:59,598 --> 01:36:02,378
So I think hap- a lot of happiness comes from that.

2005
01:36:02,378 --> 01:36:02,958
But I'm not sure.

2006
01:36:02,958 --> 01:36:03,988
I don't want to be too confident.

2007
01:36:03,988 --> 01:36:04,233
I.

2008
01:36:04,233 --> 01:36:04,408
..

2009
01:36:04,408 --> 01:36:12,208
 Being humble in the face of the uncertainty seems to be also a part of this whole happiness thing.

2010
01:36:12,208 --> 01:36:14,408
Well, I don't think there's a better way to end it than

2011
01:36:14,408 --> 01:36:17,918
uh, meaning of life and discussions of happiness.

2012
01:36:17,918 --> 01:36:19,698
So, Ilya, thank you so much.

2013
01:36:19,698 --> 01:36:22,638
You've given me a few incredible ideas.

2014
01:36:22,638 --> 01:36:24,878
You've given the world many incredible ideas.

2015
01:36:24,878 --> 01:36:27,498
I really appreciate it, and thanks for talking today.

2016
01:36:27,498 --> 01:36:27,688
Yeah.

2017
01:36:27,688 --> 01:36:28,668
Thanks for stop- stopping by.

2018
01:36:28,668 --> 01:36:30,538
I really enjoyed it.

2019
01:36:30,538 --> 01:36:33,358
Thanks for listening to this conversation with Ilya Sutskever

2020
01:36:33,358 --> 01:36:36,378
and thank you to our presenting sponsor, Cash App.

2021
01:36:36,378 --> 01:36:42,058
Please consider supporting the podcast by downloading Cash App and using code LEXPODCAST.

2022
01:36:42,058 --> 01:36:45,418
If you enjoy this podcast, subscribe on YouTube

2023
01:36:45,418 --> 01:36:47,998
review it with five stars on Apple Podcasts

2024
01:36:47,998 --> 01:36:52,838
support it on Patreon, or simply connect with me on Twitter @LexFridman.

2025
01:36:52,838 --> 01:37:00,158
And now, let me leave you with some words from Alan Turing on machine learning.

2026
01:37:00,158 --> 01:37:03,798
"Instead of trying to produce a program to simulate the adult mind

2027
01:37:03,798 --> 01:37:08,798
why not rather try to produce one which simulates the child's?

2028
01:37:08,798 --> 01:37:12,578
If this were then subjected to an appropriate course of education

2029
01:37:12,578 --> 01:37:16,112
one would obtain the adult brain.

2030
01:37:16,112 --> 01:37:27,418
" Thank you for listening, and hope to see you next time.

