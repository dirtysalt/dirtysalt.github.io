1
00:00:00,079 --> 00:00:05,000
It's hard for us humans to make any kind of clean predictions about highly non-linear dynamical systems.

2
00:00:05,000 --> 00:00:12,159
But again, to your point, we might be very surprised what classical learning systems might be able to do about even fluid.

3
00:00:12,159 --> 00:00:13,019
Yes, exactly.

4
00:00:13,019 --> 00:00:16,760
I mean, fluid dynamics, Navierâ€“Stokes equations, these are traditionally thought of as very

5
00:00:16,760 --> 00:00:20,259
very difficult, intractable problems to do on classical systems.

6
00:00:20,259 --> 00:00:21,899
They take enormous amounts of compute.

7
00:00:21,899 --> 00:00:24,059
You know, weather prediction systems, you know

8
00:00:24,059 --> 00:00:26,859
these kind of things all involve fluid dynamics calculations.

9
00:00:26,859 --> 00:00:30,159
But again, if you look at something like Veo

10
00:00:30,159 --> 00:00:33,359
our video generation model, it can model liquids quite well

11
00:00:33,359 --> 00:00:37,379
surprisingly well, and materials, specular lighting.

12
00:00:37,379 --> 00:00:39,139
I love the ones where, you know

13
00:00:39,139 --> 00:00:41,180
there's, there's people who generate videos where there's

14
00:00:41,180 --> 00:00:45,079
like, clear liquids going through hydraulic presses and then it's being squeezed out.

15
00:00:45,079 --> 00:00:49,019
I, I used to write, uh, physics engines and graphics engines in

16
00:00:49,019 --> 00:00:50,279
in my early days in gaming.

17
00:00:50,279 --> 00:00:54,559
And I know, uh, it's so painstakingly hard to build programs that can do that

18
00:00:54,559 --> 00:00:57,360
and yet somehow these systems are, you know

19
00:00:57,360 --> 00:01:00,759
reverse engineering from just watching YouTube videos.

20
00:01:00,759 --> 00:01:08,499
So th- presumably what's happening is it's extracting some underlying structure around how these materials behave.

21
00:01:08,499 --> 00:01:16,559
So perhaps there is some kind of lower dimensional manifold that can be learned if we actually fully understood what's going on under the hood.

22
00:01:16,559 --> 00:01:19,599
That's maybe, you know, maybe true of most of reality.

23
00:01:19,599 --> 00:01:25,459
The following is a conversation with Demis Hassabis

24
00:01:25,459 --> 00:01:27,539
his second time on the podcast.

25
00:01:27,539 --> 00:01:33,279
He is the leader of Google DeepMind and is now a Nobel Prize winner.

26
00:01:33,279 --> 00:01:39,039
Demis is one of the most brilliant and fascinating minds in the world today

27
00:01:39,039 --> 00:01:47,079
working on understanding and building intelligence, and exploring the big mysteries of our universe.

28
00:01:47,079 --> 00:01:50,479
This was truly an honor and a pleasure for me.

29
00:01:50,479 --> 00:01:53,599
This is the Lex Fridman Podcast.

30
00:01:53,599 --> 00:02:00,459
To support it, please check out our sponsors in the description and consider subscribing to this channel.

31
00:02:00,459 --> 00:02:05,499
And now, dear friends, here's Demis Hassabis.

32
00:02:05,499 --> 00:02:11,899
In your Nobel Prize lecture, you proposed what I think is a super interesting conjecture that

33
00:02:11,899 --> 00:02:20,806
quote, "Any pattern that can be generated or found in nature can be efficiently discovered and modeled by a classical learning algorithm.

34
00:02:20,806 --> 00:02:26,119
" What kind of patterns or systems might in- be included in that?

35
00:02:26,119 --> 00:02:29,999
Biology, chemistry, physics, maybe cosmology?

36
00:02:29,999 --> 00:02:30,439
Yup.

37
00:02:30,439 --> 00:02:31,239
Neuroscience?

38
00:02:31,239 --> 00:02:32,279
What, what are we talking about?

39
00:02:32,279 --> 00:02:32,659
Sure.

40
00:02:32,659 --> 00:02:35,219
Well, look, uh, I felt that it's sort of a tradition

41
00:02:35,219 --> 00:02:38,659
I think, of Nobel Prize lectures that you're supposed to be a little bit provocative

42
00:02:38,659 --> 00:02:40,499
and I wanted to follow that tradition.

43
00:02:40,499 --> 00:02:43,479
What I was talking about there is, if you take a step back and you look at

44
00:02:43,479 --> 00:02:44,999
um, all the work that we've done

45
00:02:44,999 --> 00:02:48,459
especially with the AlphaX projects, so I'm thinking AlphaGo

46
00:02:48,459 --> 00:02:56,259
of course AlphaFold, what they really are is we're building models of very combinatorially high dimensional spaces that

47
00:02:56,259 --> 00:02:58,279
you know, if you tried to brute force a solution

48
00:02:58,279 --> 00:03:00,899
find the best move in Go, or find the

49
00:03:00,899 --> 00:03:05,099
the exact shape of a protein, and if you enumerated all the possibilities

50
00:03:05,099 --> 00:03:06,699
you, there wouldn't be enough time in the

51
00:03:06,699 --> 00:03:08,419
in the, you know, the time of the universe.

52
00:03:08,419 --> 00:03:10,559
So you have to do something much smarter.

53
00:03:10,559 --> 00:03:14,819
And what we did in both cases was build models of those environments

54
00:03:14,819 --> 00:03:17,599
um, and that guided the search in a

55
00:03:17,599 --> 00:03:19,839
in a smart way, and that makes it tractable.

56
00:03:19,839 --> 00:03:23,739
So if you think about protein folding, which is obviously a natural system

57
00:03:23,739 --> 00:03:25,459
you know, why should that be possible?

58
00:03:25,459 --> 00:03:26,779
How does physics do that?

59
00:03:26,779 --> 00:03:29,199
You know, proteins fold in milliseconds in our bodies

60
00:03:29,199 --> 00:03:34,299
so somehow physics solves this problem that we've now also solved computationally.

61
00:03:34,299 --> 00:03:37,759
And I think the reason that's possible is that in nature

62
00:03:37,759 --> 00:03:43,679
natural systems have structure because they were subject to evolutionary processes that

63
00:03:43,679 --> 00:03:44,639
that shaped them.

64
00:03:44,639 --> 00:03:47,539
And if that's true, then you can maybe learn

65
00:03:47,539 --> 00:03:49,279
uh, uh, what that structure is.

66
00:03:49,279 --> 00:03:52,119
Th- this perspective, I think, is a really interesting one.

67
00:03:52,119 --> 00:03:55,999
You've hinted at, at it, which is almost like

68
00:03:55,999 --> 00:03:57,239
uh, crudely stated.

69
00:03:57,239 --> 00:04:01,319
Anything that can be evolved can be efficiently modeled.

70
00:04:01,319 --> 00:04:02,959
Think there's some truth to that?

71
00:04:02,959 --> 00:04:05,599
Yeah, I sometimes call it survival of the stablest

72
00:04:05,599 --> 00:04:07,799
or something like that, because, uh, i- you know

73
00:04:07,799 --> 00:04:08,236
it's, it's.

74
00:04:08,236 --> 00:04:08,299
..

75
00:04:08,299 --> 00:04:10,439
Of course there's evolution for life, uh

76
00:04:10,439 --> 00:04:12,779
living things, but there's also, you know

77
00:04:12,779 --> 00:04:16,499
if you think about geological times, so the shape of mountains

78
00:04:16,499 --> 00:04:19,339
that's been shaped by weathering processes, right

79
00:04:19,339 --> 00:04:20,759
over thousands of years.

80
00:04:20,759 --> 00:04:22,679
But then you can even take it cosmological

81
00:04:22,679 --> 00:04:25,679
the orbits of planets, the, um, shapes of asteroids.

82
00:04:25,679 --> 00:04:26,887
These have all been.

83
00:04:26,887 --> 00:04:27,059
..

84
00:04:27,059 --> 00:04:30,159
Survived kind of processes that have acted on them many

85
00:04:30,159 --> 00:04:30,819
many times.

86
00:04:30,819 --> 00:04:34,279
So if that's true, then there should be some sort of pattern

87
00:04:34,279 --> 00:04:37,399
um, that you can kind of reverse learn and

88
00:04:37,399 --> 00:04:39,759
uh, a kind of manifold really that helps you

89
00:04:39,759 --> 00:04:42,559
uh, uh, search to the right solution

90
00:04:42,559 --> 00:04:45,999
to the right shape, um, and actually allow you to predict things about it

91
00:04:45,999 --> 00:04:48,979
uh, in an efficient way, because it's not a random pattern

92
00:04:48,979 --> 00:04:49,519
right?

93
00:04:49,519 --> 00:04:51,399
So, um, it may not be possible for

94
00:04:51,399 --> 00:04:55,547
for manmade things or abstract things like factorizing large numbers because.

95
00:04:55,547 --> 00:04:55,659
..

96
00:04:55,659 --> 00:04:57,999
unless there's patterns in the number space, which there might be.

97
00:04:57,999 --> 00:05:01,459
But if there's not and it's uniform, then there's no pattern to learn

98
00:05:01,459 --> 00:05:03,799
there's no model to learn that will help you search

99
00:05:03,799 --> 00:05:05,099
so you have to do brute force.

100
00:05:05,099 --> 00:05:06,479
So in that case, you, you know

101
00:05:06,479 --> 00:05:08,619
you maybe need a quantum computer, something like this.

102
00:05:08,619 --> 00:05:11,019
But in most things in nature that we're interested in

103
00:05:11,019 --> 00:05:12,339
uh, are not like that.

104
00:05:12,339 --> 00:05:17,499
They have structure, um, that evolved for a reason and survived over time.

105
00:05:17,499 --> 00:05:21,039
And if that's true, I think that's potentially learnable by a neural network.

106
00:05:21,039 --> 00:05:26,639
It's like nature's doing a search process, and it's so fascinating that it's

107
00:05:26,639 --> 00:05:31,139
in that search process, it's creating systems that could be efficiently modeled.

108
00:05:31,139 --> 00:05:32,359
That's right.

109
00:05:32,359 --> 00:05:32,939
Yeah.

110
00:05:32,939 --> 00:05:33,579
It's so interesting.

111
00:05:33,579 --> 00:05:36,559
So they can be efficiently rediscovered or recovered

112
00:05:36,559 --> 00:05:38,739
um, because nature's not random, right?

113
00:05:38,739 --> 00:05:40,699
These, uh, uh, everything that we see around us

114
00:05:40,699 --> 00:05:42,919
including, like, the elements that are more stable

115
00:05:42,919 --> 00:05:44,899
all of those things, they're subject to

116
00:05:44,899 --> 00:05:47,339
um, some kind of selection process, pressure.

117
00:05:47,339 --> 00:05:51,379
Do you think, because you're also a fan of theoretical computer science and complexity

118
00:05:51,379 --> 00:05:54,879
do you think we can come up with a kind of complexity class

119
00:05:54,879 --> 00:06:00,959
like a complexity zoo type of class, where maybe it's the set of learnable systems

120
00:06:00,959 --> 00:06:02,062
the set of.

121
00:06:02,062 --> 00:06:02,291
..

122
00:06:02,291 --> 00:06:02,520
..

123
00:06:02,520 --> 00:06:05,476
. learnable natural systems, LNS?

124
00:06:05,476 --> 00:06:05,875
Yeah.

125
00:06:05,875 --> 00:06:08,156
 This is a Demis Hassabis- A new class.

126
00:06:08,156 --> 00:06:08,455
 .

127
00:06:08,455 --> 00:06:08,455
..

128
00:06:08,455 --> 00:06:13,556
new class of systems that could be actually learnable by classical systems in this kind of way

129
00:06:13,556 --> 00:06:16,995
natural systems that can be, uh, modeled efficiently.

130
00:06:16,995 --> 00:06:17,695
Yeah.

131
00:06:17,695 --> 00:06:24,995
I mean, I'm, I've always been fascinated by the P equals NP question and what is modelable by classical systems

132
00:06:24,995 --> 00:06:25,345
i.

133
00:06:25,345 --> 00:06:25,475
e.

134
00:06:25,475 --> 00:06:28,555
non-quantum systems, you know, Turing machines, in effect.

135
00:06:28,555 --> 00:06:31,295
And that's exactly what I'm working on, actually

136
00:06:31,295 --> 00:06:33,955
in kind of my few moments of spare time with a few colleagues

137
00:06:33,955 --> 00:06:35,876
about w- is should there be, you know

138
00:06:35,876 --> 00:06:44,135
maybe a new class or problem that is solvable by this type of neural network process and kind of mapped onto these natural systems

139
00:06:44,135 --> 00:06:47,936
so, you know, the things that exist in physics and have structure.

140
00:06:47,936 --> 00:06:50,355
So I think that could be a very interesting

141
00:06:50,355 --> 00:06:51,996
uh, new way of thinking about it.

142
00:06:51,996 --> 00:06:54,715
And it sort of fits with the way I think about physics in general

143
00:06:54,715 --> 00:06:57,376
which is the, you know, I think information is primary.

144
00:06:57,376 --> 00:07:00,375
Information's the most sort of fundamental unit of the universe

145
00:07:00,375 --> 00:07:02,055
more fundamental than energy and matter.

146
00:07:02,055 --> 00:07:04,195
I think they can all be converted into each other

147
00:07:04,195 --> 00:07:07,335
but I think of the universe as a kind of informational system.

148
00:07:07,335 --> 00:07:10,335
So when you think of it, the universe is an informational system

149
00:07:10,335 --> 00:07:12,615
then the P equals NP question is a

150
00:07:12,615 --> 00:07:14,375
is a physics question.

151
00:07:14,375 --> 00:07:15,135
That's right.

152
00:07:15,135 --> 00:07:20,215
 And, and it's a question that can help us actually solve the entirety of this whole thing going on.

153
00:07:20,215 --> 00:07:21,875
Yeah, I think it's one of the most

154
00:07:21,875 --> 00:07:25,655
uh, fundamental questions, actually, if you think of physics as informational

155
00:07:25,655 --> 00:07:27,555
uh, and, and the answer to that

156
00:07:27,555 --> 00:07:28,855
I think, is gonna be, you know

157
00:07:28,855 --> 00:07:29,495
very enlightening.

158
00:07:29,495 --> 00:07:32,375
More specific to the P and NP question

159
00:07:32,375 --> 00:07:36,895
this, again, some of the stuff we're saying is kinda crazy right now

160
00:07:36,895 --> 00:07:43,255
just like the Christian Anfinsen Nobel Prize speech controversial thing that he said sounded crazy

161
00:07:43,255 --> 00:07:46,215
and then you went and got a Nobel Prize for this with John Jumper

162
00:07:46,215 --> 00:07:47,095
solved the problem.

163
00:07:47,095 --> 00:07:49,855
So let me, let me just stick to the P equals NP.

164
00:07:49,855 --> 00:07:58,475
Do you think there's something in this thing we're talking about that could be shown if you g- can do something like

165
00:07:58,475 --> 00:08:05,075
uh, polynomial time or constant time compute ahead of time and construct this gigantic model

166
00:08:05,075 --> 00:08:10,075
then you can solve some of these extremely difficult problems

167
00:08:10,075 --> 00:08:12,255
in a theoretical computer science kind of way?

168
00:08:12,255 --> 00:08:14,175
Yeah, I think that there are, uh

169
00:08:14,175 --> 00:08:17,455
actually a huge class of problems that could be couched in this way

170
00:08:17,455 --> 00:08:19,735
the way we did AlphaGo and the way we did AlphaFold

171
00:08:19,735 --> 00:08:23,835
where, you know, you, you model what the dynamics of the system is

172
00:08:23,835 --> 00:08:25,995
the, the, the, the properties of that system

173
00:08:25,995 --> 00:08:35,215
the environment that you're trying to understand, and then that makes the search for the solution or the prediction of the next step efficient

174
00:08:35,215 --> 00:08:38,895
basically polynomial time, so tractable by a

175
00:08:38,895 --> 00:08:42,155
uh, classical system, uh, which a neural network is.

176
00:08:42,155 --> 00:08:45,475
It runs on normal computers, right, classical computers

177
00:08:45,475 --> 00:08:47,255
uh, Turing machines in effect.

178
00:08:47,255 --> 00:08:50,775
And, um, I think it's one of the most interesting questions there is

179
00:08:50,775 --> 00:08:52,755
is how far can that paradigm go?

180
00:08:52,755 --> 00:08:55,135
You know, I think we've proven, uh

181
00:08:55,135 --> 00:08:58,075
and the AI community in general, that classical systems

182
00:08:58,075 --> 00:09:01,015
Turing machines, can go a lot further than we previously thought.

183
00:09:01,015 --> 00:09:07,295
You know, they can do things like model the structures of proteins and play Go to better than world champion level

184
00:09:07,295 --> 00:09:10,775
and, uh, you know, a lot of people would've thought maybe 10

185
00:09:10,775 --> 00:09:16,275
20 years ago that was decades away, or maybe you would need some sort of quantum machines to

186
00:09:16,275 --> 00:09:19,555
to, quantum systems to be able to do things like protein folding.

187
00:09:19,555 --> 00:09:21,915
And so, I think we haven't really

188
00:09:21,915 --> 00:09:25,255
uh, even sort of scratched the surface yet of what

189
00:09:25,255 --> 00:09:28,175
uh, classical systems, so-called, uh, uh

190
00:09:28,175 --> 00:09:28,815
could do.

191
00:09:28,815 --> 00:09:31,455
And of course, AGI being built on a

192
00:09:31,455 --> 00:09:34,215
on a neural network system, on top of a neural network system

193
00:09:34,215 --> 00:09:37,915
on top of a classical computer would be the ultimate expression of that.

194
00:09:37,915 --> 00:09:40,175
And I think the limit, the, you know

195
00:09:40,175 --> 00:09:42,755
the, the, what, what the bounds of that kind of system

196
00:09:42,755 --> 00:09:44,775
what it can do is a very interesting question and

197
00:09:44,775 --> 00:09:47,475
and, and directly speaks to the P equals NP question.

198
00:09:47,475 --> 00:09:49,935
What, what do you think, again, hypothetical

199
00:09:49,935 --> 00:09:54,655
might be outside of this, maybe emergent phenomena?

200
00:09:54,655 --> 00:09:57,215
Like if you look at cellular automata- Mm-hmm.

201
00:09:57,215 --> 00:09:57,228
.

202
00:09:57,228 --> 00:09:57,255
..

203
00:09:57,255 --> 00:10:00,795
some of the, you have extremely simple systems and then some complexity emerges.

204
00:10:00,795 --> 00:10:01,075
Yes.

205
00:10:01,075 --> 00:10:09,915
Maybe that would be outside, or even would you guess even that might be amenable to efficient modeling by a classical machine?

206
00:10:09,915 --> 00:10:12,555
Yeah, I think those systems would be right on the boundary

207
00:10:12,555 --> 00:10:12,975
right?

208
00:10:12,975 --> 00:10:16,115
So, um, I think most emergent systems

209
00:10:16,115 --> 00:10:19,395
cellular automata, things like that, could be modelable by a classical system.

210
00:10:19,395 --> 00:10:22,755
You just sort of do a forward simulation of it and it'd probably be efficient enough.

211
00:10:22,755 --> 00:10:29,215
Um, of course, there's the question of things like chaotic systems where the initial conditions really matter

212
00:10:29,215 --> 00:10:31,175
and then you get to some, you know

213
00:10:31,175 --> 00:10:32,555
uncorrelated end state.

214
00:10:32,555 --> 00:10:34,655
Now, those could be difficult to model.

215
00:10:34,655 --> 00:10:37,195
So I think these are kind of the open questions

216
00:10:37,195 --> 00:10:41,775
but I think when you step back and look at what we've done with the systems and the

217
00:10:41,775 --> 00:10:51,555
and the problems that we've solved, and then you look at things like VO3 on like video generation sort of rendering physics and lighting and things like that

218
00:10:51,555 --> 00:10:54,555
you know, really core fundamental things in physics

219
00:10:54,555 --> 00:10:56,295
um, it's pretty interesting.

220
00:10:56,295 --> 00:10:59,575
I think it's telling us something quite fundamental about how the universe is structured

221
00:10:59,575 --> 00:11:00,215
in my opinion.

222
00:11:00,215 --> 00:11:02,235
Um, so, you know, in, in a way

223
00:11:02,235 --> 00:11:03,755
that's what I want to build AGI for

224
00:11:03,755 --> 00:11:05,715
is to help, uh, us, uh

225
00:11:05,715 --> 00:11:09,295
as scientists answer these questions, uh, like P equals NP.

226
00:11:09,295 --> 00:11:14,815
Yeah, I think, uh, we might be continuously surprised about what is modelable by classical computers.

227
00:11:14,815 --> 00:11:19,335
I mean, AlphaFold 3 on the interaction side is surprising

228
00:11:19,335 --> 00:11:22,835
that you can make any kind of progress on that direction.

229
00:11:22,835 --> 00:11:28,682
Alpha Genome is surprising, that you can map the genetic code to the function.

230
00:11:28,682 --> 00:11:28,874
..

231
00:11:28,874 --> 00:11:31,171
. kind of playing with the emergent kind of phenomena

232
00:11:31,171 --> 00:11:33,252
you think there's so many combinatorial options that

233
00:11:33,252 --> 00:11:34,198
and then here you go-  .

234
00:11:34,198 --> 00:11:34,212
..

235
00:11:34,212 --> 00:11:36,851
is you can find the kernel that is efficiently modeled.

236
00:11:36,851 --> 00:11:38,851
Yes, because, uh, there's some structure

237
00:11:38,851 --> 00:11:43,352
there's some landscape, you know, in the energy landscape or whatever it is that you can follow

238
00:11:43,352 --> 00:11:44,571
some grading you can follow.

239
00:11:44,571 --> 00:11:47,491
And of course, what neural networks are very good at is following gradients.

240
00:11:47,491 --> 00:11:52,411
And so if there's one to follow an object- and you can specify the objective function correctly

241
00:11:52,411 --> 00:11:56,131
you know, you don't have to deal with all that complexity

242
00:11:56,131 --> 00:12:00,271
which I think is how we maybe have naively thought about it for decades

243
00:12:00,271 --> 00:12:00,992
those problems.

244
00:12:00,992 --> 00:12:05,271
If you just enumerate all the possibilities, it looks totally intractable and there's many

245
00:12:05,271 --> 00:12:06,051
many problems like that.

246
00:12:06,051 --> 00:12:10,651
And then you think, well, it's like 10 to 300 pop- possible protein structures

247
00:12:10,651 --> 00:12:12,731
uh, 10 to the 100 and, you know

248
00:12:12,731 --> 00:12:14,752
70 possible go positions.

249
00:12:14,752 --> 00:12:17,031
All of these are way more than atoms in the universe.

250
00:12:17,031 --> 00:12:22,492
So how could one possibly find the, the right solution or predict the next step and

251
00:12:22,492 --> 00:12:22,731
and it.

252
00:12:22,731 --> 00:12:22,771
..

253
00:12:22,771 --> 00:12:26,871
but it turns out that it is possible and of course reality nature does do it

254
00:12:26,871 --> 00:12:27,571
right?

255
00:12:27,571 --> 00:12:28,551
Proteins do fold.

256
00:12:28,551 --> 00:12:30,803
So that, that gives you confidence that there must be.

257
00:12:30,803 --> 00:12:30,891
..

258
00:12:30,891 --> 00:12:33,251
if we understood how physics was doing that

259
00:12:33,251 --> 00:12:35,465
uh, in a sense, uh, then.

260
00:12:35,465 --> 00:12:35,591
..

261
00:12:35,591 --> 00:12:38,791
and we could mimic that process, I model that process

262
00:12:38,791 --> 00:12:42,091
uh, it should be possible on our classical systems is

263
00:12:42,091 --> 00:12:44,431
is, is basically what the conjecture is about.

264
00:12:44,431 --> 00:12:49,411
And of course there's non-linear dynamical systems, highly non-linear dynamical systems

265
00:12:49,411 --> 00:12:50,931
everything involving fluid.

266
00:12:50,931 --> 00:12:51,531
Yes.

267
00:12:51,531 --> 00:12:51,991
Right.

268
00:12:51,991 --> 00:12:52,496
You know, there.

269
00:12:52,496 --> 00:12:52,531
..

270
00:12:52,531 --> 00:12:55,251
recently a conversation with Terence Tao who mathematically

271
00:12:55,251 --> 00:13:09,611
uh, he contends with a very difficult aspect of systems that have some singularities in them that break the mathematics and it's just hard for us humans to make any kind of clean predictions about highly non-linear dynamical systems.

272
00:13:09,611 --> 00:13:16,991
But again, to your point, we might be very surprised what classical learning systems might be able to do about even fluid.

273
00:13:16,991 --> 00:13:17,851
Yes, exactly.

274
00:13:17,851 --> 00:13:21,611
I mean, fluid dynamics, Navier-Stokes equations, these are traditionally thought of as very

275
00:13:21,611 --> 00:13:25,731
very difficult intractable kind of problems to do on classical systems.

276
00:13:25,731 --> 00:13:27,831
They take enormous amounts of compute, you know

277
00:13:27,831 --> 00:13:32,351
weather prediction systems, you know, these kind of things all involve fluid dynamics calculations.

278
00:13:32,351 --> 00:13:33,851
And, um.

279
00:13:33,851 --> 00:13:34,011
..

280
00:13:34,011 --> 00:13:37,031
but again, if you look at something like Veo

281
00:13:37,031 --> 00:13:40,251
our video generation model, it can model liquids quite well

282
00:13:40,251 --> 00:13:41,351
surprisingly well.

283
00:13:41,351 --> 00:13:44,271
And materials, specular lighting.

284
00:13:44,271 --> 00:13:46,051
I love the ones where, you know

285
00:13:46,051 --> 00:13:51,971
there's, there's people who generate videos where there's like clear liquids going through hydraulic presses and then it's being squeezed out.

286
00:13:51,971 --> 00:13:55,931
I used to write, uh, physics engines and graphics engines in

287
00:13:55,931 --> 00:14:03,831
in my early days in gaming and I know it's just so painstakingly hard to build programs that can do that and yet somehow these systems are

288
00:14:03,831 --> 00:14:07,651
you know, reverse engineering from just watching YouTube videos.

289
00:14:07,651 --> 00:14:15,391
So presumably what's happening is it's extracting some underlying structure around how these materials behave.

290
00:14:15,391 --> 00:14:23,471
So perhaps there is some kind of lower dimensional manifold that can be learned if we actually fully understood what's going on under the hood.

291
00:14:23,471 --> 00:14:26,411
That's maybe, you know, maybe true of most of reality.

292
00:14:26,411 --> 00:14:31,151
Yeah, I've been continuously precisely by this aspect of Veo 3.

293
00:14:31,151 --> 00:14:35,511
I think a lot of people highlight different aspects including the comedic and the media- Yes.

294
00:14:35,511 --> 00:14:35,517
.

295
00:14:35,517 --> 00:14:35,531
..

296
00:14:35,531 --> 00:14:36,171
all that kind of stuff.

297
00:14:36,171 --> 00:14:46,611
And then th- the ultra-realistic ability to capture humans in a really nice way that's compelling and g- feels close to reality and then combine that with native audio.

298
00:14:46,611 --> 00:14:48,991
All of those are marvelous things about Veo 3

299
00:14:48,991 --> 00:14:53,251
but the exactly the thing you're mentioning which is the physics- Yeah.

300
00:14:53,251 --> 00:14:53,351
.

301
00:14:53,351 --> 00:14:53,551
..

302
00:14:53,551 --> 00:14:56,251
is not perfect but it's pretty damn good.

303
00:14:56,251 --> 00:15:04,371
And then the, the really interesting scientific question is what is it understanding about our world in order to be able to do that?

304
00:15:04,371 --> 00:15:07,571
Because if the cynical take with diffusion models

305
00:15:07,571 --> 00:15:09,911
there's no way it understands anything.

306
00:15:09,911 --> 00:15:10,131
Mm-hmm.

307
00:15:10,131 --> 00:15:11,401
But it seems.

308
00:15:11,401 --> 00:15:11,491
..

309
00:15:11,491 --> 00:15:21,191
I mean, I don't think you can generate that kind of video without understanding and then our own philosophical notion of what it means to understand then is like brought to the surface.

310
00:15:21,191 --> 00:15:21,571
Like, do.

311
00:15:21,571 --> 00:15:21,691
..

312
00:15:21,691 --> 00:15:25,111
to what degree do you think Veo 3 understands our world?

313
00:15:25,111 --> 00:15:28,571
I think to the extent that it can predict the next frames

314
00:15:28,571 --> 00:15:31,285
you know, in a coherent way, that's some.

315
00:15:31,285 --> 00:15:31,371
..

316
00:15:31,371 --> 00:15:33,471
that is a form, you know, of understanding

317
00:15:33,471 --> 00:15:33,651
right?

318
00:15:33,651 --> 00:15:35,811
Not in the anthropomorphic version of, you know

319
00:15:35,811 --> 00:15:39,391
it's not some kind of deep philosophical understanding of what's going on.

320
00:15:39,391 --> 00:15:41,831
I don't think these systems have that, but they

321
00:15:41,831 --> 00:15:45,011
they certainly have, uh, modeled enough of the dynamics

322
00:15:45,011 --> 00:15:50,211
you know, put it that way, that they can pretty accurately generate whatever it is

323
00:15:50,211 --> 00:15:53,491
eight seconds of consistent video that by eye at least

324
00:15:53,491 --> 00:15:56,951
you know, kind of a glance is quite hard to distinguish what the issues are.

325
00:15:56,951 --> 00:15:59,331
And imagine that in two or three more years time

326
00:15:59,331 --> 00:16:02,159
that's the thing I'm thinking about and how incredible that will.

327
00:16:02,159 --> 00:16:02,211
..

328
00:16:02,211 --> 00:16:04,591
they will look, uh, given where we've come from

329
00:16:04,591 --> 00:16:05,951
you know, the early versions of that

330
00:16:05,951 --> 00:16:07,951
uh, uh, one or two years ago.

331
00:16:07,951 --> 00:16:12,871
And so, um, the rate of progress is incredible and I think

332
00:16:12,871 --> 00:16:16,011
um, I'm like you is like a lot of people love all of the

333
00:16:16,011 --> 00:16:18,384
the, the stand-up comedians and the, the.

334
00:16:18,384 --> 00:16:18,451
..

335
00:16:18,451 --> 00:16:21,231
it actually captures a lot of human dynamics very well and

336
00:16:21,231 --> 00:16:26,911
and body language but actually the thing I'm most impressed with and fascinated by is the physics behavior

337
00:16:26,911 --> 00:16:32,771
the lighting and materials and liquids and it's pretty amazing that it can do that.

338
00:16:32,771 --> 00:16:39,051
And I think that shows, uh, that it has some notion of at least intuitive physics

339
00:16:39,051 --> 00:16:39,651
right?

340
00:16:39,651 --> 00:16:41,731
Um, how things are supposed to work

341
00:16:41,731 --> 00:16:44,271
uh, intuitively maybe the way that, uh

342
00:16:44,271 --> 00:16:46,511
a human child would understand physics, right?

343
00:16:46,511 --> 00:16:49,851
As opposed to a, you know, a PhD student really

344
00:16:49,851 --> 00:16:51,631
uh, being able to unpack all the equations.

345
00:16:51,631 --> 00:16:53,551
It's more of an intuitive physics understanding.

346
00:16:53,551 --> 00:16:58,511
Well, that intuitive physics understanding, that's the base layer

347
00:16:58,511 --> 00:17:01,051
that's the thing people sometimes call a common sense.

348
00:17:01,051 --> 00:17:01,511
Mm-hmm.

349
00:17:01,511 --> 00:17:05,151
It really understands something that I think that really surprises a lot of people.

350
00:17:05,151 --> 00:17:11,371
It blows my mind that I just didn't think it would be possible to generate that level of realism without understanding.

351
00:17:11,371 --> 00:17:11,731
Mm-hmm.

352
00:17:11,731 --> 00:17:18,451
There's this notion that you can only understand the physical world by having an embodied AI system

353
00:17:18,451 --> 00:17:20,651
a robot that interacts with that world.

354
00:17:20,651 --> 00:17:23,171
That's the only way to construct an understanding of that world.

355
00:17:23,171 --> 00:17:23,431
Yeah.

356
00:17:23,431 --> 00:17:27,231
But Veo 3 is directly challenging that- Right.

357
00:17:27,231 --> 00:17:27,243
.

358
00:17:27,243 --> 00:17:27,271
..

359
00:17:27,271 --> 00:17:27,871
it feels like.

360
00:17:27,871 --> 00:17:28,290
Yes.

361
00:17:28,290 --> 00:17:29,811
And this is very interesting, you know

362
00:17:29,811 --> 00:17:30,631
even now if we.

363
00:17:30,631 --> 00:17:30,751
..

364
00:17:30,751 --> 00:17:32,711
if you were to ask me five, 10 years ago

365
00:17:32,711 --> 00:17:35,151
I would have said even though I was immersed in all of this

366
00:17:35,151 --> 00:17:38,193
I would have said, "Well yeah, you probably need to understand intuitive physics.

367
00:17:38,193 --> 00:17:40,571
" You know, like if I push this off the table

368
00:17:40,571 --> 00:17:43,051
this glass it will maybe shatter, you know

369
00:17:43,051 --> 00:17:45,251
um, and the, and the liquid will spill out

370
00:17:45,251 --> 00:17:45,451
right?

371
00:17:45,451 --> 00:17:47,032
So we know all of these things.

372
00:17:47,032 --> 00:17:50,279
But I thought that, you know, and there's a lot of theories in neuroscience

373
00:17:50,279 --> 00:17:52,579
it's called action and perception, where, you know

374
00:17:52,579 --> 00:17:56,080
you, you need to act in the world to really truly perceive it in a deep way.

375
00:17:56,080 --> 00:17:58,420
And there was a lot of theories about

376
00:17:58,420 --> 00:18:01,139
you need embodied intelligence or robotics or something

377
00:18:01,139 --> 00:18:03,620
or maybe at least simulated action, uh

378
00:18:03,620 --> 00:18:06,519
so that you would understand things like intuitive physics.

379
00:18:06,519 --> 00:18:10,439
But it seems like, um, you can understand it through passive observation

380
00:18:10,439 --> 00:18:12,699
which is pretty surprising to me, and

381
00:18:12,699 --> 00:18:16,519
and again, I think hints at something underlying about the nature of

382
00:18:16,519 --> 00:18:18,579
uh, reality, in, in, in my opinion

383
00:18:18,579 --> 00:18:20,580
beyond, um, just the, you know

384
00:18:20,580 --> 00:18:22,180
the cool videos that it generates.

385
00:18:22,180 --> 00:18:27,879
Um, and, and of course there's next stages is maybe even making those videos interactive so

386
00:18:27,879 --> 00:18:31,119
uh, one can actually step into them and move around them

387
00:18:31,119 --> 00:18:35,139
um, which would be really mind-blowing, especially given my games background.

388
00:18:35,139 --> 00:18:36,760
 So  you can imagine.

389
00:18:36,760 --> 00:18:37,740
Uh, and then, and then I think

390
00:18:37,740 --> 00:18:40,639
you know, you're, we're starting to get towards what I would call a world model

391
00:18:40,639 --> 00:18:44,280
a model of how the world works, the mechanics of the world

392
00:18:44,280 --> 00:18:46,779
the physics of the world, and the things in that world.

393
00:18:46,779 --> 00:18:50,019
And of course that's what you would need for a true AGI system.

394
00:18:50,019 --> 00:18:52,059
I have to talk to you about video games.

395
00:18:52,059 --> 00:18:52,199
Yes.

396
00:18:52,199 --> 00:18:53,739
So you, you were being a bit trolly.

397
00:18:53,739 --> 00:18:57,659
  I, I think you're, you're having more and more fun on Twitter

398
00:18:57,659 --> 00:18:58,999
on X, which is great to see.

399
00:18:58,999 --> 00:19:04,599
So a guy named Jimmy Apples tweeted, "Let me play a video game of my VO3 videos already.

400
00:19:04,599 --> 00:19:06,719
Uh, Google cooked so good.

401
00:19:06,719 --> 00:19:08,899
Playable world models wen?

402
00:19:08,899 --> 00:19:10,919
" Spelled W-E-N question mark.

403
00:19:10,919 --> 00:19:13,139
Um, and then you quote tweeted that with

404
00:19:13,139 --> 00:19:14,677
"Now wouldn't that be something.

405
00:19:14,677 --> 00:19:15,379
" Mm-hmm.

406
00:19:15,379 --> 00:19:19,019
So how, how hard is it to build game worlds with AI?

407
00:19:19,019 --> 00:19:21,299
Maybe can you look out into the future

408
00:19:21,299 --> 00:19:24,159
uh, of video games- Mm.

409
00:19:24,159 --> 00:19:24,179
.

410
00:19:24,179 --> 00:19:24,219
..

411
00:19:24,219 --> 00:19:25,259
five, 10 years out?

412
00:19:25,259 --> 00:19:25,579
Mm.

413
00:19:25,579 --> 00:19:26,739
What do you think that looks like?

414
00:19:26,739 --> 00:19:29,179
Well, games were my first love really

415
00:19:29,179 --> 00:19:32,859
and doing AI for games was the first thing I did professionally in

416
00:19:32,859 --> 00:19:35,219
in my teenage years, and, and was the first

417
00:19:35,219 --> 00:19:37,539
uh, major AI systems that I built.

418
00:19:37,539 --> 00:19:39,939
And, uh, I always wanna, I have

419
00:19:39,939 --> 00:19:42,479
I wanna scratch that itch one day and come back to that.

420
00:19:42,479 --> 00:19:44,179
So, you know, and I will do

421
00:19:44,179 --> 00:19:47,419
I think, and, um, I think I'd sort of dream about

422
00:19:47,419 --> 00:19:52,419
you know, what would I have done back in the '90s if I'd had access to the kind of AI systems we have today?

423
00:19:52,419 --> 00:19:55,199
And I think you could build absolutely mind-blowing games.

424
00:19:55,199 --> 00:19:56,919
Um, and I think the next stage is

425
00:19:56,919 --> 00:19:58,170
I always used to love making.

426
00:19:58,170 --> 00:19:58,259
..

427
00:19:58,259 --> 00:20:00,799
All the games I've made are open-world games.

428
00:20:00,799 --> 00:20:01,019
Mm-hmm.

429
00:20:01,019 --> 00:20:05,239
So they're games where there's a simulation and then there's AI characters

430
00:20:05,239 --> 00:20:11,439
and then the player, uh, interacts with that simulation and the simulation adapts to the way the player plays.

431
00:20:11,439 --> 00:20:13,919
And I always thought they were the coolest games because

432
00:20:13,919 --> 00:20:18,819
uh, so games like Theme Park that I worked on where everybody's game experience would be unique to them

433
00:20:18,819 --> 00:20:21,599
right, because you're kinda co-creating the game

434
00:20:21,599 --> 00:20:22,139
right?

435
00:20:22,139 --> 00:20:25,079
Uh, we set up the parameters, we set up initial conditions

436
00:20:25,079 --> 00:20:27,059
and then you as the player immersed in it

437
00:20:27,059 --> 00:20:29,539
and then you are co-creating it with the

438
00:20:29,539 --> 00:20:30,699
with the simulation.

439
00:20:30,699 --> 00:20:33,799
But of course, it's very hard to program open-world games.

440
00:20:33,799 --> 00:20:35,519
You know, you've got to be able to create

441
00:20:35,519 --> 00:20:37,719
uh, content whichever direction the player goes in

442
00:20:37,719 --> 00:20:40,799
and you want it to be compelling no matter what the player chooses.

443
00:20:40,799 --> 00:20:44,379
Um, and so it was always quite difficult to build

444
00:20:44,379 --> 00:20:46,759
uh, things like Cellular Automata, actually, type of

445
00:20:46,759 --> 00:20:49,919
those kind of classical systems which created some emergent behavior.

446
00:20:49,919 --> 00:20:51,599
Um, but they're always a little bit fragile

447
00:20:51,599 --> 00:20:52,499
a little bit limited.

448
00:20:52,499 --> 00:20:55,119
Now we're maybe on the cusp in the next few years

449
00:20:55,119 --> 00:21:00,019
five, 10 years, of having AI systems that can truly create around your imagination

450
00:21:00,019 --> 00:21:05,659
um, can nar- and sort of dynamically change the story and storytell the narrative around

451
00:21:05,659 --> 00:21:09,119
uh, and make it dramatic no matter what you end up choosing.

452
00:21:09,119 --> 00:21:12,259
So it's like the ultimate choose-your-own-adventure sort of game.

453
00:21:12,259 --> 00:21:17,979
And, uh, you know, I think maybe we're within reach if you think of a kind of interactive version of VO

454
00:21:17,979 --> 00:21:21,439
uh, and then f- wind that forward five to 10 years and

455
00:21:21,439 --> 00:21:23,439
um, you know, imagine how good it's gonna be.

456
00:21:23,439 --> 00:21:24,559
Yeah.

457
00:21:24,559 --> 00:21:26,439
So you said a lot of super interesting stuff there.

458
00:21:26,439 --> 00:21:31,979
So one, the open world, built into that is a deep personalization

459
00:21:31,979 --> 00:21:33,239
the way you've described it.

460
00:21:33,239 --> 00:21:33,479
Mm.

461
00:21:33,479 --> 00:21:35,299
So it's not just that it's open world

462
00:21:35,299 --> 00:21:37,979
like you can open any door and there'll be something there.

463
00:21:37,979 --> 00:21:45,199
It's that the choice of which door you open in an unconstrained way defines the worlds you see.

464
00:21:45,199 --> 00:21:49,359
So some games try to do that, they give you choice- Yes.

465
00:21:49,359 --> 00:21:49,379
.

466
00:21:49,379 --> 00:21:49,419
..

467
00:21:49,419 --> 00:21:51,239
but it's really just an illusion of choice- Yes.

468
00:21:51,239 --> 00:21:51,245
.

469
00:21:51,245 --> 00:21:51,259
..

470
00:21:51,259 --> 00:21:53,919
because the only, uh, uh, like

471
00:21:53,919 --> 00:21:55,699
like Stanley Parable, is, is- Yeah.

472
00:21:55,699 --> 00:21:55,705
.

473
00:21:55,705 --> 00:21:55,719
..

474
00:21:55,719 --> 00:21:56,799
a game I recently  played.

475
00:21:56,799 --> 00:22:01,139
It's, it's, it's really, there's a couple of doors and it really just takes you down a narrative.

476
00:22:01,139 --> 00:22:03,979
Stanley Parable is a great video game I recommend people play- Yeah.

477
00:22:03,979 --> 00:22:04,052
.

478
00:22:04,052 --> 00:22:04,199
..

479
00:22:04,199 --> 00:22:06,259
that kinda, uh, in a meta way

480
00:22:06,259 --> 00:22:12,259
uh,  mocks the illusion of choice and there's philosophical notions of free will and so on.

481
00:22:12,259 --> 00:22:17,879
But, uh, I do, like one of my favorite games of Elder Scrolls is Daggerfall

482
00:22:17,879 --> 00:22:22,039
I believe, that they really played with the

483
00:22:22,039 --> 00:22:24,819
like, random generation of the dungeons- Yeah.

484
00:22:24,819 --> 00:22:24,919
.

485
00:22:24,919 --> 00:22:25,119
..

486
00:22:25,119 --> 00:22:27,059
of if you could step in and they- Yes.

487
00:22:27,059 --> 00:22:27,059
.

488
00:22:27,059 --> 00:22:27,059
.

489
00:22:27,059 --> 00:22:29,079
give you this feeling of an open world.

490
00:22:29,079 --> 00:22:33,339
And there, you mentioned interactivity, you don't need to interact.

491
00:22:33,339 --> 00:22:35,759
That, that's a first step, is you don't need to interact that much

492
00:22:35,759 --> 00:22:37,499
you just, when you open the door

493
00:22:37,499 --> 00:22:41,039
whatever you see is randomly generated for you.

494
00:22:41,039 --> 00:22:41,379
Yeah.

495
00:22:41,379 --> 00:22:46,139
And that's already an incredible experience, because you might be the only person to ever see that.

496
00:22:46,139 --> 00:22:46,399
Yeah.

497
00:22:46,399 --> 00:22:47,359
Exactly.

498
00:22:47,359 --> 00:22:48,503
And- and so.

499
00:22:48,503 --> 00:22:48,679
..

500
00:22:48,679 --> 00:22:52,159
But what you'd like is a little bit better than sort- sort of a random generation

501
00:22:52,159 --> 00:22:52,419
right?

502
00:22:52,419 --> 00:22:53,935
So you'd like, uh.

503
00:22:53,935 --> 00:22:54,119
..

504
00:22:54,119 --> 00:22:55,819
And- and also better than a simple A

505
00:22:55,819 --> 00:22:58,179
B hardcoded choice, right?

506
00:22:58,179 --> 00:23:00,899
That's not really a open world, right?

507
00:23:00,899 --> 00:23:03,399
Like as, as you say, it's just giving you the illusion of choice.

508
00:23:03,399 --> 00:23:04,459
What you want to be able to do is

509
00:23:04,459 --> 00:23:06,799
uh, is potentially anything in that game environment.

510
00:23:06,799 --> 00:23:11,399
Um, and I think the only way you can do that is to have

511
00:23:11,399 --> 00:23:14,239
uh, generated systems, systems that, uh

512
00:23:14,239 --> 00:23:15,339
will generate that on the fly.

513
00:23:15,339 --> 00:23:18,399
Of course you can't create infinite amounts of game assets

514
00:23:18,399 --> 00:23:18,559
right?

515
00:23:18,559 --> 00:23:21,699
It's expensive enough already how triple A games are made today.

516
00:23:21,699 --> 00:23:25,959
And that was obvious to, to us back in the '90s when I was working on all these games.

517
00:23:25,959 --> 00:23:28,039
I think maybe Black & White, uh

518
00:23:28,039 --> 00:23:30,779
was the game that I worked on early stages of that

519
00:23:30,779 --> 00:23:32,979
that had the, still probably the best AI

520
00:23:32,979 --> 00:23:33,899
learning AI, in it.

521
00:23:33,899 --> 00:23:36,819
It was an early reinforcement learning system that you

522
00:23:36,819 --> 00:23:41,379
you know, you were, you were looking after this mythical creature and growing it and nurturing it

523
00:23:41,379 --> 00:23:45,659
and depending how you treated it, it would treat the villagers in that world in the same way.

524
00:23:45,659 --> 00:23:47,159
So if you were mean to it, it would be mean.

525
00:23:47,159 --> 00:23:48,899
If you were good, it would be protective.

526
00:23:48,899 --> 00:23:52,219
And so it was really a reflection of the way you played it.

527
00:23:52,219 --> 00:23:54,103
So actually, all of the, uh.

528
00:23:54,103 --> 00:23:54,199
..

529
00:23:54,199 --> 00:23:56,379
I've been working on sort of simulations and AI

530
00:23:56,379 --> 00:23:59,999
uh, uh, through the medium of games at the beginning of my career

531
00:23:59,999 --> 00:24:03,679
and, and really the whole of what I do today is still a follow-on from

532
00:24:03,679 --> 00:24:07,799
uh, those early more hardcoded ways of doing the AI to now

533
00:24:07,799 --> 00:24:11,559
you know, fully general learning systems that- that are trying to achieve the same thing.

534
00:24:11,559 --> 00:24:12,639
Yeah.

535
00:24:12,639 --> 00:24:15,499
It's been, uh, interesting, hilarious, and

536
00:24:15,499 --> 00:24:19,519
uh, fun to watch you and Elon obviously itching to create games

537
00:24:19,519 --> 00:24:20,639
'cause you're both gamers.

538
00:24:20,639 --> 00:24:24,058
And one of the sad aspects of your-.

539
00:24:24,058 --> 00:24:24,351
..

540
00:24:24,351 --> 00:24:26,971
uh, incredible success in so many domains of science

541
00:24:26,971 --> 00:24:28,991
like serious adult stuff- Yeah.

542
00:24:28,991 --> 00:24:29,091
.

543
00:24:29,091 --> 00:24:29,291
..

544
00:24:29,291 --> 00:24:32,371
that you might not have time to really create a game.

545
00:24:32,371 --> 00:24:36,271
You might end up creating the tooling that others would create the game.

546
00:24:36,271 --> 00:24:36,292
Right.

547
00:24:36,292 --> 00:24:39,151
And you have to watch-  Yeah, exactly.

548
00:24:39,151 --> 00:24:39,157
.

549
00:24:39,157 --> 00:24:39,171
..

550
00:24:39,171 --> 00:24:42,271
other, others create the thing you've always dreamed of.

551
00:24:42,271 --> 00:24:48,991
Do you think it's possible you can somehow in your extremely busy schedule actually find time to create something like Black & White

552
00:24:48,991 --> 00:24:52,132
some, some, uh, an actual video game.

553
00:24:52,132 --> 00:24:56,551
Where, like, you could m- m- make the childhood dream- Yeah.

554
00:24:56,551 --> 00:24:56,551
.

555
00:24:56,551 --> 00:24:56,551
.

556
00:24:56,551 --> 00:24:57,751
 become, become a reality?

557
00:24:57,751 --> 00:25:00,071
Well, you know, uh, there's two things one can think about that is maybe

558
00:25:00,071 --> 00:25:02,011
uh, with vibe coding as it gets better-  Yes.

559
00:25:02,011 --> 00:25:02,011
.

560
00:25:02,011 --> 00:25:02,011
.

561
00:25:02,011 --> 00:25:03,931
there's a possibility that I could, you know- Sure.

562
00:25:03,931 --> 00:25:03,931
.

563
00:25:03,931 --> 00:25:03,931
.

564
00:25:03,931 --> 00:25:06,312
one could do that actually in, in your spare time.

565
00:25:06,312 --> 00:25:07,912
So I'm quite excited about that as a

566
00:25:07,912 --> 00:25:09,292
as-  That would be my project if

567
00:25:09,292 --> 00:25:11,631
if I got the time to do some vibe coding.

568
00:25:11,631 --> 00:25:13,791
Um, I'm actually itching to do that.

569
00:25:13,791 --> 00:25:15,171
And then the other thing is, you know

570
00:25:15,171 --> 00:25:20,831
maybe it's a sabbatical after AGI has been safely stewarded into the world and delivered into the world

571
00:25:20,831 --> 00:25:23,452
you know, that and then working on my physics theory

572
00:25:23,452 --> 00:25:25,131
you know, as we talked about in the beginning

573
00:25:25,131 --> 00:25:26,118
those would be the two.

574
00:25:26,118 --> 00:25:26,292
..

575
00:25:26,292 --> 00:25:29,012
my, my two post-AGI projects, let's call it that way.

576
00:25:29,012 --> 00:25:31,251
I would, I would love to see which post- The ultimate game.

577
00:25:31,251 --> 00:25:31,351
.

578
00:25:31,351 --> 00:25:31,551
..

579
00:25:31,551 --> 00:25:33,231
post-AGI, which you choose.

580
00:25:33,231 --> 00:25:38,731
Solving, uh, the,  the problem that some of the smartest people in human history contended with

581
00:25:38,731 --> 00:25:43,051
so P equals NP,  or creating a cool video game.

582
00:25:43,051 --> 00:25:43,451
Yeah.

583
00:25:43,451 --> 00:25:44,192
Well, but they- Right?

584
00:25:44,192 --> 00:25:44,192
.

585
00:25:44,192 --> 00:25:44,192
.

586
00:25:44,192 --> 00:25:44,411
but they might.

587
00:25:44,411 --> 00:25:44,451
..

588
00:25:44,451 --> 00:25:45,852
But in my world, they'd be related- Sure.

589
00:25:45,852 --> 00:25:45,852
.

590
00:25:45,852 --> 00:25:45,852
.

591
00:25:45,852 --> 00:25:48,932
because it would be an open world simulated game

592
00:25:48,932 --> 00:25:51,272
uh, as realistic as possible.

593
00:25:51,272 --> 00:25:53,211
So, you know, what, what is

594
00:25:53,211 --> 00:25:54,091
what is the universe?

595
00:25:54,091 --> 00:25:56,072
That's, that's, that's speaking to the same question

596
00:25:56,072 --> 00:25:56,231
right?

597
00:25:56,231 --> 00:25:57,111
NP equals NP.

598
00:25:57,111 --> 00:25:59,192
I think all these things are related, at least in my mind.

599
00:25:59,192 --> 00:26:01,351
I mean, in a really serious way

600
00:26:01,351 --> 00:26:06,671
I think v- video games sometimes are looked down upon as just this fun side activity

601
00:26:06,671 --> 00:26:10,291
but especially as, uh, AI does more and more of

602
00:26:10,291 --> 00:26:13,772
um, the difficult, uh, boring tasks

603
00:26:13,772 --> 00:26:16,391
something that we in, in modern world call work

604
00:26:16,391 --> 00:26:21,631
you know, video games is the thing in which we may find meaning

605
00:26:21,631 --> 00:26:23,751
in w- which we may find, like

606
00:26:23,751 --> 00:26:24,911
what to do with our time.

607
00:26:24,911 --> 00:26:29,731
You could create incredibly rich, meaningful experiences.

608
00:26:29,731 --> 00:26:31,271
Like, that's what human life is.

609
00:26:31,271 --> 00:26:35,831
And then in video games, you can create more sophisticated

610
00:26:35,831 --> 00:26:39,951
more diverse ways of living.

611
00:26:39,951 --> 00:26:40,651
Yeah.

612
00:26:40,651 --> 00:26:40,812
Right?

613
00:26:40,812 --> 00:26:41,271
That's the whole idea.

614
00:26:41,271 --> 00:26:41,732
I think so.

615
00:26:41,732 --> 00:26:43,811
I mean, those of us who love games

616
00:26:43,811 --> 00:26:45,851
and I still do, is, is, is

617
00:26:45,851 --> 00:26:50,372
um, you know, it almost can let your imagination run wild

618
00:26:50,372 --> 00:26:50,671
right?

619
00:26:50,671 --> 00:26:52,392
Like, I, I used to love games

620
00:26:52,392 --> 00:26:56,432
um, and working on games so much because it's the fusion

621
00:26:56,432 --> 00:26:58,511
especially in the '90s and 2- early 2000s

622
00:26:58,511 --> 00:27:00,591
the sort of golden era, maybe the '80s of

623
00:27:00,591 --> 00:27:02,771
of, of ga- of the games industry

624
00:27:02,771 --> 00:27:05,652
and it was all being discovered, new genres were being discovered.

625
00:27:05,652 --> 00:27:06,751
We weren't just making games.

626
00:27:06,751 --> 00:27:10,671
We felt we were, we were creating a new entertainment medium that never existed before

627
00:27:10,671 --> 00:27:14,324
uh, especially with these open world games and simulation games where you would co-create.

628
00:27:14,324 --> 00:27:14,411
..

629
00:27:14,411 --> 00:27:16,431
you, as the player, were co-creating the story.

630
00:27:16,431 --> 00:27:20,231
There's no other media, uh, entertainment media where you do that

631
00:27:20,231 --> 00:27:22,791
where you as the audience actually co-create the

632
00:27:22,791 --> 00:27:23,412
the story.

633
00:27:23,412 --> 00:27:25,871
And of course now with multiplayer games as well

634
00:27:25,871 --> 00:27:28,331
it can be a s- very social activity

635
00:27:28,331 --> 00:27:32,132
and can explore all kinds of interesting worlds in that.

636
00:27:32,132 --> 00:27:33,952
But on the other hand, you know

637
00:27:33,952 --> 00:27:37,771
it's very important to, um, also enjoy and experience

638
00:27:37,771 --> 00:27:39,092
uh, the physical world.

639
00:27:39,092 --> 00:27:40,651
But the question is then, you know

640
00:27:40,651 --> 00:27:45,231
I think we're gonna have to kind of confront the question again of what is the fundamental n- nature of reality

641
00:27:45,231 --> 00:27:50,711
uh, what is g- gonna be the difference between these increasingly realistic simulations and

642
00:27:50,711 --> 00:27:54,171
uh, multiplayer ones, and e- emergent, um

643
00:27:54,171 --> 00:27:55,331
and what we do in the real world?

644
00:27:55,331 --> 00:28:00,331
Yeah, there's clearly a huge amount of value to experiencing the real world

645
00:28:00,331 --> 00:28:00,971
nature.

646
00:28:00,971 --> 00:28:05,952
There's also a huge amount of value in experiencing other humans directly in person

647
00:28:05,952 --> 00:28:06,951
the way we're sitting here today.

648
00:28:06,951 --> 00:28:07,251
Yes.

649
00:28:07,251 --> 00:28:12,131
But we need to really scientifically, rigorously answer the question

650
00:28:12,131 --> 00:28:12,751
why?

651
00:28:12,751 --> 00:28:13,092
Yeah.

652
00:28:13,092 --> 00:28:13,691
Exactly.

653
00:28:13,691 --> 00:28:16,111
And which aspect of that can be mapped- Yeah.

654
00:28:16,111 --> 00:28:16,117
.

655
00:28:16,117 --> 00:28:16,131
..

656
00:28:16,131 --> 00:28:17,191
into the virtual world?

657
00:28:17,191 --> 00:28:17,691
Exactly.

658
00:28:17,691 --> 00:28:19,431
And- And it's not, it's not enough to say

659
00:28:19,431 --> 00:28:22,431
"Yeah, you should go touch grass and hang out in nature.

660
00:28:22,431 --> 00:28:23,591
" It's like, why- Yeah.

661
00:28:23,591 --> 00:28:23,591
.

662
00:28:23,591 --> 00:28:23,591
.

663
00:28:23,591 --> 00:28:24,491
exactly- Yeah.

664
00:28:24,491 --> 00:28:24,497
.

665
00:28:24,497 --> 00:28:24,511
..

666
00:28:24,511 --> 00:28:25,311
is that valuable?

667
00:28:25,311 --> 00:28:25,811
Yes.

668
00:28:25,811 --> 00:28:28,391
And I guess that's maybe the thing that's been

669
00:28:28,391 --> 00:28:30,931
uh, haunting me or obsessing me from the beginning of my career.

670
00:28:30,931 --> 00:28:32,751
If, if you think about all the different things I've done

671
00:28:32,751 --> 00:28:32,984
that's.

672
00:28:32,984 --> 00:28:33,051
..

673
00:28:33,051 --> 00:28:34,431
they're all related in that way.

674
00:28:34,431 --> 00:28:38,691
The simulation, nature of reality, and what is the bounds of

675
00:28:38,691 --> 00:28:39,831
you know, what can be modeled.

676
00:28:39,831 --> 00:28:42,591
Sorry for the ridiculous questions, but so far

677
00:28:42,591 --> 00:28:44,231
what is the greatest video game of all time?

678
00:28:44,231 --> 00:28:44,851
 What's up there?

679
00:28:44,851 --> 00:28:47,851
What, what makes- Well, my favorite one of all time is Civilization

680
00:28:47,851 --> 00:28:48,851
I, I have to say.

681
00:28:48,851 --> 00:28:54,251
That, that was the, the, the Civilization I and Civilization II are my favorite games of all time.

682
00:28:54,251 --> 00:28:59,881
Um- I can only assume you've avoided the most recent one because it would probably.

683
00:28:59,881 --> 00:29:00,031
..

684
00:29:00,031 --> 00:29:01,831
you would d- that would be your sabbatical

685
00:29:01,831 --> 00:29:02,991
that w- you would disappear.

686
00:29:02,991 --> 00:29:03,591
Yes.

687
00:29:03,591 --> 00:29:04,331
 Exactly.

688
00:29:04,331 --> 00:29:06,411
They take a lot of time, these Civilization games

689
00:29:06,411 --> 00:29:08,031
so, uh, I got to be careful with them.

690
00:29:08,031 --> 00:29:09,231
Fun question.

691
00:29:09,231 --> 00:29:13,071
You and Elon seem to be somehow solid gamers.

692
00:29:13,071 --> 00:29:16,791
Uh, is there a connection between being great at gaming and

693
00:29:16,791 --> 00:29:19,731
and, uh, being great leaders of AI companies?

694
00:29:19,731 --> 00:29:20,731
I don't know.

695
00:29:20,731 --> 00:29:20,901
I.

696
00:29:20,901 --> 00:29:20,991
..

697
00:29:20,991 --> 00:29:21,771
It's an interesting one.

698
00:29:21,771 --> 00:29:24,331
I mean, uh, we both love games and

699
00:29:24,331 --> 00:29:26,991
uh, it's interesting, he wrote games as well to start off with.

700
00:29:26,991 --> 00:29:31,651
It's probably a v- especially in the era I grew up in where home computers were.

701
00:29:31,651 --> 00:29:31,771
..

702
00:29:31,771 --> 00:29:34,351
just became a thing, you know, in the late '80s and '90s

703
00:29:34,351 --> 00:29:35,271
especially in the UK.

704
00:29:35,271 --> 00:29:38,131
I had a Spectrum and then a, a Commodore Amiga 500

705
00:29:38,131 --> 00:29:38,631
which was my- Nice.

706
00:29:38,631 --> 00:29:38,631
.

707
00:29:38,631 --> 00:29:38,631
.

708
00:29:38,631 --> 00:29:41,771
m- my favorite computer ever, and that's where I learned all my programming.

709
00:29:41,771 --> 00:29:43,811
And of course, it's a very fun thing

710
00:29:43,811 --> 00:29:46,251
uh, to program is to program games.

711
00:29:46,251 --> 00:29:49,071
So I think it's a great way to learn programming

712
00:29:49,071 --> 00:29:52,951
probably still is, and, um, and then of course

713
00:29:52,951 --> 00:29:55,631
I immediately took it in directions of AI and simulations

714
00:29:55,631 --> 00:29:55,961
which.

715
00:29:55,961 --> 00:29:56,071
..

716
00:29:56,071 --> 00:29:59,071
so I ma- was able to express my interest in

717
00:29:59,071 --> 00:30:03,691
in games, uh, and my sort of wider scientific interests all together.

718
00:30:03,691 --> 00:30:07,151
And then the final thing I think that's great about games is it fuses

719
00:30:07,151 --> 00:30:10,751
um, artistic design, you know, art

720
00:30:10,751 --> 00:30:14,231
with the, the, the most cutting-edge programming.

721
00:30:14,231 --> 00:30:17,891
Um, so again, in the '90s, all of the most interesting

722
00:30:17,891 --> 00:30:20,211
uh, technical advances were happening in gaming

723
00:30:20,211 --> 00:30:23,411
whether that was AI, graphics, physics engines

724
00:30:23,411 --> 00:30:25,491
uh, hardware, even GPUs, of course

725
00:30:25,491 --> 00:30:26,971
were designed for gaming originally.

726
00:30:26,971 --> 00:30:30,811
Um, so e- everything that was pushing computing forward in the

727
00:30:30,811 --> 00:30:33,091
in the '90s was due to gaming.

728
00:30:33,091 --> 00:30:37,211
So u- interestingly, that was where the forefront of research was going on

729
00:30:37,211 --> 00:30:39,971
and there was this incredible fusion with, with art

730
00:30:39,971 --> 00:30:43,291
um, y- you know, graphics, but also music

731
00:30:43,291 --> 00:30:45,791
and just a whole new media of storytelling

732
00:30:45,791 --> 00:30:46,671
and I love that.

733
00:30:46,671 --> 00:30:49,631
For me, it's this sort of multidisciplinary kind of effort is

734
00:30:49,631 --> 00:30:52,817
again, something I've enjoyed my whole, my whole life.

735
00:30:52,817 --> 00:30:56,595
I have to ask you, I almost forgot about w- one of the many

736
00:30:56,595 --> 00:31:00,176
and I would say one of the most incredible things recently

737
00:31:00,176 --> 00:31:03,755
uh, that somehow didn't yet get enough attention is AlphaEvolve.

738
00:31:03,755 --> 00:31:09,615
We talked about evolution a little bit, but it's the Google DeepMind system that evolves algorithms.

739
00:31:09,615 --> 00:31:09,955
Yeah.

740
00:31:09,955 --> 00:31:15,075
Are these kinds of evolution-like techniques promising as a component of a future superintelligent system?

741
00:31:15,075 --> 00:31:17,255
So for people who don't know, it's kind of

742
00:31:17,255 --> 00:31:17,543
um.

743
00:31:17,543 --> 00:31:17,736
..

744
00:31:17,736 --> 00:31:23,355
I don't know if it's fair to say it's LLM-guided evolution search.

745
00:31:23,355 --> 00:31:23,675
Yeah.

746
00:31:23,675 --> 00:31:26,695
So it's ev- e- evolution algorithms are doing the search

747
00:31:26,695 --> 00:31:28,635
and LLMs are telling you where.

748
00:31:28,635 --> 00:31:29,596
Yes, exactly.

749
00:31:29,596 --> 00:31:32,496
So LLMs are kind of proposing some possible solutions

750
00:31:32,496 --> 00:31:36,015
and then you do, you use evolutionary computing on top to

751
00:31:36,015 --> 00:31:38,555
to, to find some novel part of the

752
00:31:38,555 --> 00:31:39,775
of the search space.

753
00:31:39,775 --> 00:31:43,895
So actually, I think it's an example of very promising directions

754
00:31:43,895 --> 00:31:49,935
where you combine LLMs or foundation models with other computational techniques.

755
00:31:49,935 --> 00:31:53,955
Evolutionary methods is one, but you could also imagine Monte Carlo tree search.

756
00:31:53,955 --> 00:31:59,395
Basically, many types of search algorithms or reasoning algorithms sort of on top of

757
00:31:59,395 --> 00:32:02,175
or using, the foundation models as a basis.

758
00:32:02,175 --> 00:32:05,235
So I actually think there's quite a lot of interesting

759
00:32:05,235 --> 00:32:08,935
uh, things to be discovered probably with these sort of hybrid systems

760
00:32:08,935 --> 00:32:09,535
let's call them.

761
00:32:09,535 --> 00:32:12,655
But not to romanticize evolution- Yeah.

762
00:32:12,655 --> 00:32:12,668
.

763
00:32:12,668 --> 00:32:12,695
..

764
00:32:12,695 --> 00:32:17,115
I'm only human, but do, do you think there's some value in whatever that mechanism is?

765
00:32:17,115 --> 00:32:18,995
'Cause we already talked about natural systems.

766
00:32:18,995 --> 00:32:25,855
Do you think where there's a lot of low-hanging fruit of us understanding being mo- being able to model

767
00:32:25,855 --> 00:32:29,975
um, being able to simulate evolution and then using that

768
00:32:29,975 --> 00:32:34,975
whatever we understand about that nature-inspired mechanism to

769
00:32:34,975 --> 00:32:36,955
to then do search better and better and better?

770
00:32:36,955 --> 00:32:37,075
Yes.

771
00:32:37,075 --> 00:32:38,835
So if you think about, uh, again

772
00:32:38,835 --> 00:32:41,815
br- a b- uh, breaking down the sys- sort of systems we've built

773
00:32:41,815 --> 00:32:44,455
uh, to their really fundamental core, you've got

774
00:32:44,455 --> 00:32:48,615
like, the model of the, of the underlying dynamics of the system.

775
00:32:48,615 --> 00:32:51,555
Uh, and then if you want to discover something new

776
00:32:51,555 --> 00:32:54,575
something novel that hasn't been seen before, um

777
00:32:54,575 --> 00:32:59,835
then you need some kind of search process on top to take you to a novel region of the

778
00:32:59,835 --> 00:33:01,715
of the, of the search space.

779
00:33:01,715 --> 00:33:04,455
And, um, you can do that in a number of ways.

780
00:33:04,455 --> 00:33:06,055
Evolutionary computing is one.

781
00:33:06,055 --> 00:33:09,175
Um, with AlphaGo, we just used Monte Carlo tree search

782
00:33:09,175 --> 00:33:09,715
right?

783
00:33:09,715 --> 00:33:12,315
And that's what found Move 37, the new

784
00:33:12,315 --> 00:33:15,575
uh, kind of never-seen-before strategy in Go.

785
00:33:15,575 --> 00:33:19,175
And so that's how you can go beyond potentially what is already known.

786
00:33:19,175 --> 00:33:22,295
So the model can model everything that you currently know about

787
00:33:22,295 --> 00:33:22,535
right?

788
00:33:22,535 --> 00:33:24,035
All the data that you currently have.

789
00:33:24,035 --> 00:33:25,595
But then how do you go beyond that?

790
00:33:25,595 --> 00:33:28,275
So that starts to speak about the ideas of creativity.

791
00:33:28,275 --> 00:33:32,055
How can these systems create something new, fi- discover something new?

792
00:33:32,055 --> 00:33:35,415
Obviously, this is super relevant for scientific discovery or pushing med.

793
00:33:35,415 --> 00:33:35,475
..

794
00:33:35,475 --> 00:33:38,215
science and medicine forward, which we want to do with these systems.

795
00:33:38,215 --> 00:33:41,755
And you can actually bolt on some, uh

796
00:33:41,755 --> 00:33:48,455
fairly simple search systems on top of these models and get you into a new region of space.

797
00:33:48,455 --> 00:33:50,075
Of course, you also have to, um

798
00:33:50,075 --> 00:33:54,275
make sure that, uh, you're not searching that space totally randomly or it would be too big.

799
00:33:54,275 --> 00:33:58,315
So you have to have some objective function that you're trying to optimize and hill climb towards

800
00:33:58,315 --> 00:34:00,035
and that guides that search.

801
00:34:00,035 --> 00:34:03,055
But there's some mechanism of evolution that are interesting

802
00:34:03,055 --> 00:34:09,034
maybe in the space of programs, but then the space of programs is an extremely important space because you can probably generalize to

803
00:34:09,034 --> 00:34:10,534
uh, to everything.

804
00:34:10,534 --> 00:34:10,915
You know?

805
00:34:10,915 --> 00:34:12,295
Uh, but, you know, for example

806
00:34:12,295 --> 00:34:13,074
mutation.

807
00:34:13,074 --> 00:34:18,835
So it's not just Monte Carlo tree search where it's like a search.

808
00:34:18,835 --> 00:34:19,155
Mm-hmm.

809
00:34:19,155 --> 00:34:22,195
You could, every once in a while alt- Combine things

810
00:34:22,195 --> 00:34:22,335
yeah.

811
00:34:22,335 --> 00:34:23,074
Combine things- Yeah.

812
00:34:23,074 --> 00:34:23,074
.

813
00:34:23,074 --> 00:34:23,074
.

814
00:34:23,074 --> 00:34:24,574
alter c- like, sub.

815
00:34:24,574 --> 00:34:24,695
..

816
00:34:24,695 --> 00:34:26,155
like components of a thing.

817
00:34:26,155 --> 00:34:26,514
Yes.

818
00:34:26,514 --> 00:34:31,495
So then, you know, what evolution is really good at is not just the natural selection.

819
00:34:31,495 --> 00:34:37,735
It's combining things and building increasingly complex hierar- hierarchical systems.

820
00:34:37,735 --> 00:34:38,034
Yes.

821
00:34:38,034 --> 00:34:40,255
So that component's super interesting- Yeah.

822
00:34:40,255 --> 00:34:40,315
.

823
00:34:40,315 --> 00:34:40,435
..

824
00:34:40,435 --> 00:34:43,094
especially like with AlphaEvolve and the space of programs.

825
00:34:43,094 --> 00:34:44,014
Yeah, exactly.

826
00:34:44,014 --> 00:34:47,875
So there's a b- You can get a bit of an extra property out of evolutionary systems

827
00:34:47,875 --> 00:34:51,074
which is some new emergent capability may come about.

828
00:34:51,074 --> 00:34:51,175
Mm-hmm.

829
00:34:51,175 --> 00:34:51,255
Yes.

830
00:34:51,255 --> 00:34:51,415
Right?

831
00:34:51,415 --> 00:34:53,415
Of course, like happened with life.

832
00:34:53,415 --> 00:34:59,835
Interestingly, with naive, uh, sort of traditional evolutionary computing methods without LLMs and the modern AI

833
00:34:59,835 --> 00:35:00,975
the problem with them were.

834
00:35:00,975 --> 00:35:01,015
..

835
00:35:01,015 --> 00:35:01,035
they were.

836
00:35:01,035 --> 00:35:01,035
..

837
00:35:01,035 --> 00:35:01,455
there was a.

838
00:35:01,455 --> 00:35:01,515
..

839
00:35:01,515 --> 00:35:03,535
they were very well studied in the '90s an

840
00:35:03,535 --> 00:35:06,695
an, an, an early 2000s and some promising results.

841
00:35:06,695 --> 00:35:10,955
But the problem was they could never work out how to evolve new properties

842
00:35:10,955 --> 00:35:12,155
new emergent properties.

843
00:35:12,155 --> 00:35:15,195
You always had a sort of subset of the properties that you put into the system.

844
00:35:15,195 --> 00:35:18,995
But maybe if we combine them with these foundation models

845
00:35:18,995 --> 00:35:20,995
perhaps we can overcome that limitation.

846
00:35:20,995 --> 00:35:26,055
Obviously, uh, natural evolution clearly did, because it evo- it did evolve new capabilities

847
00:35:26,055 --> 00:35:26,575
right?

848
00:35:26,575 --> 00:35:28,475
So, uh, bacteria to where we are now.

849
00:35:28,475 --> 00:35:29,501
So clearly the.

850
00:35:29,501 --> 00:35:29,555
..

851
00:35:29,555 --> 00:35:34,035
it must be possible with evolutionary systems to generate

852
00:35:34,035 --> 00:35:37,675
uh, new patterns, you know, f- going back to the first thing we talked about

853
00:35:37,675 --> 00:35:41,115
and, uh, new capabilities and emergent properties.

854
00:35:41,115 --> 00:35:43,935
And maybe we're on the cusp of discovering how to do that.

855
00:35:43,935 --> 00:35:48,215
Yeah, listen, uh, AlphaEvolve is one of the coolest things I've ever seen.

856
00:35:48,215 --> 00:35:48,931
I've, I've, uh.

857
00:35:48,931 --> 00:35:49,075
..

858
00:35:49,075 --> 00:35:50,415
Uh, uh, on my desk at home

859
00:35:50,415 --> 00:35:53,695
you know, most of my time is spent behind that computer just programming.

860
00:35:53,695 --> 00:35:58,975
And next to the, the three screens is a s- a skull of a

861
00:35:58,975 --> 00:36:05,315
a Tiktaalik which is one of the early organisms that crawled out of the water onto land.

862
00:36:05,315 --> 00:36:09,075
And I just kind of watch that little guy.

863
00:36:09,075 --> 00:36:11,181
 And it's like you.

864
00:36:11,181 --> 00:36:11,255
..

865
00:36:11,255 --> 00:36:11,488
The, the.

866
00:36:11,488 --> 00:36:11,495
..

867
00:36:11,495 --> 00:36:16,535
whatever the computation mechanism of evolution is, it's quite incredible.

868
00:36:16,535 --> 00:36:16,815
Yes.

869
00:36:16,815 --> 00:36:18,415
It's truly, truly incredible.

870
00:36:18,415 --> 00:36:18,695
Yeah.

871
00:36:18,695 --> 00:36:22,055
Now, whether that's exactly the thing we need to do to do our search

872
00:36:22,055 --> 00:36:25,715
but never, never, uh, dismiss the power of nature what

873
00:36:25,715 --> 00:36:26,475
what it did here.

874
00:36:26,475 --> 00:36:26,855
Yeah.

875
00:36:26,855 --> 00:36:28,015
And it's amazing.

876
00:36:28,015 --> 00:36:30,855
Um, whi- which is a relatively simple algorithm

877
00:36:30,855 --> 00:36:31,475
right?

878
00:36:31,475 --> 00:36:32,295
Effectively.

879
00:36:32,295 --> 00:36:35,015
And it can generate all of this immense complexity

880
00:36:35,015 --> 00:36:35,895
emerges.

881
00:36:35,895 --> 00:36:39,115
Obviously running over, you know, 4 billion years of time

882
00:36:39,115 --> 00:36:40,883
but, but it's, it's, it's.

883
00:36:40,883 --> 00:36:41,015
..

884
00:36:41,015 --> 00:36:42,255
You know, you can think about that as

885
00:36:42,255 --> 00:36:47,675
again, a, a, a proc- a search process that ran over the physics substrate of the universe for

886
00:36:47,675 --> 00:36:49,635
uh, a long amount of computational time.

887
00:36:49,635 --> 00:36:52,455
But then it generated all this incredible, uh

888
00:36:52,455 --> 00:36:54,179
rich diversity.

889
00:36:54,179 --> 00:36:56,139
So, uh, so many questions I wanna ask you.

890
00:36:56,139 --> 00:36:57,819
So one, you do have a dream.

891
00:36:57,819 --> 00:37:00,300
One of the natural systems you want to

892
00:37:00,300 --> 00:37:02,780
uh, try to model is a, is a cell.

893
00:37:02,780 --> 00:37:03,339
Yes.

894
00:37:03,339 --> 00:37:04,419
That's a beautiful dream.

895
00:37:04,419 --> 00:37:06,999
Uh, I could ask you about that.

896
00:37:06,999 --> 00:37:10,779
I also just, for that purpose, on the AI scientist front

897
00:37:10,779 --> 00:37:11,820
just broadly.

898
00:37:11,820 --> 00:37:15,719
So there's a essay, uh, from Daniel Kakutaiyo

899
00:37:15,719 --> 00:37:22,419
Scott Alexander, and others that outline steps along the way to get to ASI and has a lot of interesting ideas

900
00:37:22,419 --> 00:37:22,939
uh, in it.

901
00:37:22,939 --> 00:37:31,379
One of which is, uh, including a superhuman coder and a superhuman AI researcher and in that

902
00:37:31,379 --> 00:37:34,719
there's a term of research taste that's really interesting.

903
00:37:34,719 --> 00:37:48,379
So in everything you've seen, do you think it's possible for AI systems to have research taste to help you in the way that AI co-scientist does to help steer human

904
00:37:48,379 --> 00:37:56,359
um, human brilliant scientists and then bo- potentially by itself to figure out what are the directions

905
00:37:56,359 --> 00:37:59,559
eh, where you want to gen- generate truly novel ideas?

906
00:37:59,559 --> 00:38:04,039
'Cause that seems to be like a really important component of how to do great science.

907
00:38:04,039 --> 00:38:04,259
Yeah.

908
00:38:04,259 --> 00:38:06,539
I think that's gonna be one of the hardest things to

909
00:38:06,539 --> 00:38:09,519
to, uh, mimic or model is, is this

910
00:38:09,519 --> 00:38:11,559
this idea of taste or, or judgment.

911
00:38:11,559 --> 00:38:14,199
I think that's what separates the, you know

912
00:38:14,199 --> 00:38:16,299
the, the great scientists from the good scientists.

913
00:38:16,299 --> 00:38:18,779
Like all, all professional scientists are good technically

914
00:38:18,779 --> 00:38:18,939
right?

915
00:38:18,939 --> 00:38:20,699
Otherwise they wouldn't have bi- made it, uh

916
00:38:20,699 --> 00:38:23,059
that far in, in academia and things like that.

917
00:38:23,059 --> 00:38:27,839
But then do you have the taste to sort of sniff out what the right direction is

918
00:38:27,839 --> 00:38:30,159
what the right experiment is, what the right question is?

919
00:38:30,159 --> 00:38:31,199
So the qu- it's the, it's.

920
00:38:31,199 --> 00:38:31,279
..

921
00:38:31,279 --> 00:38:34,059
Picking the right question is, is the hardest part of science

922
00:38:34,059 --> 00:38:37,519
um, and, and making the right hypothesis and

923
00:38:37,519 --> 00:38:41,099
um, that's what, you know, today's systems definitely they can't do.

924
00:38:41,099 --> 00:38:45,019
So, you know, I often say it's harder to come up with a conjecture

925
00:38:45,019 --> 00:38:47,259
a really good conjecture than it is to solve it.

926
00:38:47,259 --> 00:38:50,619
So we may have systems soon that can solve pretty hard conjectures.

927
00:38:50,619 --> 00:38:52,469
Um, you know, I, I.

928
00:38:52,469 --> 00:38:52,599
..

929
00:38:52,599 --> 00:38:55,079
Um, in math Olympiad problems where we

930
00:38:55,079 --> 00:38:57,579
we, you know, Alpha proved last year our system got

931
00:38:57,579 --> 00:38:58,999
you know, silver medal in that.

932
00:38:58,999 --> 00:39:00,119
Really hard problems.

933
00:39:00,119 --> 00:39:03,159
Maybe eventually we'll be able to solve a Millennium Prize kind of problem

934
00:39:03,159 --> 00:39:09,459
but could a system come up with a conjecture worthy of study that someone like Terence Tao would have gone

935
00:39:09,459 --> 00:39:09,979
"You know what?

936
00:39:09,979 --> 00:39:15,559
That's a really deep question about the nature of maths or the nature of numbers or the nature of physics"?

937
00:39:15,559 --> 00:39:20,559
And that is far harder type of creativity and we don't really know.

938
00:39:20,559 --> 00:39:20,679
..

939
00:39:20,679 --> 00:39:25,199
Today's systems clearly can't do that and we're not quite sure what that mechanism would be.

940
00:39:25,199 --> 00:39:29,239
This kind of leap of imagination like, like Einstein had when he came up with

941
00:39:29,239 --> 00:39:33,079
you know, special relativity and then general relativity with the knowledge he had at the time.

942
00:39:33,079 --> 00:39:36,585
As for, for conjecture, the.

943
00:39:36,585 --> 00:39:36,819
..

944
00:39:36,819 --> 00:39:41,439
You want to come up with a thing that's interesting and it's amenable to proof.

945
00:39:41,439 --> 00:39:41,839
Yes.

946
00:39:41,839 --> 00:39:44,779
So, like, it's easy to come up with a thing that's extremely difficult.

947
00:39:44,779 --> 00:39:45,059
Yeah.

948
00:39:45,059 --> 00:39:49,619
It's easy to come up with a thing that's extremely easy but tha- at that very edge- That sweet spot

949
00:39:49,619 --> 00:39:49,939
right?

950
00:39:49,939 --> 00:39:54,799
Of, of basically advancing the science and splitting the hypothesis space into two ideally

951
00:39:54,799 --> 00:39:55,019
right?

952
00:39:55,019 --> 00:39:56,859
Whether if it's true or not true, you

953
00:39:56,859 --> 00:40:00,159
you've learnt something really useful and, um

954
00:40:00,159 --> 00:40:04,499
and, and that's hard and, and, and making something that's also

955
00:40:04,499 --> 00:40:09,839
uh, you know, falsifiable and within sort of the technologies that you have

956
00:40:09,839 --> 00:40:11,199
you currently have available.

957
00:40:11,199 --> 00:40:16,199
So it's a very creative process actually, highly creative process that

958
00:40:16,199 --> 00:40:21,039
um, I, I think just the kind of naive search on top of a model won't be enough for that.

959
00:40:21,039 --> 00:40:21,799
Okay.

960
00:40:21,799 --> 00:40:24,959
The idea of splitting the hypothesis space in two is super interesting.

961
00:40:24,959 --> 00:40:29,095
So, uh, I've heard you say that there's basically no failure in.

962
00:40:29,095 --> 00:40:29,279
..

963
00:40:29,279 --> 00:40:32,210
Or failure's extremely valuable if it's done.

964
00:40:32,210 --> 00:40:32,359
..

965
00:40:32,359 --> 00:40:34,979
if you construct the questions right, if you construct the experiments right

966
00:40:34,979 --> 00:40:39,079
if you design them right, that failure and success are both useful.

967
00:40:39,079 --> 00:40:40,279
So- Yes.

968
00:40:40,279 --> 00:40:40,285
.

969
00:40:40,285 --> 00:40:40,299
..

970
00:40:40,299 --> 00:40:42,859
perhaps because it splits the hypothesis base in two.

971
00:40:42,859 --> 00:40:43,879
It's like a binary- Yes.

972
00:40:43,879 --> 00:40:43,879
.

973
00:40:43,879 --> 00:40:43,879
.

974
00:40:43,879 --> 00:40:44,199
search?

975
00:40:44,199 --> 00:40:44,859
That's right.

976
00:40:44,859 --> 00:40:46,379
So when you do, like, you know

977
00:40:46,379 --> 00:40:52,679
real blue sky research, there's no such thing as failure really as long as you're picking experiments and hypotheses that

978
00:40:52,679 --> 00:40:56,119
that, that, that meaningfully split the hypothesis space so

979
00:40:56,119 --> 00:41:00,579
you know, and you learn something, you can learn something kind of equally valuable from a

980
00:41:00,579 --> 00:41:02,039
an experiment that doesn't work.

981
00:41:02,039 --> 00:41:05,799
That should tell you if you've designed an experiment well and your hypotheses are

982
00:41:05,799 --> 00:41:08,799
are interesting, it should tell you a lot about where to go next.

983
00:41:08,799 --> 00:41:10,979
And, um, and then it's j- you're

984
00:41:10,979 --> 00:41:13,399
you're effectively doing a search process, um

985
00:41:13,399 --> 00:41:16,119
and using that information in, in, you know

986
00:41:16,119 --> 00:41:16,979
very helpful ways.

987
00:41:16,979 --> 00:41:21,499
So to go to your dream of, uh

988
00:41:21,499 --> 00:41:26,619
modeling a cell, um, what are the big challenges that lay ahead for us to make that happen?

989
00:41:26,619 --> 00:41:29,012
We should maybe highlight that AlphaFold.

990
00:41:29,012 --> 00:41:29,099
..

991
00:41:29,099 --> 00:41:30,899
I mean, there's just so many leaps.

992
00:41:30,899 --> 00:41:31,399
Yeah.

993
00:41:31,399 --> 00:41:34,299
So AlphaFold solved, if it's fair to say

994
00:41:34,299 --> 00:41:38,479
protein folding and there's so many incredible things we could talk about there including the open sourcing

995
00:41:38,479 --> 00:41:40,979
uh, the, everything you've released.

996
00:41:40,979 --> 00:41:45,219
AlphaFold3 is doing protein RNA, DNA interactions- Mm-hmm.

997
00:41:45,219 --> 00:41:45,319
.

998
00:41:45,319 --> 00:41:45,519
..

999
00:41:45,519 --> 00:41:48,439
which is super complicated and, and fascinating.

1000
00:41:48,439 --> 00:41:50,419
There's a amenable to modeling.

1001
00:41:50,419 --> 00:41:54,739
Alpha Genome, uh, predicts how small genetic changes

1002
00:41:54,739 --> 00:41:57,639
like if we think about single mutations, how they link to actual

1003
00:41:57,639 --> 00:41:59,099
uh, function.

1004
00:41:59,099 --> 00:42:01,212
So, um, those are.

1005
00:42:01,212 --> 00:42:01,279
..

1006
00:42:01,279 --> 00:42:03,039
It seems like it's creeping along .

1007
00:42:03,039 --> 00:42:03,279
Yes.

1008
00:42:03,279 --> 00:42:04,043
So sophistic8.

1009
00:42:04,043 --> 00:42:04,139
..

1010
00:42:04,139 --> 00:42:10,759
To, to much more complicated, uh, things like a cell but a cell has a lot of really complicated components.

1011
00:42:10,759 --> 00:42:10,959
Yeah.

1012
00:42:10,959 --> 00:42:16,559
So what I've tried to do throughout my career is I have these really grand dreams and then I try to

1013
00:42:16,559 --> 00:42:19,479
as you've noticed, and then I try to break but I try to break them down.

1014
00:42:19,479 --> 00:42:21,419
Uh, an- you know, it's easy to have a kind of

1015
00:42:21,419 --> 00:42:24,319
a, a c- crazy ambitious dream but the

1016
00:42:24,319 --> 00:42:27,199
the, the trick is how do you break it down into manageable

1017
00:42:27,199 --> 00:42:32,499
achievable, uh, interim steps that are meaningful and useful in their own right?

1018
00:42:32,499 --> 00:42:36,139
And so Virtual Cell, which is what I call the project of modeling a cell

1019
00:42:36,139 --> 00:42:37,979
uh, I've had this idea, you know

1020
00:42:37,979 --> 00:42:41,839
of wanting to do that for maybe more like 25 years and

1021
00:42:41,839 --> 00:42:45,659
uh, I used to talk with Paul Nurse who is a bit of a mentor of mine in biology.

1022
00:42:45,659 --> 00:42:48,339
He runs the, the, you know, he founded the Crick Institute and

1023
00:42:48,339 --> 00:42:50,559
and won the Nobel Prize in, in 2001.

1024
00:42:50,559 --> 00:42:52,323
Uh, i- i- is, is.

1025
00:42:52,323 --> 00:42:52,499
..

1026
00:42:52,499 --> 00:42:54,025
We've been talking about it since-.

1027
00:42:54,025 --> 00:42:54,199
..

1028
00:42:54,199 --> 00:42:55,539
you know, t- before the- you know

1029
00:42:55,539 --> 00:42:56,260
in the 90s.

1030
00:42:56,260 --> 00:42:59,400
And, um, and I come, used to come back to it every five years

1031
00:42:59,400 --> 00:43:06,839
is like, what would you need to model of the full internals of a cell so that you could do experiments on the virtual cell and what those experiment

1032
00:43:06,839 --> 00:43:12,480
p- you know, in silico, and those predictions would be useful for you to save you a lot of time in the wet lab

1033
00:43:12,480 --> 00:43:12,919
right?

1034
00:43:12,919 --> 00:43:13,659
That would be the dream.

1035
00:43:13,659 --> 00:43:17,739
Maybe you could 100X speed up experiments by doing most of it in silico.

1036
00:43:17,739 --> 00:43:21,200
The search in silico and then you do the validation step in the wet lab.

1037
00:43:21,200 --> 00:43:23,119
That would be, that's the, that's the dream.

1038
00:43:23,119 --> 00:43:25,899
And so, um, but maybe now, finally

1039
00:43:25,899 --> 00:43:28,000
uh, so I was trying to build these components

1040
00:43:28,000 --> 00:43:30,660
AlphaFold being one, that, that would, uh

1041
00:43:30,660 --> 00:43:34,120
allow you eventually to model the full interaction

1042
00:43:34,120 --> 00:43:36,400
a full simulation of a cell.

1043
00:43:36,400 --> 00:43:38,339
And I'd probably start with a yeast cell

1044
00:43:38,339 --> 00:43:43,499
and partly that's what Paul Nurse studied, because a yeast cell is like a full organism that's a single cell

1045
00:43:43,499 --> 00:43:43,899
right?

1046
00:43:43,899 --> 00:43:46,699
So it's a kind of simplest single cell organism.

1047
00:43:46,699 --> 00:43:49,259
And so it's not just a cell, it's a full organism.

1048
00:43:49,259 --> 00:43:55,979
And, um, and yeast is very well understood and so that would be a good candidate for uh

1049
00:43:55,979 --> 00:43:58,159
uh, uh a kind of full simulated model.

1050
00:43:58,159 --> 00:44:03,059
Now, AlphaFold is the, is the solution to the kind of static picture of what does a

1051
00:44:03,059 --> 00:44:05,719
what does a protein look, 3D structure protein look like

1052
00:44:05,719 --> 00:44:06,979
a static picture of it.

1053
00:44:06,979 --> 00:44:10,379
But we know that biology, all the interesting things happen with the dynamics

1054
00:44:10,379 --> 00:44:11,199
the interactions.

1055
00:44:11,199 --> 00:44:14,439
And that's what AlphaFold3 is, is the first step towards

1056
00:44:14,439 --> 00:44:15,939
is modeling those interactions.

1057
00:44:15,939 --> 00:44:18,039
So first of all pairwise, you know

1058
00:44:18,039 --> 00:44:20,539
proteins with proteins, proteins with RNA and DNA.

1059
00:44:20,539 --> 00:44:24,519
But then, um, the next step after that would be modeling maybe a whole pathway

1060
00:44:24,519 --> 00:44:27,939
maybe like the TOR pathway that's involved in cancer or something like this

1061
00:44:27,939 --> 00:44:30,279
and then eventually you might be able to model

1062
00:44:30,279 --> 00:44:31,279
you know, a whole cell.

1063
00:44:31,279 --> 00:44:36,199
Also, there's another complexity here that stuff in a cell happens at different timescales.

1064
00:44:36,199 --> 00:44:38,059
Is that tricky?

1065
00:44:38,059 --> 00:44:40,159
It's like there, you know, protein, uh

1066
00:44:40,159 --> 00:44:43,059
folding is, you know, super fast.

1067
00:44:43,059 --> 00:44:43,359
Yes.

1068
00:44:43,359 --> 00:44:46,520
Um, I don't know all the biological mechanisms- Yeah.

1069
00:44:46,520 --> 00:44:46,526
.

1070
00:44:46,526 --> 00:44:46,540
..

1071
00:44:46,540 --> 00:44:47,779
but some of them take a long time.

1072
00:44:47,779 --> 00:44:47,980
Yeah.

1073
00:44:47,980 --> 00:44:49,639
And so is that, that's a level

1074
00:44:49,639 --> 00:44:52,859
so the levels of interaction has a different temporal scale- Yeah.

1075
00:44:52,859 --> 00:44:52,865
.

1076
00:44:52,865 --> 00:44:52,879
..

1077
00:44:52,879 --> 00:44:54,059
that you have to be able to model.

1078
00:44:54,059 --> 00:44:54,959
So that would be hard.

1079
00:44:54,959 --> 00:45:00,119
So you'd probably need several simulated systems that can interact at these different temporal dynamics

1080
00:45:00,119 --> 00:45:02,799
or at least, uh, maybe it's like a hierarchical system

1081
00:45:02,799 --> 00:45:05,319
so, um, you can jump up or down the

1082
00:45:05,319 --> 00:45:06,979
the different temporal stages.

1083
00:45:06,979 --> 00:45:12,859
So can you avoid, I mean one of the challenges here is not

1084
00:45:12,859 --> 00:45:15,699
avoid simulating, for example, the, the

1085
00:45:15,699 --> 00:45:18,199
the quantum mechanical aspects of any of this

1086
00:45:18,199 --> 00:45:18,559
right?

1087
00:45:18,559 --> 00:45:20,019
You want to not over model.

1088
00:45:20,019 --> 00:45:27,519
You could skip ahead to just model the really high level things that get you a really good estimate of what's going to happen.

1089
00:45:27,519 --> 00:45:27,579
Yes.

1090
00:45:27,579 --> 00:45:36,879
So you, you got to make a decision when you're modeling any natural system what is the cutoff level of the granularity that you're going to model it to that then captures the dynamics that you're interested in.

1091
00:45:36,879 --> 00:45:41,159
So probably for a cell I, I would hope that would be the protein level

1092
00:45:41,159 --> 00:45:44,719
uh, and that one wouldn't have to go down to the atomic level.

1093
00:45:44,719 --> 00:45:49,239
Um, so, you know, but of course that's where AlphaFold stuff kicks in.

1094
00:45:49,239 --> 00:45:51,419
So that would be kind of the basis

1095
00:45:51,419 --> 00:45:53,759
and then you'd build these, um, uh

1096
00:45:53,759 --> 00:46:00,319
higher level simulations that, um, take those as building blocks and then you get the emergent behavior.

1097
00:46:00,319 --> 00:46:03,739
Apologize for the pothead questions ahead of time but

1098
00:46:03,739 --> 00:46:05,145
uh, will-  .

1099
00:46:05,145 --> 00:46:05,159
..

1100
00:46:05,159 --> 00:46:10,479
do you think, uh, we'll be able to simulate or model the origin of life?

1101
00:46:10,479 --> 00:46:15,439
So being able to simulate the first from

1102
00:46:15,439 --> 00:46:19,519
from non-living organisms the, the birth of a living organism.

1103
00:46:19,519 --> 00:46:21,119
I think that's, uh, one of the

1104
00:46:21,119 --> 00:46:23,819
of course one of the deepest and most fascinating questions.

1105
00:46:23,819 --> 00:46:25,959
Um, I love that area of biology.

1106
00:46:25,959 --> 00:46:29,219
You know, uh, there's people, like there's a great book by Nick Lane

1107
00:46:29,219 --> 00:46:31,979
one of the top, top experts in this area called The

1108
00:46:31,979 --> 00:46:34,759
The Ten Great Inventions of, of, of Evolution.

1109
00:46:34,759 --> 00:46:38,759
I think it's fantastic and it also speaks to what the great filters might be be- you know

1110
00:46:38,759 --> 00:46:40,679
prior or are they ahead of us.

1111
00:46:40,679 --> 00:46:43,519
I think, I think they're most likely in the past if you read that book

1112
00:46:43,519 --> 00:46:45,719
of how unlikely to go, you know

1113
00:46:45,719 --> 00:46:52,979
have any life at all and then single cell to multi-cell seems an unbelievably big jump that took like a billion years I think- Yeah.

1114
00:46:52,979 --> 00:46:52,985
.

1115
00:46:52,985 --> 00:46:52,999
..

1116
00:46:52,999 --> 00:46:53,979
and on earth to do, right?

1117
00:46:53,979 --> 00:46:55,199
So it shows you how hard it was

1118
00:46:55,199 --> 00:46:55,399
right?

1119
00:46:55,399 --> 00:46:57,159
 Bacteria were super happy for a very long time.

1120
00:46:57,159 --> 00:46:59,939
For a very long time before they captured mitochondria somehow

1121
00:46:59,939 --> 00:47:00,219
right?

1122
00:47:00,219 --> 00:47:04,659
I don't see why not, why AI couldn't help with that

1123
00:47:04,659 --> 00:47:05,719
some kind of simulation.

1124
00:47:05,719 --> 00:47:10,019
Again, it's again, it's a bit of a search process through a combinatorial space.

1125
00:47:10,019 --> 00:47:12,859
Here's like all the, you know, the chemical soup that

1126
00:47:12,859 --> 00:47:14,879
that you start with, the primordial soup that

1127
00:47:14,879 --> 00:47:17,519
you know, maybe was on earth near these hot vents

1128
00:47:17,519 --> 00:47:19,079
here's some initial conditions.

1129
00:47:19,079 --> 00:47:22,539
Can you, uh, generate something that looks like a cell?

1130
00:47:22,539 --> 00:47:26,459
So perhaps that would be a next stage after the Virtual Cell project is well how

1131
00:47:26,459 --> 00:47:30,719
how could you actually, um, something like that emerge from the chemical soup?

1132
00:47:30,719 --> 00:47:34,499
Well, I would love it if there was a Move 37 for the origin of life.

1133
00:47:34,499 --> 00:47:34,859
Yeah.

1134
00:47:34,859 --> 00:47:37,559
 I think that's one of the sort of great mysteries.

1135
00:47:37,559 --> 00:47:40,299
I think ultimately what we'll figure out is they're a continuum

1136
00:47:40,299 --> 00:47:42,899
there's no such thing as a line between non-living and living.

1137
00:47:42,899 --> 00:47:45,059
But if we can make that rigorous- Yes.

1138
00:47:45,059 --> 00:47:45,099
.

1139
00:47:45,099 --> 00:47:45,179
..

1140
00:47:45,179 --> 00:47:48,439
th- that the very thing from the bi- Big Bang to today

1141
00:47:48,439 --> 00:47:50,039
it's been the same process.

1142
00:47:50,039 --> 00:47:55,639
If we can break down that wall that we've constructed in our minds of the actual origin of

1143
00:47:55,639 --> 00:47:58,279
from non-living to living and it's not a line

1144
00:47:58,279 --> 00:48:02,259
that it's a continuum that connects physics and chemistry and biology.

1145
00:48:02,259 --> 00:48:02,559
Yeah.

1146
00:48:02,559 --> 00:48:03,619
There's no line.

1147
00:48:03,619 --> 00:48:11,299
I mean this is my whole reason why I worked on AI and AGI my whole life because I think it can be the ultimate tool to help us answer these kind of questions.

1148
00:48:11,299 --> 00:48:13,919
And I don't really understand why, um

1149
00:48:13,919 --> 00:48:16,759
you know, the average person doesn't think

1150
00:48:16,759 --> 00:48:18,619
like worry about this stuff more.

1151
00:48:18,619 --> 00:48:24,399
 Like how, how can we not have a good definition of life and non- and non- living and non-living and-  .

1152
00:48:24,399 --> 00:48:24,399
..

1153
00:48:24,399 --> 00:48:28,459
the nature of time and let alone consciousness and gravity and all these things.

1154
00:48:28,459 --> 00:48:31,019
It's, it's just and quantum mechanics weirdness.

1155
00:48:31,019 --> 00:48:33,739
It's just, to me it's, I've always had this

1156
00:48:33,739 --> 00:48:35,959
this sort of screaming at me in my face.

1157
00:48:35,959 --> 00:48:38,099
 The whole, and that scream is getting louder.

1158
00:48:38,099 --> 00:48:40,599
You know it's like how, what is going on here?

1159
00:48:40,599 --> 00:48:44,079
You know in, in, and I mean that in the deeper sense like in the

1160
00:48:44,079 --> 00:48:47,259
you know, the nature of reality which has to be the ultimate question- Yeah.

1161
00:48:47,259 --> 00:48:47,265
.

1162
00:48:47,265 --> 00:48:47,279
..

1163
00:48:47,279 --> 00:48:48,879
uh, that would answer all of these things.

1164
00:48:48,879 --> 00:48:50,299
It's sort of crazy if you think about it.

1165
00:48:50,299 --> 00:48:56,379
We can stare at each other and e- all these living things all the time we can inspect it with microscopes and take it apart

1166
00:48:56,379 --> 00:49:00,979
uh, almost down to the atomic level and yet we still can't answer that clearly- Yeah.

1167
00:49:00,979 --> 00:49:00,985
.

1168
00:49:00,985 --> 00:49:00,999
..

1169
00:49:00,999 --> 00:49:03,919
in a simple way, that question of how do you define living?

1170
00:49:03,919 --> 00:49:04,524
Yeah.

1171
00:49:04,524 --> 00:49:04,710
..

1172
00:49:04,710 --> 00:49:05,703
. it's kind of amazing.

1173
00:49:05,703 --> 00:49:05,983
Yeah.

1174
00:49:05,983 --> 00:49:09,224
Living, you can kind of talk your way out of thinking about

1175
00:49:09,224 --> 00:49:10,724
but, like, consciousness?

1176
00:49:10,724 --> 00:49:14,303
Like we have this very obviously subjective conscious experience

1177
00:49:14,303 --> 00:49:16,163
like we're at the center of our own world and it

1178
00:49:16,163 --> 00:49:19,263
it feels like something and then how, how

1179
00:49:19,263 --> 00:49:21,183
how are you not screaming-  Yeah.

1180
00:49:21,183 --> 00:49:21,283
.

1181
00:49:21,283 --> 00:49:21,483
..

1182
00:49:21,483 --> 00:49:22,743
at the mystery of it all?

1183
00:49:22,743 --> 00:49:22,884
Right.

1184
00:49:22,884 --> 00:49:23,259
We haven't.

1185
00:49:23,259 --> 00:49:23,344
..

1186
00:49:23,344 --> 00:49:29,923
I mean, but really, humans have been contending with the mystery of the world around them for a long

1187
00:49:29,923 --> 00:49:30,108
long.

1188
00:49:30,108 --> 00:49:30,183
..

1189
00:49:30,183 --> 00:49:33,943
There's a lot of mysteries, like what's up with the sun and

1190
00:49:33,943 --> 00:49:35,743
and the rain?

1191
00:49:35,743 --> 00:49:36,243
Yeah.

1192
00:49:36,243 --> 00:49:37,303
Like what's that about?

1193
00:49:37,303 --> 00:49:39,504
And then like last year, we had a lot of rain

1194
00:49:39,504 --> 00:49:41,224
and this year, we don't have rain.

1195
00:49:41,224 --> 00:49:42,443
Like what did we do wrong?

1196
00:49:42,443 --> 00:49:44,603
Humans have been asking that question - Yeah.

1197
00:49:44,603 --> 00:49:44,603
.

1198
00:49:44,603 --> 00:49:44,603
.

1199
00:49:44,603 --> 00:49:45,283
for a long time.

1200
00:49:45,283 --> 00:49:45,583
Exactly.

1201
00:49:45,583 --> 00:49:46,123
So we're quite.

1202
00:49:46,123 --> 00:49:46,184
..

1203
00:49:46,184 --> 00:49:49,383
I guess we've developed a lot of mechanisms to cope with this- Yeah.

1204
00:49:49,383 --> 00:49:49,389
.

1205
00:49:49,389 --> 00:49:49,403
..

1206
00:49:49,403 --> 00:49:51,894
uh, these deep mysteries that we can't fully.

1207
00:49:51,894 --> 00:49:51,984
..

1208
00:49:51,984 --> 00:49:54,103
we can see, but we can't fully understand

1209
00:49:54,103 --> 00:49:56,203
and we have to, have to just get on with daily life.

1210
00:49:56,203 --> 00:49:56,563
Yeah.

1211
00:49:56,563 --> 00:49:57,509
And, and, and we get.

1212
00:49:57,509 --> 00:49:57,563
..

1213
00:49:57,563 --> 00:49:58,763
we keep ourselves busy, right?

1214
00:49:58,763 --> 00:50:00,483
In a way that we keep ourselves distracted.

1215
00:50:00,483 --> 00:50:04,883
  I mean, weather is one of the most important questions of human history.

1216
00:50:04,883 --> 00:50:05,453
We still.

1217
00:50:05,453 --> 00:50:05,583
..

1218
00:50:05,583 --> 00:50:09,403
That's, that's the go-to small talk direction of-  Yes.

1219
00:50:09,403 --> 00:50:09,543
 .

1220
00:50:09,543 --> 00:50:09,543
..

1221
00:50:09,543 --> 00:50:09,963
of the weather.

1222
00:50:09,963 --> 00:50:10,683
Especially in England.

1223
00:50:10,683 --> 00:50:11,103
Yeah.

1224
00:50:11,103 --> 00:50:12,340
 And then it's.

1225
00:50:12,340 --> 00:50:12,443
..

1226
00:50:12,443 --> 00:50:16,564
Which is, you know, famously is an extremely difficult system to model.

1227
00:50:16,564 --> 00:50:16,684
Yeah.

1228
00:50:16,684 --> 00:50:20,183
And, uh, even that system, uh

1229
00:50:20,183 --> 00:50:22,823
uh, Google DeepMind has made progress on.

1230
00:50:22,823 --> 00:50:23,143
Yes.

1231
00:50:23,143 --> 00:50:23,483
We've.

1232
00:50:23,483 --> 00:50:23,544
..

1233
00:50:23,544 --> 00:50:23,743
Yeah.

1234
00:50:23,743 --> 00:50:27,263
We've created the, the best weather prediction systems in the world

1235
00:50:27,263 --> 00:50:34,063
and they're better than traditional fluid dynamics sort of systems that are usually calculated on massive supercomputers

1236
00:50:34,063 --> 00:50:35,883
takes days to calculate it.

1237
00:50:35,883 --> 00:50:40,624
Um, we've managed to model a lot of the weather dynamics with neural network systems

1238
00:50:40,624 --> 00:50:48,063
with our WeatherNext system and, again, it's interesting that those kinds of dynamics can be modeled even though they're very complicated

1239
00:50:48,063 --> 00:50:50,563
almost bordering on chaotic systems in some cases.

1240
00:50:50,563 --> 00:50:52,823
A lot of the interesting aspects of that

1241
00:50:52,823 --> 00:50:56,143
um, uh, can be modeled by these neural network systems

1242
00:50:56,143 --> 00:50:58,263
including very recently we had, you know

1243
00:50:58,263 --> 00:51:01,243
cyclone prediction of where, you know, paths of hurricanes might go.

1244
00:51:01,243 --> 00:51:03,683
Of course, super useful, super important for the world

1245
00:51:03,683 --> 00:51:08,143
and, and, and it's super important to do that very timely and very quickly and as well as accurately.

1246
00:51:08,143 --> 00:51:10,623
And, uh, I think it's a very promising direction

1247
00:51:10,623 --> 00:51:12,743
again, of, you know, simulating and

1248
00:51:12,743 --> 00:51:18,123
uh, uh, so that you can run forward predictions and simulations of very complicated real-world systems.

1249
00:51:18,123 --> 00:51:20,663
I should mention that, uh, I've got a chance in

1250
00:51:20,663 --> 00:51:24,943
uh, Texas to meet a community of folks called the Storm Chasers.

1251
00:51:24,943 --> 00:51:25,343
Yes.

1252
00:51:25,343 --> 00:51:28,683
And w- what's really incredible about them, I need to talk to them more

1253
00:51:28,683 --> 00:51:30,143
is they're extremely tech-savvy.

1254
00:51:30,143 --> 00:51:30,263
Mm-hmm.

1255
00:51:30,263 --> 00:51:34,023
Because what they have to do is they have to use models to predict where the storm is.

1256
00:51:34,023 --> 00:51:34,043
Yeah.

1257
00:51:34,043 --> 00:51:34,687
 So they're.

1258
00:51:34,687 --> 00:51:34,743
..

1259
00:51:34,743 --> 00:51:35,288
 It's this.

1260
00:51:35,288 --> 00:51:35,363
..

1261
00:51:35,363 --> 00:51:37,623
it's, it's this beautiful mix of, like

1262
00:51:37,623 --> 00:51:38,463
crazy- Yeah.

1263
00:51:38,463 --> 00:51:38,469
.

1264
00:51:38,469 --> 00:51:38,483
..

1265
00:51:38,483 --> 00:51:41,163
enough to, like, go into the eye of the storm- Yeah.

1266
00:51:41,163 --> 00:51:41,169
.

1267
00:51:41,169 --> 00:51:41,183
..

1268
00:51:41,183 --> 00:51:45,803
and, like, in order to protect your life and predict where the extreme events are going to be

1269
00:51:45,803 --> 00:51:49,183
they have to have increasingly sophisticated models of

1270
00:51:49,183 --> 00:51:49,863
uh, of weather.

1271
00:51:49,863 --> 00:51:50,283
Yeah.

1272
00:51:50,283 --> 00:51:50,683
Yeah.

1273
00:51:50,683 --> 00:51:52,643
It's a, it's a b- a beautiful balance of

1274
00:51:52,643 --> 00:51:57,263
like, being in it as living organisms and the

1275
00:51:57,263 --> 00:51:58,243
the cutting edge of science.

1276
00:51:58,243 --> 00:52:00,323
Um, they actually might be using, uh

1277
00:52:00,323 --> 00:52:01,709
DeepMind's system, so that's.

1278
00:52:01,709 --> 00:52:01,803
..

1279
00:52:01,803 --> 00:52:02,303
Yeah, they are.

1280
00:52:02,303 --> 00:52:03,063
Hopefully, they are.

1281
00:52:03,063 --> 00:52:04,863
And I'd, I'd love to join them on one of those trips.

1282
00:52:04,863 --> 00:52:05,963
 They look amazing, right?

1283
00:52:05,963 --> 00:52:06,063
That's great.

1284
00:52:06,063 --> 00:52:07,503
To actually experience it one time.

1285
00:52:07,503 --> 00:52:08,363
E- exactly.

1286
00:52:08,363 --> 00:52:08,383
Yeah.

1287
00:52:08,383 --> 00:52:11,323
And then also to experience the correct prediction- Yeah.

1288
00:52:11,323 --> 00:52:11,343
.

1289
00:52:11,343 --> 00:52:11,383
..

1290
00:52:11,383 --> 00:52:12,803
of where something will come- Yeah.

1291
00:52:12,803 --> 00:52:12,809
.

1292
00:52:12,809 --> 00:52:12,823
..

1293
00:52:12,823 --> 00:52:14,203
and how it's going to evolve.

1294
00:52:14,203 --> 00:52:15,003
It's incredible.

1295
00:52:15,003 --> 00:52:15,343
Yeah.

1296
00:52:15,343 --> 00:52:19,143
You've estimated that we'll have AGI by 2030.

1297
00:52:19,143 --> 00:52:22,683
Um, so there's interesting questions around that.

1298
00:52:22,683 --> 00:52:25,663
How will we actually know that we got there

1299
00:52:25,663 --> 00:52:30,103
uh, and, uh, what may be the move

1300
00:52:30,103 --> 00:52:32,643
quote, "Move 37" of AGI?

1301
00:52:32,643 --> 00:52:35,683
My estimate is sort of 50% chance by

1302
00:52:35,683 --> 00:52:37,803
in the next five years, so, you know

1303
00:52:37,803 --> 00:52:39,243
by 2030, let's say.

1304
00:52:39,243 --> 00:52:42,703
And, uh, so I think there's a good chance that that could happen.

1305
00:52:42,703 --> 00:52:45,223
Part of it is what, what is your definition of AGI?

1306
00:52:45,223 --> 00:52:47,183
Of course, people are arguing about that now and

1307
00:52:47,183 --> 00:52:50,383
and, uh, mine's quite a high bar and always has been of

1308
00:52:50,383 --> 00:52:53,783
like, can we match the cognitive functions that the brain has

1309
00:52:53,783 --> 00:52:54,303
right?

1310
00:52:54,303 --> 00:52:57,663
So we know our brains are pretty much general Turing machines

1311
00:52:57,663 --> 00:53:03,223
approximate, and of course, we created incredible modern civilization with our minds

1312
00:53:03,223 --> 00:53:06,343
so that sh- also speaks to how general the brain is.

1313
00:53:06,343 --> 00:53:09,283
And, um, for us to know we have a

1314
00:53:09,283 --> 00:53:11,343
a true AGI, we would have to

1315
00:53:11,343 --> 00:53:13,563
like, make sure that it has all those capabilities.

1316
00:53:13,563 --> 00:53:17,143
It isn't kind of a jagged intelligence where some things it's really good at

1317
00:53:17,143 --> 00:53:19,463
like today's systems, but other things it's really

1318
00:53:19,463 --> 00:53:20,763
uh, flawed at.

1319
00:53:20,763 --> 00:53:23,343
And, and that's what we currently have with today's systems

1320
00:53:23,343 --> 00:53:27,723
they're not consistent, so you'd want that consistency of intelligence across the board.

1321
00:53:27,723 --> 00:53:29,903
And then we have some missing, I think

1322
00:53:29,903 --> 00:53:35,663
capabilities, like sort of, uh, the true invention capabilities and creativity that we were talking about earlier

1323
00:53:35,663 --> 00:53:36,923
so you'd want to see those.

1324
00:53:36,923 --> 00:53:38,283
How you test that?

1325
00:53:38,283 --> 00:53:40,003
Um, I think you just test it.

1326
00:53:40,003 --> 00:53:45,463
One way to do it would be kind of brute force test of tens of thousands of cognitive tasks that- Mm-hmm.

1327
00:53:45,463 --> 00:53:45,476
.

1328
00:53:45,476 --> 00:53:45,503
..

1329
00:53:45,503 --> 00:53:47,883
um, you know, we know that humans can do

1330
00:53:47,883 --> 00:53:51,983
uh, and maybe also make the system available to

1331
00:53:51,983 --> 00:53:54,603
uh, a few hundred of the world's top experts

1332
00:53:54,603 --> 00:53:57,683
uh, the Terence Taus of each, each subject area

1333
00:53:57,683 --> 00:53:59,157
and see if they can find.

1334
00:53:59,157 --> 00:53:59,283
..

1335
00:53:59,283 --> 00:54:02,763
You know, give th- give them a month or two and see if they can find a

1336
00:54:02,763 --> 00:54:04,023
an obvious flaw in the system.

1337
00:54:04,023 --> 00:54:06,083
And if they can't, then I think you're

1338
00:54:06,083 --> 00:54:07,994
you're pretty, uh, you know, pretty.

1339
00:54:07,994 --> 00:54:08,083
..

1340
00:54:08,083 --> 00:54:09,703
you can be pretty confident we have a

1341
00:54:09,703 --> 00:54:10,983
a fully general system.

1342
00:54:10,983 --> 00:54:15,983
Maybe to push back a little bit, it seems like humans are really incredible as the

1343
00:54:15,983 --> 00:54:20,163
the intelligence improves across all domains to take it for granted.

1344
00:54:20,163 --> 00:54:20,703
Mm-hmm.

1345
00:54:20,703 --> 00:54:23,543
Uh, like you mentioned Terence Tau, uh

1346
00:54:23,543 --> 00:54:27,163
th- these brilliant experts, they might quickly

1347
00:54:27,163 --> 00:54:31,223
in a span of weeks, take for granted all the incredible things they can do and then focus in

1348
00:54:31,223 --> 00:54:32,657
"Well, ha ha, right there.

1349
00:54:32,657 --> 00:54:36,603
" You know, I, I consider myself a h- first of all

1350
00:54:36,603 --> 00:54:37,063
human.

1351
00:54:37,063 --> 00:54:37,463
Yeah.

1352
00:54:37,463 --> 00:54:38,863
 That's good.

1353
00:54:38,863 --> 00:54:39,103
Yeah.

1354
00:54:39,103 --> 00:54:41,083
 Uh, I identify as human.

1355
00:54:41,083 --> 00:54:42,936
Um,  the.

1356
00:54:42,936 --> 00:54:43,123
..

1357
00:54:43,123 --> 00:54:43,793
I.

1358
00:54:43,793 --> 00:54:43,923
..

1359
00:54:43,923 --> 00:54:46,103
You know, some people listen to me talk and they're like

1360
00:54:46,103 --> 00:54:48,303
"That guy is not good at talking, the stuttering

1361
00:54:48,303 --> 00:54:48,508
the.

1362
00:54:48,508 --> 00:54:48,611
..

1363
00:54:48,611 --> 00:54:48,943
" You know.

1364
00:54:48,943 --> 00:54:53,043
 So, like, e- even humans have obvious

1365
00:54:53,043 --> 00:54:56,503
across domains, limits, uh, even just outside of- Of course.

1366
00:54:56,503 --> 00:54:56,503
.

1367
00:54:56,503 --> 00:54:56,503
.

1368
00:54:56,503 --> 00:54:56,745
calc.

1369
00:54:56,745 --> 00:54:56,843
..

1370
00:54:56,843 --> 00:54:58,203
mathematics and physics and so on.

1371
00:54:58,203 --> 00:54:59,683
It.

1372
00:54:59,683 --> 00:54:59,843
..

1373
00:54:59,843 --> 00:55:04,043
I, I, I wonder if it will take something like a Move 37

1374
00:55:04,043 --> 00:55:05,583
so on the positive side- Yeah.

1375
00:55:05,583 --> 00:55:05,583
.

1376
00:55:05,583 --> 00:55:05,583
.

1377
00:55:05,583 --> 00:55:09,063
versus like a barrage of 10,000 cognitive tasks- Yeah.

1378
00:55:09,063 --> 00:55:09,063
.

1379
00:55:09,063 --> 00:55:09,063
.

1380
00:55:09,063 --> 00:55:09,513
where.

1381
00:55:09,513 --> 00:55:09,656
..

1382
00:55:09,656 --> 00:55:13,115
It'll be one or two where it's like- Yes.

1383
00:55:13,115 --> 00:55:13,141
.

1384
00:55:13,141 --> 00:55:13,195
..

1385
00:55:13,195 --> 00:55:14,275
holy shit, this is special.

1386
00:55:14,275 --> 00:55:15,275
So I think there are, exactly.

1387
00:55:15,275 --> 00:55:19,775
So I think there's the sort of blanket testing to just make sure you've got the consistency

1388
00:55:19,775 --> 00:55:23,875
but I think there are the sort of lighthouse moments

1389
00:55:23,875 --> 00:55:26,755
like the Move 37, that w- I would be looking for.

1390
00:55:26,755 --> 00:55:32,276
So one would be inventing a new conjecture or a new hypothesis about physics

1391
00:55:32,276 --> 00:55:33,815
like Einstein did.

1392
00:55:33,815 --> 00:55:37,016
So maybe you could even run the back test of that very rigorously

1393
00:55:37,016 --> 00:55:42,696
like l- have a cutoff of, knowledge cutoff of 1900 and then give the system everything that was

1394
00:55:42,696 --> 00:55:45,076
you know, that was written up to 1900 and then

1395
00:55:45,076 --> 00:55:48,776
and then see if it could come up with special relativity and general relativity

1396
00:55:48,776 --> 00:55:49,675
right, like Einstein did.

1397
00:55:49,675 --> 00:55:49,835
Mm-hmm.

1398
00:55:49,835 --> 00:55:51,395
That, that would be an interesting test.

1399
00:55:51,395 --> 00:55:55,455
Another one would be, can it invent a game like Go?

1400
00:55:55,455 --> 00:55:58,276
Not just come up with Move 37, a new strategy

1401
00:55:58,276 --> 00:56:00,036
but can it invent a game that's as deep

1402
00:56:00,036 --> 00:56:02,795
as aesthetically beautiful, as elegant as Go?

1403
00:56:02,795 --> 00:56:05,775
And those are the sorts of things I would be looking out for.

1404
00:56:05,775 --> 00:56:08,515
Uh, uh, a- and probably a system being able to do

1405
00:56:08,515 --> 00:56:10,475
uh, uh, several of those things, right?

1406
00:56:10,475 --> 00:56:12,515
For it to be very general, um

1407
00:56:12,515 --> 00:56:13,655
not just one domain.

1408
00:56:13,655 --> 00:56:15,875
And so I think that would be the signs

1409
00:56:15,875 --> 00:56:17,475
at least that I would be looking for

1410
00:56:17,475 --> 00:56:20,335
that we've got a system that's AGI level.

1411
00:56:20,335 --> 00:56:23,835
And then maybe to fill that out, you would also check the consistency

1412
00:56:23,835 --> 00:56:26,855
you know, make sure there's no holes in that system either.

1413
00:56:26,855 --> 00:56:30,635
Yeah, something like a, a new conjecture or scientific discovery

1414
00:56:30,635 --> 00:56:32,135
that would be a cool feeling.

1415
00:56:32,135 --> 00:56:32,415
Yeah.

1416
00:56:32,415 --> 00:56:33,855
That would be amazing.

1417
00:56:33,855 --> 00:56:35,495
So it's not, not just helping us do that

1418
00:56:35,495 --> 00:56:37,675
but actually coming up with something brand new.

1419
00:56:37,675 --> 00:56:39,835
And you would be in the room for that.

1420
00:56:39,835 --> 00:56:40,175
Exactly.

1421
00:56:40,175 --> 00:56:44,815
And so it would be like probably two or three months before announcing it.

1422
00:56:44,815 --> 00:56:45,415
Mm-hmm.

1423
00:56:45,415 --> 00:56:48,655
And you would just be sitting there trying not to tweet.

1424
00:56:48,655 --> 00:56:51,375
  Something like that, exactly.

1425
00:56:51,375 --> 00:56:53,335
It's like, what is this amazing new- Yeah.

1426
00:56:53,335 --> 00:56:53,341
.

1427
00:56:53,341 --> 00:56:53,355
..

1428
00:56:53,355 --> 00:56:54,815
you know, physics, uh, idea.

1429
00:56:54,815 --> 00:56:58,155
And then we'd probably check it with world experts in that domain- Yeah.

1430
00:56:58,155 --> 00:56:58,161
.

1431
00:56:58,161 --> 00:56:58,175
..

1432
00:56:58,175 --> 00:57:02,315
right, and validate it and kind of go through its workings.

1433
00:57:02,315 --> 00:57:05,175
And it, I guess it would be explaining its workings too.

1434
00:57:05,175 --> 00:57:07,635
Um, yeah, be an amazing moment.

1435
00:57:07,635 --> 00:57:11,175
Do you worry that we as humans, even expert humans like you

1436
00:57:11,175 --> 00:57:11,855
might miss it?

1437
00:57:11,855 --> 00:57:14,435
Might miss- Well, it may be c- pretty complicated

1438
00:57:14,435 --> 00:57:15,351
so it could be.

1439
00:57:15,351 --> 00:57:15,415
..

1440
00:57:15,415 --> 00:57:17,855
The analogy I give there is I don't think it will be

1441
00:57:17,855 --> 00:57:21,495
um, uh, uh, totally mysterious to the

1442
00:57:21,495 --> 00:57:24,175
to the best human scientists, but it may be a bit like

1443
00:57:24,175 --> 00:57:31,695
for example, in chess if I was to talk to Garry Kasparov or Magnus Carlsen and play a game with them and they make a brilliant move

1444
00:57:31,695 --> 00:57:33,715
I might not be able to come up with that move

1445
00:57:33,715 --> 00:57:37,335
but they could explain why afterwards that move made sense.

1446
00:57:37,335 --> 00:57:39,635
And we would be able to understand it to some degree.

1447
00:57:39,635 --> 00:57:41,995
Not to the level they do, but i- i- you know

1448
00:57:41,995 --> 00:57:44,755
if they were good at explaining, which is actually part of intelligence too

1449
00:57:44,755 --> 00:57:48,095
is being able to explain in a simple way that what you're thinking about

1450
00:57:48,095 --> 00:57:52,715
um, uh, I, I think that that would be very possible for the best human scientists.

1451
00:57:52,715 --> 00:57:55,835
But I wonder, maybe you can, you can educate me on this side of Go

1452
00:57:55,835 --> 00:58:02,355
I wonder if there's moves for Magnus or Garry where they at first will dismiss it as a bad move.

1453
00:58:02,355 --> 00:58:03,415
Yeah, sure.

1454
00:58:03,415 --> 00:58:04,375
There could be.

1455
00:58:04,375 --> 00:58:07,615
But then afterwards, they'll figure out with their intuition that

1456
00:58:07,615 --> 00:58:09,155
that this, why this works and then

1457
00:58:09,155 --> 00:58:10,326
and then, and then empirically.

1458
00:58:10,326 --> 00:58:10,415
..

1459
00:58:10,415 --> 00:58:13,615
The nice thing about games is, one of the great things about games is you can em- it's

1460
00:58:13,615 --> 00:58:14,875
it's a sort of scientific test.

1461
00:58:14,875 --> 00:58:15,171
Does it.

1462
00:58:15,171 --> 00:58:15,275
..

1463
00:58:15,275 --> 00:58:16,555
Do you win the game or not win?

1464
00:58:16,555 --> 00:58:19,735
And then, um, that tells you, okay

1465
00:58:19,735 --> 00:58:21,295
that move in the end was good.

1466
00:58:21,295 --> 00:58:22,635
That strategy was good.

1467
00:58:22,635 --> 00:58:24,695
And then you can go back and analyze that and

1468
00:58:24,695 --> 00:58:28,275
and, and, and, and explain even to yourself a little bit more why.

1469
00:58:28,275 --> 00:58:29,315
Explore around it.

1470
00:58:29,315 --> 00:58:32,035
And that's how chess analysis and things like that work.

1471
00:58:32,035 --> 00:58:33,975
So perhaps that's why my brain works like that

1472
00:58:33,975 --> 00:58:37,435
'cause I, I've been doing that since I was four and you're train- you know

1473
00:58:37,435 --> 00:58:39,715
tra- it's sort of hardcore training in that way.

1474
00:58:39,715 --> 00:58:43,375
But even, even now like when I generate code

1475
00:58:43,375 --> 00:58:55,195
there, there is this kind of nuanced fascinating con- contention that's happening where I might at first identify as a set of generated code is incorrect in

1476
00:58:55,195 --> 00:58:59,335
in some interesting nuanced ways, but then I'm always have to ask the question

1477
00:58:59,335 --> 00:59:02,755
is there a deeper insight here that, uh

1478
00:59:02,755 --> 00:59:04,395
that I'm the one who's incorrect?

1479
00:59:04,395 --> 00:59:05,035
Mm-hmm.

1480
00:59:05,035 --> 00:59:06,463
And th- that's going to.

1481
00:59:06,463 --> 00:59:06,535
..

1482
00:59:06,535 --> 00:59:08,155
As the systems get more and more intelligent

1483
00:59:08,155 --> 00:59:09,335
you're gonna have to contend with that.

1484
00:59:09,335 --> 00:59:13,155
It's like what, what, what do you m- is this a bug or a- Yes.

1485
00:59:13,155 --> 00:59:13,155
.

1486
00:59:13,155 --> 00:59:13,155
.

1487
00:59:13,155 --> 00:59:14,515
feature, what you just came up with.

1488
00:59:14,515 --> 00:59:16,555
Yeah, and they're gonna be pretty complicated to do

1489
00:59:16,555 --> 00:59:17,551
but of course it will be.

1490
00:59:17,551 --> 00:59:17,695
..

1491
00:59:17,695 --> 00:59:21,875
You could imagine also AI systems that are producing that code or whatever that is

1492
00:59:21,875 --> 00:59:25,435
and then human programmers looking at, but also not unaided

1493
00:59:25,435 --> 00:59:27,415
with the help of AI tools as well.

1494
00:59:27,415 --> 00:59:30,129
So it's gonna be kind of an interesting.

1495
00:59:30,129 --> 00:59:30,235
..

1496
00:59:30,235 --> 00:59:32,535
You know, d- maybe different AI tools to the ones- Yeah.

1497
00:59:32,535 --> 00:59:32,561
.

1498
00:59:32,561 --> 00:59:32,615
..

1499
00:59:32,615 --> 00:59:35,895
that they're more va- you know, mo- kind of monitoring tools to the ones that generated it.

1500
00:59:35,895 --> 00:59:38,375
So if we look at a AGI system

1501
00:59:38,375 --> 00:59:41,495
sorry to bring it back up, but AlphaEvolve.

1502
00:59:41,495 --> 00:59:41,655
Yeah.

1503
00:59:41,655 --> 00:59:43,175
Super cool.

1504
00:59:43,175 --> 00:59:46,235
So AlphaEvolve enables, on the programming side

1505
00:59:46,235 --> 00:59:50,715
something like recursive self-improvement, uh, potentially.

1506
00:59:50,715 --> 00:59:51,555
Like what.

1507
00:59:51,555 --> 00:59:51,675
..

1508
00:59:51,675 --> 00:59:54,015
If we can imagine what that AGI system

1509
00:59:54,015 --> 00:59:58,815
maybe not the first version, but a few versions beyond that

1510
00:59:58,815 --> 01:00:00,235
what does that actually look like?

1511
01:00:00,235 --> 01:00:01,335
Do you think it will be simple?

1512
01:00:01,335 --> 01:00:06,015
Do you think it will be something like a self-improving program and a simple one?

1513
01:00:06,015 --> 01:00:08,555
I mean, potentially that's possible, I would say.

1514
01:00:08,555 --> 01:00:10,175
Um, I'm not sure it's even desirable

1515
01:00:10,175 --> 01:00:12,555
because that's a kind of like hard take off scenario.

1516
01:00:12,555 --> 01:00:12,835
Yeah.

1517
01:00:12,835 --> 01:00:16,475
But, but you, y- these current systems like AlphaEvolve

1518
01:00:16,475 --> 01:00:20,055
they have, you know, human in the loop deciding on various things.

1519
01:00:20,055 --> 01:00:22,455
They're separate hybrid systems that interact.

1520
01:00:22,455 --> 01:00:25,735
Uh, one could imagine eventually doing that end-to-end.

1521
01:00:25,735 --> 01:00:27,855
I don't see why that wouldn't be possible

1522
01:00:27,855 --> 01:00:35,415
but right now, um, you know, I think the systems are not good enough to do that in terms of coming up with the architecture of the code.

1523
01:00:35,415 --> 01:00:40,735
Um, and again, it's a little bit reconnected to this idea of coming up with a new conjectural hypothesis.

1524
01:00:40,735 --> 01:00:41,188
How.

1525
01:00:41,188 --> 01:00:41,295
..

1526
01:00:41,295 --> 01:00:45,195
Like, they, they're good if you give them very specific instructions about what you're trying to do.

1527
01:00:45,195 --> 01:00:48,935
Um, but if you give them a very vague high level instruction

1528
01:00:48,935 --> 01:00:50,295
that wouldn't work currently.

1529
01:00:50,295 --> 01:00:54,675
Like, uh, and I think that's related to this idea of like invent a game as good as Go

1530
01:00:54,675 --> 01:00:55,075
right?

1531
01:00:55,075 --> 01:00:56,255
Imagine that was the prompt.

1532
01:00:56,255 --> 01:00:58,255
That's, that's pretty underspecified.

1533
01:00:58,255 --> 01:01:01,255
And so the current systems wouldn't know, I think

1534
01:01:01,255 --> 01:01:04,075
what to do with that, how to narrow that down to something tractable.

1535
01:01:04,075 --> 01:01:05,555
And I think there's similar like, look

1536
01:01:05,555 --> 01:01:07,775
just make a better version of yourself, that's too

1537
01:01:07,775 --> 01:01:09,355
that's too unconstrained.

1538
01:01:09,355 --> 01:01:10,815
But we've done it in s- you know

1539
01:01:10,815 --> 01:01:12,255
in, and as you know with AlphaEvolve

1540
01:01:12,255 --> 01:01:14,615
like things like faster matrix multiplication.

1541
01:01:14,615 --> 01:01:14,835
Mm-hmm.

1542
01:01:14,835 --> 01:01:18,635
So when you, when you hone it down to a very specific thing you want

1543
01:01:18,635 --> 01:01:21,215
um, it's very good at incrementally improving that.

1544
01:01:21,215 --> 01:01:21,415
For sure.

1545
01:01:21,415 --> 01:01:23,895
But at the moment these are more like incremental improvements

1546
01:01:23,895 --> 01:01:25,815
sort of small iterations.

1547
01:01:25,815 --> 01:01:29,315
Whereas if, you know, i- i- if you wanted a big leap in

1548
01:01:29,315 --> 01:01:32,575
uh, understanding, you'd need a, you'd need a much larger

1549
01:01:32,575 --> 01:01:33,715
uh, advance.

1550
01:01:33,715 --> 01:01:35,915
Yeah, but it could also be sort of

1551
01:01:35,915 --> 01:01:37,935
to push back against hard take off scenario

1552
01:01:37,935 --> 01:01:40,717
it could be just a sequence of-.

1553
01:01:40,717 --> 01:01:40,979
..

1554
01:01:40,979 --> 01:01:44,759
um, incremental improvements, like matrix multiplication.

1555
01:01:44,759 --> 01:01:49,980
Like it has to sit there for days thinking how to incrementally improve a thing

1556
01:01:49,980 --> 01:01:51,719
and that it does so recursively.

1557
01:01:51,719 --> 01:01:53,740
And as you do more and more improvement

1558
01:01:53,740 --> 01:01:55,219
it'll slow down.

1559
01:01:55,219 --> 01:01:55,659
Right.

1560
01:01:55,659 --> 01:01:59,499
So there'll be like a- like a- the path to AGI won't be like a

1561
01:01:59,499 --> 01:01:59,869
a.

1562
01:01:59,869 --> 01:02:00,079
..

1563
01:02:00,079 --> 01:02:03,180
It'll be a gradual improvement over time.

1564
01:02:03,180 --> 01:02:03,499
Yes.

1565
01:02:03,499 --> 01:02:06,239
If it was just incremental improvements, that's how it would look.

1566
01:02:06,239 --> 01:02:09,339
So the question is, could it come up with a new leap

1567
01:02:09,339 --> 01:02:10,759
like the transformers architecture?

1568
01:02:10,759 --> 01:02:11,079
Yeah.

1569
01:02:11,079 --> 01:02:14,240
Like could it have done that back in 2017 when

1570
01:02:14,240 --> 01:02:15,600
you know, we did it and Brain did it?

1571
01:02:15,600 --> 01:02:18,959
And it's- it's not clear that- that these systems

1572
01:02:18,959 --> 01:02:20,759
something like AlphaFold wouldn't be able to do

1573
01:02:20,759 --> 01:02:22,160
make such a big leap.

1574
01:02:22,160 --> 01:02:23,919
So for sure these systems are good.

1575
01:02:23,919 --> 01:02:26,659
We have systems I think that can do incremental hill climbing.

1576
01:02:26,659 --> 01:02:27,079
Mm-hmm.

1577
01:02:27,079 --> 01:02:32,519
And that's a kind of bigger question about is that all that's needed from here or do we actually need one or two more

1578
01:02:32,519 --> 01:02:34,359
um, uh, big breakthroughs?

1579
01:02:34,359 --> 01:02:38,760
And can the same kind of systems provide the breakthroughs also?

1580
01:02:38,760 --> 01:02:40,679
So make it a bunch of S-curves.

1581
01:02:40,679 --> 01:02:44,159
Like incremental improvement, but also every once in a while

1582
01:02:44,159 --> 01:02:44,679
leaps.

1583
01:02:44,679 --> 01:02:50,279
Yeah, I don't think a- a- anyone has systems that can sh- have shown unequivocally those big leaps.

1584
01:02:50,279 --> 01:02:51,232
The- the- the.

1585
01:02:51,232 --> 01:02:51,299
..

1586
01:02:51,299 --> 01:02:51,399
Right?

1587
01:02:51,399 --> 01:02:54,859
We have a lot of systems that do the hill climbing of the S-curve that you're currently on.

1588
01:02:54,859 --> 01:02:55,419
Yeah.

1589
01:02:55,419 --> 01:02:57,679
And that would be the Move 37, is a leap.

1590
01:02:57,679 --> 01:02:59,079
Yeah, I think would be a leap.

1591
01:02:59,079 --> 01:03:00,679
Um, something like that.

1592
01:03:00,679 --> 01:03:05,559
Uh, do you think s- the scaling laws are holding strong on the pre-training

1593
01:03:05,559 --> 01:03:07,099
post-training, test time, compute?

1594
01:03:07,099 --> 01:03:12,839
Uh, do you, uh, on the flip side of that anticipate AI progress hitting a wall?

1595
01:03:12,839 --> 01:03:16,739
We certainly feel there's a lot more room just in the scaling

1596
01:03:16,739 --> 01:03:19,459
so, um, actually all steps, pre-training

1597
01:03:19,459 --> 01:03:21,979
post-training and inference time.

1598
01:03:21,979 --> 01:03:26,359
So, uh, there's sort of three scalings that are happening c- concurrently.

1599
01:03:26,359 --> 01:03:27,747
Um, and we.

1600
01:03:27,747 --> 01:03:27,899
..

1601
01:03:27,899 --> 01:03:31,199
A- again there, it's about how innovative you can be

1602
01:03:31,199 --> 01:03:34,639
and we, you know, we pride ourselves on having the broadest and

1603
01:03:34,639 --> 01:03:36,659
um, deepest research bench.

1604
01:03:36,659 --> 01:03:38,719
Uh, we have amazing, you know, incredible

1605
01:03:38,719 --> 01:03:42,099
uh, researchers and, uh, people like Noam Shazeer who

1606
01:03:42,099 --> 01:03:44,899
you know, came up with transformers and s- and Dave Silver

1607
01:03:44,899 --> 01:03:47,459
you know, who led the AlphaGo project and so on.

1608
01:03:47,459 --> 01:03:54,959
And, um, it's- it's- it's w- that research base means that if some new- new breakthrough is required

1609
01:03:54,959 --> 01:03:59,639
like an AlphaGo or transformers, uh, I would back us to be the place that does that.

1610
01:03:59,639 --> 01:04:02,159
So I actually quite like it when the terrain gets harder

1611
01:04:02,159 --> 01:04:02,579
right?

1612
01:04:02,579 --> 01:04:05,879
Because then it veers more from just engineering-  Yeah.

1613
01:04:05,879 --> 01:04:05,979
.

1614
01:04:05,979 --> 01:04:06,179
..

1615
01:04:06,179 --> 01:04:07,899
to- to true research, and, you know

1616
01:04:07,899 --> 01:04:10,779
re- or research plus engineering, and that's our sweet spot

1617
01:04:10,779 --> 01:04:12,239
and I- I think that's harder.

1618
01:04:12,239 --> 01:04:15,359
It's harder to invent things than to- than to

1619
01:04:15,359 --> 01:04:16,979
um, you know, fast follow.

1620
01:04:16,979 --> 01:04:20,039
And, um, so, you know, we don't know.

1621
01:04:20,039 --> 01:04:27,779
I would say it's a, it's kind of 50/50 whether new things are needed or whether the scaling the existing stuff is gonna be enough.

1622
01:04:27,779 --> 01:04:30,219
And so in true kind of empirical fashion

1623
01:04:30,219 --> 01:04:32,799
we're pushing both of those as hard as possible.

1624
01:04:32,799 --> 01:04:35,199
The new blue sky ideas and, you know

1625
01:04:35,199 --> 01:04:36,839
maybe about half our resources are on that

1626
01:04:36,839 --> 01:04:42,219
and then- and then, uh, scaling to the max the- the current- the current capabilities.

1627
01:04:42,219 --> 01:04:45,039
And, um, we're still seeing some, you know

1628
01:04:45,039 --> 01:04:48,439
fantastic progress on, uh, each different version of Gemini.

1629
01:04:48,439 --> 01:04:51,599
That's interesting the way you put it in- in terms of the deep bench

1630
01:04:51,599 --> 01:04:59,159
that if, uh, progress towards AGI is more than just scaling compute

1631
01:04:59,159 --> 01:05:01,999
and so the engineering side of the problem

1632
01:05:01,999 --> 01:05:06,739
and is more on the scientific side where there's breakthroughs needed

1633
01:05:06,739 --> 01:05:09,539
then you feel confident DeepMind is well, uh

1634
01:05:09,539 --> 01:05:11,959
Google DeepMind is well-positioned to- Yes.

1635
01:05:11,959 --> 01:05:11,999
.

1636
01:05:11,999 --> 01:05:12,079
..

1637
01:05:12,079 --> 01:05:13,339
ki- kick ass in that domain.

1638
01:05:13,339 --> 01:05:16,979
Well, I mean, if you look at the history of the last decade or 15 years- Yeah.

1639
01:05:16,979 --> 01:05:17,039
.

1640
01:05:17,039 --> 01:05:17,159
..

1641
01:05:17,159 --> 01:05:18,699
um, it's been, uh, you know

1642
01:05:18,699 --> 01:05:23,759
maybe, I don't know, 80, 90% of the breakthroughs that mo- that underpins modern AI field today was from

1643
01:05:23,759 --> 01:05:26,459
you know, originally Google Brain, Google Research and DeepMind

1644
01:05:26,459 --> 01:05:27,399
so yeah.

1645
01:05:27,399 --> 01:05:29,319
I would back that to continue hopefully.

1646
01:05:29,319 --> 01:05:32,219
  Uh, so on the data side

1647
01:05:32,219 --> 01:05:34,659
are you concerned about running out of high-quality data

1648
01:05:34,659 --> 01:05:36,439
especially high-quality human data?

1649
01:05:36,439 --> 01:05:40,359
I'm not very worried about that, partly because I think there's enough data

1650
01:05:40,359 --> 01:05:44,059
uh, on, and it's b- been proven to get the systems to be pretty good.

1651
01:05:44,059 --> 01:05:46,819
And this goes back to simulations again.

1652
01:05:46,819 --> 01:05:55,379
If you- do we have enough data to make simulations or so that you can create more synthetic data that are from the right distribution?

1653
01:05:55,379 --> 01:05:56,439
Obviously that's the key.

1654
01:05:56,439 --> 01:05:59,399
So you need enough real world data in order to be able to

1655
01:05:59,399 --> 01:06:01,619
uh, uh, create those kinds of generator

1656
01:06:01,619 --> 01:06:02,599
data generators.

1657
01:06:02,599 --> 01:06:05,259
And, um, I think that we're at that step at the moment.

1658
01:06:05,259 --> 01:06:08,919
Yeah, you've done a lot of incredible stuff on the side of science and biology.

1659
01:06:08,919 --> 01:06:09,219
Mm-hmm.

1660
01:06:09,219 --> 01:06:11,939
D- doing a lot with not so much data.

1661
01:06:11,939 --> 01:06:12,319
Yeah.

1662
01:06:12,319 --> 01:06:13,679
I mean, it's still a lot of data

1663
01:06:13,679 --> 01:06:16,279
but I guess enough to get off- To get that going.

1664
01:06:16,279 --> 01:06:16,879
Exactly.

1665
01:06:16,879 --> 01:06:16,959
Yeah.

1666
01:06:16,959 --> 01:06:17,039
Yeah.

1667
01:06:17,039 --> 01:06:17,067
It's.

1668
01:06:17,067 --> 01:06:17,079
..

1669
01:06:17,079 --> 01:06:17,659
Exactly.

1670
01:06:17,659 --> 01:06:21,739
Uh, how crucial is the scaling of compute to building AGI?

1671
01:06:21,739 --> 01:06:24,879
This is a question that's an engineering question.

1672
01:06:24,879 --> 01:06:28,059
It's a almost a geopolitical question- Mm-hmm.

1673
01:06:28,059 --> 01:06:28,125
.

1674
01:06:28,125 --> 01:06:28,259
..

1675
01:06:28,259 --> 01:06:33,419
because it also integrated into that is the supply chains and energy.

1676
01:06:33,419 --> 01:06:33,859
Yes.

1677
01:06:33,859 --> 01:06:35,259
A thing that you care a lot about

1678
01:06:35,259 --> 01:06:37,119
which is, um, potentially fusion.

1679
01:06:37,119 --> 01:06:37,299
Yes.

1680
01:06:37,299 --> 01:06:39,159
So innovating on the side of energy also.

1681
01:06:39,159 --> 01:06:39,479
Yeah.

1682
01:06:39,479 --> 01:06:41,819
Do you think we're gonna keep scaling compute?

1683
01:06:41,819 --> 01:06:43,619
I think so, for several reasons.

1684
01:06:43,619 --> 01:06:46,799
I think compute, there's- there's the amount of compute you have for training.

1685
01:06:46,799 --> 01:06:48,839
Uh, often it needs to be co-located

1686
01:06:48,839 --> 01:06:50,859
so actually even like, you know, uh

1687
01:06:50,859 --> 01:06:53,619
bandwidth constraints between data centers can affect that.

1688
01:06:53,619 --> 01:06:56,959
So it's- it's- it's, there's additional constraints even there.

1689
01:06:56,959 --> 01:07:00,919
And that- that's important for training obviously the largest models you can.

1690
01:07:00,919 --> 01:07:07,779
But there's also because now AI systems are in products and being used by billions of people around the world

1691
01:07:07,779 --> 01:07:10,039
you need a ton of inference compute now.

1692
01:07:10,039 --> 01:07:11,979
Um, and then on top of that

1693
01:07:11,979 --> 01:07:14,959
there's the thinking systems, the new paradigm

1694
01:07:14,959 --> 01:07:16,839
uh, of the last year that, uh

1695
01:07:16,839 --> 01:07:20,619
where they get smarter the longer amount of inference time you give them at test time.

1696
01:07:20,619 --> 01:07:23,619
So all of those things need a lot of compute

1697
01:07:23,619 --> 01:07:26,459
and I don't really see that slowing down.

1698
01:07:26,459 --> 01:07:29,319
Um, and, uh, as AI systems become better

1699
01:07:29,319 --> 01:07:31,739
they'll become more useful and there'll be more demand for them.

1700
01:07:31,739 --> 01:07:35,779
So both from the training side, the training side actually is- is only just one part of that

1701
01:07:35,779 --> 01:07:39,839
may even become the smaller part of- of what's needed-  yeah.

1702
01:07:39,839 --> 01:07:39,839
.

1703
01:07:39,839 --> 01:07:39,839
.

1704
01:07:39,839 --> 01:07:42,439
um, uh, uh, in the overall compute that- that's required.

1705
01:07:42,439 --> 01:07:45,639
Yeah, that's one sort of almost memey kind of thing

1706
01:07:45,639 --> 01:07:48,959
which is like the success and the incredible aspects of VO3.

1707
01:07:48,959 --> 01:07:50,923
 There's, uh.

1708
01:07:50,923 --> 01:07:51,059
..

1709
01:07:51,059 --> 01:07:52,139
People kind of make fun of, like

1710
01:07:52,139 --> 01:07:54,059
the more successful it becomes, you know

1711
01:07:54,059 --> 01:07:55,319
the servers are sweating.

1712
01:07:55,319 --> 01:07:55,960
Yes.

1713
01:07:55,960 --> 01:07:56,459
 Exactly.

1714
01:07:56,459 --> 01:07:56,779
They're melting.

1715
01:07:56,779 --> 01:07:57,199
To do inference.

1716
01:07:57,199 --> 01:07:57,840
 Yeah, yeah.

1717
01:07:57,840 --> 01:07:58,279
Exactly.

1718
01:07:58,279 --> 01:08:01,939
We did a little video of, of the se- of the servers frying eggs and things.

1719
01:08:01,939 --> 01:08:01,959
Yeah.

1720
01:08:01,959 --> 01:08:03,739
And, um, that's right.

1721
01:08:03,739 --> 01:08:06,099
And, and, and we're gonna have to figure out how to do that.

1722
01:08:06,099 --> 01:08:09,139
Um, there's a lot of interesting hardware innovations that we do.

1723
01:08:09,139 --> 01:08:11,619
As you know, we have our own TPU line and we're looking at

1724
01:08:11,619 --> 01:08:15,599
like, inference-only things, inference-only chips and how we can make those more efficient.

1725
01:08:15,599 --> 01:08:20,639
We're also very interested in building AI systems and we have done the help with energy usage.

1726
01:08:20,639 --> 01:08:23,839
So help, um, data center energy, like

1727
01:08:23,839 --> 01:08:26,158
for the cooling systems be efficient, um

1728
01:08:26,158 --> 01:08:30,520
grid optimization, um, and then eventually, things like helping with

1729
01:08:30,520 --> 01:08:32,459
uh, plasma containment fusion reactors.

1730
01:08:32,459 --> 01:08:36,439
We've done lots of work on that with Commonwealth Fusion and also

1731
01:08:36,439 --> 01:08:39,100
uh, one could imagine reactor design, um

1732
01:08:39,100 --> 01:08:43,459
and then material design, I think, is one of the most exciting new types of solar material

1733
01:08:43,459 --> 01:08:49,319
solar panel material, superc- room temperature superconductors has always been on my list of dream breakthroughs and

1734
01:08:49,319 --> 01:08:50,859
um, optimal batteries.

1735
01:08:50,859 --> 01:08:52,618
And I think a solution to any, you know

1736
01:08:52,618 --> 01:08:55,939
one of those things would be absolutely revolutionary for

1737
01:08:55,939 --> 01:08:58,238
you know, climate and energy usage.

1738
01:08:58,238 --> 01:09:00,519
And we're probably close, you know, again

1739
01:09:00,519 --> 01:09:04,618
in the next five years to having AI systems that can materially help with those problems.

1740
01:09:04,618 --> 01:09:06,125
If you were to bet.

1741
01:09:06,125 --> 01:09:06,299
..

1742
01:09:06,299 --> 01:09:07,538
Sorry for the ridiculous question.

1743
01:09:07,538 --> 01:09:07,578
Yeah.

1744
01:09:07,578 --> 01:09:10,118
But what, what is the main source of energy

1745
01:09:10,118 --> 01:09:13,519
uh, in, like, 20, 30, 40 years?

1746
01:09:13,519 --> 01:09:15,179
Do you think it's gonna be nuclear fusion?

1747
01:09:15,179 --> 01:09:18,639
I think fusion and solar are the two that I

1748
01:09:18,639 --> 01:09:19,578
I would bet on.

1749
01:09:19,578 --> 01:09:21,479
Um, solar, I mean, you know

1750
01:09:21,479 --> 01:09:23,399
it's the fusion reactor in the sky, of course.

1751
01:09:23,399 --> 01:09:26,259
And I think really, the, the problem there is

1752
01:09:26,259 --> 01:09:27,979
is, is batteries and transmission.

1753
01:09:27,979 --> 01:09:30,339
So, you know, as well as more efficient

1754
01:09:30,339 --> 01:09:32,618
more and more efficient solar material, perhaps eventually

1755
01:09:32,618 --> 01:09:36,299
you know, in space, you know, these kind of Dyson sphere type ideas.

1756
01:09:36,299 --> 01:09:40,139
And fusion, I think, is definitely doable

1757
01:09:40,139 --> 01:09:45,698
it seems, uh, if we have the right design of reactor and we can control the plasma and

1758
01:09:45,698 --> 01:09:47,038
uh, fast enough and so on.

1759
01:09:47,038 --> 01:09:50,279
And I think both of those things will actually get solved.

1760
01:09:50,279 --> 01:09:51,578
So we'll probably have at least.

1761
01:09:51,578 --> 01:09:51,679
..

1762
01:09:51,679 --> 01:09:54,779
Those are probably the two primary sources of renewable

1763
01:09:54,779 --> 01:09:57,719
clean, almost free, or perhaps free energy.

1764
01:09:57,719 --> 01:09:59,459
What a time to be alive.

1765
01:09:59,459 --> 01:10:03,199
If I, uh, traveled into the future with you

1766
01:10:03,199 --> 01:10:11,379
100 years from now, how much would you be surprised if we've passed a type I Kardashev scale civilization?

1767
01:10:11,379 --> 01:10:16,239
I would not be that surprised if there's a h- like a 100-year time scale from here.

1768
01:10:16,239 --> 01:10:21,199
I mean, I think it's pretty clear if we crack the energy problems in one of the ways we've just discussed

1769
01:10:21,199 --> 01:10:23,539
fusion or, or very efficient solar.

1770
01:10:23,539 --> 01:10:28,379
Um, then if energy is kind of free and renewable and clean

1771
01:10:28,379 --> 01:10:32,139
um, then that solves a whole bunch of other problems.

1772
01:10:32,139 --> 01:10:37,239
So for example, the water access problem goes away because you can just use desalination.

1773
01:10:37,239 --> 01:10:39,419
We have the technology, it's just too expensive.

1774
01:10:39,419 --> 01:10:43,859
So only, you know, uh, fairly wealthy countries like Singapore and Israel and so on

1775
01:10:43,859 --> 01:10:45,099
like, actually use it.

1776
01:10:45,099 --> 01:10:47,119
But, but if it was, uh, cheap

1777
01:10:47,119 --> 01:10:49,439
then ev- then, you know, all countries that have a coast could.

1778
01:10:49,439 --> 01:10:51,279
But also, you'd have unlimited rocket fuel.

1779
01:10:51,279 --> 01:10:55,619
You could just separate seawater out into hydrogen and oxygen using energy

1780
01:10:55,619 --> 01:10:56,859
and that's rocket fuel.

1781
01:10:56,859 --> 01:10:59,479
So, uh, combined with, you know

1782
01:10:59,479 --> 01:11:05,279
Elon's amazing self-landing rockets, then it could be like- it would sort of- like a bus service to

1783
01:11:05,279 --> 01:11:05,979
to space.

1784
01:11:05,979 --> 01:11:10,199
So that opens up, you know, incredible new resources and domains.

1785
01:11:10,199 --> 01:11:14,919
Uh, asteroid mining I think will become a thing and maximum human flourishing to the stars.

1786
01:11:14,919 --> 01:11:16,939
Like, that's what I, uh, dream about as well

1787
01:11:16,939 --> 01:11:20,819
is like Carl Sagan's sort of idea of bringing consciousness to the universe

1788
01:11:20,819 --> 01:11:22,019
waking up the universe.

1789
01:11:22,019 --> 01:11:27,959
And I, I think human civilization will do that in the full sense of time if we get AI right and

1790
01:11:27,959 --> 01:11:30,499
uh, and, and, and crack some of these problems with it.

1791
01:11:30,499 --> 01:11:32,499
Yeah, I wonder what it would look like if you're just

1792
01:11:32,499 --> 01:11:34,679
uh, tourists flying through space.

1793
01:11:34,679 --> 01:11:37,459
You would probably notice Earth.

1794
01:11:37,459 --> 01:11:37,619
..

1795
01:11:37,619 --> 01:11:42,019
Because if you solve the energy problem, you would see a lot of space rockets probably.

1796
01:11:42,019 --> 01:11:42,079
Mm-hmm.

1797
01:11:42,079 --> 01:11:45,359
So it would be like traffic  here in London- Yep.

1798
01:11:45,359 --> 01:11:45,365
.

1799
01:11:45,365 --> 01:11:45,379
..

1800
01:11:45,379 --> 01:11:46,199
but in space.

1801
01:11:46,199 --> 01:11:47,019
 Yes, exactly.

1802
01:11:47,019 --> 01:11:48,099
It's just a lot of rockets.

1803
01:11:48,099 --> 01:11:48,439
Yes.

1804
01:11:48,439 --> 01:11:52,039
And then you would probably see, floating in space

1805
01:11:52,039 --> 01:11:54,339
some kind of source of energy like solar- Yeah.

1806
01:11:54,339 --> 01:11:54,432
.

1807
01:11:54,432 --> 01:11:54,619
..

1808
01:11:54,619 --> 01:11:55,139
potentially.

1809
01:11:55,139 --> 01:11:58,519
So Earth would just look more, on the surface

1810
01:11:58,519 --> 01:12:00,539
more, um, technological.

1811
01:12:00,539 --> 01:12:05,839
And then, then you would use the power of that energy then to preserve the natural- Yes.

1812
01:12:05,839 --> 01:12:05,939
.

1813
01:12:05,939 --> 01:12:06,139
..

1814
01:12:06,139 --> 01:12:07,819
like, the rainforest and all that kind of stuff.

1815
01:12:07,819 --> 01:12:07,859
Exactly.

1816
01:12:07,859 --> 01:12:10,259
Because for the first time in, in human history

1817
01:12:10,259 --> 01:12:13,499
we wouldn't be, uh, resource constrained.

1818
01:12:13,499 --> 01:12:13,819
Mm-hmm.

1819
01:12:13,819 --> 01:12:19,659
And I think that could be amazing new era for humanity where it's not zero sum

1820
01:12:19,659 --> 01:12:20,219
right?

1821
01:12:20,219 --> 01:12:20,259
Mm-hmm.

1822
01:12:20,259 --> 01:12:21,699
I have this land, you don't have it.

1823
01:12:21,699 --> 01:12:22,810
Or if we take.

1824
01:12:22,810 --> 01:12:22,919
..

1825
01:12:22,919 --> 01:12:24,939
You know, if the tigers have their forest

1826
01:12:24,939 --> 01:12:26,909
then the, the local villagers can't.

1827
01:12:26,909 --> 01:12:27,019
..

1828
01:12:27,019 --> 01:12:28,179
What are they gonna use?

1829
01:12:28,179 --> 01:12:30,459
I, I, I think that this will help a lot.

1830
01:12:30,459 --> 01:12:33,639
No, it won't solve all problems because there's still other human

1831
01:12:33,639 --> 01:12:36,479
uh, foibles that will, will, will still exist

1832
01:12:36,479 --> 01:12:38,559
but it will at least remove one, I think

1833
01:12:38,559 --> 01:12:42,159
one of the big vectors which is scarcity of resources

1834
01:12:42,159 --> 01:12:44,939
you know, including land and raw materials and energy.

1835
01:12:44,939 --> 01:12:46,931
And, um, we know we should be.

1836
01:12:46,931 --> 01:12:46,979
..

1837
01:12:46,979 --> 01:12:51,199
Some of us call it, like, and others call it about this kind of radical abundance era where

1838
01:12:51,199 --> 01:12:53,599
um, there's plenty of resources to go around.

1839
01:12:53,599 --> 01:12:57,019
Of course, the next big question is making sure that that's fairly

1840
01:12:57,019 --> 01:13:00,739
you know, shared fairly, uh, and everyone in society benefits from that.

1841
01:13:00,739 --> 01:13:03,831
So there is something about human nature where I go.

1842
01:13:03,831 --> 01:13:04,039
..

1843
01:13:04,039 --> 01:13:06,359
You know, it's like , it's like Borat

1844
01:13:06,359 --> 01:13:07,419
like, "My neighbor.

1845
01:13:07,419 --> 01:13:08,107
" Like.

1846
01:13:08,107 --> 01:13:08,239
..

1847
01:13:08,239 --> 01:13:09,899
 Like, you start trouble.

1848
01:13:09,899 --> 01:13:15,519
We, we, we do start conflicts and that's why games throughout

1849
01:13:15,519 --> 01:13:18,879
as I'm learning actually more and more, even in ancient history

1850
01:13:18,879 --> 01:13:22,579
served the purpose of pushing people away from war- Yes.

1851
01:13:22,579 --> 01:13:22,579
.

1852
01:13:22,579 --> 01:13:22,579
.

1853
01:13:22,579 --> 01:13:23,579
actual hot war.

1854
01:13:23,579 --> 01:13:28,839
So maybe we can figure out increasingly sophisticated video games that pull us.

1855
01:13:28,839 --> 01:13:28,919
..

1856
01:13:28,919 --> 01:13:32,139
They, they give us that, uh- Visual.

1857
01:13:32,139 --> 01:13:33,159
Scratch the itch- Yeah.

1858
01:13:33,159 --> 01:13:33,165
.

1859
01:13:33,165 --> 01:13:33,179
..

1860
01:13:33,179 --> 01:13:36,799
of, like, conflict, whatever that is abo- about us

1861
01:13:36,799 --> 01:13:46,959
the human nature, and then avoid the actual hot wars that would come with increasingly sophisticated technologies because we're now.

1862
01:13:46,959 --> 01:13:47,099
..

1863
01:13:47,099 --> 01:13:53,159
We've long passed the stage where the weapons we're able to create can actually just destroy all of human civilizations.

1864
01:13:53,159 --> 01:13:53,179
Yeah.

1865
01:13:53,179 --> 01:13:54,152
So it's no longer.

1866
01:13:54,152 --> 01:13:54,299
..

1867
01:13:54,299 --> 01:13:58,022
Um, that's no longer a great way.

1868
01:13:58,022 --> 01:13:58,304
..

1869
01:13:58,304 --> 01:13:58,586
..

1870
01:13:58,586 --> 01:14:00,928
. to, uh, start shit with your neighbor.

1871
01:14:00,928 --> 01:14:03,787
It's better to play a game of chess and- Or football.

1872
01:14:03,787 --> 01:14:03,820
.

1873
01:14:03,820 --> 01:14:03,887
..

1874
01:14:03,887 --> 01:14:04,427
or football.

1875
01:14:04,427 --> 01:14:04,555
Or, or.

1876
01:14:04,555 --> 01:14:04,627
..

1877
01:14:04,627 --> 01:14:04,847
Yeah.

1878
01:14:04,847 --> 01:14:05,148
Yeah.

1879
01:14:05,148 --> 01:14:05,762
And I think.

1880
01:14:05,762 --> 01:14:05,827
..

1881
01:14:05,827 --> 01:14:08,339
I mean, I think that's what, why modern sport is so.

1882
01:14:08,339 --> 01:14:08,467
..

1883
01:14:08,467 --> 01:14:10,168
And I love football, watching it and

1884
01:14:10,168 --> 01:14:11,847
and I just feel like, uh.

1885
01:14:11,847 --> 01:14:11,967
..

1886
01:14:11,967 --> 01:14:14,168
And I used to play it a lot as well and it's

1887
01:14:14,168 --> 01:14:17,687
it's, it's, it's, it's very visceral and it's tribal- Mm-hmm.

1888
01:14:17,687 --> 01:14:17,694
.

1889
01:14:17,694 --> 01:14:17,708
..

1890
01:14:17,708 --> 01:14:20,578
and I think it does channel a lot of those energies into a.

1891
01:14:20,578 --> 01:14:20,708
..

1892
01:14:20,708 --> 01:14:23,808
Which I think is a kinda human need to belong to some

1893
01:14:23,808 --> 01:14:24,787
some group.

1894
01:14:24,787 --> 01:14:27,687
And, um, but into a, into a

1895
01:14:27,687 --> 01:14:29,867
into a fun way, a, a healthy way

1896
01:14:29,867 --> 01:14:31,728
and, and a not, a not destructive way

1897
01:14:31,728 --> 01:14:33,767
kinda constructive, uh, thing.

1898
01:14:33,767 --> 01:14:42,167
And I think going back to games again is I think they're originally why they're so great as well for kids to play things like chess is they're great little microcosm simulations of the world.

1899
01:14:42,167 --> 01:14:42,547
Mm-hmm.

1900
01:14:42,547 --> 01:14:43,808
They, they are simulations of the world too.

1901
01:14:43,808 --> 01:14:47,287
They're simplified versions of some real-world situation, whether it's poker or

1902
01:14:47,287 --> 01:14:49,595
or Go or chess, different aspects.

1903
01:14:49,595 --> 01:14:49,687
..

1904
01:14:49,687 --> 01:14:50,387
Or diplomacy.

1905
01:14:50,387 --> 01:14:50,507
Mm-hmm.

1906
01:14:50,507 --> 01:14:53,147
Different aspects of, of the real world.

1907
01:14:53,147 --> 01:14:55,927
And it allows you to practice at them too and

1908
01:14:55,927 --> 01:15:00,567
and 'cause, you know, how many times do you get to practice a massive decision moment in your life?

1909
01:15:00,567 --> 01:15:03,367
You know, what job to take, what university to go to.

1910
01:15:03,367 --> 01:15:04,767
You know, you get maybe, I don't know

1911
01:15:04,767 --> 01:15:07,467
a dozen or so key decisions one has to make

1912
01:15:07,467 --> 01:15:09,307
and you gotta make those as best as you can.

1913
01:15:09,307 --> 01:15:12,107
Um, and games is a kind of safe environment

1914
01:15:12,107 --> 01:15:16,127
repeatable environment, where you can get better at your decision-making process.

1915
01:15:16,127 --> 01:15:21,427
Um, and it maybe has this in- a- additional benefit of channeling some energies into

1916
01:15:21,427 --> 01:15:24,347
uh, into more creative and constructive pursuits.

1917
01:15:24,347 --> 01:15:26,487
Well, I think it's also really important to practice

1918
01:15:26,487 --> 01:15:28,447
um, losing and winning.

1919
01:15:28,447 --> 01:15:29,087
Right.

1920
01:15:29,087 --> 01:15:30,620
Like losing is a really.

1921
01:15:30,620 --> 01:15:30,727
..

1922
01:15:30,727 --> 01:15:32,007
You know, that's why I love games.

1923
01:15:32,007 --> 01:15:34,007
That's why I love even, um, things like

1924
01:15:34,007 --> 01:15:35,467
uh, Brazilian jiu-jitsu- Yeah.

1925
01:15:35,467 --> 01:15:35,567
.

1926
01:15:35,567 --> 01:15:35,767
..

1927
01:15:35,767 --> 01:15:39,687
where you can get your ass kicked in a safe environment over and over.

1928
01:15:39,687 --> 01:15:42,233
It reminds you about the way.

1929
01:15:42,233 --> 01:15:42,347
..

1930
01:15:42,347 --> 01:15:44,167
about physics, about the way the world works

1931
01:15:44,167 --> 01:15:46,427
about sometimes you lose, sometimes you win.

1932
01:15:46,427 --> 01:15:48,327
You can still be friends with everybody.

1933
01:15:48,327 --> 01:15:48,547
Yeah.

1934
01:15:48,547 --> 01:15:51,987
But that, that feeling of losing, I mean

1935
01:15:51,987 --> 01:15:53,967
it's a weird one for us humans to

1936
01:15:53,967 --> 01:15:56,507
like, really, like, make sense of.

1937
01:15:56,507 --> 01:15:57,887
Like, that's just part of life.

1938
01:15:57,887 --> 01:15:59,907
That is a fundamental part of life is losing.

1939
01:15:59,907 --> 01:16:00,527
Yeah.

1940
01:16:00,527 --> 01:16:02,347
And I think in martial arts as I understand it

1941
01:16:02,347 --> 01:16:04,127
but also in things like, like chess

1942
01:16:04,127 --> 01:16:04,367
is a lo-.

1943
01:16:04,367 --> 01:16:04,407
..

1944
01:16:04,407 --> 01:16:07,267
At least the way I took it, it's a lot to do with self-improvement- Mm-hmm.

1945
01:16:07,267 --> 01:16:07,300
.

1946
01:16:07,300 --> 01:16:07,367
..

1947
01:16:07,367 --> 01:16:08,447
self-knowledge.

1948
01:16:08,447 --> 01:16:11,507
You know, the, "Okay, so I did this thing.

1949
01:16:11,507 --> 01:16:13,687
" It's not about really being the other person.

1950
01:16:13,687 --> 01:16:16,067
It's about maximizing your own potential.

1951
01:16:16,067 --> 01:16:17,407
If you do it in a healthy way

1952
01:16:17,407 --> 01:16:20,767
you learn to use victory and losses in a way.

1953
01:16:20,767 --> 01:16:22,327
Don't get carried away with victory- Yeah.

1954
01:16:22,327 --> 01:16:22,413
.

1955
01:16:22,413 --> 01:16:22,587
..

1956
01:16:22,587 --> 01:16:24,587
and, and think you're the just the best in the world.

1957
01:16:24,587 --> 01:16:24,869
Keep.

1958
01:16:24,869 --> 01:16:24,927
..

1959
01:16:24,927 --> 01:16:29,627
And, and, and the losses keep you humble and always knowing there's always something more to learn

1960
01:16:29,627 --> 01:16:32,287
there's always a bigger expert that you can mentor you.

1961
01:16:32,287 --> 01:16:33,687
You know, I think you learn that

1962
01:16:33,687 --> 01:16:35,527
I, I, I'm pretty sure, in martial arts and

1963
01:16:35,527 --> 01:16:37,947
and, and I think that's also, uh

1964
01:16:37,947 --> 01:16:39,787
the way that at least I was trained in chess.

1965
01:16:39,787 --> 01:16:43,307
And so in the same way, and it can be very hardcore and very important

1966
01:16:43,307 --> 01:16:47,087
and, of course, you wanna win, but you also need to learn how to deal with setbacks

1967
01:16:47,087 --> 01:16:49,009
uh, in a, in a healthy way that.

1968
01:16:49,009 --> 01:16:49,107
..

1969
01:16:49,107 --> 01:16:50,727
Um, and, and, and wire that

1970
01:16:50,727 --> 01:16:54,447
that feeling that you have when you lose something into a constructive thing of

1971
01:16:54,447 --> 01:16:56,847
"Next time, I'm gonna improve this," right?

1972
01:16:56,847 --> 01:16:57,857
Or, "get better at this.

1973
01:16:57,857 --> 01:17:00,367
" There is something that's a source of happiness

1974
01:17:00,367 --> 01:17:02,607
a source of meaning, that improvement step.

1975
01:17:02,607 --> 01:17:04,147
It's not about the winning or losing.

1976
01:17:04,147 --> 01:17:05,127
Yeah, it's the mastery.

1977
01:17:05,127 --> 01:17:05,527
Yeah.

1978
01:17:05,527 --> 01:17:07,247
There's nothing more satisfying in a way.

1979
01:17:07,247 --> 01:17:09,507
It's like, "Oh, wow, this thing I couldn't do before

1980
01:17:09,507 --> 01:17:10,375
now I can.

1981
01:17:10,375 --> 01:17:13,747
" And, and, and again, games and physical sports and

1982
01:17:13,747 --> 01:17:16,047
and mental sports, their wa- their ways of measuring

1983
01:17:16,047 --> 01:17:19,167
they're beautiful because you can measure that, that progress.

1984
01:17:19,167 --> 01:17:19,387
Yeah.

1985
01:17:19,387 --> 01:17:19,587
Right?

1986
01:17:19,587 --> 01:17:20,767
I mean, th- there's something about.

1987
01:17:20,767 --> 01:17:20,887
..

1988
01:17:20,887 --> 01:17:22,767
That is why I love role-playing games, like the

1989
01:17:22,767 --> 01:17:25,327
uh, number go up of, like, my-  Yes.

1990
01:17:25,327 --> 01:17:25,333
.

1991
01:17:25,333 --> 01:17:25,347
..

1992
01:17:25,347 --> 01:17:26,287
on the skill tree.

1993
01:17:26,287 --> 01:17:26,587
Exactly.

1994
01:17:26,587 --> 01:17:28,987
 Like literally, that is a source of meaning for us humans.

1995
01:17:28,987 --> 01:17:32,187
Whatever our- Yeah, we're quite, we're, we're quite addicted to this sort of-  Yeah.

1996
01:17:32,187 --> 01:17:34,947
These numbers going up and, uh, and

1997
01:17:34,947 --> 01:17:35,467
and- Yeah.

1998
01:17:35,467 --> 01:17:35,467
.

1999
01:17:35,467 --> 01:17:35,467
.

2000
01:17:35,467 --> 01:17:36,007
and, and, and maybe-  .

2001
01:17:36,007 --> 01:17:36,007
..

2002
01:17:36,007 --> 01:17:37,327
that's why we make games like that- Yeah.

2003
01:17:37,327 --> 01:17:37,347
.

2004
01:17:37,347 --> 01:17:37,387
..

2005
01:17:37,387 --> 01:17:39,147
because obviously that is something we're.

2006
01:17:39,147 --> 01:17:39,287
..

2007
01:17:39,287 --> 01:17:41,767
We're, we're hill-climbing systems ourselves, right?

2008
01:17:41,767 --> 01:17:42,384
Yeah, it's.

2009
01:17:42,384 --> 01:17:42,447
..

2010
01:17:42,447 --> 01:17:44,647
It would be quite sad if we didn't have- Yeah.

2011
01:17:44,647 --> 01:17:44,647
.

2012
01:17:44,647 --> 01:17:44,647
.

2013
01:17:44,647 --> 01:17:46,487
any mechanism by- Different color belts.

2014
01:17:46,487 --> 01:17:46,567
 Exactly.

2015
01:17:46,567 --> 01:17:46,860
All of the.

2016
01:17:46,860 --> 01:17:46,907
..

2017
01:17:46,907 --> 01:17:48,367
We do, we do this everywhere, right?

2018
01:17:48,367 --> 01:17:48,907
Where we just-  .

2019
01:17:48,907 --> 01:17:48,907
..

2020
01:17:48,907 --> 01:17:50,387
have this thing that it's great.

2021
01:17:50,387 --> 01:17:50,501
It's.

2022
01:17:50,501 --> 01:17:50,547
..

2023
01:17:50,547 --> 01:17:51,907
And, uh, I don't wanna dismiss that

2024
01:17:51,907 --> 01:17:53,647
that there is a source of deep meaning- Yeah.

2025
01:17:53,647 --> 01:17:53,653
.

2026
01:17:53,653 --> 01:17:53,667
..

2027
01:17:53,667 --> 01:17:54,147
as humans.

2028
01:17:54,147 --> 01:17:57,467
Uh, so one of the incredible stories on the business

2029
01:17:57,467 --> 01:18:00,447
on the leadership side is, um, what Google has

2030
01:18:00,447 --> 01:18:01,667
has done over the past year.

2031
01:18:01,667 --> 01:18:08,087
 So I, uh, I think it's fair to say that Google was losing on the LLM product side

2032
01:18:08,087 --> 01:18:10,207
uh, a year ago when Gemini won five

2033
01:18:10,207 --> 01:18:11,707
and now it's winning- Mm-hmm.

2034
01:18:11,707 --> 01:18:11,720
.

2035
01:18:11,720 --> 01:18:11,747
..

2036
01:18:11,747 --> 01:18:12,660
with Gemini 2.

2037
01:18:12,660 --> 01:18:15,467
5 and you took the helm and you led this effort.

2038
01:18:15,467 --> 01:18:17,947
What did it take to go from, let's say

2039
01:18:17,947 --> 01:18:20,967
quote-unquote, "losing" to, quote-unquote, "winning" in the

2040
01:18:20,967 --> 01:18:22,147
in, in the span of a year?

2041
01:18:22,147 --> 01:18:25,387
Yeah, well, firstly, it's absolutely incredible team that we have

2042
01:18:25,387 --> 01:18:27,747
you know, led by Koray and Jeff Dean and

2043
01:18:27,747 --> 01:18:31,047
and Oriol and the amazing team we have on Gemini

2044
01:18:31,047 --> 01:18:32,727
absolutely world-class.

2045
01:18:32,727 --> 01:18:35,327
So you can't do it without the best talent.

2046
01:18:35,327 --> 01:18:37,738
Um, and of course, you have.

2047
01:18:37,738 --> 01:18:37,847
..

2048
01:18:37,847 --> 01:18:39,687
You know, we have a lot of great compute as well

2049
01:18:39,687 --> 01:18:42,807
but then it's the research culture we've created- Mm-hmm.

2050
01:18:42,807 --> 01:18:42,813
.

2051
01:18:42,813 --> 01:18:42,827
..

2052
01:18:42,827 --> 01:18:43,047
right?

2053
01:18:43,047 --> 01:18:46,707
And basically coming together, both different groups in

2054
01:18:46,707 --> 01:18:47,427
in Google.

2055
01:18:47,427 --> 01:18:49,467
You know, there was Google Brain, world-class team

2056
01:18:49,467 --> 01:18:59,127
and, and then the old DeepMind, and pulling together all the best people and the best ideas and gathering around to make the absolute greatest system we could.

2057
01:18:59,127 --> 01:19:03,487
And it has been hard, um, but we're all very competitive

2058
01:19:03,487 --> 01:19:06,267
uh, and we, you know, love research.

2059
01:19:06,267 --> 01:19:07,507
This is so fun to do.

2060
01:19:07,507 --> 01:19:09,047
Um, and we've.

2061
01:19:09,047 --> 01:19:09,127
..

2062
01:19:09,127 --> 01:19:10,707
You know, it's great to see our trajectory.

2063
01:19:10,707 --> 01:19:13,067
It wasn't a given, but we're very pleased with

2064
01:19:13,067 --> 01:19:14,313
um, the, the.

2065
01:19:14,313 --> 01:19:14,387
..

2066
01:19:14,387 --> 01:19:17,167
where we are and the rate of progress is the most important thing.

2067
01:19:17,167 --> 01:19:18,967
So if you look at where we've come

2068
01:19:18,967 --> 01:19:19,167
two.

2069
01:19:19,167 --> 01:19:19,227
..

2070
01:19:19,227 --> 01:19:21,687
from two years ago to one year ago to now

2071
01:19:21,687 --> 01:19:24,887
you know, I think our, we call it relentless progress

2072
01:19:24,887 --> 01:19:27,987
along with relentless shipping of that progress, is

2073
01:19:27,987 --> 01:19:29,307
um, being very successful.

2074
01:19:29,307 --> 01:19:32,307
And, you know, um, it's unbelievably competitive

2075
01:19:32,307 --> 01:19:34,747
uh, the whole space, the whole AI space

2076
01:19:34,747 --> 01:19:37,567
with some of the greatest entrepreneurs and leaders

2077
01:19:37,567 --> 01:19:43,487
uh, and companies in the world all competing now because everyone's realized how important AI is.

2078
01:19:43,487 --> 01:19:46,947
Um, and it's very, you know, been pleasing for us to see that progress.

2079
01:19:46,947 --> 01:19:49,047
You know, Google's a gigantic company.

2080
01:19:49,047 --> 01:19:50,234
Uh, can you speak to.

2081
01:19:50,234 --> 01:19:50,452
..

2082
01:19:50,452 --> 01:19:50,670
..

2083
01:19:50,670 --> 01:19:54,380
. the natural things that happen in that case is the bureaucracy that emerges.

2084
01:19:54,380 --> 01:19:55,419
Like you want to be careful.

2085
01:19:55,419 --> 01:20:01,019
Like, you know, like, the- the- the natural kind of there's- there's meetings and there's- Yeah.

2086
01:20:01,019 --> 01:20:01,032
.

2087
01:20:01,032 --> 01:20:01,059
..

2088
01:20:01,059 --> 01:20:02,119
managers and that.

2089
01:20:02,119 --> 01:20:07,099
Like what- what are some of the challenges from a leadership perspective of breaking through that in order to

2090
01:20:07,099 --> 01:20:07,960
like you said, ship?

2091
01:20:07,960 --> 01:20:08,560
Like the amou- Yeah.

2092
01:20:08,560 --> 01:20:08,566
.

2093
01:20:08,566 --> 01:20:08,580
..

2094
01:20:08,580 --> 01:20:10,319
the number of products- Yeah.

2095
01:20:10,319 --> 01:20:10,325
.

2096
01:20:10,325 --> 01:20:10,339
..

2097
01:20:10,339 --> 01:20:14,079
Gemini-related products that's been shipped over the past year is just insane.

2098
01:20:14,079 --> 01:20:14,699
Right.

2099
01:20:14,699 --> 01:20:15,399
It is.

2100
01:20:15,399 --> 01:20:16,659
 Yeah, exactly.

2101
01:20:16,659 --> 01:20:18,640
That's- that's what relentlessness looks like.

2102
01:20:18,640 --> 01:20:22,359
Um, I think it's- it's a question of like any big company

2103
01:20:22,359 --> 01:20:27,339
you know, ends up having, uh, a- a lot of layers of management and things like that.

2104
01:20:27,339 --> 01:20:28,919
It's sort of the nature of how it works.

2105
01:20:28,919 --> 01:20:35,000
Um, but I still operate, and I was always operating with old DeepMind as a- as a startup still.

2106
01:20:35,000 --> 01:20:35,279
Mm-hmm.

2107
01:20:35,279 --> 01:20:37,099
Large one, but still as a startup.

2108
01:20:37,099 --> 01:20:40,259
And that's what we still act like today in- as- with Google DeepMind.

2109
01:20:40,259 --> 01:20:46,459
And acting with decisiveness and the energy that you get from the best smaller organizations.

2110
01:20:46,459 --> 01:20:51,059
And we try to get the best of both worlds where we have this incredible billions of users

2111
01:20:51,059 --> 01:20:57,439
surfaces, uh, incredible products that we can power up with our AI and our- and our research.

2112
01:20:57,439 --> 01:20:59,020
Um, and that's amazing.

2113
01:20:59,020 --> 01:20:59,486
And you can.

2114
01:20:59,486 --> 01:20:59,560
..

2115
01:20:59,560 --> 01:21:01,679
You know, there's very few places in the world you can get that.

2116
01:21:01,679 --> 01:21:04,019
Do incredible world-class research on the one hand

2117
01:21:04,019 --> 01:21:08,000
and then plug it in and- and improve billions of people's lives the next day.

2118
01:21:08,000 --> 01:21:10,519
Uh, that's a pretty amazing combination.

2119
01:21:10,519 --> 01:21:19,099
And we're continually fighting and cutting away bureaucracy to allow the research culture and the relentless shipping culture to flourish.

2120
01:21:19,099 --> 01:21:22,939
And I think we've got a pretty good balance whilst being responsible with it

2121
01:21:22,939 --> 01:21:24,219
you know, as you have to be

2122
01:21:24,219 --> 01:21:26,239
uh, as a large company and also

2123
01:21:26,239 --> 01:21:28,239
uh, with a number of, you know

2124
01:21:28,239 --> 01:21:30,279
uh, huge product surfaces that we have.

2125
01:21:30,279 --> 01:21:33,779
Uh, so funny thing you mentioned about like the- the surface with a billion.

2126
01:21:33,779 --> 01:21:36,019
I- I had a conversation with a guy named

2127
01:21:36,019 --> 01:21:40,139
um, brilliant guy, uh, here at the British Museum called Irvin Finkel.

2128
01:21:40,139 --> 01:21:46,719
He's a world expert at cuneiforms, which is a ancient writing on tablets.

2129
01:21:46,719 --> 01:21:46,839
Yep.

2130
01:21:46,839 --> 01:21:51,019
And he doesn't know about ChatGPT or Gemini.

2131
01:21:51,019 --> 01:21:51,079
Mm-hmm.

2132
01:21:51,079 --> 01:21:52,739
He doesn't even know anything about AI.

2133
01:21:52,739 --> 01:21:52,959
Mm-hmm.

2134
01:21:52,959 --> 01:21:58,039
But his first encounter with this AI is AI mode on  Google.

2135
01:21:58,039 --> 01:21:58,119
Yes.

2136
01:21:58,119 --> 01:21:58,379
Yes.

2137
01:21:58,379 --> 01:22:00,070
He's like, "Is that what you're talking about?

2138
01:22:00,070 --> 01:22:00,179
" Yeah.

2139
01:22:00,179 --> 01:22:01,605
"This AI mode?

2140
01:22:01,605 --> 01:22:07,959
"   And then, you know, it's just inc- it's just a reminder that there's a large part of the world that doesn't know about this AI thing.

2141
01:22:07,959 --> 01:22:08,519
Yeah.

2142
01:22:08,519 --> 01:22:09,439
 And then- I know, it's funny

2143
01:22:09,439 --> 01:22:12,699
because if you live on, uh, X and Twitter and I mean

2144
01:22:12,699 --> 01:22:13,939
it's sort of, at least my feed

2145
01:22:13,939 --> 01:22:14,719
it's all AI.

2146
01:22:14,719 --> 01:22:16,439
And- and there's certain places where, you know

2147
01:22:16,439 --> 01:22:19,019
in the Valley and certain pockets where everyone's just

2148
01:22:19,019 --> 01:22:20,419
all they're thinking about is AI.

2149
01:22:20,419 --> 01:22:24,619
But a lot of the normal world hasn't- hasn't come across it yet

2150
01:22:24,619 --> 01:22:28,439
but- And that's a great responsibility to w- the- their first interaction- Yeah.

2151
01:22:28,439 --> 01:22:28,539
.

2152
01:22:28,539 --> 01:22:28,739
..

2153
01:22:28,739 --> 01:22:33,379
on the- the- the grand scale of the rural India or an- anywhere across the world

2154
01:22:33,379 --> 01:22:34,119
like you get to- Right.

2155
01:22:34,119 --> 01:22:34,399
Right.

2156
01:22:34,399 --> 01:22:35,879
And we want it to be as good as possible.

2157
01:22:35,879 --> 01:22:37,899
And in a lot of cases, it's just under the hood

2158
01:22:37,899 --> 01:22:41,119
powering, making something like maps or search work better.

2159
01:22:41,119 --> 01:22:44,699
And, um, and e- ideally for a lot of those people

2160
01:22:44,699 --> 01:22:45,679
it should just be seamless.

2161
01:22:45,679 --> 01:22:48,099
It's just new technology that makes their lives more

2162
01:22:48,099 --> 01:22:50,159
you know, productive and- and- and helps them.

2163
01:22:50,159 --> 01:22:58,139
A bunch of folks on the Gemini product and engineering teams spoken extremely highly of you on another dimension that I almost didn't e- even expect

2164
01:22:58,139 --> 01:23:05,379
because I kind of th- think of you as the like deep scientist in caring about these big research scientific questions.

2165
01:23:05,379 --> 01:23:07,619
But they also said you're a great product guy.

2166
01:23:07,619 --> 01:23:12,019
Like how to create a thing that a lot of people would use and enjoy using.

2167
01:23:12,019 --> 01:23:12,219
Mm-hmm.

2168
01:23:12,219 --> 01:23:18,759
So can you maybe speak to what it takes to create a- a- AI-based product that a lot of people would enjoy using?

2169
01:23:18,759 --> 01:23:19,219
Yeah.

2170
01:23:19,219 --> 01:23:24,679
Well, I mean, again, that comes back from my game design days where I used to design games for millions of gamers.

2171
01:23:24,679 --> 01:23:25,759
People forget about that.

2172
01:23:25,759 --> 01:23:28,939
I've- I've had experience with cutting edge technology in product.

2173
01:23:28,939 --> 01:23:29,079
Right.

2174
01:23:29,079 --> 01:23:31,579
That- that- that- that is how games was in the '90s.

2175
01:23:31,579 --> 01:23:40,919
And so I love actually the combination of cutting edge research and then being applied in a product and- and to power a new experience.

2176
01:23:40,919 --> 01:23:44,879
And so, um, I think it's the same skill really of- of

2177
01:23:44,879 --> 01:23:47,959
you know, imagining what it would be like to use it viscerally

2178
01:23:47,959 --> 01:23:51,099
um, and having good taste, coming back to earlier.

2179
01:23:51,099 --> 01:23:53,699
The same thing that's useful in science, um

2180
01:23:53,699 --> 01:23:57,159
I think is- is- can also be useful in- in product design.

2181
01:23:57,159 --> 01:23:59,999
And, um, I- I've just had a very

2182
01:23:59,999 --> 01:24:02,479
you know, always been a sort of multidisciplinary person

2183
01:24:02,479 --> 01:24:05,559
so I don't see, uh, the boundaries really between

2184
01:24:05,559 --> 01:24:08,999
you know, arts and sciences or product and research.

2185
01:24:08,999 --> 01:24:10,379
It's- it's a continuum for me.

2186
01:24:10,379 --> 01:24:11,487
I mean, I only work on.

2187
01:24:11,487 --> 01:24:11,639
..

2188
01:24:11,639 --> 01:24:13,279
I like working on products that are cutting edge.

2189
01:24:13,279 --> 01:24:14,179
I wouldn't be able to, you know

2190
01:24:14,179 --> 01:24:16,279
have cutting edge technology under the hood.

2191
01:24:16,279 --> 01:24:19,439
I wouldn't be excited about them if they were just run-of-the-mill products.

2192
01:24:19,439 --> 01:24:23,579
Um, so it requires this invention, creativity cap- capability.

2193
01:24:23,579 --> 01:24:23,759
Mm-hmm.

2194
01:24:23,759 --> 01:24:27,319
What are some specific things you kind of learned about when you

2195
01:24:27,319 --> 01:24:30,459
um, even on the LLM side, you're interacting with Gemini.

2196
01:24:30,459 --> 01:24:30,819
Yeah.

2197
01:24:30,819 --> 01:24:34,019
You're like, this doesn't feel like the layout

2198
01:24:34,019 --> 01:24:35,599
the- the interface- Yeah.

2199
01:24:35,599 --> 01:24:35,605
.

2200
01:24:35,605 --> 01:24:35,619
..

2201
01:24:35,619 --> 01:24:40,999
maybe the trade-off between the latency, like how- how to present to the user

2202
01:24:40,999 --> 01:24:42,879
how long to wait- Mm-hmm.

2203
01:24:42,879 --> 01:24:42,939
.

2204
01:24:42,939 --> 01:24:43,059
..

2205
01:24:43,059 --> 01:24:46,239
and how that waiting is shown or the reasoning capabilities.

2206
01:24:46,239 --> 01:24:48,079
There's some interesting things, because like you said

2207
01:24:48,079 --> 01:24:48,999
it's a very cutting edge.

2208
01:24:48,999 --> 01:24:50,259
We don't know- Yeah.

2209
01:24:50,259 --> 01:24:50,265
.

2210
01:24:50,265 --> 01:24:50,279
..

2211
01:24:50,279 --> 01:24:52,719
how to present it- how to present it correctly.

2212
01:24:52,719 --> 01:24:55,079
So is there some specific things you've- you've learned?

2213
01:24:55,079 --> 01:24:57,539
I mean, i- it's such a fast evolving space.

2214
01:24:57,539 --> 01:24:59,379
We're evaluating this all the time.

2215
01:24:59,379 --> 01:25:03,879
But where we are today is that you want to continually simplify things

2216
01:25:03,879 --> 01:25:06,759
um, the int- whether that's the interface or- or- Simplify

2217
01:25:06,759 --> 01:25:06,799
yeah.

2218
01:25:06,799 --> 01:25:06,799
.

2219
01:25:06,799 --> 01:25:06,799
.

2220
01:25:06,799 --> 01:25:09,139
the inter- uh, the- what you build on top of the model.

2221
01:25:09,139 --> 01:25:11,159
You kind of want to get out of the way of the model.

2222
01:25:11,159 --> 01:25:15,079
The model train is coming down the track and it's improving unbelievably fast

2223
01:25:15,079 --> 01:25:17,059
this relentless progress we talked about earlier.

2224
01:25:17,059 --> 01:25:18,392
You know, you look at 2.

2225
01:25:18,392 --> 01:25:19,405
5 versus 1.

2226
01:25:19,405 --> 01:25:21,879
5 and it's just a gigantic improvement.

2227
01:25:21,879 --> 01:25:24,459
And we expect that again for the future versions.

2228
01:25:24,459 --> 01:25:26,499
And so the models are becoming more capable.

2229
01:25:26,499 --> 01:25:27,419
So you've got.

2230
01:25:27,419 --> 01:25:27,479
..

2231
01:25:27,479 --> 01:25:30,399
The interesting thing about the design space in- in- in today's world

2232
01:25:30,399 --> 01:25:34,939
these AI first products is you've got to design not for what the thing can do today

2233
01:25:34,939 --> 01:25:37,559
the technology can do today, but in a year's time.

2234
01:25:37,559 --> 01:25:41,499
So you actually have to be a very technical product person

2235
01:25:41,499 --> 01:25:45,899
because, uh, you've got to kind of have a good intuition for and feel for

2236
01:25:45,899 --> 01:25:49,279
okay, that thing that I'm dreaming about now can't be done today

2237
01:25:49,279 --> 01:25:55,119
but is the research track on schedule to basically intercept that in six months or a year's time?

2238
01:25:55,119 --> 01:25:58,939
So you kind of got to intercept where this highly changing technology is going

2239
01:25:58,939 --> 01:26:00,879
as well as the, um, uh, uh

2240
01:26:00,879 --> 01:26:07,619
new capabilities are coming online all the time that you- you didn't realize before that can allow like deep research to work

2241
01:26:07,619 --> 01:26:09,719
or now we've got video generation.

2242
01:26:09,719 --> 01:26:10,779
What do we do with that?

2243
01:26:10,779 --> 01:26:13,507
Um, this multimodal stuff, you know, is it.

2244
01:26:13,507 --> 01:26:13,619
..

2245
01:26:13,619 --> 01:26:18,079
One question I have is, is it really going to be the current UI that we have today

2246
01:26:18,079 --> 01:26:19,319
these textbox chats?

2247
01:26:19,319 --> 01:26:23,819
It seems very unlikely give- once you think about these super multimodal

2248
01:26:23,819 --> 01:26:25,219
uh, uh, systems.

2249
01:26:25,219 --> 01:26:27,539
Shouldn't it be something more like Minority Report where you're- Mm-hmm.

2250
01:26:27,539 --> 01:26:27,572
.

2251
01:26:27,572 --> 01:26:27,639
..

2252
01:26:27,639 --> 01:26:31,999
you're sort of vibing with it in a- in a co- in a kind of collaborative way

2253
01:26:31,999 --> 01:26:32,419
right?

2254
01:26:32,419 --> 01:26:33,619
It seems very restricted today.

2255
01:26:33,619 --> 01:26:39,799
I think we'll look back on today's interfaces and products and systems as quite archaic in maybe in a

2256
01:26:39,799 --> 01:26:40,799
just a couple of years.

2257
01:26:40,799 --> 01:26:48,019
So I think there's a lot of sp- space actually for innovation to happen on the product side as well as the- the research side.

2258
01:26:48,019 --> 01:26:52,079
And then we were offline talking about this keyboard is the- the open question is how

2259
01:26:52,079 --> 01:26:55,703
when, and how much will we move to audio?

2260
01:26:55,703 --> 01:26:55,951
..

2261
01:26:55,951 --> 01:26:58,975
. as a primary way of interacting with the machines around us

2262
01:26:58,975 --> 01:27:00,735
versus typing stuff.

2263
01:27:00,735 --> 01:27:01,055
Yeah.

2264
01:27:01,055 --> 01:27:03,655
I mean, typing is a very low bandwidth way of doing it

2265
01:27:03,655 --> 01:27:05,215
even if you're a very fast, you know

2266
01:27:05,215 --> 01:27:05,815
typer.

2267
01:27:05,815 --> 01:27:09,475
And I think we're gonna have to start utilizing other devices

2268
01:27:09,475 --> 01:27:11,916
whether that's smart glasses, you know, i- uh

2269
01:27:11,916 --> 01:27:17,075
audio, earbuds, um, and eventually maybe some sorts of neural devices

2270
01:27:17,075 --> 01:27:21,475
where we can increase the, the input and the output bandwidth to something

2271
01:27:21,475 --> 01:27:24,075
uh, you know, maybe 100X of what it is today.

2272
01:27:24,075 --> 01:27:35,335
I think that, you know, underappreciated art form is the interface design because I think you cannot unlock the power of the intelligence of a system if you don't have the right interface.

2273
01:27:35,335 --> 01:27:35,455
Mm-hmm.

2274
01:27:35,455 --> 01:27:38,496
The interface is really the way you unlock its power.

2275
01:27:38,496 --> 01:27:38,695
Yeah.

2276
01:27:38,695 --> 01:27:41,795
And it's such an interesting question of how to do that.

2277
01:27:41,795 --> 01:27:41,995
Yeah.

2278
01:27:41,995 --> 01:27:42,768
So how, how.

2279
01:27:42,768 --> 01:27:42,915
..

2280
01:27:42,915 --> 01:27:46,815
You would think, like, getting out of the way is a real art form.

2281
01:27:46,815 --> 01:27:47,155
Yes.

2282
01:27:47,155 --> 01:27:50,535
You know, it's the sort of thing that I guess Steve Jobs always talked about

2283
01:27:50,535 --> 01:27:50,736
right?

2284
01:27:50,736 --> 01:27:53,235
It's simplicity, beauty, and elegance that we want

2285
01:27:53,235 --> 01:27:53,575
right?

2286
01:27:53,575 --> 01:27:54,355
And we're not the.

2287
01:27:54,355 --> 01:27:54,435
..

2288
01:27:54,435 --> 01:27:58,135
nobody's there yet, in my opinion, and that's what I would like us to get to.

2289
01:27:58,135 --> 01:28:00,175
Again, it sort of speaks to like Go again

2290
01:28:00,175 --> 01:28:01,715
right, as a game, the most elegant

2291
01:28:01,715 --> 01:28:02,435
beautiful game.

2292
01:28:02,435 --> 01:28:03,055
Can you.

2293
01:28:03,055 --> 01:28:03,175
..

2294
01:28:03,175 --> 01:28:03,508
You know, the.

2295
01:28:03,508 --> 01:28:03,575
..

2296
01:28:03,575 --> 01:28:05,835
Uh, uh, can you make an interface as beautiful as that?

2297
01:28:05,835 --> 01:28:08,375
And actually, I think we're gonna enter an era of

2298
01:28:08,375 --> 01:28:14,021
uh, AI-generated interfaces that are probably personalized to you so it fits the way that you.

2299
01:28:14,021 --> 01:28:14,175
..

2300
01:28:14,175 --> 01:28:17,355
your aesthetic, your feel, the way that your brain works.

2301
01:28:17,355 --> 01:28:22,035
And, um, and, and, and the AI kind of generates that depending on the task

2302
01:28:22,035 --> 01:28:22,235
you know?

2303
01:28:22,235 --> 01:28:24,635
It feels like that's probably the direction we'll end up in.

2304
01:28:24,635 --> 01:28:29,015
Yeah, 'cause some people are power users and they want every single parameter on the screen- Right.

2305
01:28:29,015 --> 01:28:29,021
.

2306
01:28:29,021 --> 01:28:29,035
..

2307
01:28:29,035 --> 01:28:32,695
everything, e- everything based like perhaps me with a keyboard- Yeah.

2308
01:28:32,695 --> 01:28:32,701
.

2309
01:28:32,701 --> 01:28:32,715
..

2310
01:28:32,715 --> 01:28:35,115
keyboard-based navigation, I like to have shortcuts for everything

2311
01:28:35,115 --> 01:28:38,675
and some people like the minimalism of- Just hide all of that complexity

2312
01:28:38,675 --> 01:28:39,415
yeah, exactly.

2313
01:28:39,415 --> 01:28:39,481
.

2314
01:28:39,481 --> 01:28:39,615
..

2315
01:28:39,615 --> 01:28:39,635
c- completely.

2316
01:28:39,635 --> 01:28:39,955
Yeah.

2317
01:28:39,955 --> 01:28:43,735
Uh, well, I'm glad you have a Steve Jobs mode in you as well.

2318
01:28:43,735 --> 01:28:43,775
.

2319
01:28:43,775 --> 01:28:44,395
This is great.

2320
01:28:44,395 --> 01:28:46,335
Einstein mode, Steve Jobs mode.

2321
01:28:46,335 --> 01:28:50,035
Um, all right, let me try to trick you into answering a question.

2322
01:28:50,035 --> 01:28:51,915
When, when will Gemini 3 come out?

2323
01:28:51,915 --> 01:28:52,175
.

2324
01:28:52,175 --> 01:28:54,115
Is it before or after GTA 6?

2325
01:28:54,115 --> 01:28:54,215
.

2326
01:28:54,215 --> 01:28:55,895
The world waits for both.

2327
01:28:55,895 --> 01:28:59,495
And what does it take to go from 2.

2328
01:28:59,495 --> 01:29:01,135
5 to 3.

2329
01:29:01,135 --> 01:29:04,815
0, because it seems like there's been a lot of releases of 2.

2330
01:29:04,815 --> 01:29:07,015
5 which are already leaps in performance?

2331
01:29:07,015 --> 01:29:07,155
Mm.

2332
01:29:07,155 --> 01:29:09,935
So what, what does it even mean to go to a new version?

2333
01:29:09,935 --> 01:29:11,455
Is it e- about performance?

2334
01:29:11,455 --> 01:29:15,815
Is this about a completely different flavor of an experience?

2335
01:29:15,815 --> 01:29:16,475
Yeah.

2336
01:29:16,475 --> 01:29:18,795
Well, so the way it works with our different

2337
01:29:18,795 --> 01:29:20,947
uh, version numbers is we.

2338
01:29:20,947 --> 01:29:21,155
..

2339
01:29:21,155 --> 01:29:22,591
you know, we try to collect.

2340
01:29:22,591 --> 01:29:22,695
..

2341
01:29:22,695 --> 01:29:26,055
So maybe it takes, you know, roughly six months or something to

2342
01:29:26,055 --> 01:29:31,855
to do a new, uh, kind of full run and the full productization of a new version.

2343
01:29:31,855 --> 01:29:35,355
And during that time, lots of new interesting research

2344
01:29:35,355 --> 01:29:37,655
iterations, and ideas come up.

2345
01:29:37,655 --> 01:29:37,955
Mm-hmm.

2346
01:29:37,955 --> 01:29:39,735
And we sort of collect them all together that

2347
01:29:39,735 --> 01:29:44,875
you know, you could imagine the last six months worth of interesting ideas on the architecture front

2348
01:29:44,875 --> 01:29:47,095
um, maybe it's on the data front

2349
01:29:47,095 --> 01:29:50,135
it's like many different possible things, and we collect

2350
01:29:50,135 --> 01:29:55,195
package that all up, test which ones are likely to be useful for the next iteration

2351
01:29:55,195 --> 01:29:58,255
and then bundle that all together, and then we start the new

2352
01:29:58,255 --> 01:30:00,295
you know, giant hero training run.

2353
01:30:00,295 --> 01:30:00,635
.

2354
01:30:00,635 --> 01:30:00,835
Right?

2355
01:30:00,835 --> 01:30:02,875
And, and then, uh, and then of course

2356
01:30:02,875 --> 01:30:03,975
that gets monitored.

2357
01:30:03,975 --> 01:30:05,801
Uh, and then at the end, then there's the.

2358
01:30:05,801 --> 01:30:05,895
..

2359
01:30:05,895 --> 01:30:07,875
o- of the pre-training, then there's all the post-training

2360
01:30:07,875 --> 01:30:10,375
there's many different ways of doing that, different ways of patching it

2361
01:30:10,375 --> 01:30:14,235
so there's a whole experimenting phase there, which you can also get a lot of gains out

2362
01:30:14,235 --> 01:30:18,135
and that's where you see the version numbers usually referring to the base model

2363
01:30:18,135 --> 01:30:19,395
the pre-train model- Mm-hmm.

2364
01:30:19,395 --> 01:30:19,395
.

2365
01:30:19,395 --> 01:30:19,395
.

2366
01:30:19,395 --> 01:30:22,165
and then the interim versions of 2.

2367
01:30:22,165 --> 01:30:24,075
5, you know, and the different sizes

2368
01:30:24,075 --> 01:30:26,875
and the different little, uh, additions, they're often

2369
01:30:26,875 --> 01:30:30,235
uh, patches or post-training ideas that can be done afterwards

2370
01:30:30,235 --> 01:30:32,595
uh, off the same basic architecture.

2371
01:30:32,595 --> 01:30:33,855
And then of course, on top of that

2372
01:30:33,855 --> 01:30:37,475
we also have different sizes, Pro and Flash and Flash Lite

2373
01:30:37,475 --> 01:30:39,855
that are often distilled from the biggest ones

2374
01:30:39,855 --> 01:30:42,215
you know, the Flash model from the Pro model.

2375
01:30:42,215 --> 01:30:45,875
And that means we have a range of different choices

2376
01:30:45,875 --> 01:30:50,695
uh, if you are the developer, of do you wanna pri- prioritize performance or speed

2377
01:30:50,695 --> 01:30:51,115
right- Mm-hmm.

2378
01:30:51,115 --> 01:30:51,115
.

2379
01:30:51,115 --> 01:30:51,115
.

2380
01:30:51,115 --> 01:30:51,695
and cost?

2381
01:30:51,695 --> 01:30:55,095
And we like to think of this Pareto frontier well of

2382
01:30:55,095 --> 01:30:56,395
of, you know, on the one hand

2383
01:30:56,395 --> 01:30:58,795
uh, the y-axis is, you know, like performance

2384
01:30:58,795 --> 01:31:00,975
and then the, the, the x-axis is

2385
01:31:00,975 --> 01:31:03,295
you know, cost or latency and, and speed

2386
01:31:03,295 --> 01:31:08,375
uh, basically, and we, we have models that completely define the frontier.

2387
01:31:08,375 --> 01:31:12,735
So whatever your trade-off is that you want as an individual user or as a

2388
01:31:12,735 --> 01:31:16,895
as a developer, you should find one of our models satisfies that constraint.

2389
01:31:16,895 --> 01:31:21,015
So behind the version changes, there is a big hero run.

2390
01:31:21,015 --> 01:31:21,695
Yes.

2391
01:31:21,695 --> 01:31:28,815
And then there's, uh, just an insane complexity of productization

2392
01:31:28,815 --> 01:31:33,495
then there's the distillation of the different sizes along that Pareto front

2393
01:31:33,495 --> 01:31:36,555
and then as, uh, with each step you take

2394
01:31:36,555 --> 01:31:38,395
you realize there might be a cool product

2395
01:31:38,395 --> 01:31:39,735
these side quests.

2396
01:31:39,735 --> 01:31:40,935
Yes, exactly.

2397
01:31:40,935 --> 01:31:41,121
But.

2398
01:31:41,121 --> 01:31:41,175
..

2399
01:31:41,175 --> 01:31:45,615
And then you also don't want to take too many side quests because then you have a million versions of a million products- Yes

2400
01:31:45,615 --> 01:31:46,415
yes, precisely.

2401
01:31:46,415 --> 01:31:46,435
.

2402
01:31:46,435 --> 01:31:46,475
..

2403
01:31:46,475 --> 01:31:47,235
and it's very, it's very unclear.

2404
01:31:47,235 --> 01:31:47,495
Yeah.

2405
01:31:47,495 --> 01:31:49,855
But you also get super excited 'cause it's super cool.

2406
01:31:49,855 --> 01:31:50,155
Yeah.

2407
01:31:50,155 --> 01:31:50,986
Like how does even.

2408
01:31:50,986 --> 01:31:51,055
..

2409
01:31:51,055 --> 01:31:52,935
You look at Veo's, v- very cool.

2410
01:31:52,935 --> 01:31:53,275
Yeah.

2411
01:31:53,275 --> 01:31:54,895
How does it fit into the bigger thing

2412
01:31:54,895 --> 01:31:54,915
right?

2413
01:31:54,915 --> 01:31:55,555
Yes, exactly.

2414
01:31:55,555 --> 01:31:55,875
Yeah.

2415
01:31:55,875 --> 01:31:56,195
Exactly.

2416
01:31:56,195 --> 01:31:57,373
And then you're constantly.

2417
01:31:57,373 --> 01:31:57,515
..

2418
01:31:57,515 --> 01:32:00,795
this process of converging upstream we call it

2419
01:32:00,795 --> 01:32:03,895
you know, ideas from the, from the product surfaces or

2420
01:32:03,895 --> 01:32:07,215
or, or from the post-training, and, and even further downstream

2421
01:32:07,215 --> 01:32:07,546
and that.

2422
01:32:07,546 --> 01:32:07,615
..

2423
01:32:07,615 --> 01:32:09,955
you, you kind of upstream that into the

2424
01:32:09,955 --> 01:32:12,055
the core model training for the next run.

2425
01:32:12,055 --> 01:32:12,355
Mm-hmm.

2426
01:32:12,355 --> 01:32:12,515
Right?

2427
01:32:12,515 --> 01:32:16,575
So then the main model, the main Gemini track becomes more and more general.

2428
01:32:16,575 --> 01:32:16,795
Mm-hmm.

2429
01:32:16,795 --> 01:32:18,655
And eventually, you know, AGI.

2430
01:32:18,655 --> 01:32:19,475
.

2431
01:32:19,475 --> 01:32:20,075
.

2432
01:32:20,075 --> 01:32:21,335
One hero run at a time.

2433
01:32:21,335 --> 01:32:21,855
Yes, exactly.

2434
01:32:21,855 --> 01:32:22,075
.

2435
01:32:22,075 --> 01:32:23,195
A few hero runs later.

2436
01:32:23,195 --> 01:32:24,335
Uh, yeah.

2437
01:32:24,335 --> 01:32:27,375
So sometimes when you release these new versions

2438
01:32:27,375 --> 01:32:31,415
or every version really, uh, are benchmarks

2439
01:32:31,415 --> 01:32:35,555
um, productive or counterproductive for showing the performance of a model?

2440
01:32:35,555 --> 01:32:37,695
You need them, and, and I.

2441
01:32:37,695 --> 01:32:37,775
..

2442
01:32:37,775 --> 01:32:39,655
but it's important that you don't overfit to them

2443
01:32:39,655 --> 01:32:40,115
right?

2444
01:32:40,115 --> 01:32:42,215
So they shouldn't be the end, the be-all and end-all.

2445
01:32:42,215 --> 01:32:45,175
So there's, there's LM Arena, or it used to be called LMSYS

2446
01:32:45,175 --> 01:32:48,335
that's one of them that turned out sort of organically to be one of the

2447
01:32:48,335 --> 01:32:50,515
the main ways people like to test these systems

2448
01:32:50,515 --> 01:32:51,575
at least the chatbots.

2449
01:32:51,575 --> 01:32:54,519
Um, obviously there's loads of academic benchmarks on.

2450
01:32:54,519 --> 01:32:54,695
..

2451
01:32:54,695 --> 01:32:58,075
from, from the test, uh, mathematics and coding ability

2452
01:32:58,075 --> 01:33:01,635
general language ability, science ability, and so on

2453
01:33:01,635 --> 01:33:04,555
and then we have our own internal benchmarks that we care about.

2454
01:33:04,555 --> 01:33:07,235
It's a kind of multi-objective, y- you know

2455
01:33:07,235 --> 01:33:08,975
optimization problem, right?

2456
01:33:08,975 --> 01:33:11,075
You w- you don't wanna be good at just one thing.

2457
01:33:11,075 --> 01:33:13,915
We're trying to build general systems that are good across the board.

2458
01:33:13,915 --> 01:33:17,575
And you try and make no-regret, uh, improvements.

2459
01:33:17,575 --> 01:33:19,416
So where you're improving, like- Yeah.

2460
01:33:19,416 --> 01:33:19,416
.

2461
01:33:19,416 --> 01:33:19,416
.

2462
01:33:19,416 --> 01:33:23,775
you know, coding, uh, but it doesn't reduce your performance in other areas

2463
01:33:23,775 --> 01:33:23,975
right?

2464
01:33:23,975 --> 01:33:25,781
So that's the hard part because you, you can.

2465
01:33:25,781 --> 01:33:25,875
..

2466
01:33:25,875 --> 01:33:29,016
Of course, you could put more coding data in or you could put more

2467
01:33:29,016 --> 01:33:31,556
um, I don't know, gaming data in.

2468
01:33:31,556 --> 01:33:34,095
But then does it make worse your language

2469
01:33:34,095 --> 01:33:36,716
uh, system or, or, uh, in

2470
01:33:36,716 --> 01:33:39,275
in your translation systems and other things that you care about?

2471
01:33:39,275 --> 01:33:39,906
So it's.

2472
01:33:39,906 --> 01:33:39,975
..

2473
01:33:39,975 --> 01:33:45,056
You've got to kind of continually monitor this l- increasingly larger and larger suite of

2474
01:33:45,056 --> 01:33:46,076
of benchmarks.

2475
01:33:46,076 --> 01:33:48,496
And also there's, uh, when you stick them into products

2476
01:33:48,496 --> 01:33:56,175
these models, you also care about the direct usage and the direct stats and the signals that you're getting from the end users

2477
01:33:56,175 --> 01:33:58,995
whether they're coders or, or, or the average person using

2478
01:33:58,995 --> 01:34:00,295
uh, using the chat interfaces.

2479
01:34:00,295 --> 01:34:02,775
Yeah, because ultimately you want to measure the usefulness

2480
01:34:02,775 --> 01:34:05,115
but it's so hard to convert that into a number.

2481
01:34:05,115 --> 01:34:05,476
Right.

2482
01:34:05,476 --> 01:34:08,555
It's, it's really vibe-based benchmarks- Yes.

2483
01:34:08,555 --> 01:34:08,562
.

2484
01:34:08,562 --> 01:34:08,576
..

2485
01:34:08,576 --> 01:34:11,355
across a large number of users and it's hard to know.

2486
01:34:11,355 --> 01:34:11,875
And I.

2487
01:34:11,875 --> 01:34:11,996
..

2488
01:34:11,996 --> 01:34:13,771
It would be just terrifying to me to.

2489
01:34:13,771 --> 01:34:13,876
..

2490
01:34:13,876 --> 01:34:16,275
You know you have a much smarter model

2491
01:34:16,275 --> 01:34:19,535
but it's just something vibe-based.

2492
01:34:19,535 --> 01:34:21,475
It's not, not, not quite working.

2493
01:34:21,475 --> 01:34:23,347
That's just sc- scary because.

2494
01:34:23,347 --> 01:34:23,435
..

2495
01:34:23,435 --> 01:34:28,895
And everything you just said, it has to be smart and useful across so many domains.

2496
01:34:28,895 --> 01:34:35,155
So you, you get super excited because it's all of a sudden solving programming problems it'd never been able to solve before

2497
01:34:35,155 --> 01:34:38,155
but now it's crappy at poetry or something.

2498
01:34:38,155 --> 01:34:38,555
Yes.

2499
01:34:38,555 --> 01:34:38,635
Right.

2500
01:34:38,635 --> 01:34:39,009
And it's just.

2501
01:34:39,009 --> 01:34:39,055
..

2502
01:34:39,055 --> 01:34:39,695
I don't know.

2503
01:34:39,695 --> 01:34:40,545
That's a stressful.

2504
01:34:40,545 --> 01:34:40,655
..

2505
01:34:40,655 --> 01:34:43,335
That's so difficult, um, because- To balance

2506
01:34:43,335 --> 01:34:43,455
yeah.

2507
01:34:43,455 --> 01:34:43,461
.

2508
01:34:43,461 --> 01:34:43,475
..

2509
01:34:43,475 --> 01:34:45,715
to balance and because you can't really trust the benchmarks- Mm-hmm.

2510
01:34:45,715 --> 01:34:45,748
.

2511
01:34:45,748 --> 01:34:45,815
..

2512
01:34:45,815 --> 01:34:48,115
you really have to trust the end users.

2513
01:34:48,115 --> 01:34:48,435
Yeah.

2514
01:34:48,435 --> 01:34:51,655
And then other things that are even more esoteric come into play like

2515
01:34:51,655 --> 01:34:55,555
um, you know, the style of the persona of the

2516
01:34:55,555 --> 01:34:57,579
the, the system, you know, how it.

2517
01:34:57,579 --> 01:34:57,715
..

2518
01:34:57,715 --> 01:34:58,835
You know, is it verbose?

2519
01:34:58,835 --> 01:35:00,055
Is it succinct?

2520
01:35:00,055 --> 01:35:01,275
Is it humorous?

2521
01:35:01,275 --> 01:35:02,083
You know, and they.

2522
01:35:02,083 --> 01:35:02,135
..

2523
01:35:02,135 --> 01:35:03,695
And different people like different things.

2524
01:35:03,695 --> 01:35:03,855
Mm-hmm.

2525
01:35:03,855 --> 01:35:06,095
So, um, you know, it's very interesting.

2526
01:35:06,095 --> 01:35:10,955
It's almost like cutting edge part of psychology research or pers- personality research.

2527
01:35:10,955 --> 01:35:12,455
You know, I used to do that in my PhD

2528
01:35:12,455 --> 01:35:13,995
like five factor personality.

2529
01:35:13,995 --> 01:35:16,295
What do we actually want our assistants to be like?

2530
01:35:16,295 --> 01:35:19,115
And different people will like different things as well.

2531
01:35:19,115 --> 01:35:25,195
So these are all just sort of new problems in product space that I don't think have ever really been tackled before but

2532
01:35:25,195 --> 01:35:27,715
um, we're going to sort of ha- rapidly have to deal with now.

2533
01:35:27,715 --> 01:35:29,875
 I think it's a super fascinating space

2534
01:35:29,875 --> 01:35:31,515
developing the character of the thing.

2535
01:35:31,515 --> 01:35:31,755
Yeah.

2536
01:35:31,755 --> 01:35:34,995
And in so doing, it puts a mirror to ourselves

2537
01:35:34,995 --> 01:35:37,215
what are the kind of things, um

2538
01:35:37,215 --> 01:35:38,195
that we like?

2539
01:35:38,195 --> 01:35:42,235
Because prompt engineering allows you to control a lot of those elements but can the product

2540
01:35:42,235 --> 01:35:46,935
uh, make it easier for you to

2541
01:35:46,935 --> 01:35:50,355
uh, control the different flavors of those experiences

2542
01:35:50,355 --> 01:35:51,895
the different characters that you interact with?

2543
01:35:51,895 --> 01:35:52,895
Yeah, exactly.

2544
01:35:52,895 --> 01:35:53,199
So.

2545
01:35:53,199 --> 01:35:53,375
..

2546
01:35:53,375 --> 01:35:55,995
So what's the probability of Google DeepMind winning?

2547
01:35:55,995 --> 01:35:58,175
Well, I don't see it as sort of winning.

2548
01:35:58,175 --> 01:35:59,619
I mean, I think we need to.

2549
01:35:59,619 --> 01:35:59,795
..

2550
01:35:59,795 --> 01:36:04,735
I think winning is the wrong way to look at it given how important and consequential what it is we're building.

2551
01:36:04,735 --> 01:36:06,380
So funnily enough, I don't.

2552
01:36:06,380 --> 01:36:06,515
..

2553
01:36:06,515 --> 01:36:10,615
I try not to view it like a game or competition even though that's a lot of my mindset.

2554
01:36:10,615 --> 01:36:12,068
It's, it's about st-.

2555
01:36:12,068 --> 01:36:12,135
..

2556
01:36:12,135 --> 01:36:13,729
In my view, all of us have.

2557
01:36:13,729 --> 01:36:13,815
..

2558
01:36:13,815 --> 01:36:15,395
those of us at the leading edge, uh

2559
01:36:15,395 --> 01:36:22,395
have a responsibility to, um, steward this unbelievable technology that could be used for incredible good but also has risks

2560
01:36:22,395 --> 01:36:26,555
um, steward it safely into the world for the benefit of humanity.

2561
01:36:26,555 --> 01:36:28,895
That's always, um, what I've, um

2562
01:36:28,895 --> 01:36:32,115
uh, uh, I dreamed about and what we've always tried to do.

2563
01:36:32,115 --> 01:36:34,495
And I hope that's what eventually the community

2564
01:36:34,495 --> 01:36:40,035
maybe the international community, will rally around when it becomes obvious that as we get closer and closer to

2565
01:36:40,035 --> 01:36:42,595
to AGI that, um, that's what's needed.

2566
01:36:42,595 --> 01:36:44,015
I agree with you.

2567
01:36:44,015 --> 01:36:45,275
I think that's beautifully put.

2568
01:36:45,275 --> 01:36:51,695
You've said that, um, you talk to and are on good terms with the leads of some of these

2569
01:36:51,695 --> 01:36:53,975
uh, labs as the competition heats up.

2570
01:36:53,975 --> 01:36:58,315
Uh, how hard is it to maintain sort of those relationships?

2571
01:36:58,315 --> 01:37:00,075
It's been okay so far.

2572
01:37:00,075 --> 01:37:02,655
I try to pride myself in being, uh

2573
01:37:02,655 --> 01:37:03,795
collaborative.

2574
01:37:03,795 --> 01:37:05,115
I'm a collaborative person.

2575
01:37:05,115 --> 01:37:06,815
Research is a collaborative endeavor.

2576
01:37:06,815 --> 01:37:08,455
Science is a collaborative endeavor, right?

2577
01:37:08,455 --> 01:37:11,155
It's all good for humanity in the end if you cure incredible

2578
01:37:11,155 --> 01:37:13,995
you know, terrible diseases and you come up with an incredible cure.

2579
01:37:13,995 --> 01:37:16,235
This is net win for humanity.

2580
01:37:16,235 --> 01:37:19,455
And the same with energy, all of the things that I'm interested in

2581
01:37:19,455 --> 01:37:21,255
in, in helping solve with AI.

2582
01:37:21,255 --> 01:37:26,515
So I just want that technology to exist in the world and be used for the right things and

2583
01:37:26,515 --> 01:37:27,879
and, and the kind of.

2584
01:37:27,879 --> 01:37:27,975
..

2585
01:37:27,975 --> 01:37:32,345
The benefits of that, the productivity benefits of that being shared for every.

2586
01:37:32,345 --> 01:37:32,395
..

2587
01:37:32,395 --> 01:37:33,395
the benefit of everyone.

2588
01:37:33,395 --> 01:37:36,575
So I try to maintain good relations with all the leading lab

2589
01:37:36,575 --> 01:37:37,335
uh, people.

2590
01:37:37,335 --> 01:37:39,375
They have very interesting characters, many of them

2591
01:37:39,375 --> 01:37:40,275
as you might expect.

2592
01:37:40,275 --> 01:37:40,615
Yeah .

2593
01:37:40,615 --> 01:37:42,215
Um, but yeah, I'm on good terms

2594
01:37:42,215 --> 01:37:44,455
I, I hope with pretty much all of them and

2595
01:37:44,455 --> 01:37:46,875
uh, I, I think that's gonna be important when

2596
01:37:46,875 --> 01:37:49,695
when things get even more serious than they are now

2597
01:37:49,695 --> 01:37:52,735
uh, that there are those communication channels.

2598
01:37:52,735 --> 01:37:52,995
Mm-hmm.

2599
01:37:52,995 --> 01:37:55,935
And, uh, that's what will facilitate, uh

2600
01:37:55,935 --> 01:37:58,795
cooperation or collaboration if that's what rec- is required

2601
01:37:58,795 --> 01:38:00,275
especially on things like safety.

2602
01:38:00,275 --> 01:38:02,675
Yeah, I hope there's some collaboration on stuff that's

2603
01:38:02,675 --> 01:38:09,875
uh, sort of less high stakes and in so doing serves as a mechanism for maintaining friendships and relationships.

2604
01:38:09,875 --> 01:38:14,995
So for example, I think the internet would love it if you and Elon somehow collaborated on creating a video game

2605
01:38:14,995 --> 01:38:15,615
that kind of thing.

2606
01:38:15,615 --> 01:38:16,155
Right .

2607
01:38:16,155 --> 01:38:16,283
That.

2608
01:38:16,283 --> 01:38:16,335
..

2609
01:38:16,335 --> 01:38:19,895
I think that enables camaraderie and good terms.

2610
01:38:19,895 --> 01:38:23,035
And also you two are legit gamers so it's just fun to- Yeah.

2611
01:38:23,035 --> 01:38:23,075
.

2612
01:38:23,075 --> 01:38:23,155
..

2613
01:38:23,155 --> 01:38:23,635
fun to create something.

2614
01:38:23,635 --> 01:38:27,055
Yeah, that would be awesome and we've talked about that in the past and it may be a cool thing that

2615
01:38:27,055 --> 01:38:28,195
that, you know, we can do.

2616
01:38:28,195 --> 01:38:29,835
And I agree with you, it'd be nice to have

2617
01:38:29,835 --> 01:38:33,195
um, kind of side projects in a way where

2618
01:38:33,195 --> 01:38:38,215
where w- one can just lean in to the collaboration aspect of it and it's a sort of

2619
01:38:38,215 --> 01:38:40,815
uh, win-win for both sides and it's

2620
01:38:40,815 --> 01:38:41,027
um.

2621
01:38:41,027 --> 01:38:41,155
..

2622
01:38:41,155 --> 01:38:43,235
And it kind of builds up that, that

2623
01:38:43,235 --> 01:38:44,755
that, uh, collaborative muscle.

2624
01:38:44,755 --> 01:38:49,155
I see the scientific endeavor as that kind of side project for humanity.

2625
01:38:49,155 --> 01:38:49,355
Yeah.

2626
01:38:49,355 --> 01:38:50,632
And I, I think Deep.

2627
01:38:50,632 --> 01:38:50,735
..

2628
01:38:50,735 --> 01:38:52,755
Google DeepMind has been really pushing that.

2629
01:38:52,755 --> 01:38:54,271
Uh, I would love it if.

2630
01:38:54,271 --> 01:38:54,335
..

2631
01:38:54,335 --> 01:39:01,395
to see other labs do more scientific stuff and then collaborate because it just seems like easier to collaborate on the big scientific questions.

2632
01:39:01,395 --> 01:39:03,021
I agree and, uh, I would love to see.

2633
01:39:03,021 --> 01:39:03,075
..

2634
01:39:03,075 --> 01:39:03,566
A lot of people.

2635
01:39:03,566 --> 01:39:03,615
..

2636
01:39:03,615 --> 01:39:06,935
All of the other labs talk about science but I think we're really the only ones- Yeah.

2637
01:39:06,935 --> 01:39:06,955
.

2638
01:39:06,955 --> 01:39:06,995
..

2639
01:39:06,995 --> 01:39:14,155
using it for science and doing that and that's why projects like AlphaFold are so important to me and I think to our mission is to show

2640
01:39:14,155 --> 01:39:16,068
uh, how AI can.

2641
01:39:16,068 --> 01:39:16,215
..

2642
01:39:16,215 --> 01:39:16,629
This.

2643
01:39:16,629 --> 01:39:16,715
..

2644
01:39:16,715 --> 01:39:20,955
You know, be clearly used in a very concrete way for the benefit of humanity and

2645
01:39:20,955 --> 01:39:28,071
and also we spun out companies like Isomorphic off the back of AlphaFold to do drug discovery and it's going really well and build sort of.

2646
01:39:28,071 --> 01:39:28,215
..

2647
01:39:28,215 --> 01:39:30,535
You know, you can think of build additional AlphaFold type

2648
01:39:30,535 --> 01:39:34,775
type systems to go into chemistry space to help accelerate drug design.

2649
01:39:34,775 --> 01:39:37,455
And the examples I think we need to show

2650
01:39:37,455 --> 01:39:42,306
uh, and society needs to understand are where AI can bring these huge benefits.

2651
01:39:42,306 --> 01:39:47,663
Well, from the bottom of my heart, thank you for pushing the scientific efforts forward w- with rigor

2652
01:39:47,663 --> 01:39:49,503
with fun, with humility, all of it.

2653
01:39:49,503 --> 01:39:50,623
I just love to see it.

2654
01:39:50,623 --> 01:39:52,363
And still talking about P equals NP, I mean

2655
01:39:52,363 --> 01:39:53,343
it's just incredible.

2656
01:39:53,343 --> 01:39:54,483
So I love it.

2657
01:39:54,483 --> 01:39:57,624
Uh, there are- there- there's been 

2658
01:39:57,624 --> 01:39:59,503
uh, seemingly a war for talent.

2659
01:39:59,503 --> 01:40:00,963
Some of it is meme, I don't know.

2660
01:40:00,963 --> 01:40:09,283
Um, what do you think about Meta buying up talent with huge salaries and- and the heating up of this battle for talent?

2661
01:40:09,283 --> 01:40:13,443
And I- I should say that I think a lot of people see DeepMind as a really great place to do

2662
01:40:13,443 --> 01:40:15,344
uh, cutting edge work- Mm-hmm.

2663
01:40:15,344 --> 01:40:15,377
.

2664
01:40:15,377 --> 01:40:15,444
..

2665
01:40:15,444 --> 01:40:17,723
for the reasons that you've outlined is- Yeah.

2666
01:40:17,723 --> 01:40:17,723
.

2667
01:40:17,723 --> 01:40:17,723
.

2668
01:40:17,723 --> 01:40:20,664
like there's this vibrant scientific culture.

2669
01:40:20,664 --> 01:40:21,443
Yeah.

2670
01:40:21,443 --> 01:40:22,844
Well, look, uh, of course, um

2671
01:40:22,844 --> 01:40:26,143
you know, there's a strategy that- that Meta is taking right now.

2672
01:40:26,143 --> 01:40:29,004
I think that, um, from my perspective at least

2673
01:40:29,004 --> 01:40:31,723
I think the people that are real, uh

2674
01:40:31,723 --> 01:40:35,723
believers in the mission of AGI and what it can do and understand the real consequences

2675
01:40:35,723 --> 01:40:39,063
both good and bad from that and what's- what that responsibility entails

2676
01:40:39,063 --> 01:40:41,523
I think they're mostly doing it to be

2677
01:40:41,523 --> 01:40:44,343
like myself, to be on the frontier of that research.

2678
01:40:44,343 --> 01:40:50,283
So, you know, they can help influence the way that goes and steward that technology safely into the world.

2679
01:40:50,283 --> 01:40:52,823
And, you know, Meta right now are not at the frontier

2680
01:40:52,823 --> 01:40:54,903
maybe they'll- they'll manage to get back on there

2681
01:40:54,903 --> 01:41:00,503
and, um, you know, it's probably rational what they're doing from their perspective because they're behind and they need to do something.

2682
01:41:00,503 --> 01:41:04,003
But I think, um, there's more important things than- than just money.

2683
01:41:04,003 --> 01:41:05,683
Of course, one has to pay, you know

2684
01:41:05,683 --> 01:41:07,583
people at market rates and all of these things

2685
01:41:07,583 --> 01:41:08,903
and that continues to go up.

2686
01:41:08,903 --> 01:41:15,483
 Um, but as pro- And- and- and I was expecting this because more and more people are finally realizing

2687
01:41:15,483 --> 01:41:18,803
leaders or companies, what I've always known for 30 plus years now

2688
01:41:18,803 --> 01:41:23,523
which is that AGI is the most important technology probably that's ever gonna be invented.

2689
01:41:23,523 --> 01:41:26,803
So in some senses it's- it's rational to be doing that.

2690
01:41:26,803 --> 01:41:29,163
But I also think there's a much bigger question.

2691
01:41:29,163 --> 01:41:32,443
I mean, people in AI these days are very well paid.

2692
01:41:32,443 --> 01:41:35,423
You know, I- I remember when we were starting out back in 2010

2693
01:41:35,423 --> 01:41:38,683
you know, I didn't even pay myself for a couple of years 'cause it wasn't enough money.

2694
01:41:38,683 --> 01:41:39,783
We couldn't raise any money.

2695
01:41:39,783 --> 01:41:42,283
And these days interns are being paid, you know

2696
01:41:42,283 --> 01:41:42,783
the amount-  .

2697
01:41:42,783 --> 01:41:42,783
..

2698
01:41:42,783 --> 01:41:45,343
that we raised as our first entire seed round.

2699
01:41:45,343 --> 01:41:46,583
So it's pretty funny.

2700
01:41:46,583 --> 01:41:52,063
And I remember the days where we used to- I used to have to- to work for free and p- and almost pay my own way to do an internship

2701
01:41:52,063 --> 01:41:52,283
right?

2702
01:41:52,283 --> 01:41:53,543
Now it's all the other way around.

2703
01:41:53,543 --> 01:41:54,663
But that's just how it is.

2704
01:41:54,663 --> 01:41:55,583
It's the new world.

2705
01:41:55,583 --> 01:41:58,203
And, um, but I think that, you know

2706
01:41:58,203 --> 01:42:03,083
we've been discussing like what happens post-AGI and energy systems are solved and so on

2707
01:42:03,083 --> 01:42:04,703
what is even money going to mean?

2708
01:42:04,703 --> 01:42:12,563
So I think, uh, you know, and the economy and- and we're gonna have much bigger issues to work through and how does the economy function in that world and companies.

2709
01:42:12,563 --> 01:42:14,343
So I think, you know, it's a little bit

2710
01:42:14,343 --> 01:42:16,803
uh, of a side issue about, uh

2711
01:42:16,803 --> 01:42:19,063
uh, salaries and things of like that today.

2712
01:42:19,063 --> 01:42:25,383
Yeah, when you're facing such gigantic consequences and- and g- gigantic fascinating scientific questions

2713
01:42:25,383 --> 01:42:25,423
for sure.

2714
01:42:25,423 --> 01:42:25,423
Right.

2715
01:42:25,423 --> 01:42:26,883
Which may be only a few years away

2716
01:42:26,883 --> 01:42:27,503
so.

2717
01:42:27,503 --> 01:42:30,483
So on a practical sort of pragmatic sense

2718
01:42:30,483 --> 01:42:33,983
uh, if we zoom in on jobs and can look at programmers

2719
01:42:33,983 --> 01:42:38,223
because it seems like AI systems are currently doing incredibly well at programming

2720
01:42:38,223 --> 01:42:39,263
and increasingly so.

2721
01:42:39,263 --> 01:42:41,723
So a lot of people that, uh

2722
01:42:41,723 --> 01:42:46,623
program for a living, love programming, are worried they will lose their jobs.

2723
01:42:46,623 --> 01:42:49,663
How worried should they be, do you think?

2724
01:42:49,663 --> 01:42:52,103
And what's the right way to, uh

2725
01:42:52,103 --> 01:42:58,003
sort of adjust to the new reality and ensure that you survive and thrive as a human in the programming world?

2726
01:42:58,003 --> 01:43:02,363
Well, it's interesting that programming, and it's again counterintuitive to what we thought

2727
01:43:02,363 --> 01:43:09,903
uh, years ago maybe, that some of the skills that we think of as harder skills are turned out maybe to be the easier ones for various reasons.

2728
01:43:09,903 --> 01:43:14,983
But, you know, coding and math because you can create a lot of synthetic data and verify if that data's correct.

2729
01:43:14,983 --> 01:43:15,243
Mm-hmm.

2730
01:43:15,243 --> 01:43:16,823
So b- because of that nature of that

2731
01:43:16,823 --> 01:43:19,903
it's easier to make things like synthetic data to train from.

2732
01:43:19,903 --> 01:43:21,683
Um, it's also an area, of course

2733
01:43:21,683 --> 01:43:23,623
we're all interested in 'cause we- as programmers

2734
01:43:23,623 --> 01:43:23,843
right?

2735
01:43:23,843 --> 01:43:26,903
To help us, um, get faster at it and more productive.

2736
01:43:26,903 --> 01:43:28,823
So I think the- for the next era

2737
01:43:28,823 --> 01:43:37,183
like the next five, 10 years, I think what we're gonna find is people who are kind of embraced these technologies become almost at one with them

2738
01:43:37,183 --> 01:43:40,523
um, whether that's in the creative industries or the technical industries

2739
01:43:40,523 --> 01:43:43,803
will become sort of superhumanly productive, I think.

2740
01:43:43,803 --> 01:43:47,803
So the great programmers will be even better but they'll be even 10x even what they are today.

2741
01:43:47,803 --> 01:43:53,423
And because there you'll be able to use their skills to utilize the- the tools to the maximum

2742
01:43:53,423 --> 01:43:55,963
uh, you know, exploit them to the maximum.

2743
01:43:55,963 --> 01:43:59,443
And, um, so I think that's what we're gonna see in the next domain.

2744
01:43:59,443 --> 01:44:02,083
Um, so that's gonna cause quite a lot of change

2745
01:44:02,083 --> 01:44:02,623
right?

2746
01:44:02,623 --> 01:44:03,543
And so that's coming.

2747
01:44:03,543 --> 01:44:05,163
A lot of people benefit from that.

2748
01:44:05,163 --> 01:44:09,003
So I think one example of that is if coding becomes easier

2749
01:44:09,003 --> 01:44:13,043
um, m- it becomes available to many more creatives to do more.

2750
01:44:13,043 --> 01:44:19,083
Uh, and, uh, but I think the top programmers will still have huge advantages as terms of specifying

2751
01:44:19,083 --> 01:44:21,943
going back to specifying what the architecture should be

2752
01:44:21,943 --> 01:44:24,243
the question should be how to guide these

2753
01:44:24,243 --> 01:44:27,843
um, uh, coding assistants in a way that's useful

2754
01:44:27,843 --> 01:44:30,603
you know, check whether the code they produce is good.

2755
01:44:30,603 --> 01:44:32,983
So I think there's plenty of, um

2756
01:44:32,983 --> 01:44:35,823
uh, headroom there for the foreseeable, you know

2757
01:44:35,823 --> 01:44:36,543
next few years.

2758
01:44:36,543 --> 01:44:38,903
So I think there's- th- there's several interesting things there.

2759
01:44:38,903 --> 01:44:49,223
One is there's a- a lot of imperative to just get better and better consistently of using these tools so that you're- that you're riding the wave of the improvement- improving models- Yes.

2760
01:44:49,223 --> 01:44:49,276
.

2761
01:44:49,276 --> 01:44:49,383
..

2762
01:44:49,383 --> 01:44:51,103
versus like competing against them.

2763
01:44:51,103 --> 01:44:51,583
Yeah.

2764
01:44:51,583 --> 01:44:56,603
But s- sadly, but that's the- the nature of- of life on Earth.

2765
01:44:56,603 --> 01:45:02,063
Um, there could be a huge amount of value to certain kinds of programming at the cutting edge

2766
01:45:02,063 --> 01:45:04,703
and less value to other kinds.

2767
01:45:04,703 --> 01:45:06,963
For example, it could be like, you know

2768
01:45:06,963 --> 01:45:08,583
front end- Mm-hmm.

2769
01:45:08,583 --> 01:45:08,583
.

2770
01:45:08,583 --> 01:45:08,583
.

2771
01:45:08,583 --> 01:45:12,988
web design might, uh, be more amenable to-.

2772
01:45:12,988 --> 01:45:13,291
..

2773
01:45:13,291 --> 01:45:14,771
to, to, as you, as you mentioned

2774
01:45:14,771 --> 01:45:16,871
to generation- Hmm.

2775
01:45:16,871 --> 01:45:16,924
.

2776
01:45:16,924 --> 01:45:17,031
..

2777
01:45:17,031 --> 01:45:18,331
uh, by AI systems.

2778
01:45:18,331 --> 01:45:21,352
It may be, for example, game engine design or something like this- Yeah.

2779
01:45:21,352 --> 01:45:21,352
.

2780
01:45:21,352 --> 01:45:21,352
.

2781
01:45:21,352 --> 01:45:26,631
or backend design or, or guiding systems in high performance situations

2782
01:45:26,631 --> 01:45:29,771
high performance programming type of design decisions.

2783
01:45:29,771 --> 01:45:31,752
That might be extremely valuable.

2784
01:45:31,752 --> 01:45:31,992
Mm-hmm.

2785
01:45:31,992 --> 01:45:33,351
But it will shift- Yeah.

2786
01:45:33,351 --> 01:45:33,384
.

2787
01:45:33,384 --> 01:45:33,451
..

2788
01:45:33,451 --> 01:45:37,671
where the humans are needed most and that's scary for people to address.

2789
01:45:37,671 --> 01:45:38,011
Yeah, I can.

2790
01:45:38,011 --> 01:45:38,091
..

2791
01:45:38,091 --> 01:45:38,891
I think that's right.

2792
01:45:38,891 --> 01:45:39,257
The, the.

2793
01:45:39,257 --> 01:45:39,331
..

2794
01:45:39,331 --> 01:45:42,151
A- any time where there's a lot of disruption and change.

2795
01:45:42,151 --> 01:45:43,211
You know, and we've had this.

2796
01:45:43,211 --> 01:45:44,071
It's not just this time.

2797
01:45:44,071 --> 01:45:47,051
We've had this in many times in human history with the internet

2798
01:45:47,051 --> 01:45:50,611
um, mobile, but before that was the Industrial Revolution.

2799
01:45:50,611 --> 01:45:54,531
Um, and it's gonna be one of those eras where there will be a lot of change.

2800
01:45:54,531 --> 01:45:57,131
I think there will be new jobs we can't even imagine today

2801
01:45:57,131 --> 01:45:58,551
just like the internet created.

2802
01:45:58,551 --> 01:46:04,451
And then those people with the right skillsets to ride that wave will become incredibly

2803
01:46:04,451 --> 01:46:05,391
uh, valuable.

2804
01:46:05,391 --> 01:46:05,812
Right?

2805
01:46:05,812 --> 01:46:06,451
Those skills.

2806
01:46:06,451 --> 01:46:09,811
But maybe people will have to relearn or adapt a bit

2807
01:46:09,811 --> 01:46:11,172
uh, their current skills.

2808
01:46:11,172 --> 01:46:15,624
And it's the, the thing that's gonna be harder to deal with this time around is the.

2809
01:46:15,624 --> 01:46:15,711
..

2810
01:46:15,711 --> 01:46:22,131
I think what we're gonna see is something like probably 10 times the impact the Industrial Revolution had and

2811
01:46:22,131 --> 01:46:23,811
but 10 times faster as well.

2812
01:46:23,811 --> 01:46:24,711
Right?

2813
01:46:24,711 --> 01:46:26,951
So instead of 100 years, it takes 10 years.

2814
01:46:26,951 --> 01:46:28,122
And so that's gonna make.

2815
01:46:28,122 --> 01:46:28,191
..

2816
01:46:28,191 --> 01:46:31,771
You know, it's like 100X, uh, the impact and the speed combined.

2817
01:46:31,771 --> 01:46:35,971
So that's what's I think gonna make it more difficult for society to

2818
01:46:35,971 --> 01:46:39,191
to, to deal with and, it's g- there's a lot to think through

2819
01:46:39,191 --> 01:46:41,891
and I think we need to be discussing that right now.

2820
01:46:41,891 --> 01:46:47,311
And I, I, you know, I encourage top economists in the world and philosophers to start thinking about

2821
01:46:47,311 --> 01:46:52,191
um, uh, how should, is society gonna be affected by this and what should we do

2822
01:46:52,191 --> 01:46:54,991
including things like, um, u- you know

2823
01:46:54,991 --> 01:46:57,631
uh, universal basic provision or something like that.

2824
01:46:57,631 --> 01:47:01,331
Where a lot of the, um, inc- increased productivity

2825
01:47:01,331 --> 01:47:03,931
uh, gets shared out and distributed, uh

2826
01:47:03,931 --> 01:47:04,811
to society.

2827
01:47:04,811 --> 01:47:08,411
Um, and maybe in the form of surface- services and other things

2828
01:47:08,411 --> 01:47:13,731
where if you want more than that, you still go and get some incredibly rare skills and things like that

2829
01:47:13,731 --> 01:47:15,931
um, and, and make yourself unique.

2830
01:47:15,931 --> 01:47:19,151
Um, but, uh, uh, but there's a basic provision that is provided.

2831
01:47:19,151 --> 01:47:21,431
And if you think of government as technology

2832
01:47:21,431 --> 01:47:25,631
there's also interesting questions, not just in economics but just politics.

2833
01:47:25,631 --> 01:47:37,291
How do you design a system that's responding to the rapidly changing times such that you can represent the different pain that people feel from the different groups?

2834
01:47:37,291 --> 01:47:41,731
And how do you reallocate resources in a way that

2835
01:47:41,731 --> 01:47:47,571
um, addresses that pain and represents the hope and the pain and the fears of different people

2836
01:47:47,571 --> 01:47:50,291
uh, in a way that doesn't lead to division?

2837
01:47:50,291 --> 01:47:58,551
Because politicians are often really good at sort of fueling the division and using that to get elected.

2838
01:47:58,551 --> 01:48:03,051
The other m- c- defining the other and then saying- Yeah.

2839
01:48:03,051 --> 01:48:03,051
.

2840
01:48:03,051 --> 01:48:03,051
.

2841
01:48:03,051 --> 01:48:04,991
"That's bad," and so based on that.

2842
01:48:04,991 --> 01:48:10,151
I think that's often counterproductive to leveraging a rapidly changing technology

2843
01:48:10,151 --> 01:48:12,071
how to, uh, help the world flourish.

2844
01:48:12,071 --> 01:48:19,691
So we almost, uh, need to improve our political s- systems as well rapidly if you think of them as a technology.

2845
01:48:19,691 --> 01:48:20,291
Definitely.

2846
01:48:20,291 --> 01:48:22,651
And I think, I think we'll need new governance

2847
01:48:22,651 --> 01:48:26,571
uh, structures, institutions probably, to help with this transition.

2848
01:48:26,571 --> 01:48:30,911
So I think political philosophy and political science is gonna be key

2849
01:48:30,911 --> 01:48:31,951
uh, to that.

2850
01:48:31,951 --> 01:48:33,991
But I think the number one thing, first of all

2851
01:48:33,991 --> 01:48:37,471
uh, is to create more abundance of resources

2852
01:48:37,471 --> 01:48:37,971
right?

2853
01:48:37,971 --> 01:48:38,211
Mm-hmm.

2854
01:48:38,211 --> 01:48:40,171
Then there's the qu- So that's the number one thing.

2855
01:48:40,171 --> 01:48:42,651
Increase productivity, get more resources.

2856
01:48:42,651 --> 01:48:45,531
Maybe eventually get out of the zero-some situation.

2857
01:48:45,531 --> 01:48:48,311
Then the second question is how to use

2858
01:48:48,311 --> 01:48:50,471
uh, those resources and distribute those resources.

2859
01:48:50,471 --> 01:48:51,191
But yeah.

2860
01:48:51,191 --> 01:48:53,711
You can't do that without having that abundance first.

2861
01:48:53,711 --> 01:48:56,971
Uh, you mentioned to me, uh, the book The Maniac

2862
01:48:56,971 --> 01:48:59,871
uh, by Benjamin Libetut.

2863
01:48:59,871 --> 01:49:02,351
A book on, uh, first of all

2864
01:49:02,351 --> 01:49:02,931
about you.

2865
01:49:02,931 --> 01:49:03,991
There's a bio about you.

2866
01:49:03,991 --> 01:49:06,011
Um, - It's strange, yeah.

2867
01:49:06,011 --> 01:49:06,811
It's unclear.

2868
01:49:06,811 --> 01:49:07,331
Yes, sure.

2869
01:49:07,331 --> 01:49:10,811
 It's unclear how much is fiction, how much is reality.

2870
01:49:10,811 --> 01:49:14,171
Um, but I think the central figure there is

2871
01:49:14,171 --> 01:49:15,231
uh, John von Neumann.

2872
01:49:15,231 --> 01:49:19,011
I would say it's a haunting and beautiful exploration of madness and genius

2873
01:49:19,011 --> 01:49:23,491
and let's say the double-edge, uh, sword of discovery.

2874
01:49:23,491 --> 01:49:27,291
And, you know, for, um, people who don't know

2875
01:49:27,291 --> 01:49:29,411
John von Neumann is a kind of legendary mind.

2876
01:49:29,411 --> 01:49:31,011
He contributed to quantum mechanics.

2877
01:49:31,011 --> 01:49:33,231
He was on the Manhattan Project.

2878
01:49:33,231 --> 01:49:35,771
He is widely considered to be the father of

2879
01:49:35,771 --> 01:49:39,191
or pioneer of the modern computer and AI and so on.

2880
01:49:39,191 --> 01:49:40,111
So there's.

2881
01:49:40,111 --> 01:49:40,211
..

2882
01:49:40,211 --> 01:49:43,951
Many people say he's, like, one of the smartest humans ever.

2883
01:49:43,951 --> 01:49:44,091
Mm-hmm.

2884
01:49:44,091 --> 01:49:45,051
Which is fascinating.

2885
01:49:45,051 --> 01:49:53,991
And what's also fascinating is as a person who saw nuclear science and physics become the atomic bomb

2886
01:49:53,991 --> 01:49:59,831
so you, you got to see ideas become a thing that has a huge amount of impact on the world.

2887
01:49:59,831 --> 01:50:03,131
He also foresaw the same thing for computing.

2888
01:50:03,131 --> 01:50:03,611
Yeah.

2889
01:50:03,611 --> 01:50:04,655
He s- he.

2890
01:50:04,655 --> 01:50:04,751
..

2891
01:50:04,751 --> 01:50:07,371
And that's the, a little bit, again

2892
01:50:07,371 --> 01:50:10,571
beautiful and haunting aspect of the book, um

2893
01:50:10,571 --> 01:50:14,751
than taking a leap forward and looking at this Lee Sedol

2894
01:50:14,751 --> 01:50:24,571
AlphaZero, AlphaGo, AlphaZero big moment that maybe John v- von Neumann's thinking was brought to

2895
01:50:24,571 --> 01:50:26,351
to, to, to reality.

2896
01:50:26,351 --> 01:50:28,471
So I, I, I guess the question is

2897
01:50:28,471 --> 01:50:31,911
um, what do you think if you got to hang out with John von Neumann

2898
01:50:31,911 --> 01:50:32,591
uh, now?

2899
01:50:32,591 --> 01:50:34,511
What, what would he say-  .

2900
01:50:34,511 --> 01:50:34,511
..

2901
01:50:34,511 --> 01:50:35,371
about what's going on?

2902
01:50:35,371 --> 01:50:36,811
Well, that would be an amazing experience.

2903
01:50:36,811 --> 01:50:39,151
You know, he's a f- a fantastic mind and

2904
01:50:39,151 --> 01:50:44,171
and I also love the way he, he spent a lot of his time at Princeton at the Institute of Advanced Study.

2905
01:50:44,171 --> 01:50:46,451
It's a very special place for thinking.

2906
01:50:46,451 --> 01:50:50,771
And, um, it's amazing at how much of a polymath he was and the

2907
01:50:50,771 --> 01:50:53,391
the spread of things he helped invent, including

2908
01:50:53,391 --> 01:50:57,071
of course, the von Neumann architecture that all the modern computers are based on.

2909
01:50:57,071 --> 01:51:00,271
And, um, he had amazing foresight.

2910
01:51:00,271 --> 01:51:03,271
I think he would've loved where we are today

2911
01:51:03,271 --> 01:51:04,871
and he would've, um.

2912
01:51:04,871 --> 01:51:04,951
..

2913
01:51:04,951 --> 01:51:06,851
I think he would've really enjoyed AlphaGo- Okay.

2914
01:51:06,851 --> 01:51:06,871
 .

2915
01:51:06,871 --> 01:51:06,871
..

2916
01:51:06,871 --> 01:51:08,011
being a, you know, games- Yes.

2917
01:51:08,011 --> 01:51:09,331
He also did game theory.

2918
01:51:09,331 --> 01:51:14,351
I think he foresaw a lot of what would happen with learning machine systems that

2919
01:51:14,351 --> 01:51:16,891
that, that are kind of grown, I think he called it

2920
01:51:16,891 --> 01:51:17,991
rather than programmed.

2921
01:51:17,991 --> 01:51:19,073
I'm not sure how even.

2922
01:51:19,073 --> 01:51:19,171
..

2923
01:51:19,171 --> 01:51:20,731
Maybe he wouldn't even be that surprised.

2924
01:51:20,731 --> 01:51:24,983
There's the fruition of what I think he already foresaw in the 1950s.

2925
01:51:24,983 --> 01:51:26,679
I wonder what advice he would give.

2926
01:51:26,679 --> 01:51:27,599
He got to see- Hmm.

2927
01:51:27,599 --> 01:51:27,632
.

2928
01:51:27,632 --> 01:51:27,699
..

2929
01:51:27,699 --> 01:51:30,239
the building of the atomic bomb with the Manhattan Project.

2930
01:51:30,239 --> 01:51:30,360
Yeah.

2931
01:51:30,360 --> 01:51:33,739
I'm sure there is interesting stuff that maybe is not talked about enough

2932
01:51:33,739 --> 01:51:36,800
maybe some bureaucratic aspect, maybe the influence of politicians

2933
01:51:36,800 --> 01:51:43,319
maybe, maybe not enough of picking up the phone and talking to people that are called enemies- Hmm.

2934
01:51:43,319 --> 01:51:43,339
.

2935
01:51:43,339 --> 01:51:43,380
..

2936
01:51:43,380 --> 01:51:44,540
by the said politicians.

2937
01:51:44,540 --> 01:51:48,159
There might be something like deep wisdom that we just may have lost from that time

2938
01:51:48,159 --> 01:51:48,559
actually.

2939
01:51:48,559 --> 01:51:50,219
Yeah, I'm sure, I'm sure there is.

2940
01:51:50,219 --> 01:51:51,599
I mean, I've t- we, we, you know

2941
01:51:51,599 --> 01:51:52,227
studied.

2942
01:51:52,227 --> 01:51:52,299
..

2943
01:51:52,299 --> 01:51:54,099
I read a lot of books for that time as well

2944
01:51:54,099 --> 01:51:57,280
chronical time, um, and some brilliant people involved.

2945
01:51:57,280 --> 01:51:58,219
But I, I agree with you.

2946
01:51:58,219 --> 01:52:01,560
I think maybe there needs to be more dialogue and understanding.

2947
01:52:01,560 --> 01:52:03,700
Um, I hope we can learn from those

2948
01:52:03,700 --> 01:52:04,619
those times.

2949
01:52:04,619 --> 01:52:08,008
I think the difference here is that the AI has so many.

2950
01:52:08,008 --> 01:52:08,100
..

2951
01:52:08,100 --> 01:52:09,619
it's a multi-use technology.

2952
01:52:09,619 --> 01:52:11,852
Obviously, we're trying to do things like the.

2953
01:52:11,852 --> 01:52:11,979
..

2954
01:52:11,979 --> 01:52:14,780
like solve, you know, all diseases, um

2955
01:52:14,780 --> 01:52:17,839
uh, help with energy, uh, and scarcity

2956
01:52:17,839 --> 01:52:19,179
these incredible things.

2957
01:52:19,179 --> 01:52:21,179
This is why all of us and, and myself

2958
01:52:21,179 --> 01:52:24,379
you know, I worked, started on this journey 30 plus years ago.

2959
01:52:24,379 --> 01:52:27,499
And, um, but of course there are risks too.

2960
01:52:27,499 --> 01:52:31,799
And probably von Neumann, my guess is he foresaw both.

2961
01:52:31,799 --> 01:52:34,499
And, um, and I think he sort of said

2962
01:52:34,499 --> 01:52:36,039
I think is to his wife, that

2963
01:52:36,039 --> 01:52:37,009
that, that it would be a.

2964
01:52:37,009 --> 01:52:37,119
..

2965
01:52:37,119 --> 01:52:37,423
this is.

2966
01:52:37,423 --> 01:52:37,519
..

2967
01:52:37,519 --> 01:52:40,659
computers would be even more impactful in the world.

2968
01:52:40,659 --> 01:52:42,259
And as we just discussed, you know

2969
01:52:42,259 --> 01:52:42,959
I think that's right.

2970
01:52:42,959 --> 01:52:46,899
I think it's going to be ten times at least of the industrial revolution.

2971
01:52:46,899 --> 01:52:47,919
So I think he's right.

2972
01:52:47,919 --> 01:52:49,959
So I think he would have been, I imagine

2973
01:52:49,959 --> 01:52:53,059
fascinated by, uh, uh, uh, where we are now.

2974
01:52:53,059 --> 01:52:54,705
And I think one of the.

2975
01:52:54,705 --> 01:52:54,839
..

2976
01:52:54,839 --> 01:53:00,719
maybe you can correct me, but one of the takeaways from the book is that reason

2977
01:53:00,719 --> 01:53:04,019
as, uh, said in the book, mad dreams of reason

2978
01:53:04,019 --> 01:53:09,819
is not enough for guiding humanity as we build these super powerful technology

2979
01:53:09,819 --> 01:53:11,419
that there's something else.

2980
01:53:11,419 --> 01:53:13,639
I mean, there's also like a religious component.

2981
01:53:13,639 --> 01:53:13,999
Hmm.

2982
01:53:13,999 --> 01:53:16,579
Whatever God, whatever religion gives, it give.

2983
01:53:16,579 --> 01:53:16,659
..

2984
01:53:16,659 --> 01:53:19,659
it pulls at something in the human spirit that raw

2985
01:53:19,659 --> 01:53:22,539
cold reason doesn't give us.

2986
01:53:22,539 --> 01:53:23,679
And I, I agree with that.

2987
01:53:23,679 --> 01:53:26,319
I think we need to approach it with whatever you want to call it

2988
01:53:26,319 --> 01:53:29,159
the, uh, spiritual dimension or humanist dimension.

2989
01:53:29,159 --> 01:53:30,599
It doesn't have to be to do with religion

2990
01:53:30,599 --> 01:53:31,019
right?

2991
01:53:31,019 --> 01:53:33,219
But this idea of, of a soul

2992
01:53:33,219 --> 01:53:35,479
what makes us human, this spark that we have

2993
01:53:35,479 --> 01:53:38,439
perhaps it's to do with consciousness when we finally understand that

2994
01:53:38,439 --> 01:53:41,619
um, I think that has to be at the heart of the endeavor.

2995
01:53:41,619 --> 01:53:41,999
Mm-hmm, mm-hmm.

2996
01:53:41,999 --> 01:53:45,239
Um, and technology, I've always seen technology as the enabler

2997
01:53:45,239 --> 01:53:45,739
right?

2998
01:53:45,739 --> 01:53:50,759
The tools the- that enable us to, to flourish and to understand more about the

2999
01:53:50,759 --> 01:53:51,359
the world.

3000
01:53:51,359 --> 01:53:53,179
And I, I'm sort of with Feynman on this

3001
01:53:53,179 --> 01:53:57,859
and he used to always talk about science and art being companions

3002
01:53:57,859 --> 01:53:58,239
right?

3003
01:53:58,239 --> 01:54:01,019
You can understand it from both sides, the beauty of a flower

3004
01:54:01,019 --> 01:54:05,419
how beautiful it is, and also understand why the colors of the flower evolved like that

3005
01:54:05,419 --> 01:54:05,759
right?

3006
01:54:05,759 --> 01:54:07,479
That just makes it more beautiful that, that

3007
01:54:07,479 --> 01:54:09,299
that just the intrinsic beauty of the flower.

3008
01:54:09,299 --> 01:54:12,039
And, and I've always sort of seen it like that.

3009
01:54:12,039 --> 01:54:14,119
And maybe, you know, in the Renaissance times

3010
01:54:14,119 --> 01:54:16,719
the great discovers then, like people like da Vinci

3011
01:54:16,719 --> 01:54:17,530
you know, they were.

3012
01:54:17,530 --> 01:54:17,599
..

3013
01:54:17,599 --> 01:54:20,819
I don't think he saw any difference between science and art

3014
01:54:20,819 --> 01:54:22,619
uh, and perhaps religion, right?

3015
01:54:22,619 --> 01:54:22,799
They were.

3016
01:54:22,799 --> 01:54:22,839
..

3017
01:54:22,839 --> 01:54:23,405
everything was.

3018
01:54:23,405 --> 01:54:23,519
..

3019
01:54:23,519 --> 01:54:26,219
it's just part of being human and, um

3020
01:54:26,219 --> 01:54:28,779
being inspired about the world around us.

3021
01:54:28,779 --> 01:54:29,799
And that's what I.

3022
01:54:29,799 --> 01:54:29,999
..

3023
01:54:29,999 --> 01:54:31,579
the philosophy I try to take.

3024
01:54:31,579 --> 01:54:34,219
And, um, one of my favorite philosophers is Spinoza

3025
01:54:34,219 --> 01:54:36,499
and I think he combined that all very well

3026
01:54:36,499 --> 01:54:40,559
you know, this idea of trying to understand the universe and understanding our place in it

3027
01:54:40,559 --> 01:54:44,239
and that was his kind of way of understanding religion.

3028
01:54:44,239 --> 01:54:46,019
And I think that's quite beautiful.

3029
01:54:46,019 --> 01:54:47,039
And for me, every.

3030
01:54:47,039 --> 01:54:47,119
..

3031
01:54:47,119 --> 01:54:49,839
all of these things are related, interrelated

3032
01:54:49,839 --> 01:54:53,259
the technology and, um, what it means to be human.

3033
01:54:53,259 --> 01:54:57,619
And, uh, I think it's very important though that we remember that as.

3034
01:54:57,619 --> 01:54:57,859
..

3035
01:54:57,859 --> 01:55:00,639
when we're immersed in the technology and the

3036
01:55:00,639 --> 01:55:01,599
the research.

3037
01:55:01,599 --> 01:55:04,039
I think a lot of researchers that I see in

3038
01:55:04,039 --> 01:55:09,239
in our field are a little bit too narrow and only understand the technology.

3039
01:55:09,239 --> 01:55:11,759
And I think also that's why it's important for

3040
01:55:11,759 --> 01:55:13,339
um, this to be debated at.

3041
01:55:13,339 --> 01:55:13,419
..

3042
01:55:13,419 --> 01:55:14,619
by society at large.

3043
01:55:14,619 --> 01:55:16,539
And I'm very supportive of things like this

3044
01:55:16,539 --> 01:55:19,499
the AI summits that will happen and governments understanding it

3045
01:55:19,499 --> 01:55:26,559
and I think that's one good thing about the chatbot era and the product era of AI is that everyday person can actually feel and

3046
01:55:26,559 --> 01:55:29,039
and interact with cutting edge AI and, and

3047
01:55:29,039 --> 01:55:30,479
and feel, feel it for themselves.

3048
01:55:30,479 --> 01:55:33,899
Yeah, because they, they force the technologist to have the human conversation.

3049
01:55:33,899 --> 01:55:34,559
Yeah, for sure.

3050
01:55:34,559 --> 01:55:34,939
Yeah.

3051
01:55:34,939 --> 01:55:36,659
That's the hopeful aspect of it, like you said

3052
01:55:36,659 --> 01:55:41,911
it's a dual use technology that we're forcefully integrating the entire humanity into it by.

3053
01:55:41,911 --> 01:55:42,119
..

3054
01:55:42,119 --> 01:55:43,759
into the discussion about AI- Mm-hmm.

3055
01:55:43,759 --> 01:55:43,825
.

3056
01:55:43,825 --> 01:55:43,959
..

3057
01:55:43,959 --> 01:55:50,719
because ultimately AI, AGI will be used for things that states use technologies for

3058
01:55:50,719 --> 01:55:52,839
which is, uh, conflict and so on.

3059
01:55:52,839 --> 01:55:59,699
And the more we, uh, uh, integrate humans into this picture by having  chats with them

3060
01:55:59,699 --> 01:56:02,269
the more it will guide- Yeah, be able to adapt.

3061
01:56:02,269 --> 01:56:02,379
..

3062
01:56:02,379 --> 01:56:06,779
society will be able to adapt to these technologies like we've always done in the past with

3063
01:56:06,779 --> 01:56:10,159
with, uh, the incredible technologies we've invented in the past.

3064
01:56:10,159 --> 01:56:16,699
Do you think there will be something like a Manhattan Project where

3065
01:56:16,699 --> 01:56:27,139
um, there will be an escalation of the power of this technology and states in their old way of thinking will try to use it as weapons technologies and there will be this kind of escalation?

3066
01:56:27,139 --> 01:56:28,039
I hope not.

3067
01:56:28,039 --> 01:56:30,699
Um, I think that would be, uh

3068
01:56:30,699 --> 01:56:33,919
very dangerous to do, and I think also

3069
01:56:33,919 --> 01:56:37,319
um, you know, not the right use of the technology.

3070
01:56:37,319 --> 01:56:38,939
I, I hope we'll end up with more

3071
01:56:38,939 --> 01:56:42,399
something more collaborative if needed, like more like a

3072
01:56:42,399 --> 01:56:44,199
like a CERN project- Yeah.

3073
01:56:44,199 --> 01:56:44,205
.

3074
01:56:44,205 --> 01:56:44,219
..

3075
01:56:44,219 --> 01:56:56,479
you know, where, um, it's research focused and the best minds in the world come together to carefully complete the final steps and make sure it's responsibly done before

3076
01:56:56,479 --> 01:56:58,579
you know, like, uh, deploying it to the world.

3077
01:56:58,579 --> 01:56:59,379
We'll see.

3078
01:56:59,379 --> 01:57:02,139
I mean, it's difficult with the current geopolitical climate

3079
01:57:02,139 --> 01:57:04,839
I think, uh, to, to see cooperation

3080
01:57:04,839 --> 01:57:09,239
but things can change and, um, I think at least on the scientific level

3081
01:57:09,239 --> 01:57:11,499
it's important for the researchers to, to

3082
01:57:11,499 --> 01:57:13,619
to, to keep in touch and, and

3083
01:57:13,619 --> 01:57:15,039
and keep close to each other on.

3084
01:57:15,039 --> 01:57:15,119
..

3085
01:57:15,119 --> 01:57:16,459
at least on those kinds of topics.

3086
01:57:16,459 --> 01:57:19,419
Yeah, and I, I personally believe on the education side and

3087
01:57:19,419 --> 01:57:23,439
um, immigration side, it would be great if both directions

3088
01:57:23,439 --> 01:57:25,082
uh, people from the West-.

3089
01:57:25,082 --> 01:57:25,264
..

3090
01:57:25,264 --> 01:57:27,663
China and China back.

3091
01:57:27,663 --> 01:57:32,643
I mean, there is some like family human aspect of people just intermixing- Yeah.

3092
01:57:32,643 --> 01:57:32,716
.

3093
01:57:32,716 --> 01:57:32,863
..

3094
01:57:32,863 --> 01:57:34,923
and thereby those ties grow strong.

3095
01:57:34,923 --> 01:57:47,244
So you can't sort of divide against each other this kind of old-school way of thinking and so uh multi uh multicultural multidisciplinary research teams working on scientific questions that's like the hope.

3096
01:57:47,244 --> 01:57:51,903
Don't, don't let the warm, the leaders that are warmongers divide us.

3097
01:57:51,903 --> 01:57:55,383
I think science is the ultimately really beautiful connector.

3098
01:57:55,383 --> 01:57:55,664
Yeah.

3099
01:57:55,664 --> 01:57:59,603
Science has always been uh I think quite uh a very collaborative endeavor.

3100
01:57:59,603 --> 01:57:59,963
Mm-hmm.

3101
01:57:59,963 --> 01:58:01,883
And you know scientists know that it's, it's

3102
01:58:01,883 --> 01:58:04,964
it's a collective endeavor as well and we can all learn from each other.

3103
01:58:04,964 --> 01:58:07,863
So perhaps it could be a vector to get a bit of cooperation.

3104
01:58:07,863 --> 01:58:10,083
What's your ridiculous question?

3105
01:58:10,083 --> 01:58:11,003
What's your PDOOM?

3106
01:58:11,003 --> 01:58:13,644
Probability that human civilization destroys itself.

3107
01:58:13,644 --> 01:58:17,564
Well look, I-I don't have a  it's a

3108
01:58:17,564 --> 01:58:20,003
you know, I don't have a PDOOM number.

3109
01:58:20,003 --> 01:58:25,723
The reason I don't is because I think it would imply a level of precision that is not there.

3110
01:58:25,723 --> 01:58:29,183
So like I don't know how people are getting their PDOOM numbers.

3111
01:58:29,183 --> 01:58:34,743
I think it's a kind of a little bit of a ridiculous notion because um what I would say is

3112
01:58:34,743 --> 01:58:38,463
it's definitely non-zero and it's probably non-neg- negligible.

3113
01:58:38,463 --> 01:58:44,703
So that in itself is pretty sobering and my-my view is it's just hugely uncertain

3114
01:58:44,703 --> 01:58:45,363
right?

3115
01:58:45,363 --> 01:58:47,523
What this technology is going to be able to do?

3116
01:58:47,523 --> 01:58:49,223
How fast are they going to take off?

3117
01:58:49,223 --> 01:58:50,723
How controllable are they going to be?

3118
01:58:50,723 --> 01:58:54,963
Some things may turn out to be and hopefully like way easier than we thought

3119
01:58:54,963 --> 01:58:55,463
right?

3120
01:58:55,463 --> 01:59:01,523
Um, but it may be there's some really hard um uh-uh problems that are harder than we guess today.

3121
01:59:01,523 --> 01:59:09,463
And I think uh we don't know that for sure and so in under those conditions of a lot of uncertainty but huge stakes both ways.

3122
01:59:09,463 --> 01:59:12,863
You know on the one hand we, we could solve all diseases

3123
01:59:12,863 --> 01:59:19,963
energy problems, the, the, the scarcity problem and then travel to the stars and conquerors of the stars and maximum human flourishing.

3124
01:59:19,963 --> 01:59:22,423
On the other hand, is this sort of PDOOM scenarios.

3125
01:59:22,423 --> 01:59:25,063
So given the uncertainty around it and the importance of it

3126
01:59:25,063 --> 01:59:30,443
it's clear to me the only rational sensible approach is to proceed with cautious optimism.

3127
01:59:30,443 --> 01:59:37,483
So we want the outc- we want the um eh the benefits of course uh and uh all of the

3128
01:59:37,483 --> 01:59:45,403
the amazing things that AI can bring and actually I would be really worried for humanity if I if given the o-other challenges that we have

3129
01:59:45,403 --> 01:59:49,103
climate, dise- you know aging, resources, all of that.

3130
01:59:49,103 --> 01:59:52,263
If I didn't know something like AI was coming down the line- Mm-hmm.

3131
01:59:52,263 --> 01:59:52,263
.

3132
01:59:52,263 --> 01:59:52,263
.

3133
01:59:52,263 --> 01:59:52,523
right?

3134
01:59:52,523 --> 01:59:54,463
How would we solve all those other problems?

3135
01:59:54,463 --> 01:59:55,563
I think it's hard.

3136
01:59:55,563 --> 01:59:59,523
Um, so I think we, you know it could be amazingly transformative for good.

3137
01:59:59,523 --> 02:00:06,023
Um, but on the other hand you know there are these risks that we know are there but we can't quite quantify.

3138
02:00:06,023 --> 02:00:17,763
So the, the best thing to do is to use the scientific method to do more research to try and uh more precisely define those risks and of course address them.

3139
02:00:17,763 --> 02:00:19,803
Um, and I think that's what we're doing.

3140
02:00:19,803 --> 02:00:26,463
I think there probably needs to be uh ten times more effort on that than there is now as we're getting closer and closer to the

3141
02:00:26,463 --> 02:00:28,003
to the, to the AGI line.

3142
02:00:28,003 --> 02:00:29,963
What would be the source of worry for you more?

3143
02:00:29,963 --> 02:00:34,723
Would it be human caused or AI, AGI caused?

3144
02:00:34,723 --> 02:00:35,323
Yeah.

3145
02:00:35,323 --> 02:00:42,983
Humans abusing the technology versus AGI itself through mechanism that you've spoken about which is fascinating deception or this kind of stuff- Yes.

3146
02:00:42,983 --> 02:00:43,036
.

3147
02:00:43,036 --> 02:00:43,143
..

3148
02:00:43,143 --> 02:00:45,623
getting better and better and better secretly and then escapes.

3149
02:00:45,623 --> 02:00:49,983
I think they, they operate over different time scales and they're equally important to address.

3150
02:00:49,983 --> 02:01:00,543
So there's just the, the, the common garden or variety of like you know bad actors using new technology uh in this case general purpose technology and repurposing it for harmful ends.

3151
02:01:00,543 --> 02:01:15,923
And that's a huge uh risk and I think there has a lot of complications because generally you know I'm in huge favor of open science and open source and in fact we did it with all our science projects like AlphaFold and all of those things uh for the benefit of

3152
02:01:15,923 --> 02:01:17,583
of the scientific community.

3153
02:01:17,583 --> 02:01:32,303
Um, but how does one restrict bad actors access to these powerful systems whether they're individuals or even rogue states uh and but enable access at the same time to good actors to maximally build on top of?

3154
02:01:32,303 --> 02:01:36,603
It's a pretty tricky problem that there's I've not heard a clear solution to.

3155
02:01:36,603 --> 02:01:46,143
So there's the bad actor use case problem and then there's obviously uh as the systems become more agentic and closer to AGI um and more autonomous

3156
02:01:46,143 --> 02:01:51,783
how do we ensure the guard rails and they stick to what we want them to do uh and under our control?

3157
02:01:51,783 --> 02:01:56,723
Yeah I tend to maybe my mind is limited worry more about the humans

3158
02:01:56,723 --> 02:01:57,843
so the bad actors.

3159
02:01:57,843 --> 02:01:57,963
Mm.

3160
02:01:57,963 --> 02:02:11,143
And there it could be uh in part how do you not put destructive technology in the hands of bad actors but another part from again geopolitical technology perspective how do you reduce the number of bad actors in the world?

3161
02:02:11,143 --> 02:02:14,123
That's, that's also an interesting human problem.

3162
02:02:14,123 --> 02:02:14,603
Yeah.

3163
02:02:14,603 --> 02:02:15,823
It's a hard problem.

3164
02:02:15,823 --> 02:02:26,763
I mean look we, we can um maybe also use the technology itself to help um uh early warning on some of the bad actor use cases right?

3165
02:02:26,763 --> 02:02:35,963
Whether that's bio or nuclear or whatever it is like AI could be potentially helpful there as long as the AI that you're using is itself reliable

3166
02:02:35,963 --> 02:02:36,503
right?

3167
02:02:36,503 --> 02:02:40,523
So it's a sort of interlocking problem and that's what makes it very tricky and

3168
02:02:40,523 --> 02:02:49,703
and again it may require some agreement internationally at least between China and the- and the US of some uh basic standards

3169
02:02:49,703 --> 02:02:50,103
right.

3170
02:02:50,103 --> 02:02:53,463
I have to ask you about the book The Maniac.

3171
02:02:53,463 --> 02:02:55,903
There's this, this the hand of God moment

3172
02:02:55,903 --> 02:02:57,823
Lee Sedol's move 78- Mm.

3173
02:02:57,823 --> 02:02:57,889
.

3174
02:02:57,889 --> 02:02:58,023
..

3175
02:02:58,023 --> 02:03:05,492
that perhaps  the last time a human did a move of sort of pure human genius.

3176
02:03:05,492 --> 02:03:08,475
And beat AlphaGo or, like, broke its brain.

3177
02:03:08,475 --> 02:03:08,676
Yes.

3178
02:03:08,676 --> 02:03:14,556
If, sorry to anthropomorphize, but it's an interesting moment because I think in so many domains it will keep happening.

3179
02:03:14,556 --> 02:03:14,956
Yeah.

3180
02:03:14,956 --> 02:03:17,155
It's a special moment and, you know

3181
02:03:17,155 --> 02:03:18,916
it was great for Lee Sedol and, you know

3182
02:03:18,916 --> 02:03:22,155
I think it's in a way they were sort of inspiring each other.

3183
02:03:22,155 --> 02:03:26,195
We as a team were inspired by Lee Sedol's brilliance and nobleness

3184
02:03:26,195 --> 02:03:29,335
and then maybe he got inspired by, you know

3185
02:03:29,335 --> 02:03:34,155
what AlphaGo was doing to then conjure this incredible inspirational moment.

3186
02:03:34,155 --> 02:03:36,035
It's all, you know, captured very well in the

3187
02:03:36,035 --> 02:03:37,395
in the documentary about it.

3188
02:03:37,395 --> 02:03:37,695
It is.

3189
02:03:37,695 --> 02:03:41,995
And, um, I think that'll continue in many domains where there's this

3190
02:03:41,995 --> 02:03:43,395
at least for the, for the, again

3191
02:03:43,395 --> 02:03:45,695
for the foreseeable, uh, future of, like

3192
02:03:45,695 --> 02:03:48,396
yeah, the hu- humans bringing in their ingenuity

3193
02:03:48,396 --> 02:03:51,535
um, and asking the right question, let's say

3194
02:03:51,535 --> 02:03:54,935
uh, and then utilizing these tools, uh

3195
02:03:54,935 --> 02:03:57,775
in a way that, um, then cracks a problem.

3196
02:03:57,775 --> 02:03:58,375
Yeah.

3197
02:03:58,375 --> 02:03:58,546
What.

3198
02:03:58,546 --> 02:03:58,615
..

3199
02:03:58,615 --> 02:04:05,295
As the AI becomes smarter and smarter, one of the interesting questions we can ask ourselves is what makes humans special?

3200
02:04:05,295 --> 02:04:11,155
It does feel, um, perhaps biased that we humans are deeply special.

3201
02:04:11,155 --> 02:04:13,495
I don't know if it's our intelligence.

3202
02:04:13,495 --> 02:04:16,655
Uh, it could be something else that

3203
02:04:16,655 --> 02:04:20,235
that other thing that's outside the mad dreams of reason.

3204
02:04:20,235 --> 02:04:23,516
I think that's what I've always imagined, uh

3205
02:04:23,516 --> 02:04:25,476
when I was a kid and starting on this journey of

3206
02:04:25,476 --> 02:04:29,215
like, um, I was of course fascinated by things like consciousness.

3207
02:04:29,215 --> 02:04:32,255
Did, did a neuroscience PhD to look at how the brain works

3208
02:04:32,255 --> 02:04:33,715
especially imagination and memory.

3209
02:04:33,715 --> 02:04:35,135
I focused on the hippocampus.

3210
02:04:35,135 --> 02:04:37,116
And it's sort of gonna be interesting.

3211
02:04:37,116 --> 02:04:38,175
I always thought the best way.

3212
02:04:38,175 --> 02:04:38,255
..

3213
02:04:38,255 --> 02:04:45,775
Of course one can come philosophize about it and have thought experiments and maybe even do actual experiments like you do in neuroscience on

3214
02:04:45,775 --> 02:04:49,875
on real brains, but in the end I always imagine that building AI

3215
02:04:49,875 --> 02:04:55,115
a kind of intelligent artifact, and then comparing that to the human mind and seeing what the differences were

3216
02:04:55,115 --> 02:04:58,995
uh, would be the best way to uncover what's special about the human mind

3217
02:04:58,995 --> 02:05:00,735
if indeed there is anything special.

3218
02:05:00,735 --> 02:05:04,081
And I suspect there probably is, but it's gonna be hard to de-.

3219
02:05:04,081 --> 02:05:04,155
..

3220
02:05:04,155 --> 02:05:06,415
You know, I think this journey we're on will help us

3221
02:05:06,415 --> 02:05:08,515
uh, understand that and define that.

3222
02:05:08,515 --> 02:05:15,995
And, you know, there may be a difference between carbon-based substrates that we are and silicon ones when they process information.

3223
02:05:15,995 --> 02:05:18,135
You know, one of the best definitions I like of

3224
02:05:18,135 --> 02:05:22,235
of, of consciousness is it's the way information feels when we process it

3225
02:05:22,235 --> 02:05:22,615
right?

3226
02:05:22,615 --> 02:05:22,835
 Yeah.

3227
02:05:22,835 --> 02:05:23,995
Um, it could be.

3228
02:05:23,995 --> 02:05:24,440
I mean, it doesn't help.

3229
02:05:24,440 --> 02:05:24,475
..

3230
02:05:24,475 --> 02:05:26,355
It's not a very helpful scientific explanation- Right.

3231
02:05:26,355 --> 02:05:26,361
.

3232
02:05:26,361 --> 02:05:26,375
..

3233
02:05:26,375 --> 02:05:28,715
but I think it's kind of interesting intuit- intuitive one.

3234
02:05:28,715 --> 02:05:30,875
And, um, and so, you know

3235
02:05:30,875 --> 02:05:33,655
on this, this, this journey, this scientific journey we're on

3236
02:05:33,655 --> 02:05:36,335
we'll, I think, um, help uncover that mystery.

3237
02:05:36,335 --> 02:05:37,415
Yeah.

3238
02:05:37,415 --> 02:05:39,695
What I cannot create, I do not understand.

3239
02:05:39,695 --> 02:05:41,955
That's, uh, somebody you deeply admire, Richard Feynman

3240
02:05:41,955 --> 02:05:42,695
like you mentioned.

3241
02:05:42,695 --> 02:05:50,475
You also reach, um, for the, the Wigner's dreams of universality that he saw in constrained domains

3242
02:05:50,475 --> 02:05:53,695
but also broadly generally in, in mathematics and so on and so on.

3243
02:05:53,695 --> 02:05:54,035
Mm-hmm.

3244
02:05:54,035 --> 02:05:56,595
So many aspects on which you're pushing towards.

3245
02:05:56,595 --> 02:05:58,815
Not to start trouble at the end, but

3246
02:05:58,815 --> 02:06:00,695
uh,  Roger Penrose.

3247
02:06:00,695 --> 02:06:00,995
Yes.

3248
02:06:00,995 --> 02:06:01,935
Okay.

3249
02:06:01,935 --> 02:06:04,335
  So, uh, you know, do

3250
02:06:04,335 --> 02:06:05,727
do you think consciousness.

3251
02:06:05,727 --> 02:06:05,855
..

3252
02:06:05,855 --> 02:06:09,815
There's this hard problem of consciousness, how information feels.

3253
02:06:09,815 --> 02:06:13,675
Um, do you think consciousness, first of all

3254
02:06:13,675 --> 02:06:15,375
is a computation?

3255
02:06:15,375 --> 02:06:18,775
And if it is, if it's information processing

3256
02:06:18,775 --> 02:06:20,255
like you said everything is- Mm-hmm.

3257
02:06:20,255 --> 02:06:20,308
.

3258
02:06:20,308 --> 02:06:20,415
..

3259
02:06:20,415 --> 02:06:23,415
is it something that could be modeled by a classical computer?

3260
02:06:23,415 --> 02:06:24,035
Yeah.

3261
02:06:24,035 --> 02:06:25,995
Or is it a quantum mechanical in nature?

3262
02:06:25,995 --> 02:06:28,095
Well, look, Penrose is an amazing thinker

3263
02:06:28,095 --> 02:06:29,755
one of the greatest of the modern era

3264
02:06:29,755 --> 02:06:31,955
and he, we've had a lot of discussions about this.

3265
02:06:31,955 --> 02:06:34,735
Of course, we cordially disagree, which is

3266
02:06:34,735 --> 02:06:36,543
you know, I, I feel like, um.

3267
02:06:36,543 --> 02:06:36,615
..

3268
02:06:36,615 --> 02:06:44,192
I mean, he collaborated with a lot of good neuroscientists to see if he could find mechanisms for quantum mechanics behavior in the brain and they.

3269
02:06:44,192 --> 02:06:44,295
..

3270
02:06:44,295 --> 02:06:46,435
Uh, to my knowledge, they haven't found anything

3271
02:06:46,435 --> 02:06:47,995
um, convincing yet.

3272
02:06:47,995 --> 02:06:49,539
So my betting is there is.

3273
02:06:49,539 --> 02:06:49,675
..

3274
02:06:49,675 --> 02:06:49,927
is.

3275
02:06:49,927 --> 02:06:50,095
..

3276
02:06:50,095 --> 02:06:51,815
that, that, that it's mostly, you know

3277
02:06:51,815 --> 02:06:54,375
it is just classical computing that's going on in the brain

3278
02:06:54,375 --> 02:06:57,375
which suggests that all the phenomena, uh

3279
02:06:57,375 --> 02:07:01,275
are modelable or mimickable by a classical computer.

3280
02:07:01,275 --> 02:07:02,155
But we'll see.

3281
02:07:02,155 --> 02:07:06,975
You know, there, there may be this final mysterious things of the feeling of consciousness

3282
02:07:06,975 --> 02:07:12,215
the qualia, these kinds of things that philosophers debate where it's unique to the substrate.

3283
02:07:12,215 --> 02:07:14,766
We may even come towards understanding that when.

3284
02:07:14,766 --> 02:07:14,835
..

3285
02:07:14,835 --> 02:07:17,355
if we do things like Neuralink and, and o- uh

3286
02:07:17,355 --> 02:07:22,035
have neural interfaces to the AI systems, which I think we probably will eventually

3287
02:07:22,035 --> 02:07:24,855
um, maybe to keep up with the AI systems.

3288
02:07:24,855 --> 02:07:29,335
Uh, we might actually be able to feel for ourselves what it's like to compute on silicon

3289
02:07:29,335 --> 02:07:29,955
right?

3290
02:07:29,955 --> 02:07:33,295
So, um, and maybe that will tell us.

3291
02:07:33,295 --> 02:07:35,335
Uh, so I think-  Yeah.

3292
02:07:35,335 --> 02:07:35,335
.

3293
02:07:35,335 --> 02:07:35,335
.

3294
02:07:35,335 --> 02:07:36,515
it's, it's gonna be interesting.

3295
02:07:36,515 --> 02:07:41,655
And I, I had a debate once with the late Daniel Dennett about why do we think each other are conscious?

3296
02:07:41,655 --> 02:07:42,975
Okay, so it's for two reasons.

3297
02:07:42,975 --> 02:07:45,755
One is you're exhibiting the same behavior that I am.

3298
02:07:45,755 --> 02:07:47,075
So that's one thing.

3299
02:07:47,075 --> 02:07:49,135
Behaviorally, you seem like a conscious being

3300
02:07:49,135 --> 02:07:49,715
if I am.

3301
02:07:49,715 --> 02:07:51,755
But the second thing, which is often overlooked

3302
02:07:51,755 --> 02:07:53,735
is that we're all running on the same substrate.

3303
02:07:53,735 --> 02:07:56,875
So if you're behaving in the same way and we're running on the same substrate

3304
02:07:56,875 --> 02:08:01,175
it's most parsimonious to assume you're feeling the same experience that I'm feeling.

3305
02:08:01,175 --> 02:08:04,675
But with a AI our that's on silicon

3306
02:08:04,675 --> 02:08:06,595
we won't be able to rely on the second part.

3307
02:08:06,595 --> 02:08:10,355
Even if it exhibits the first part, the behavior looks like a behavior of a conscious being

3308
02:08:10,355 --> 02:08:12,275
it might even claim it is, um

3309
02:08:12,275 --> 02:08:13,091
but we.

3310
02:08:13,091 --> 02:08:13,315
..

3311
02:08:13,315 --> 02:08:16,275
but, but we wouldn't know how it actually felt.

3312
02:08:16,275 --> 02:08:18,335
Um, and it probably couldn't know we.

3313
02:08:18,335 --> 02:08:18,455
..

3314
02:08:18,455 --> 02:08:19,095
what we felt.

3315
02:08:19,095 --> 02:08:20,855
At least in the first stages.

3316
02:08:20,855 --> 02:08:23,815
Maybe when we get to super intelligence and the technologies that builds

3317
02:08:23,815 --> 02:08:25,495
perhaps we'll, we'll be able to, um

3318
02:08:25,495 --> 02:08:26,095
bridge that.

3319
02:08:26,095 --> 02:08:32,195
No, I mean, that's a huge test for radical empathy is to empathize with a different substrate.

3320
02:08:32,195 --> 02:08:32,595
Right.

3321
02:08:32,595 --> 02:08:33,995
Exactly.

3322
02:08:33,995 --> 02:08:35,535
 I mean- We never had to confront that before.

3323
02:08:35,535 --> 02:08:36,535
Yeah.

3324
02:08:36,535 --> 02:08:36,595
Yeah.

3325
02:08:36,595 --> 02:08:38,875
So maybe, maybe through brain-computer inter- interfaces

3326
02:08:38,875 --> 02:08:41,935
be able to truly empathize what it feels like to be a computer.

3327
02:08:41,935 --> 02:08:46,035
To compute- Well, for information to be computed not on a carbon system.

3328
02:08:46,035 --> 02:08:47,932
I mean, that's d- deeply excite.

3329
02:08:47,932 --> 02:08:47,995
..

3330
02:08:47,995 --> 02:08:50,235
I mean, some people kind of think about that with plants

3331
02:08:50,235 --> 02:08:51,755
with other life forms, which are different.

3332
02:08:51,755 --> 02:08:52,215
Yes, it could be.

3333
02:08:52,215 --> 02:08:52,715
Exactly.

3334
02:08:52,715 --> 02:08:56,549
Sim- similar substrate, but d- d- sufficiently far enough-.

3335
02:08:56,549 --> 02:08:56,723
..

3336
02:08:56,723 --> 02:08:58,363
on the, uh, evolutionary tree.

3337
02:08:58,363 --> 02:08:58,863
Yup.

3338
02:08:58,863 --> 02:09:02,570
That it's- requires a radical empathy, but to do that with a computer.

3339
02:09:02,570 --> 02:09:02,683
..

3340
02:09:02,683 --> 02:09:06,844
I mean, no, we sort of- there are animal studies on this of like- of course higher animals like

3341
02:09:06,844 --> 02:09:10,163
you know, killer whales and dolphins and dogs and

3342
02:09:10,163 --> 02:09:11,303
and monkeys.

3343
02:09:11,303 --> 02:09:13,083
You know, they have some- and elephants.

3344
02:09:13,083 --> 02:09:15,903
You know, they have some aspects certainly of consciousness

3345
02:09:15,903 --> 02:09:19,443
right, even though they're not- might not be that- that- that smart on an IQ sense.

3346
02:09:19,443 --> 02:09:23,763
So it's a- we can already empathize with that and maybe even some of our systems one day.

3347
02:09:23,763 --> 02:09:25,563
Like we built this thing called Dolphin Gemma

3348
02:09:25,563 --> 02:09:30,244
you know, which can- we- have one- a version of our system was trained on dolphin and whale sounds

3349
02:09:30,244 --> 02:09:34,104
and maybe we'll be able to build a- an interpreter or translator at some point

3350
02:09:34,104 --> 02:09:35,044
which would be pretty cool.

3351
02:09:35,044 --> 02:09:37,884
What gives you hope for the future of human civilization?

3352
02:09:37,884 --> 02:09:43,304
Well, what gives me hope is, I think our almost limitless ingenuity

3353
02:09:43,304 --> 02:09:44,003
first of all.

3354
02:09:44,003 --> 02:09:48,304
I think the best of us and the best human minds are incredible.

3355
02:09:48,304 --> 02:09:51,783
Um, and, you know, I love l- w- you know

3356
02:09:51,783 --> 02:09:55,864
meeting and watching, uh, any human that's the top of their game

3357
02:09:55,864 --> 02:09:58,203
whether that's sport or science or art.

3358
02:09:58,203 --> 02:10:00,703
You know, it's- it's- it's just nothing more wonderful than that

3359
02:10:00,703 --> 02:10:02,283
seeing them in their element, in flow.

3360
02:10:02,283 --> 02:10:04,323
Um, I think it's almost limitless.

3361
02:10:04,323 --> 02:10:07,363
You know, our brains are general systems

3362
02:10:07,363 --> 02:10:08,423
intelligent systems.

3363
02:10:08,423 --> 02:10:11,283
So, I think it's almost limitless what we can potentially do with them.

3364
02:10:11,283 --> 02:10:14,523
And then the other thing is our extreme adaptability.

3365
02:10:14,523 --> 02:10:18,983
I think it's gonna be okay in terms of- there's gonna be a lot of change

3366
02:10:18,983 --> 02:10:24,403
but the- but look where we are now with our- effectively our hunter-gatherer brains.

3367
02:10:24,403 --> 02:10:26,243
How is it we can, you know

3368
02:10:26,243 --> 02:10:28,103
we can c- cope with the modern world

3369
02:10:28,103 --> 02:10:28,463
right?

3370
02:10:28,463 --> 02:10:30,025
F- flying on planes.

3371
02:10:30,025 --> 02:10:30,163
..

3372
02:10:30,163 --> 02:10:30,283
 .

3373
02:10:30,283 --> 02:10:30,283
..

3374
02:10:30,283 --> 02:10:31,546
doing podcasts.

3375
02:10:31,546 --> 02:10:31,703
..

3376
02:10:31,703 --> 02:10:31,923
 Yeah.

3377
02:10:31,923 --> 02:10:31,943
.

3378
02:10:31,943 --> 02:10:31,983
..

3379
02:10:31,983 --> 02:10:34,803
you know, playing computer games and virtual simulations.

3380
02:10:34,803 --> 02:10:35,043
Yeah.

3381
02:10:35,043 --> 02:10:38,163
I mean, it's already mind-blowing given that our mind was- was developed for

3382
02:10:38,163 --> 02:10:40,883
you know, hunting buffalos on the- on the tundra.

3383
02:10:40,883 --> 02:10:44,283
And- and so, w- I think this is just the next step and

3384
02:10:44,283 --> 02:10:50,273
and- and it's actually kind of interesting to see how h- h- society has already adapted to this mind-blowing AI technology.

3385
02:10:50,273 --> 02:10:50,363
..

3386
02:10:50,363 --> 02:10:50,403
Yeah.

3387
02:10:50,403 --> 02:10:50,409
.

3388
02:10:50,409 --> 02:10:50,423
..

3389
02:10:50,423 --> 02:10:51,303
we have today already.

3390
02:10:51,303 --> 02:10:51,323
Yeah.

3391
02:10:51,323 --> 02:10:53,359
It's sort of like, "Oh, I talk to chatbots.

3392
02:10:53,359 --> 02:10:54,303
"  Totally fine.

3393
02:10:54,303 --> 02:10:57,743
Like- And it's, uh, very possible that this very podcast activity

3394
02:10:57,743 --> 02:11:00,743
which I'm here for, will be completely replaced by AI.

3395
02:11:00,743 --> 02:11:01,037
 Well.

3396
02:11:01,037 --> 02:11:01,043
..

3397
02:11:01,043 --> 02:11:02,923
I'm very replaceable and I'm r- waiting for it.

3398
02:11:02,923 --> 02:11:04,003
Not to the level that you can do it

3399
02:11:04,003 --> 02:11:04,703
Lex, I don't think.

3400
02:11:04,703 --> 02:11:05,383
Ah, thank you.

3401
02:11:05,383 --> 02:11:07,423
That's-  That's what we humans do to each other.

3402
02:11:07,423 --> 02:11:07,443
Yes.

3403
02:11:07,443 --> 02:11:08,143
 We complement.

3404
02:11:08,143 --> 02:11:08,563
Exactly.

3405
02:11:08,563 --> 02:11:09,323
All right.

3406
02:11:09,323 --> 02:11:12,903
And, uh, I'm, uh, deeply grateful for us humans to have this c- uh

3407
02:11:12,903 --> 02:11:16,463
infinite capacity for curiosity, adaptability, like you said

3408
02:11:16,463 --> 02:11:18,454
and also compassion and ability to love.

3409
02:11:18,454 --> 02:11:18,603
..

3410
02:11:18,603 --> 02:11:19,083
Exactly.

3411
02:11:19,083 --> 02:11:19,089
.

3412
02:11:19,089 --> 02:11:19,103
..

3413
02:11:19,103 --> 02:11:19,963
all of those human things.

3414
02:11:19,963 --> 02:11:21,123
All the things that are deeply human.

3415
02:11:21,123 --> 02:11:23,563
Well, this is a huge honor, Demis.

3416
02:11:23,563 --> 02:11:25,963
You're one of the truly special humans in the world.

3417
02:11:25,963 --> 02:11:29,763
Uh, thank you so much for doing what you do and for talking today.

3418
02:11:29,763 --> 02:11:30,983
Oh, thank you very much, Lex.

3419
02:11:30,983 --> 02:11:34,763
Thanks for listening to this conversation with Demis Hassabis.

3420
02:11:34,763 --> 02:11:41,483
To support this podcast, please check out our sponsors in the description and consider subscribing to this channel.

3421
02:11:41,483 --> 02:11:48,163
And now, let me answer some questions and try to articulate some things I've been thinking about.

3422
02:11:48,163 --> 02:11:52,523
If you would like to submit questions, including in audio and video form

3423
02:11:52,523 --> 02:11:54,336
go to lexfrumman.

3424
02:11:54,336 --> 02:11:55,263
com/ama.

3425
02:11:55,263 --> 02:11:58,103
I got a lot of amazing questions, thoughts

3426
02:11:58,103 --> 02:11:59,203
and requests from folks.

3427
02:11:59,203 --> 02:12:01,283
I'll keep trying to pick some, uh

3428
02:12:01,283 --> 02:12:05,343
randomly and comment on it at the end of every episode.

3429
02:12:05,343 --> 02:12:09,663
I got a note on May 21st this year that said

3430
02:12:09,663 --> 02:12:10,383
"Hi Lex.

3431
02:12:10,383 --> 02:12:17,023
20 years ago today, David Foster Wallace delivered his famous 'This is water' speech at

3432
02:12:17,023 --> 02:12:18,283
uh, Kenyon College.

3433
02:12:18,283 --> 02:12:20,440
What do you think of this speech?

3434
02:12:20,440 --> 02:12:30,203
" Well, first, I think this is probably one of the greatest and most unique commencement speeches ever given.

3435
02:12:30,203 --> 02:12:34,263
But of course I have many favorites, including the one by Steve Jobs.

3436
02:12:34,263 --> 02:12:40,643
And David Foster Wallace is one of my favorite writers and one of my favorite humans.

3437
02:12:40,643 --> 02:12:50,063
There's a tragic honesty to his work, and it always felt as if he was engaging in a- a constant battle with his own mind

3438
02:12:50,063 --> 02:12:57,763
and the writing, his writing, were kind of his notes from the front lines of that battle.

3439
02:12:57,763 --> 02:13:00,503
Now, on to the speech.

3440
02:13:00,503 --> 02:13:01,723
Let me quote some parts.

3441
02:13:01,723 --> 02:13:06,063
There's of course the parable of the fish and the water that goes

3442
02:13:06,063 --> 02:13:13,343
"There are these two young fish swimming along and they happen to meet an older fish swimming the other way

3443
02:13:13,343 --> 02:13:16,303
who nods at them and says, 'Morning

3444
02:13:16,303 --> 02:13:16,903
boys.

3445
02:13:16,903 --> 02:13:18,085
How's the water?

3446
02:13:18,085 --> 02:13:22,163
' And the two young fish swim on for a bit

3447
02:13:22,163 --> 02:13:25,703
and then eventually one of them looks over at the other and goes

3448
02:13:25,703 --> 02:13:27,888
'What the hell is water?

3449
02:13:27,888 --> 02:13:32,623
'" In this speech, David Foster Wallace goes on to say

3450
02:13:32,623 --> 02:13:40,703
"The point of the fish story is merely that the most obvious important realities are often the ones that are hardest to see and talk about.

3451
02:13:40,703 --> 02:13:43,543
Stated as an English sentence, of course

3452
02:13:43,543 --> 02:13:50,283
this is just a banal platitude, but the fact is that in the day-to-day trenches of adult existence

3453
02:13:50,283 --> 02:13:53,463
banal platitudes can have a life or death importance

3454
02:13:53,463 --> 02:13:57,809
or so I wish to suggest to you in this dry and lovely morning.

3455
02:13:57,809 --> 02:14:02,483
" I have several takeaways from this parable and the speech that follows.

3456
02:14:02,483 --> 02:14:06,203
First, I think we must question everything

3457
02:14:06,203 --> 02:14:09,803
and in particular the most basic assumptions about our reality

3458
02:14:09,803 --> 02:14:13,623
our life, and the very nature of existence

3459
02:14:13,623 --> 02:14:17,163
and that this project is a deeply personal one.

3460
02:14:17,163 --> 02:14:22,463
In some fundamental sense, nobody can really help you in this process of discovery.

3461
02:14:22,463 --> 02:14:25,903
The call to action here, I think

3462
02:14:25,903 --> 02:14:29,103
from, uh, David Foster Wallace, as he puts it

3463
02:14:29,103 --> 02:14:33,003
is to, quote, "To be just a little less arrogant

3464
02:14:33,003 --> 02:14:38,763
to have just a little more critical awareness about myself and my certainties

3465
02:14:38,763 --> 02:14:43,903
because a huge percentage of the stuff that I tend to be automatically certain of is

3466
02:14:43,903 --> 02:14:47,589
it turns out, totally wrong and deluded.

3467
02:14:47,589 --> 02:14:52,283
" All right, back to me, Lex speaking.

3468
02:14:52,283 --> 02:14:59,063
Second takeaway is that the central spiritual battles of our life are not fought on a

3469
02:14:59,063 --> 02:15:03,563
uh, mountaintop somewhere at a meditation retreat.

3470
02:15:03,563 --> 02:15:07,111
But it is fought in the mundane moments of daily life.

3471
02:15:07,111 --> 02:15:17,591
Third takeaway is that we too easily give away our time and attention to the multitude of distractions that the world feeds us

3472
02:15:17,591 --> 02:15:21,532
the insatiable black holes of attention.

3473
02:15:21,532 --> 02:15:30,191
David Foster Wallace's call to action in this case is to be deeply aware of the beauty in each moment

3474
02:15:30,191 --> 02:15:33,151
and to find meaning in the mundane.

3475
02:15:33,151 --> 02:15:37,931
I often quote David Foster Wallace in his advice that

3476
02:15:37,931 --> 02:15:40,400
"The key to life is to be un-borable

3477
02:15:40,400 --> 02:15:42,711
" and I think this is exactly right.

3478
02:15:42,711 --> 02:15:48,932
Every moment, every object, every experience, when looked at closely enough

3479
02:15:48,932 --> 02:15:53,092
contains within it infinite richness to explore.

3480
02:15:53,092 --> 02:16:00,051
And since, uh, Demis Hassabis of this very podcast episode and I are such fans of Richard Feynman

3481
02:16:00,051 --> 02:16:02,971
allow me to, uh, also quote Mr.

3482
02:16:02,971 --> 02:16:04,851
Feynman on this topic as well.

3483
02:16:04,851 --> 02:16:13,651
Quote, "I have a friend who's an artist and has sometimes taken a view which I don't agree with very well.

3484
02:16:13,651 --> 02:16:17,706
He'll hold up a flower and say, 'Look how beautiful it is

3485
02:16:17,706 --> 02:16:19,031
' and I'll agree.

3486
02:16:19,031 --> 02:16:22,891
Then he says, 'I, as an artist

3487
02:16:22,891 --> 02:16:25,291
can see how beautiful this is, but you

3488
02:16:25,291 --> 02:16:29,405
as a scientist, take this all apart and it becomes a dull thing.

3489
02:16:29,405 --> 02:16:32,471
' And I think that's kind of nutty.

3490
02:16:32,471 --> 02:16:37,831
First of all, the beauty that he sees is available to other people

3491
02:16:37,831 --> 02:16:39,630
and to me too, I believe.

3492
02:16:39,630 --> 02:16:44,630
Although I may not be quite as refined aesthetically as he is

3493
02:16:44,630 --> 02:16:47,251
I can appreciate the beauty of a flower.

3494
02:16:47,251 --> 02:16:52,091
At the same time, I see much more about the flower than he sees.

3495
02:16:52,091 --> 02:16:57,790
I can imagine the cells in there, the complicated actions inside which also have beauty.

3496
02:16:57,790 --> 02:17:01,391
I mean, it's not just beauty at this dimension

3497
02:17:01,391 --> 02:17:02,751
at one centimeter.

3498
02:17:02,751 --> 02:17:06,731
There's also beauty at the smaller dimensions, the inner structure.

3499
02:17:06,731 --> 02:17:08,050
Also, the processes.

3500
02:17:08,050 --> 02:17:14,130
The fact that the colors in the flower evolved in order to attract insects to pollinate it is interesting.

3501
02:17:14,130 --> 02:17:16,931
It means that the insects can see the color.

3502
02:17:16,931 --> 02:17:18,651
It adds a question.

3503
02:17:18,651 --> 02:17:22,391
Does this aesthetic sense also exist in lower forms?

3504
02:17:22,391 --> 02:17:24,130
Why is it aesthetic?

3505
02:17:24,130 --> 02:17:29,391
All kinds of interesting questions which the science knowledge only adds to the excitement

3506
02:17:29,391 --> 02:17:32,591
the mystery, and the awe of a flower.

3507
02:17:32,591 --> 02:17:34,343
It only adds.

3508
02:17:34,343 --> 02:17:39,471
" All right, back to, uh, David Foster Wallace's speech.

3509
02:17:39,471 --> 02:17:44,531
He has a great story in there that I particularly enjoy.

3510
02:17:44,531 --> 02:17:46,091
It goes.

3511
02:17:46,091 --> 02:17:46,290
..

3512
02:17:46,290 --> 02:17:51,331
There are these two guys sitting together in a bar in the remote Alaskan wilderness.

3513
02:17:51,331 --> 02:17:55,290
One of the guys is religious, the other is an atheist.

3514
02:17:55,290 --> 02:18:02,731
And the two are arguing about the existence of God with that special intensity that comes after about the fourth beer.

3515
02:18:02,731 --> 02:18:08,171
And the atheist says, "Look, it's not like I don't have actual reasons for not believing in God.

3516
02:18:08,171 --> 02:18:13,191
It's not like I haven't ever experimented with the whole God and prayer thing.

3517
02:18:13,191 --> 02:18:18,391
Just last month, I got caught away from the camp in that terrible blizzard

3518
02:18:18,391 --> 02:18:22,511
and I was totally lost, and I couldn't see a thing

3519
02:18:22,511 --> 02:18:24,431
and it was 50 below.

3520
02:18:24,431 --> 02:18:25,511
And so I tried it.

3521
02:18:25,511 --> 02:18:27,971
I fell to my knees in the snow and cried out

3522
02:18:27,971 --> 02:18:30,370
'Oh God, if there is a God

3523
02:18:30,370 --> 02:18:33,971
I'm lost in this blizzard and I'm gonna die if you don't help me.

3524
02:18:33,971 --> 02:18:38,571
'" And now back in the bar, the religious guy looks at the atheist

3525
02:18:38,571 --> 02:18:39,210
all puzzled.

3526
02:18:39,210 --> 02:18:42,411
"Well, then you must believe now," he says.

3527
02:18:42,411 --> 02:18:45,058
"After all, there you are, alive.

3528
02:18:45,058 --> 02:18:47,851
" The atheist just rolls his eyes.

3529
02:18:47,851 --> 02:18:48,591
"No, man.

3530
02:18:48,591 --> 02:18:55,416
All that happened was a couple of Eskimos happened to be wandering by and showed me the way back to the camp.

3531
02:18:55,416 --> 02:19:01,031
" All this, I think, teaches us that everything is a matter of perspective

3532
02:19:01,031 --> 02:19:10,191
and that wisdom may arrive if we have the humility to keep shifting and expanding our perspective on the world.

3533
02:19:10,191 --> 02:19:14,431
Thank you for allowing me to talk a bit about David Foster Wallace.

3534
02:19:14,431 --> 02:19:19,050
He's one of my favorite writers, and he's a beautiful soul.

3535
02:19:19,050 --> 02:19:23,151
If I may, one more thing I wanted to briefly comment on.

3536
02:19:23,151 --> 02:19:31,050
I find myself to be in this strange position of getting attacked online often from all sides

3537
02:19:31,050 --> 02:19:35,411
including being lied about, sometimes through selective misrepresentation

3538
02:19:35,411 --> 02:19:37,630
but often through downright lies.

3539
02:19:37,630 --> 02:19:39,571
I don't know how else to put it.

3540
02:19:39,571 --> 02:19:48,231
This all breaks my heart, frankly, but I've come to understand that it's the way of the internet and the cost of the path I've chosen.

3541
02:19:48,231 --> 02:19:52,291
There's been days when it's been rough on me mentally.

3542
02:19:52,291 --> 02:19:58,111
It's not fun being lied about, especially when it's about things that are usually

3543
02:19:58,111 --> 02:20:01,651
for a long time, have been a source of happiness and joy for me.

3544
02:20:01,651 --> 02:20:04,371
But again, that's life.

3545
02:20:04,371 --> 02:20:09,811
I'll continue exploring the world of people and ideas with empathy and rigor

3546
02:20:09,811 --> 02:20:13,911
wearing my heart on my sleeve as much as I can.

3547
02:20:13,911 --> 02:20:17,351
For me, that's the only way to live.

3548
02:20:17,351 --> 02:20:22,971
Anyway, a common attack on me is about my time at MIT and Drexel

3549
02:20:22,971 --> 02:20:26,931
two great universities I love and have tremendous respect for.

3550
02:20:26,931 --> 02:20:31,691
Since a bunch of lies have accumulated online about me on these topics

3551
02:20:31,691 --> 02:20:34,551
to a sad, and at times hilarious degree

3552
02:20:34,551 --> 02:20:40,811
I thought I would once more state the obvious facts about my bio for the small number of you who may care.

3553
02:20:40,811 --> 02:20:43,631
TL;DR, two things.

3554
02:20:43,631 --> 02:20:51,351
First, as I say often, including in a recent podcast episode that somehow was listened to by many millions of people

3555
02:20:51,351 --> 02:20:56,131
I proudly went to Drexel University for my bachelor's

3556
02:20:56,131 --> 02:20:58,071
master's, and doctorate degrees.

3557
02:20:58,071 --> 02:21:08,198
Second, I am a research scientist at MIT and have been there in a paid research position for the last 10 years.

3558
02:21:08,198 --> 02:21:11,692
Allow me to elaborate a bit more on these two things now

3559
02:21:11,692 --> 02:21:14,931
but please skip if this is not at all interesting.

3560
02:21:14,931 --> 02:21:20,711
So like I said, a common attack on me is that I have no real affiliation with MIT.

3561
02:21:20,711 --> 02:21:28,891
The accusation, I guess, is that I'm falsely claiming an MIT affiliation because I taught a lecture there once.

3562
02:21:28,891 --> 02:21:34,211
Nope, that accusation against me is a complete lie.

3563
02:21:34,211 --> 02:21:44,051
I have been at MIT for over 10 years in a paid research position from 2015 to today.

3564
02:21:44,051 --> 02:21:50,271
To be extra clear, I'm a research scientist at MIT working in LIDS

3565
02:21:50,271 --> 02:21:55,551
the Laboratory for Information and Decision Systems, in the College of Computing.

3566
02:21:55,551 --> 02:21:59,631
For now, since I'm still at MIT

3567
02:21:59,631 --> 02:22:04,992
you can, uh, see me in the directory and on the various lab pages.

3568
02:22:04,992 --> 02:22:09,291
I have indeed given many lectures at MIT over the years

3569
02:22:09,291 --> 02:22:12,451
a small fraction of which I posted online.

3570
02:22:12,451 --> 02:22:17,531
Teaching, for me, always has been just for fun and not part of my research work.

3571
02:22:17,531 --> 02:22:23,871
I personally think I suck at it, but I have always learned and grown from the experience.

3572
02:22:23,871 --> 02:22:28,751
It's like Feynman spoke about, if you want to understand something deeply

3573
02:22:28,751 --> 02:22:31,511
it's good to try to teach it.

3574
02:22:31,511 --> 02:22:35,651
But like I said, my main focus has always been on research.

3575
02:22:35,651 --> 02:22:41,571
I published many peer-reviewed papers that you can see in my Google Scholar profile.

3576
02:22:41,571 --> 02:22:47,391
For my first four years at MIT, I worked extremely intensively

3577
02:22:47,391 --> 02:22:50,471
most weeks were 80 to 100-hour workweeks.

3578
02:22:50,471 --> 02:22:54,611
After that, in 2019, I still kept my research scientist position

3579
02:22:54,611 --> 02:23:00,171
but I split my time taking a leap to pursue projects in AI and robotics outside MIT

3580
02:23:00,171 --> 02:23:03,131
and to dedicate a lot of focus to the podcast.

3581
02:23:03,131 --> 02:23:09,531
As I've said, I've been continuously surprised just how many hours preparing for an episode takes.

3582
02:23:09,531 --> 02:23:13,071
There are many episodes of the podcast for which I have to read

3583
02:23:13,071 --> 02:23:18,891
write and think per 100, 200 or more hours across multiple weeks and months.

3584
02:23:18,891 --> 02:23:24,331
Since 2020, I have not actively published research papers.

3585
02:23:24,331 --> 02:23:29,431
Just like the podcast, I think it's something that's a serious full-time effort.

3586
02:23:29,431 --> 02:23:34,891
But not publishing and doing full-time research has been eating at me

3587
02:23:34,891 --> 02:23:42,631
because I love research, and I love programming and building systems that test out interesting technical ideas

3588
02:23:42,631 --> 02:23:47,311
especially in the context of human-AI or human-robot interaction.

3589
02:23:47,311 --> 02:23:50,871
I hope to change this in the coming months and years.

3590
02:23:50,871 --> 02:23:54,171
What I've come to realize about myself is

3591
02:23:54,171 --> 02:23:58,911
if I don't publish or if I don't launch systems that people use

3592
02:23:58,911 --> 02:24:01,391
I definitely feel like a piece of me is missing.

3593
02:24:01,391 --> 02:24:04,971
It legitimately is a source of happiness for me.

3594
02:24:04,971 --> 02:24:08,371
Anyway, I'm proud of my time at MIT.

3595
02:24:08,371 --> 02:24:13,951
I was and am constantly surrounded by people much smarter than me

3596
02:24:13,951 --> 02:24:17,951
many of whom have become lifelong colleagues and friends.

3597
02:24:17,951 --> 02:24:21,611
MIT is a place I go to escape the world

3598
02:24:21,611 --> 02:24:26,451
to focus on exploring fascinating questions at the cutting edge of science and engineering.

3599
02:24:26,451 --> 02:24:37,071
This, again, makes me truly happy, and it does hit pretty hard on a psychological level when I'm getting attacked over this.

3600
02:24:37,071 --> 02:24:39,851
Perhaps I'm doing something wrong.

3601
02:24:39,851 --> 02:24:42,591
If I am, I will try to do better.

3602
02:24:42,591 --> 02:24:51,611
In all this discussion of academic work, I hope you know that I don't ever mean to say that I'm an expert at anything.

3603
02:24:51,611 --> 02:24:54,691
In the podcast and in my private life

3604
02:24:54,691 --> 02:24:56,831
I don't claim to be smart.

3605
02:24:56,831 --> 02:25:00,511
In fact, I often call myself an idiot

3606
02:25:00,511 --> 02:25:01,651
and mean it.

3607
02:25:01,651 --> 02:25:04,411
I try to make fun of myself as much as possible

3608
02:25:04,411 --> 02:25:08,191
and in general, to celebrate others instead.

3609
02:25:08,191 --> 02:25:12,791
Now, to talk about Drexel University, which I also love

3610
02:25:12,791 --> 02:25:17,311
am proud of, and am deeply grateful for my time there.

3611
02:25:17,311 --> 02:25:20,491
As I said, I went to Drexel for my bachelor's

3612
02:25:20,491 --> 02:25:25,351
master's, and doctorate degrees in computer science and electrical engineering.

3613
02:25:25,351 --> 02:25:28,931
I've talked about Drexel many times, including

3614
02:25:28,931 --> 02:25:31,771
as I mentioned, at the end of a recent podcast

3615
02:25:31,771 --> 02:25:37,371
the Donald Trump episode, funny enough, that was listened to by many millions of people

3616
02:25:37,371 --> 02:25:45,651
where I answered a question about graduate school and explained my own journey at Drexel and how grateful I am for it.

3617
02:25:45,651 --> 02:25:51,851
If it's at all interesting to you, please go listen to the end of that episode or watch the related clip.

3618
02:25:51,851 --> 02:26:00,271
At Drexel, I met and worked with many brilliant researchers and mentors from whom I've learned a lot about engineering

3619
02:26:00,271 --> 02:26:01,391
science, and life.

3620
02:26:01,391 --> 02:26:04,511
There are many valuable things I gained from my time at Drexel.

3621
02:26:04,511 --> 02:26:10,111
First, I took a large number of very difficult math and theoretical computer science courses.

3622
02:26:10,111 --> 02:26:12,951
They taught me how to think deeply and rigorously

3623
02:26:12,951 --> 02:26:15,871
and also how to work hard and not give up

3624
02:26:15,871 --> 02:26:20,011
even if it feels like I'm too dumb to find a solution to a technical problem.

3625
02:26:20,011 --> 02:26:24,331
Second, I programmed a lot during that time

3626
02:26:24,331 --> 02:26:25,871
mostly C, C++.

3627
02:26:25,871 --> 02:26:30,851
I programmed robots, optimization algorithms, computer vision systems

3628
02:26:30,851 --> 02:26:34,371
wireless network protocols, multimodal machine learning systems

3629
02:26:34,371 --> 02:26:37,171
and all kinds of simulations of physical systems.

3630
02:26:37,171 --> 02:26:42,191
This is where I really developed a love for programming

3631
02:26:42,191 --> 02:26:47,511
including, yes, Emacs and the Kinesis Keyboard.

3632
02:26:47,511 --> 02:26:51,851
Uh, I also, during that time, read a lot.

3633
02:26:51,851 --> 02:26:56,111
I played a lot of guitar, wrote a lot of crappy poetry

3634
02:26:56,111 --> 02:26:58,991
and, uh, trained a lot of, uh

3635
02:26:58,991 --> 02:27:03,531
in judo and jujitsu, which I cannot sing enough praises to.

3636
02:27:03,531 --> 02:27:07,491
Jujitsu humbled me on a daily basis throughout my 20s

3637
02:27:07,491 --> 02:27:11,671
and it still does to this very day whenever I get a chance to train.

3638
02:27:11,671 --> 02:27:22,071
Anyway, I hope that the folks who occasionally get swept up in the chanting online crowds that want to tear down others don't lose themselves in it too much.

3639
02:27:22,071 --> 02:27:28,131
In the end, I still think there's more good than bad in people

3640
02:27:28,131 --> 02:27:32,411
but we're all, each of us, a mixed bag.

3641
02:27:32,411 --> 02:27:35,091
I know I am very much flawed.

3642
02:27:35,091 --> 02:27:36,751
I speak awkwardly.

3643
02:27:36,751 --> 02:27:39,211
I sometimes say stupid shit.

3644
02:27:39,211 --> 02:27:41,551
I can get irrationally emotional.

3645
02:27:41,551 --> 02:27:44,211
I can be too much of a dick when I should be kind.

3646
02:27:44,211 --> 02:27:49,051
I can lose myself in a biased rabbit hole before I wake up to the bigger

3647
02:27:49,051 --> 02:27:51,131
more accurate picture of reality.

3648
02:27:51,131 --> 02:27:56,671
I'm human, and so are you, for better or for worse.

3649
02:27:56,671 --> 02:28:03,151
And I do still believe we're in this whole beautiful mess together.

3650
02:28:03,151 --> 02:28:05,371
I love you all.

