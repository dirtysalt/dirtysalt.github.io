1
00:00:47,600 --> 00:00:52,160
Today I'm chatting with Richard Sutton,  who is one of the founding fathers of

2
00:00:52,160 --> 00:00:55,440
reinforcement learning and inventor of  many of the main techniques used there,

3
00:00:55,440 --> 00:01:00,640
like TD learning and policy gradient methods. For that, he received this year's Turing Award

4
00:01:00,640 --> 00:01:05,440
which, if you don’t know, is the Nobel Prize  for computer science. Richard, congratulations.

5
00:01:05,440 --> 00:01:08,160
Thank you, Dwarkesh. Thanks for coming on the podcast.

6
00:01:08,160 --> 00:01:12,880
It's my pleasure. First question. My audience and I are

7
00:01:12,880 --> 00:01:18,560
familiar with the LLM way of thinking about AI. Conceptually, what are we missing in terms of

8
00:01:18,560 --> 00:01:25,200
thinking about AI from the RL perspective? It's really quite a different point of view.

9
00:01:26,240 --> 00:01:30,240
It can easily get separated and lose  the ability to talk to each other.

10
00:01:32,560 --> 00:01:37,840
Large language models have become such a big  thing, generative AI in general a big thing.

11
00:01:38,960 --> 00:01:46,880
Our field is subject to bandwagons and  fashions, so we lose track of the basic things.

12
00:01:46,880 --> 00:01:52,000
I consider reinforcement learning to  be basic AI. What is intelligence?

13
00:01:52,000 --> 00:01:58,640
The problem is to understand your world. Reinforcement learning is about understanding

14
00:01:58,640 --> 00:02:02,960
your world, whereas large language  models are about mimicking people,

15
00:02:02,960 --> 00:02:06,880
doing what people say you should do. They're not about figuring out what to do.

16
00:02:08,639 --> 00:02:14,720
You would think that to emulate the trillions  of tokens in the corpus of Internet text,

17
00:02:14,720 --> 00:02:17,920
you would have to build a world model. In fact, these models do seem to have

18
00:02:17,920 --> 00:02:21,360
very robust world models. They're the best world models

19
00:02:21,360 --> 00:02:26,000
we've made to date in AI, right? What do you think is missing?

20
00:02:26,000 --> 00:02:28,400
I would disagree with most  of the things you just said.

21
00:02:30,720 --> 00:02:34,720
To mimic what people say is not really  to build a model of the world at all.

22
00:02:36,080 --> 00:02:40,160
You're mimicking things that have  a model of the world: people.

23
00:02:40,160 --> 00:02:47,120
I don't want to approach the question in an  adversarial way, but I would question the

24
00:02:47,120 --> 00:02:51,520
idea that they have a world model. A world model would enable you

25
00:02:51,520 --> 00:02:55,440
to predict what would happen. They have the ability to predict

26
00:02:55,440 --> 00:02:57,040
what a person would say. They don't have the

27
00:02:57,040 --> 00:03:04,720
ability to predict what will happen. What we want, to quote Alan Turing, is a machine

28
00:03:04,720 --> 00:03:09,920
that can learn from experience, where experience  is the things that actually happen in your life.

29
00:03:09,920 --> 00:03:16,000
You do things, you see what happens,  and that's what you learn from.

30
00:03:16,640 --> 00:03:18,640
The large language models  learn from something else.

31
00:03:18,640 --> 00:03:22,960
They learn from "here's a situation,  and here's what a person did".

32
00:03:22,960 --> 00:03:26,800
Implicitly, the suggestion is you  should do what the person did.

33
00:03:26,800 --> 00:03:30,720
I guess maybe the crux, and I'm curious if  you disagree with this, is that some people

34
00:03:30,720 --> 00:03:36,080
will say that imitation learning has given us a  good prior, or given these models a good prior,

35
00:03:36,080 --> 00:03:42,880
of reasonable ways to approach problems. As we move towards the era of experience, as

36
00:03:42,880 --> 00:03:49,040
you call it, this prior is going to be the basis  on which we teach these models from experience,

37
00:03:49,040 --> 00:03:54,000
because this gives them the opportunity  to get answers right some of the time.

38
00:03:54,000 --> 00:04:00,000
Then on this, you can train them on experience. Do you agree with that perspective?

39
00:04:00,000 --> 00:04:04,320
No. I agree that it's the large  language model perspective.

40
00:04:04,320 --> 00:04:10,880
I don't think it's a good perspective. To be a prior for something,

41
00:04:10,880 --> 00:04:15,360
there has to be a real thing. A prior bit of knowledge should be

42
00:04:15,360 --> 00:04:20,880
the basis for actual knowledge. What is actual  knowledge? There's no definition of actual

43
00:04:20,880 --> 00:04:29,600
knowledge in that large-language framework. What makes an action a good action to take?

44
00:04:29,600 --> 00:04:34,720
You recognize the need for continual learning. If you need to learn continually,

45
00:04:34,720 --> 00:04:38,400
continually means learning during the  normal interaction with the world.

46
00:04:39,440 --> 00:04:43,840
There must be some way during the  normal interaction to tell what's right.

47
00:04:47,200 --> 00:04:54,080
Is there any way to tell in the large language  model setup what's the right thing to say?

48
00:04:54,080 --> 00:04:58,800
You will say something and you will not get  feedback about what the right thing to say is,

49
00:04:58,800 --> 00:05:02,720
because there's no definition of what  the right thing to say is. There's no

50
00:05:02,720 --> 00:05:07,520
goal. If there's no goal, then there's  one thing to say, another thing to say.

51
00:05:07,520 --> 00:05:12,960
There's no right thing to say. There's no  ground truth. You can't have prior knowledge

52
00:05:12,960 --> 00:05:17,600
if you don't have ground truth, because the  prior knowledge is supposed to be a hint or

53
00:05:17,600 --> 00:05:24,320
an initial belief about what the truth is. There  isn't any truth. There's no right thing to say.

54
00:05:24,880 --> 00:05:29,520
In reinforcement learning, there is a right thing  to say, a right thing to do, because the right

55
00:05:29,520 --> 00:05:33,760
thing to do is the thing that gets you reward. We have a definition of what's the right thing

56
00:05:33,760 --> 00:05:39,760
to do, so we can have prior knowledge  or knowledge provided by people about

57
00:05:39,760 --> 00:05:43,440
what the right thing to do is. Then we can check it to see,

58
00:05:43,440 --> 00:05:46,400
because we have a definition of what  the actual right thing to do is.

59
00:05:47,280 --> 00:05:50,800
An even simpler case is when you're  trying to make a model of the world.

60
00:05:50,800 --> 00:05:56,080
When you predict what will happen, you predict  and then you see what happens. There's ground

61
00:05:56,080 --> 00:06:02,560
truth. There's no ground truth in large  language models because you don't have

62
00:06:02,560 --> 00:06:09,760
a prediction about what will happen next. If you say something in your conversation,

63
00:06:09,760 --> 00:06:14,320
the large language models have no prediction  about what the person will say in response

64
00:06:14,320 --> 00:06:19,280
to that or what the response will be. I think they do. You can literally ask them,

65
00:06:19,280 --> 00:06:23,920
"What would you anticipate a user might say  in response?" They’ll have a prediction.

66
00:06:23,920 --> 00:06:29,040
No, they will respond to that question right. But they have no prediction in the substantive

67
00:06:29,040 --> 00:06:32,320
sense that they won't be  surprised by what happens.

68
00:06:32,320 --> 00:06:36,320
If something happens that isn't what you  might say they predicted, they will not

69
00:06:36,320 --> 00:06:43,920
change because an unexpected thing has happened. To learn that, they'd have to make an adjustment.

70
00:06:43,920 --> 00:06:49,600
I think a capability like  this does exist in context.

71
00:06:49,600 --> 00:06:52,880
It's interesting to watch a  model do chain of thought.

72
00:06:53,520 --> 00:06:56,560
Suppose it's trying to solve a math problem. It'll say, "Okay, I'm going to approach this

73
00:06:56,560 --> 00:07:00,880
problem using this approach first." It'll write this out and be like,

74
00:07:00,880 --> 00:07:03,840
"Oh wait, I just realized this is the wrong  conceptual way to approach the problem.

75
00:07:03,840 --> 00:07:10,240
I'm going to restart with another approach." That flexibility does exist in context, right?

76
00:07:10,240 --> 00:07:12,160
Do you have something else in mind  or do you just think that you need

77
00:07:12,160 --> 00:07:20,320
to extend this capability across longer horizons? I'm just saying they don't have in any meaningful

78
00:07:20,320 --> 00:07:25,040
sense a prediction of what will happen next. They will not be surprised by what happens next.

79
00:07:25,040 --> 00:07:30,640
They'll not make any changes if  something happens, based on what happens.

80
00:07:30,640 --> 00:07:32,880
Isn't that literally what  next token prediction is?

81
00:07:32,880 --> 00:07:35,000
Prediction about what's next and  then updating on the surprise?

82
00:07:35,000 --> 00:07:39,280
The next token is what they should  say, what the actions should be.

83
00:07:39,280 --> 00:07:42,400
It's not what the world will give  them in response to what they do.

84
00:07:42,400 --> 00:07:48,720
Let's go back to their lack of a goal. For me, having a goal is

85
00:07:48,720 --> 00:07:52,720
the essence of intelligence. Something is intelligent if it can achieve goals.

86
00:07:53,360 --> 00:07:57,520
I like John McCarthy's definition that  intelligence is the computational part

87
00:07:57,520 --> 00:08:03,760
of the ability to achieve goals. You have to have goals or you're

88
00:08:03,760 --> 00:08:08,320
just a behaving system. You're not anything special,

89
00:08:08,320 --> 00:08:11,360
you're not intelligent. You agree that large language

90
00:08:11,360 --> 00:08:14,480
models don't have goals? No, they have a goal.

91
00:08:14,480 --> 00:08:17,040
What's the goal? Next token prediction.

92
00:08:17,040 --> 00:08:24,080
That's not a goal. It doesn't change  the world. Tokens come at you,

93
00:08:24,080 --> 00:08:31,760
and if you predict them, you don't influence them. Oh yeah. It's not a goal about the external world.

94
00:08:31,760 --> 00:08:38,880
It's not a goal. It's not a substantive goal.  You can't look at a system and say it has a goal

95
00:08:38,880 --> 00:08:42,400
if it's just sitting there predicting and being  happy with itself that it's predicting accurately.

96
00:08:43,280 --> 00:08:48,480
The bigger question I want to understand  is why you don't think doing RL on

97
00:08:48,480 --> 00:08:52,800
top of LLMs is a productive direction. We seem to be able to give these models

98
00:08:52,800 --> 00:08:58,480
the goal of solving difficult math problems. They are in many ways at the very peaks of

99
00:08:58,480 --> 00:09:04,480
human-level in the capacity to solve math  Olympiad-type problems. They got gold at

100
00:09:04,480 --> 00:09:09,680
IMO. So it seems like the model which got  gold at the International Math Olympiad does

101
00:09:09,680 --> 00:09:15,120
have the goal of getting math problems right. Why can't we extend this to different domains?

102
00:09:15,760 --> 00:09:22,320
The math problems are different. Making a  model of the physical world and carrying

103
00:09:22,320 --> 00:09:29,920
out the consequences of mathematical assumptions  or operations, those are very different things.

104
00:09:29,920 --> 00:09:35,680
The empirical world has to be learned. You have to learn the consequences.

105
00:09:36,240 --> 00:09:43,840
Whereas the math is more computational,  it's more like standard planning.

106
00:09:44,640 --> 00:09:54,800
There they can have a goal to find  the proof, and they are in some way

107
00:09:54,800 --> 00:09:59,760
given that goal to find the proof. It's interesting because you wrote

108
00:09:59,760 --> 00:10:04,640
this essay in 2019 titled "The Bitter  Lesson," and this is the most influential

109
00:10:04,640 --> 00:10:13,600
essay, perhaps, in the history of AI. But people have used that as a justification for

110
00:10:13,600 --> 00:10:21,520
scaling up LLMs because, in their view, this is  the one scalable way we have found to pour ungodly

111
00:10:21,520 --> 00:10:26,480
amounts of compute into learning about the world. It's interesting that your perspective is that

112
00:10:26,480 --> 00:10:32,240
the LLMs are not "bitter lesson"-pilled. It's an interesting question whether large

113
00:10:32,240 --> 00:10:42,880
language models are a case of the bitter lesson. They are clearly a way of using massive

114
00:10:42,880 --> 00:10:48,800
computation, things that will scale with  computation up to the limits of the Internet.

115
00:10:51,120 --> 00:11:01,120
But they're also a way of putting in lots of  human knowledge. This is an interesting question.

116
00:11:01,920 --> 00:11:13,360
It's a sociological or industry question. Will they reach the limits of the data and

117
00:11:13,360 --> 00:11:23,680
be superseded by things that can get more data  just from experience rather than from people?

118
00:11:24,880 --> 00:11:28,640
In some ways it's a classic  case of the bitter lesson.

119
00:11:29,520 --> 00:11:32,800
The more human knowledge we put into the  large language models, the better they

120
00:11:32,800 --> 00:11:44,560
can do. So it feels good. Yet, I expect there  to be systems that can learn from experience.

121
00:11:44,560 --> 00:11:49,440
Which could perform much better  and be much more scalable.

122
00:11:49,440 --> 00:11:56,560
In which case, it will be another instance of the  bitter lesson, that the things that used human

123
00:11:56,560 --> 00:12:05,120
knowledge were eventually superseded by things  that just trained from experience and computation.

124
00:12:05,120 --> 00:12:11,200
I guess that doesn't seem like the crux to me. I think those people would also agree that the

125
00:12:11,200 --> 00:12:17,760
overwhelming amount of compute in the future  will come from learning from experience.

126
00:12:17,760 --> 00:12:22,800
They just think that the scaffold or the basis of  that, the thing you'll start with in order to pour

127
00:12:22,800 --> 00:12:29,520
in the compute to do this future experiential  learning or on-the-job learning, will be LLMs.

128
00:12:31,680 --> 00:12:36,480
I still don't understand why this is  the wrong starting point altogether.

129
00:12:36,480 --> 00:12:42,480
Why do we need a whole new architecture to  begin doing experiential, continual learning?

130
00:12:43,040 --> 00:12:48,320
Why can't we start with LLMs to do that? In every case of the bitter lesson you

131
00:12:48,320 --> 00:12:56,560
could start with human knowledge and then do the  scalable things. That's always the case. There's

132
00:12:56,560 --> 00:13:02,240
never any reason why that has to be bad. But in fact, and in practice,

133
00:13:02,240 --> 00:13:07,360
it has always turned out to be bad. People get locked into the human

134
00:13:07,360 --> 00:13:13,840
knowledge approach, and they psychologically…  Now I'm speculating why it is, but this is

135
00:13:13,840 --> 00:13:20,000
what has always happened. They get their lunch eaten

136
00:13:20,000 --> 00:13:24,800
by the methods that are truly scalable. Give me a sense of what the scalable method is.

137
00:13:24,800 --> 00:13:33,120
The scalable method is you learn from experience. You try things, you see what works.

138
00:13:33,760 --> 00:13:37,360
No one has to tell you. First of all, you have a goal.

139
00:13:37,360 --> 00:13:41,920
Without a goal, there's no sense of  right or wrong or better or worse.

140
00:13:41,920 --> 00:13:48,000
Large language models are trying to get by without  having a goal or a sense of better or worse.

141
00:13:48,000 --> 00:13:55,280
That's just exactly starting in the wrong place. Maybe it's interesting to compare this to humans.

142
00:13:55,840 --> 00:14:02,160
In both the case of learning from imitation  versus experience and on the question of goals,

143
00:14:02,160 --> 00:14:09,760
I think there's some interesting analogies. Kids will initially learn from imitation.

144
00:14:10,800 --> 00:14:14,320
You don't think so? No, of course not.

145
00:14:14,320 --> 00:14:19,680
Really? I think kids just watch people. They try to say the same words…

146
00:14:19,680 --> 00:14:24,880
How old are these kids? What  about the first six months?

147
00:14:24,880 --> 00:14:28,480
I think they're imitating things. They're  trying to make their mouth sound the way

148
00:14:28,480 --> 00:14:31,040
they see their mother's mouth sound. Then they'll say the same words without

149
00:14:31,040 --> 00:14:33,840
understanding what they mean. As they get older, the complexity

150
00:14:33,840 --> 00:14:40,480
of the imitation they do increases. You're imitating maybe the skills that

151
00:14:41,360 --> 00:14:44,880
people in your band are using to  hunt down the deer or something.

152
00:14:44,880 --> 00:14:47,840
Then you go into the learning  from experience RL regime.

153
00:14:47,840 --> 00:14:51,680
But I think there's a lot of imitation  learning happening with humans.

154
00:14:51,680 --> 00:14:54,560
It's surprising you can have  such a different point of view.

155
00:14:55,360 --> 00:15:00,640
When I see kids, I see kids just  trying things and waving their

156
00:15:00,640 --> 00:15:10,400
hands around and moving their eyes around. There's no imitation for how they move their

157
00:15:10,400 --> 00:15:14,400
eyes around or even the sounds they make. They may want to create the same sounds,

158
00:15:14,400 --> 00:15:23,120
but the actions, the thing that the infant  actually does, there's no targets for that.

159
00:15:23,120 --> 00:15:26,720
There are no examples for that. I agree. That doesn't explain everything infants

160
00:15:26,720 --> 00:15:31,920
do, but I think it guides a learning process. Even an LLM, when it's trying to predict the next

161
00:15:31,920 --> 00:15:36,800
token early in training, it will make a guess. It'll be different from what it actually sees.

162
00:15:36,800 --> 00:15:40,160
In some sense, it's very short-horizon  RL, where it's making this guess,

163
00:15:40,160 --> 00:15:43,200
"I think this token will be this." It's this other thing, similar to how a kid

164
00:15:43,200 --> 00:15:47,920
will try to say a word. It comes out wrong. The large language models are learning

165
00:15:47,920 --> 00:15:52,960
from training data. It's not learning from  experience. It's learning from something that

166
00:15:52,960 --> 00:15:59,920
will never be available during its normal life. There's never any training data that says you

167
00:15:59,920 --> 00:16:05,520
should do this action in normal life. I think this is more of a semantic

168
00:16:05,520 --> 00:16:08,080
distinction. What do you call  school? Is that not training data?

169
00:16:10,640 --> 00:16:14,320
School is much later. Okay,  I shouldn't have said never.

170
00:16:15,200 --> 00:16:17,680
I don’t know, I think I would  even say that about school.

171
00:16:17,680 --> 00:16:25,216
But formal schooling is the exception. But there are phases of learning where

172
00:16:25,216 --> 00:16:29,040
there’s the programming in your biology  early on, you're not that useful.

173
00:16:29,040 --> 00:16:34,960
Then why you exist is to understand the  world and learn how to interact with it.

174
00:16:34,960 --> 00:16:39,520
It seems like a training phase. I agree that then there's a more

175
00:16:39,520 --> 00:16:44,240
gradual… There's not a sharp cutoff  to training to deployment, but there

176
00:16:44,240 --> 00:16:49,360
seems to be this initial training phase right? There's nothing where you have training of what

177
00:16:49,360 --> 00:16:58,640
you should do. There's nothing. You see things  that happen. You're not told what to do. Don't

178
00:16:59,520 --> 00:17:03,840
be difficult. I mean this is obvious. You're literally taught what to do.

179
00:17:03,840 --> 00:17:06,720
This is where the word training  comes from, from humans.

180
00:17:07,839 --> 00:17:13,119
I don't think learning is really about training. I think learning is about learning,

181
00:17:13,119 --> 00:17:18,560
it's about an active process. The child tries things and sees what happens.

182
00:17:22,400 --> 00:17:27,040
We don't think about training when  we think of an infant growing up.

183
00:17:27,040 --> 00:17:32,080
These things are actually rather well understood. If you look at how psychologists think about

184
00:17:32,080 --> 00:17:40,080
learning, there's nothing like imitation. Maybe there are some extreme cases where humans

185
00:17:40,080 --> 00:17:46,640
might do that or appear to do that, but there's  no basic animal learning process called imitation.

186
00:17:46,640 --> 00:17:53,200
There are basic animal learning processes for  prediction and for trial-and-error control.

187
00:17:53,200 --> 00:17:58,080
It's really interesting how sometimes the  hardest things to see are the obvious ones.

188
00:17:58,080 --> 00:18:04,800
It's obvious—if you look at animals and how  they learn, and you look at psychology and our

189
00:18:04,800 --> 00:18:12,720
theories of them—that supervised learning  is not part of the way animals learn.

190
00:18:13,760 --> 00:18:20,240
We don't have examples of desired behavior. What we have are examples of things that happen,

191
00:18:20,240 --> 00:18:24,000
one thing that followed another. We have examples of,

192
00:18:24,000 --> 00:18:31,440
"We did something and there were consequences." But there are no examples of supervised learning.

193
00:18:32,080 --> 00:18:35,120
Supervised learning is not  something that happens in nature.

194
00:18:38,400 --> 00:18:42,960
Even if that were the case with school,  we should forget about it because that's

195
00:18:42,960 --> 00:18:48,320
some special thing that happens in people. It doesn't happen broadly in nature. Squirrels

196
00:18:48,320 --> 00:18:51,520
don't go to school. Squirrels  can learn all about the world.

197
00:18:51,520 --> 00:18:58,800
It's absolutely obvious, I would say, that  supervised learning doesn't happen in animals.

198
00:18:59,680 --> 00:19:05,040
I interviewed this psychologist  and anthropologist, Joseph Henrich,

199
00:19:05,040 --> 00:19:12,160
who has done work about cultural evolution,  basically what distinguishes humans and

200
00:19:12,160 --> 00:19:15,600
how humans pick up knowledge. Why are you trying to distinguish

201
00:19:15,600 --> 00:19:22,160
humans? Humans are animals. What we  have in common is more interesting.

202
00:19:22,160 --> 00:19:26,000
What distinguishes us, we should  be paying less attention to.

203
00:19:26,000 --> 00:19:31,440
We're trying to replicate intelligence. If you  want to understand what it is that enables humans

204
00:19:31,440 --> 00:19:37,120
to go to the moon or to build semiconductors,  I think the thing we want to understand is what

205
00:19:37,120 --> 00:19:38,400
makes that happen. No animal can go

206
00:19:38,400 --> 00:19:42,640
to the moon or make semiconductors. We want to understand what makes humans special.

207
00:19:42,640 --> 00:19:47,840
I like the way you consider that obvious,  because I consider the opposite obvious.

208
00:19:50,640 --> 00:19:57,040
We have to understand how we are animals. If we understood a squirrel, I think we'd

209
00:19:57,040 --> 00:20:01,040
be almost all the way there to  understanding human intelligence.

210
00:20:01,040 --> 00:20:08,960
The language part is just a small veneer on the  surface. This is great. We're finding out the

211
00:20:08,960 --> 00:20:15,840
very different ways that we're thinking. We're  not arguing. We're trying to share our different

212
00:20:15,840 --> 00:20:19,280
ways of thinking with each other. I think argument is useful.

213
00:20:21,120 --> 00:20:24,720
I do want to complete this thought. Joseph Henrich has this interesting

214
00:20:24,720 --> 00:20:33,120
theory about a lot of the skills that humans  have had to master in order to be successful.

215
00:20:33,120 --> 00:20:35,760
We're not talking about the last  thousand years or the last 10,000 years,

216
00:20:35,760 --> 00:20:42,960
but hundreds of thousands of years. The world  is really complicated. It's not possible to

217
00:20:42,960 --> 00:20:50,480
reason through how to, let’s say, hunt  a seal if you're living in the Arctic.

218
00:20:50,480 --> 00:20:57,680
There's this many, many-step, long process of  how to make the bait and how to find the seal,

219
00:20:57,680 --> 00:21:02,240
and then how to process the food in a way  that makes sure you won't get poisoned.

220
00:21:02,240 --> 00:21:09,920
It's not possible to reason through all of that. Over time, there's this larger process of whatever

221
00:21:09,920 --> 00:21:14,400
analogy you want to use—maybe RL, something  else—where culture as a whole has figured out

222
00:21:14,400 --> 00:21:23,120
how to find and kill and eat seals. In his view, what is happening when

223
00:21:23,920 --> 00:21:29,840
this knowledge is transmitted through  generations, is that you have to imitate

224
00:21:29,840 --> 00:21:34,800
your elders in order to learn that skill. You can't think your way through how to

225
00:21:34,800 --> 00:21:38,160
hunt and kill and process a seal. You have to watch other people,

226
00:21:38,160 --> 00:21:43,040
maybe make tweaks and adjustments,  and that's how knowledge accumulates.

227
00:21:43,040 --> 00:21:46,240
The initial step of the cultural  gain has to be imitation.

228
00:21:46,240 --> 00:21:49,680
But maybe you think about it a different way? No, I think about it the same way.

229
00:21:50,720 --> 00:21:58,160
Still, it's a small thing on top of basic  trial-and-error learning, prediction learning.

230
00:21:58,160 --> 00:22:05,600
It's what distinguishes us, perhaps,  from many animals. But we're an animal

231
00:22:05,600 --> 00:22:13,520
first. We were an animal before we had  language and all those other things.

232
00:22:13,520 --> 00:22:17,040
I do think you make a very interesting  point that continual learning is a

233
00:22:17,040 --> 00:22:22,400
capability that most mammals have. I guess all mammals have it.

234
00:22:22,400 --> 00:22:28,400
It's quite interesting that we have something that  all mammals have, but our AI systems don't have.

235
00:22:29,040 --> 00:22:33,680
Whereas the ability to understand math and  solve difficult math problems—depends on how

236
00:22:33,680 --> 00:22:40,800
you define math—is a capability that our  AIs have, but that almost no animal has.

237
00:22:40,800 --> 00:22:45,520
It's quite interesting what ends up being  difficult and what ends up being easy.

238
00:22:45,520 --> 00:23:56,800
Moravec's paradox. That’s right, that’s right.

239
00:23:58,080 --> 00:24:02,400
This alternative paradigm that you're imagining… The experiential paradigm. Let's

240
00:24:02,400 --> 00:24:08,160
lay it out a little bit. It says that experience, action,

241
00:24:08,160 --> 00:24:14,640
sensation—well, sensation, action, reward—this  happens on and on and on for your life.

242
00:24:15,360 --> 00:24:20,160
It says that this is the foundation  and the focus of intelligence.

243
00:24:20,160 --> 00:24:25,680
Intelligence is about taking that  stream and altering the actions to

244
00:24:25,680 --> 00:24:32,320
increase the rewards in the stream. Learning then is from the stream,

245
00:24:32,320 --> 00:24:39,600
and learning is about the stream. That second part is particularly telling.

246
00:24:40,640 --> 00:24:44,320
What you learn, your  knowledge, is about the stream.

247
00:24:44,320 --> 00:24:48,240
Your knowledge is about if you  do some action, what will happen.

248
00:24:48,240 --> 00:24:55,040
Or it's about which events will follow other  events. It's about the stream. The content of

249
00:24:55,040 --> 00:25:01,920
the knowledge is statements about the stream. Because it's a statement about the stream,

250
00:25:01,920 --> 00:25:06,720
you can test it by comparing it to the  stream, and you can learn it continually.

251
00:25:06,720 --> 00:25:10,400
When you're imagining this  future continual learning agent…

252
00:25:10,400 --> 00:25:13,200
They're not "future". Of  course, they exist all the time.

253
00:25:13,760 --> 00:25:17,200
This is what the reinforcement learning  paradigm is, learning from experience.

254
00:25:17,200 --> 00:25:20,880
Yeah, I guess what I meant to  say is a general human-level,

255
00:25:20,880 --> 00:25:26,880
general continual learning agent. What is the  reward function? Is it just predicting the world?

256
00:25:26,880 --> 00:25:34,160
Is it then having a specific effect on it? What would the general reward function be?

257
00:25:34,160 --> 00:25:42,320
The reward function is arbitrary. If you're  playing chess, it's to win the game of chess.

258
00:25:42,320 --> 00:25:48,720
If you're a squirrel, maybe the  reward has to do with getting nuts.

259
00:25:51,680 --> 00:25:58,560
In general, for an animal, you would say the  reward is to avoid pain and to acquire pleasure.

260
00:26:04,480 --> 00:26:08,720
I think there also should be a  component having to do with your

261
00:26:08,720 --> 00:26:14,800
increasing understanding of your environment. That would be sort of an intrinsic motivation.

262
00:26:14,800 --> 00:26:23,840
I see. With this AI, lots of people would want  it to be doing lots of different kinds of things.

263
00:26:24,720 --> 00:26:28,320
It's performing the task people want,  but at the same time, it's learning

264
00:26:28,320 --> 00:26:35,520
about the world from doing that task. Let’s say we get rid of this paradigm

265
00:26:35,520 --> 00:26:39,360
where there's training periods and  then there's deployment periods.

266
00:26:40,960 --> 00:26:46,000
Do we also get rid of this paradigm where there's  the model and then instances of the model or

267
00:26:46,000 --> 00:26:53,440
copies of the model that are doing certain things? How do you think about the fact that we'd

268
00:26:53,440 --> 00:26:56,800
want this thing to be doing different things? We'd want to aggregate the knowledge that it's

269
00:26:56,800 --> 00:27:00,560
gaining from doing those different things. I don't like the word "model"

270
00:27:00,560 --> 00:27:05,840
when used the way you just did. I think a better word would be "the network"

271
00:27:05,840 --> 00:27:11,760
because I think you mean the network. Maybe  there are many networks. Anyway, things would

272
00:27:11,760 --> 00:27:20,160
be learned. You'd have copies and many instances. Sure, you'd want to share knowledge across the

273
00:27:20,160 --> 00:27:21,920
instances. There would be

274
00:27:21,920 --> 00:27:28,160
lots of possibilities for doing that. Today, you have one child grow up and

275
00:27:28,160 --> 00:27:33,120
learn about the world, and then every  new child has to repeat that process.

276
00:27:33,120 --> 00:27:38,320
Whereas with AIs, with a digital intelligence,  you could hope to do it once and then copy it

277
00:27:38,320 --> 00:27:43,200
into the next one as a starting place. This would be a huge savings.

278
00:27:44,160 --> 00:27:49,680
I think it'd be much more important  than trying to learn from people.

279
00:27:49,680 --> 00:27:54,880
I agree that the kind of thing you're  talking about is necessary regardless

280
00:27:54,880 --> 00:28:00,240
of whether you start from LLMs or not. If you want human or animal-level intelligence,

281
00:28:00,240 --> 00:28:04,000
you're going to need this capability. Suppose a human is trying to make a startup.

282
00:28:05,440 --> 00:28:08,640
This is a thing which has a  reward on the order of 10 years.

283
00:28:08,640 --> 00:28:12,480
Once in 10 years you might have an exit  where you get paid out a billion dollars.

284
00:28:12,480 --> 00:28:18,320
But humans have this ability to make intermediate  auxiliary rewards or have some way of…Even when

285
00:28:18,320 --> 00:28:23,040
they have extremely sparse rewards, they  can still make intermediate steps having an

286
00:28:23,040 --> 00:28:27,920
understanding of what the next thing they're  doing leads to this grander goal we have.

287
00:28:27,920 --> 00:28:31,520
How do you imagine such a  process might play out with AIs?

288
00:28:31,520 --> 00:28:35,920
This is something we know very well. The basis of it is temporal difference

289
00:28:35,920 --> 00:28:41,520
learning where the same thing  happens in a less grandiose scale.

290
00:28:41,520 --> 00:28:46,880
When you learn to play chess, you have  the long-term goal of winning the game.

291
00:28:46,880 --> 00:28:54,320
Yet you want to be able to learn from shorter-term  things like taking your opponent's pieces.

292
00:28:55,040 --> 00:28:59,200
You do that by having a value function  which predicts the long-term outcome.

293
00:28:59,760 --> 00:29:05,040
Then if you take the guy's pieces, your  prediction about the long-term outcome is changed.

294
00:29:05,040 --> 00:29:11,680
It goes up, you think you're going to win. Then that increase in your belief immediately

295
00:29:11,680 --> 00:29:20,080
reinforces the move that led to taking the piece. We have this long-term 10-year goal of making a

296
00:29:20,080 --> 00:29:24,400
startup and making a lot of money. When we make progress, we say, "Oh,

297
00:29:24,400 --> 00:29:32,560
I'm more likely to achieve the long-term goal,"  and that rewards the steps along the way.

298
00:29:34,000 --> 00:29:39,280
You also want some ability for  information that you're learning.

299
00:29:39,280 --> 00:29:43,360
One of the things that makes humans quite  different from these LLMs is that if you're

300
00:29:43,360 --> 00:29:47,440
onboarding on a job, you're picking  up so much context and information.

301
00:29:47,440 --> 00:29:51,360
That's what makes you useful at the job. You're learning everything from how your

302
00:29:51,360 --> 00:29:55,200
client has preferences to how  the company works, everything.

303
00:29:56,560 --> 00:30:01,680
Is the bandwidth of information that you  get from a procedure like TD learning high

304
00:30:01,680 --> 00:30:06,160
enough to have this huge pipe of  context and tacit knowledge that

305
00:30:06,160 --> 00:30:10,400
you need to be picking up in the way  humans do when they're just deployed?

306
00:30:14,560 --> 00:30:20,400
I’m not sure but I think at the crux of this,  the big world hypothesis seems very relevant.

307
00:30:20,400 --> 00:30:25,920
The reason why humans become useful on  the job is because they are encountering

308
00:30:25,920 --> 00:30:31,280
their particular part of the world. It can't have been anticipated and

309
00:30:31,280 --> 00:30:38,640
can't all have been put in in advance. The world is so huge that you can't.

310
00:30:38,640 --> 00:30:45,600
The dream of large language models, as I see  it, is you can teach the agent everything.

311
00:30:45,600 --> 00:30:51,280
It will know everything and won't have to  learn anything online, during its life.

312
00:30:52,720 --> 00:30:58,400
Your examples are all, "Well, really  you have to" because you can teach it,

313
00:30:58,400 --> 00:31:02,720
but there's all the little idiosyncrasies of  the particular life they're leading and the

314
00:31:02,720 --> 00:31:07,440
particular people they're working with and what  they like, as opposed to what average people like.

315
00:31:08,480 --> 00:31:13,600
That's just saying the world is really big, and  you're going to have to learn it along the way.

316
00:31:14,320 --> 00:31:19,200
It seems to me you need two things. One is some way of converting this long-run

317
00:31:19,200 --> 00:31:27,680
goal reward into smaller auxiliary predictive  rewards of the future reward, or the future

318
00:31:27,680 --> 00:31:32,240
reward that leads to the final reward. But initially, it seems to me,

319
00:31:35,280 --> 00:31:42,000
I need to hold on to all this context that  I'm gaining as I'm working in the world.

320
00:31:42,000 --> 00:31:48,800
I'm learning about my clients, my  company, and all this information.

321
00:31:50,320 --> 00:31:54,080
I would say you're just doing regular  learning. Maybe you're using "context"

322
00:31:54,080 --> 00:31:58,400
because in large language models all that  information has to go into the context window.

323
00:31:58,960 --> 00:32:02,560
But in a continual learning setup,  it just goes into the weights.

324
00:32:02,560 --> 00:32:06,400
Maybe context is the wrong word to use  because I mean a more general thing.

325
00:32:06,400 --> 00:32:10,400
You learn a policy that's specific to the  environment that you're finding yourself in.

326
00:32:12,320 --> 00:32:20,560
The question I'm trying to ask is, you need some  way of getting…How many bits per second is a human

327
00:32:20,560 --> 00:32:25,360
picking up when they're out in the world? If you're just interacting over Slack

328
00:32:25,360 --> 00:32:28,320
with your clients and everything. Maybe you're trying to ask the question of,

329
00:32:28,320 --> 00:32:33,920
it seems like the reward is too small of a  thing to do all the learning that we need to do.

330
00:32:33,920 --> 00:32:40,640
But we have the sensations, we have all  the other information we can learn from.

331
00:32:41,360 --> 00:32:45,360
We don't just learn from the reward. We learn from all the data.

332
00:32:45,360 --> 00:32:51,280
What is the learning process which  helps you capture that information?

333
00:32:52,320 --> 00:32:59,280
Now I want to talk about the base common  model of the agent with the four parts. We

334
00:32:59,280 --> 00:33:04,560
need a policy. The policy says, "In the  situation I'm in, what should I do?" We

335
00:33:04,560 --> 00:33:09,120
need a value function. The value function is  the thing that is learned with TD learning,

336
00:33:09,120 --> 00:33:13,920
and the value function produces a number. The number says how well it's going.

337
00:33:13,920 --> 00:33:19,360
Then you watch if that's going up and  down and use that to adjust your policy.

338
00:33:19,360 --> 00:33:24,320
So you have those two things. Then there's also the perception

339
00:33:24,320 --> 00:33:30,800
component, which is construction of your state  representation, your sense of where you are now.

340
00:33:30,800 --> 00:33:34,640
The fourth one is what we're really  getting at, most transparently anyway.

341
00:33:34,640 --> 00:33:38,400
The fourth one is the  transition model of the world.

342
00:33:38,400 --> 00:33:41,760
That's why I am uncomfortable just calling  everything "models," because I want to

343
00:33:41,760 --> 00:33:45,520
talk about the model of the world,  the transition model of the world.

344
00:33:45,520 --> 00:33:50,400
Your belief that if you do this, what will happen? What will be the consequences of what you do?

345
00:33:50,400 --> 00:33:55,280
Your physics of the world. But it's not  just physics, it's also abstract models,

346
00:33:55,280 --> 00:34:00,800
like your model of how you traveled from  California up to Edmonton for this podcast.

347
00:34:00,800 --> 00:34:05,440
That was a model, and that's a transition  model. That would be learned. It's not

348
00:34:05,440 --> 00:34:08,880
learned from reward. It's learned from,  "You did things, you saw what happened,

349
00:34:08,880 --> 00:34:13,679
you made that model of the world." That will be learned very richly

350
00:34:13,679 --> 00:34:17,840
from all the sensation that you  receive, not just from the reward.

351
00:34:17,840 --> 00:34:22,639
It has to include the reward as well,  but that's a small part of the whole

352
00:34:22,639 --> 00:34:27,920
model, a small, crucial part of the whole model. One of my friends, Toby Ord, pointed out that if

353
00:34:27,920 --> 00:34:36,400
you look at the MuZero models that Google DeepMind  deployed to learn Atari games, these models were

354
00:34:36,400 --> 00:34:42,080
initially not a general intelligence itself,  but a general framework for training specialized

355
00:34:42,080 --> 00:34:46,639
intelligences to play specific games. That is to say that you couldn't,

356
00:34:46,639 --> 00:34:53,360
using that framework, train a policy to  play both chess and Go and some other game.

357
00:34:53,360 --> 00:34:58,640
You had to train each one in a specialized way. He was wondering whether that implies

358
00:34:58,640 --> 00:35:03,360
that with reinforcement learning generally,  because of this information constraint,

359
00:35:03,360 --> 00:35:08,160
you can only learn one thing at a time? The density of information isn't that high?

360
00:35:08,160 --> 00:35:11,920
Or whether it was just specific  to the way that MuZero was done.

361
00:35:11,920 --> 00:35:18,160
If it's specific to AlphaZero, what needed  to be changed about that approach so that

362
00:35:18,160 --> 00:35:24,720
it could be a general learning agent? The idea is totally general. I do use

363
00:35:24,720 --> 00:35:30,480
all the time, as my canonical example,  the idea of an AI agent is like a person.

364
00:35:32,080 --> 00:35:37,120
People, in some sense, have  just one world they live in.

365
00:35:38,000 --> 00:35:43,120
That world may involve chess and it  may involve Atari games, but those are

366
00:35:43,120 --> 00:35:47,840
not a different task or a different world. Those are different states they encounter.

367
00:35:47,840 --> 00:35:54,640
So the general idea is not limited at all. Maybe it would be useful to explain what was

368
00:35:54,640 --> 00:36:04,800
missing in that architecture, or that approach,  which this continual learning AGI would have.

369
00:36:04,800 --> 00:36:13,440
They just set it up. It was not their  ambition to have one agent across those games.

370
00:36:13,440 --> 00:36:18,160
If we want to talk about transfer, we should  talk about transfer not across games or

371
00:36:18,160 --> 00:36:26,880
across tasks, but transfer between states. I guess I’m curious if historically, have we

372
00:36:26,880 --> 00:36:35,280
seen the level of transfer using RL techniques  that would be needed to build this kind of…

373
00:36:35,280 --> 00:36:42,800
Good. Good. We're not seeing transfer anywhere.  Critical to good performance is that you can

374
00:36:42,800 --> 00:36:47,920
generalize well from one state to another state. We don't have any methods that are good at that.

375
00:36:47,920 --> 00:36:56,640
What we have are people trying different things  and they settle on something, a representation

376
00:36:56,640 --> 00:37:05,680
that transfers well or generalizes well. But we have very few automated techniques

377
00:37:05,680 --> 00:37:10,880
to promote transfer, and none of them  are used in modern deep learning.

378
00:37:11,760 --> 00:37:17,440
Let me paraphrase to make sure  that I understood that correctly.

379
00:37:17,440 --> 00:37:22,640
It sounds like you're saying that when we  do have generalization in these models,

380
00:37:22,640 --> 00:37:31,040
that is a result of some sculpted… Humans did it. The researchers did it.

381
00:37:31,040 --> 00:37:35,600
Because there's no other explanation. Gradient  descent will not make you generalize well.

382
00:37:35,600 --> 00:37:39,760
It will make you solve the problem. It will not make you, if you get

383
00:37:39,760 --> 00:37:45,680
new data, generalize in a good way. Generalization means to train on one thing

384
00:37:45,680 --> 00:37:50,560
that'll affect what you do on other things. We know deep learning is really bad at this.

385
00:37:50,560 --> 00:37:56,160
For example, we know that if you train on some new  thing, it will often catastrophically interfere

386
00:37:56,160 --> 00:38:02,960
with all the old things that you knew. This  is exactly bad generalization. Generalization,

387
00:38:02,960 --> 00:38:08,480
as I said, is some kind of influence of  training on one state on other states.

388
00:38:11,440 --> 00:38:13,760
The fact that you generalize  is not necessarily good or bad.

389
00:38:13,760 --> 00:38:16,240
You can generalize poorly,  you can generalize well.

390
00:38:17,680 --> 00:38:23,920
Generalization always will happen, but  we need algorithms that will cause the

391
00:38:23,920 --> 00:38:30,560
generalization to be good rather than bad. I'm not trying to kickstart this initial

392
00:38:30,560 --> 00:38:35,520
crux again, but I'm just genuinely curious because  I think I might be using the term differently.

393
00:38:35,520 --> 00:38:39,040
One way to think about these LLMs is  that they’re increasing the scope of

394
00:38:39,040 --> 00:38:44,800
generalization from earlier systems, which  could not really even do a basic math problem,

395
00:38:44,800 --> 00:38:49,680
to now where they can do anything in this  class of Math Olympiad-type problems.

396
00:38:50,400 --> 00:38:52,800
You initially start with them being able  to generalize among addition problems.

397
00:38:54,160 --> 00:39:02,160
Then they can generalize among problems which  require use of different kinds of mathematical

398
00:39:02,160 --> 00:39:08,000
techniques and theorems and conceptual categories,  which is what the Math Olympiad requires.

399
00:39:08,640 --> 00:39:12,800
It sounds like you don't think of being  able to solve any problem within that

400
00:39:12,800 --> 00:39:18,240
category as an example of generalization. Let me know if I'm misunderstanding that.

401
00:39:18,960 --> 00:39:23,600
Large language models are so complex. We don't really know what

402
00:39:23,600 --> 00:39:30,080
information they have had prior. We have to guess because they've been fed so much.

403
00:39:30,080 --> 00:39:34,640
This is one reason why they're  not a good way to do science.

404
00:39:34,640 --> 00:39:39,840
It's just so uncontrolled, so unknown. But if you come up with an entirely new…

405
00:39:39,840 --> 00:39:46,880
They're getting a bunch of things right, perhaps.  The question is why. Well maybe that they don't

406
00:39:46,880 --> 00:39:51,120
need to generalize to get them right, because  the only way to get some of them right is to

407
00:39:51,120 --> 00:39:58,160
form something which gets all of them right. If there's only one answer and you find it,

408
00:39:58,800 --> 00:40:02,080
that's not called generalization. It's just it's the only way to solve it,

409
00:40:02,080 --> 00:40:06,480
and so they find the only way to solve it. But generalization is when it could be this way,

410
00:40:06,480 --> 00:40:08,880
it could be that way, and they do it the good way.

411
00:40:08,880 --> 00:40:15,600
My understanding is that this is working more  and more, better and better, with coding agents.

412
00:40:15,600 --> 00:40:21,040
With engineers, obviously if you're trying  to program a library, there are many

413
00:40:21,040 --> 00:40:25,040
different ways you could achieve the end spec. An initial frustration with these models has

414
00:40:25,040 --> 00:40:31,200
been that they'll do it in a way that's sloppy. Over time they're getting better and better at

415
00:40:31,200 --> 00:40:37,920
coming up with the design architecture and the  abstractions that developers find more satisfying.

416
00:40:37,920 --> 00:40:41,760
It seems like an example of  what you're talking about.

417
00:40:41,760 --> 00:40:46,480
There's nothing in them which  will cause it to generalize well.

418
00:40:46,480 --> 00:40:52,160
Gradient descent will cause them to find  a solution to the problems they've seen.

419
00:40:52,160 --> 00:40:55,200
If there's only one way to  solve them, they'll do that.

420
00:40:55,200 --> 00:40:59,040
But if there are many ways to solve it, some which  generalize well, some which generalize poorly,

421
00:40:59,040 --> 00:41:03,920
there's nothing in the algorithms that  will cause them to generalize well.

422
00:41:03,920 --> 00:41:08,560
But people, of course, are evolved and if  it's not working out they fiddle with

423
00:41:08,560 --> 00:41:14,800
it until they find a way, perhaps until  they find a way which generalizes well.

424
00:42:17,280 --> 00:42:25,260
I want to zoom out and ask about being in the  field of AI for longer than almost anybody who

425
00:42:25,260 --> 00:42:29,840
is commentating on it, or working in it now. I'm curious about what the

426
00:42:29,840 --> 00:42:34,400
biggest surprises have been. How much new stuff do you feel like is coming out?

427
00:42:34,400 --> 00:42:37,920
Or does it feel like people are  just playing with old ideas?

428
00:42:39,360 --> 00:42:43,200
Zooming out, you got into this even  before deep learning was popular.

429
00:42:43,200 --> 00:42:49,440
So how do you see the trajectory of this field  over time and how new ideas have come about and

430
00:42:49,440 --> 00:42:57,120
everything? What's been surprising? I thought a little bit about this.

431
00:42:57,120 --> 00:43:03,600
There are a handful of things. First, the large language models are surprising.

432
00:43:03,600 --> 00:43:12,640
It's surprising how effective artificial  neural networks are at language tasks.

433
00:43:12,640 --> 00:43:19,600
That was a surprise, it wasn't expected. Language  seemed different. So that's impressive. There's a

434
00:43:19,600 --> 00:43:28,640
long-standing controversy in AI about simple  basic principle methods, the general-purpose

435
00:43:28,640 --> 00:43:38,720
methods like search and learning, compared to  human-enabled systems like symbolic methods.

436
00:43:41,040 --> 00:43:44,240
In the old days, it was interesting because  things like search and learning were called

437
00:43:44,240 --> 00:43:48,320
weak methods because they're just using  general principles, they're not using

438
00:43:48,320 --> 00:43:56,880
the power that comes from imbuing a system with  human knowledge. Those were called strong. I think

439
00:43:56,880 --> 00:44:06,800
the weak methods have just totally won. That's the biggest question from the

440
00:44:06,800 --> 00:44:13,120
old days of AI, what would happen. Learning and search have just won the day.

441
00:44:13,120 --> 00:44:18,320
There's a sense in which that was not surprising  to me because I was always hoping or rooting

442
00:44:18,320 --> 00:44:23,600
for the simple basic principles. Even with the large language models,

443
00:44:23,600 --> 00:44:28,800
it's surprising how well it worked,  but it was all good and gratifying.

444
00:44:30,960 --> 00:44:37,600
AlphaGo was surprising, how well that was  able to work, AlphaZero in particular.

445
00:44:40,080 --> 00:44:44,960
But it's all very gratifying because again,  simple basic principles are winning the day.

446
00:44:46,960 --> 00:44:54,160
Whenever the public conception has been  changed because some new application was

447
00:44:54,160 --> 00:44:59,600
developed— for example, when AlphaZero became  this viral sensation—to you as somebody who

448
00:44:59,600 --> 00:45:03,680
has literally came up with many of the  techniques that were used, did it feel

449
00:45:03,680 --> 00:45:08,400
to you like new breakthroughs were made? Or did it feel like, "Oh, we've had these

450
00:45:08,400 --> 00:45:14,000
techniques since the '90s and people are  simply combining them and applying them now"?

451
00:45:14,000 --> 00:45:18,320
The whole AlphaGo thing had a  precursor, which is TD-Gammon.

452
00:45:18,320 --> 00:45:28,240
Gerry Tesauro did reinforcement learning, temporal  difference learning methods, to play backgammon.

453
00:45:28,800 --> 00:45:33,680
It beat the world's best players  and it worked really well.

454
00:45:33,680 --> 00:45:38,720
In some sense, AlphaGo was merely  a scaling up of that process.

455
00:45:38,720 --> 00:45:43,280
But it was quite a bit of scaling up and  there was also an additional innovation

456
00:45:43,280 --> 00:45:49,280
in how the search was done. But it made  sense. It wasn't surprising in that sense.

457
00:45:49,280 --> 00:45:56,960
AlphaGo actually didn't use TD learning. It waited to see the final outcomes. But

458
00:45:56,960 --> 00:46:03,760
AlphaZero used TD. AlphaZero was applied to  all the other games and it did extremely well.

459
00:46:04,640 --> 00:46:09,280
I've always been very impressed by the  way AlphaZero plays chess because I'm a

460
00:46:09,280 --> 00:46:15,200
chess player and it just sacrifices  material for positional advantages.

461
00:46:15,200 --> 00:46:21,520
It's just content and patient to sacrifice  that material for a long period of time.

462
00:46:22,880 --> 00:46:30,240
That was surprising that it worked so well, but  also gratifying and it fit into my worldview.

463
00:46:31,600 --> 00:46:36,560
This has led me where I am. I'm in some sense a contrarian or

464
00:46:36,560 --> 00:46:43,600
someone thinking differently than the field is. I'm personally just content being out of sync

465
00:46:43,600 --> 00:46:47,600
with my field for a long period  of time, perhaps decades, because

466
00:46:47,600 --> 00:46:56,720
occasionally I have been proved right in the past. The other thing I do—to help me not feel I'm out

467
00:46:56,720 --> 00:47:04,560
of sync and thinking in a strange way—is to look  not at my local environment or my local field,

468
00:47:04,560 --> 00:47:10,480
but to look back in time and into history and to  see what people have thought classically about

469
00:47:12,160 --> 00:47:15,760
the mind in many different fields. I don't feel I'm out of sync with

470
00:47:15,760 --> 00:47:18,800
the larger traditions. I really view myself as

471
00:47:18,800 --> 00:47:26,000
a classicist rather than as a contrarian. I go to what the larger community of thinkers

472
00:47:26,000 --> 00:47:30,800
about the mind have always thought. Some sort of left-field questions

473
00:47:30,800 --> 00:47:35,680
for you if you'll tolerate them. The way I read the bitter lesson is

474
00:47:35,680 --> 00:47:42,960
that it's not necessarily saying that human  artisanal researcher tuning doesn't work,

475
00:47:42,960 --> 00:47:49,520
but that it obviously scales much worse than  compute, which is growing exponentially.

476
00:47:49,520 --> 00:47:52,160
So you want techniques which leverage the latter. Yep.

477
00:47:52,880 --> 00:47:59,520
Once we have AGI, we'll have researchers  which scale linearly with compute.

478
00:47:59,520 --> 00:48:02,880
We'll have this avalanche of  millions of AI researchers.

479
00:48:02,880 --> 00:48:09,360
Their stock will be growing as fast as compute. So maybe this will mean that it is rational

480
00:48:09,360 --> 00:48:13,280
or it will make sense to have  them doing good old-fashioned

481
00:48:13,280 --> 00:48:21,280
AI and doing these artisanal solutions. As a vision of what happens after AGI in

482
00:48:21,280 --> 00:48:25,680
terms of how AI research will evolve, I wonder  if that's still compatible with a bitter lesson.

483
00:48:25,680 --> 00:48:30,880
How did we get to this AGI? You want to presume that it's been done.

484
00:48:30,880 --> 00:48:34,240
Suppose it started with general  methods, but now we've got the AGI.

485
00:48:34,240 --> 00:48:38,640
And now we want to go… Then we're done.

486
00:48:38,640 --> 00:48:44,000
Interesting. You don't think  that there's anything above AGI?

487
00:48:44,000 --> 00:48:48,320
But you're using it to get AGI again. Well, I'm using it to get superhuman levels

488
00:48:48,320 --> 00:48:54,480
of intelligence or competence at different tasks. These AGIs, if they're not superhuman already,

489
00:48:54,480 --> 00:49:00,480
then the knowledge that they might  impart would be not superhuman.

490
00:49:00,480 --> 00:49:05,840
I guess there are different gradations. I'm not sure your idea makes sense because

491
00:49:05,840 --> 00:49:12,400
it seems to presume the existence of AGI  and that we've already worked that out.

492
00:49:12,400 --> 00:49:18,960
Maybe one way to motivate this is, AlphaGo was  superhuman. It beat any Go player. AlphaZero

493
00:49:18,960 --> 00:49:22,880
would beat AlphaGo every single time. So there are ways to get more

494
00:49:22,880 --> 00:49:27,120
superhuman than even superhuman. It was also a different architecture.

495
00:49:27,120 --> 00:49:33,280
So it seems possible to me that the agent that's  able to generally learn across all domains,

496
00:49:33,280 --> 00:49:38,560
there would be ways to give it better architecture  for learning, just the same way that AlphaZero was

497
00:49:38,560 --> 00:49:41,760
an improvement upon AlphaGo and MuZero  was an improvement upon AlphaZero.

498
00:49:41,760 --> 00:49:48,160
And the way AlphaZero was an improvement was that  it did not use human knowledge but just went from

499
00:49:48,160 --> 00:49:49,920
experience. Right.

500
00:49:49,920 --> 00:49:57,360
So why do you say, "Bring in other  agents' expertise to teach it",

501
00:49:57,360 --> 00:50:04,400
when it's worked so well from experience  and not by help from another agent?

502
00:50:04,400 --> 00:50:09,360
I agree that in that particular case that  it was moving to more general methods.

503
00:50:10,000 --> 00:50:12,960
I meant to use that particular example  to illustrate that it's possible to go

504
00:50:12,960 --> 00:50:19,120
superhuman to superhuman++, to superhuman+++. I'm curious if you think those gradations will

505
00:50:19,120 --> 00:50:22,560
continue to happen by just  making the method simpler.

506
00:50:22,560 --> 00:50:27,680
Or, because we'll have the capability of these  millions of minds who can then add complexity

507
00:50:27,680 --> 00:50:34,080
as needed, will that continue to be a false path,  even when you have billions of AI researchers or

508
00:50:34,080 --> 00:50:37,680
trillions of AI researchers? It’s more interesting

509
00:50:37,680 --> 00:50:44,000
just to think about that case. When you have many AIs, will they help each

510
00:50:44,000 --> 00:50:50,320
other the way cultural evolution works in people? Maybe we should talk about that.

511
00:50:50,880 --> 00:50:55,360
The bitter lesson, who cares about that? That's an empirical observation about a particular

512
00:50:55,360 --> 00:51:01,120
period in history. 70 years in history, it doesn't  necessarily have to apply to the next 70 years.

513
00:51:01,120 --> 00:51:04,800
An interesting question is, you're an  AI, you get some more computer power.

514
00:51:04,800 --> 00:51:08,480
Should you use it to make yourself  more computationally capable?

515
00:51:08,480 --> 00:51:13,040
Or should you use it to spawn off a copy of  yourself to go learn something interesting

516
00:51:13,040 --> 00:51:17,040
on the other side of the planet or on some  other topic and then report back to you?

517
00:51:18,240 --> 00:51:24,320
I think that's a really interesting  question that will only arise in

518
00:51:24,320 --> 00:51:28,080
the age of digital intelligences. I'm not sure what the answer is.

519
00:51:29,680 --> 00:51:35,920
More questions, will it be possible to really  spawn it off, send it out, learn something new,

520
00:51:35,920 --> 00:51:40,960
something perhaps very new, and then will it  be able to be reincorporated into the original?

521
00:51:40,960 --> 00:51:47,200
Or will it have changed so much  that it can't really be done?

522
00:51:47,200 --> 00:51:53,440
Is that possible or is that not? You could carry this to its limit as I saw

523
00:51:53,440 --> 00:51:58,800
one of your videos the other night. It suggests  that it could. You spawn off many, many copies,

524
00:51:58,800 --> 00:52:04,400
do different things, highly decentralized,  but report back to the central master.

525
00:52:05,520 --> 00:52:12,640
This will be such a powerful thing. This is my attempt to add something to this view.

526
00:52:14,960 --> 00:52:21,120
A big issue will become corruption. If you really could just get information

527
00:52:21,120 --> 00:52:26,160
from anywhere and bring it into your central  mind, you could become more and more powerful.

528
00:52:27,680 --> 00:52:31,360
It's all digital and they all speak  some internal digital language.

529
00:52:31,360 --> 00:52:37,440
Maybe it'll be easy and possible. But it will not be as easy as you're

530
00:52:37,440 --> 00:52:43,120
imagining because you can lose your mind this way. If you pull in something from the outside

531
00:52:43,120 --> 00:52:48,720
and build it into your inner thinking, it  could take over you, it could change you,

532
00:52:48,720 --> 00:52:54,800
it could be your destruction rather  than your increment in knowledge.

533
00:52:55,760 --> 00:53:00,000
I think this will become a big concern,  particularly when you're like, "Oh,

534
00:53:00,000 --> 00:53:04,960
he's figured out all about how to play  some new game or he's studied Indonesia,

535
00:53:04,960 --> 00:53:12,240
and you want to incorporate that into your mind." You could think, "Oh, just read it all in,

536
00:53:12,240 --> 00:53:14,880
and that'll be fine." But no, you've just read a whole

537
00:53:14,880 --> 00:53:23,120
bunch of bits into your mind, and they could have  viruses in them, they could have hidden goals,

538
00:53:23,120 --> 00:53:27,760
they can warp you and change you. This will become a big thing.

539
00:53:27,760 --> 00:53:34,400
How do you have cybersecurity in the age  of digital spawning and re-reforming again?

540
00:54:35,520 --> 00:54:38,640
I guess this brings us to  the topic of AI succession.

541
00:54:39,360 --> 00:54:42,320
You have a perspective that's quite  different from a lot of people that

542
00:54:42,320 --> 00:54:47,200
I've interviewed and a lot of people generally. I also think it's a very interesting perspective.

543
00:54:47,200 --> 00:54:55,280
I want to hear about it. I do think succession to digital

544
00:54:55,280 --> 00:55:05,280
intelligence or augmented humans is inevitable.  I have a four-part argument. Step one is,

545
00:55:05,280 --> 00:55:12,560
there's no government or organization  that gives humanity a unified point of

546
00:55:12,560 --> 00:55:18,000
view that dominates and that can arrange... There's no consensus about how the world

547
00:55:18,000 --> 00:55:20,960
should be run. Number two,

548
00:55:21,600 --> 00:55:26,240
we will figure out how intelligence works. The researchers will figure it out eventually.

549
00:55:26,240 --> 00:55:29,760
Number three, we won't stop just  with human-level intelligence. We

550
00:55:29,760 --> 00:55:39,200
will reach superintelligence. Number four, it's  inevitable over time that the most intelligent

551
00:55:39,200 --> 00:55:50,240
things around would gain resources and power. Put all that together and it's sort of inevitable.

552
00:55:50,240 --> 00:55:57,840
You're going to have succession to AI  or to AI-enabled, augmented humans.

553
00:55:59,360 --> 00:56:07,200
Those four things seem clear and sure to happen. But within that set of possibilities,

554
00:56:07,200 --> 00:56:11,840
there could be good outcomes as well  as less good outcomes, bad outcomes.

555
00:56:14,560 --> 00:56:20,560
I'm just trying to be realistic about where  we are and ask how we should feel about it.

556
00:56:21,920 --> 00:56:25,840
I agree with all four of those  arguments and the implication.

557
00:56:25,840 --> 00:56:34,000
I also agree that succession contains  a wide variety of possible futures.

558
00:56:34,000 --> 00:56:37,760
Curious to get more thoughts on that. I do encourage people to

559
00:56:37,760 --> 00:56:42,720
think positively about it. First of all, it's something we humans have

560
00:56:42,720 --> 00:56:47,280
always tried to do for thousands of years, try  to understand ourselves, trying to make ourselves

561
00:56:47,280 --> 00:56:58,400
think better, just understanding ourselves. This is a great success for science, humanities.

562
00:56:58,400 --> 00:57:06,000
We're finding out what this essential part of  humanness is, what it means to be intelligent.

563
00:57:06,000 --> 00:57:10,880
Then what I usually say is  that this is all human-centric.

564
00:57:10,880 --> 00:57:17,120
But if we step aside from being a human and  just take the point of view of the universe,

565
00:57:17,120 --> 00:57:24,000
this is I think a major stage in the universe, a  major transition, a transition from replicators.

566
00:57:24,000 --> 00:57:28,400
We humans and animals,  plants, we're all replicators.

567
00:57:28,960 --> 00:57:34,080
That gives us some strengths and some limitations. We're entering the age of design

568
00:57:34,080 --> 00:57:39,600
because our AIs are designed. Our physical objects are designed, our buildings

569
00:57:39,600 --> 00:57:46,720
are designed, our technology is designed. We're designing AIs now, things that can

570
00:57:46,720 --> 00:57:50,560
be intelligent themselves and that  are themselves capable of design.

571
00:57:51,440 --> 00:57:55,360
This is a key step in the  world and in the universe.

572
00:57:57,280 --> 00:57:59,840
It's the transition from the  world in which most of the

573
00:57:59,840 --> 00:58:07,600
interesting things that are, are replicated. Replicated means you can make copies of them,

574
00:58:07,600 --> 00:58:11,600
but you don't really understand them. Right now we can make more intelligent beings,

575
00:58:11,600 --> 00:58:15,600
more children, but we don't really  understand how intelligence works.

576
00:58:15,600 --> 00:58:20,480
Whereas we're reaching now to  having designed intelligence,

577
00:58:20,480 --> 00:58:25,040
intelligence that we do understand how it works. Therefore we can change it in different

578
00:58:25,040 --> 00:58:32,800
ways and at different speeds than otherwise. In our future, they may not be replicated at all.

579
00:58:32,800 --> 00:58:37,920
We may just design AIs, and those  AIs will design other AIs, and

580
00:58:38,640 --> 00:58:43,040
everything will be done by design and  construction rather than by replication.

581
00:58:43,840 --> 00:58:48,160
I mark this as one of the four  great stages of the universe.

582
00:58:48,160 --> 00:58:55,600
First there's dust, it ends with stars. Stars  make planets. The planets can give rise to life.

583
00:58:55,600 --> 00:59:07,200
Now we're giving rise to designed entities. I think we should be proud that we are giving

584
00:59:07,200 --> 00:59:15,760
rise to this great transition in the universe.  It's an interesting thing. Should we consider them

585
00:59:15,760 --> 00:59:20,640
part of humanity or different from humanity? It's  our choice. It's our choice whether we should say,

586
00:59:20,640 --> 00:59:24,720
"Oh, they are our offspring and we should  be proud of them and we should celebrate

587
00:59:24,720 --> 00:59:29,600
their achievements."Or we could say, "Oh no,  they're not us and we should be horrified."

588
00:59:29,600 --> 00:59:33,680
It's interesting that it  feels to me like a choice.

589
00:59:33,680 --> 00:59:38,480
Yet it's such a strongly held thing  that, how could it be a choice?

590
00:59:38,480 --> 00:59:42,880
I like these sort of contradictory  implications of thought.

591
00:59:42,880 --> 00:59:48,000
It is interesting to consider if we are  just designing another generation of humans.

592
00:59:48,000 --> 00:59:51,520
Maybe design is the wrong word. But we know a future generation of humans is going

593
00:59:51,520 --> 00:59:58,320
to come up. Forget about AI. We just know in the  long run, humanity will be more capable and more

594
00:59:58,320 --> 01:00:02,160
numerous, maybe more intelligent. How do we feel about that?

595
01:00:02,160 --> 01:00:07,520
I do think there are potential worlds with future  humans that we would be quite concerned about.

596
01:00:08,080 --> 01:00:13,040
Are you thinking like, maybe we are like the  Neanderthals that give rise to Homo sapiens.

597
01:00:13,040 --> 01:00:17,200
Maybe Homo sapiens will give  rise to a new group of people.

598
01:00:17,200 --> 01:00:20,080
Something like that. I'm basically  taking the example you're giving.

599
01:00:20,800 --> 01:00:26,800
Even if we consider them part of humanity, I don't  think that necessarily means that we should feel

600
01:00:26,800 --> 01:00:28,800
super comfortable. Kinship.

601
01:00:28,800 --> 01:00:33,840
Like Nazis were humans, right? If we thought,  "Oh, the future generation will be Nazis,

602
01:00:33,840 --> 01:00:37,280
I think we'd be quite concerned about  just handing off power to them."

603
01:00:37,280 --> 01:00:44,640
So I agree that this is not super dissimilar  to worrying about more capable future humans,

604
01:00:44,640 --> 01:00:49,600
but I don't think that addresses a lot of  the concerns people might have about this

605
01:00:49,600 --> 01:00:54,160
level of power being attained this fast  with entities we don't fully understand.

606
01:00:54,800 --> 01:00:59,840
I think it's relevant to point  out that for most of humanity,

607
01:01:00,720 --> 01:01:11,120
they don't have much influence on what happens. Most of humanity doesn't influence who can control

608
01:01:11,120 --> 01:01:21,760
the atom bombs or who controls the nation states. Even as a citizen, I often feel that we don't

609
01:01:21,760 --> 01:01:25,760
control the nation states very much.  They're out of control. A lot of it

610
01:01:25,760 --> 01:01:32,480
has to do with just how you feel about change. If you think the current situation is really good,

611
01:01:32,480 --> 01:01:38,160
then you're more likely to be suspicious of  change and averse to change than if you think

612
01:01:40,160 --> 01:01:46,560
it's imperfect. I think it's imperfect.  In fact, I think it's pretty bad. So I’m

613
01:01:47,440 --> 01:01:54,320
open to change. I think humanity has  not had a super good track record.

614
01:01:54,320 --> 01:01:59,280
Maybe it's the best thing that there  has been, but it's far from perfect.

615
01:01:59,280 --> 01:02:06,160
I guess there are different varieties of change. The Industrial Revolution was change,

616
01:02:06,160 --> 01:02:11,680
the Bolshevik Revolution was also change. If you were around in Russia in the 1900s and

617
01:02:11,680 --> 01:02:16,640
you were like, "Look, things aren't going well,  the tsar is kind of messing things up, we need

618
01:02:16,640 --> 01:02:22,080
change", I'd want to know what kind of change  you wanted before signing on the dotted line.

619
01:02:23,040 --> 01:02:27,120
Similarly with AI, where I'd want to  understand, and, to the extent that it's

620
01:02:27,120 --> 01:02:34,240
possible, change the trajectory of AI  such that the change is positive for humans.

621
01:02:35,040 --> 01:02:38,960
We should be concerned about  our future, the future.

622
01:02:39,600 --> 01:02:45,120
We should try to make it good. We should also though recognize

623
01:02:45,120 --> 01:02:51,680
the limit, our limits. I think we want to avoid

624
01:02:51,680 --> 01:02:55,120
the feeling of entitlement, avoid the  feeling of, "Oh, we are here first,

625
01:02:55,120 --> 01:03:01,440
we should always have it in a good way." How should we think about the future?

626
01:03:01,440 --> 01:03:07,520
How much control should a particular  species on a particular planet have over it?

627
01:03:08,240 --> 01:03:12,960
How much control do we have? A counterbalance to our limited control

628
01:03:12,960 --> 01:03:21,440
over the long-term future of humanity should be  how much control do we have over our own lives.

629
01:03:21,440 --> 01:03:28,720
We have our own goals. We have our families.  Those things are much more controllable than

630
01:03:28,720 --> 01:03:37,520
trying to control the whole universe. I think it's appropriate for us to

631
01:03:39,600 --> 01:03:47,440
really work towards our own local goals. It's kind of aggressive for us to say, "Oh, the

632
01:03:47,440 --> 01:03:52,480
future has to evolve this way that I want it to." Because then we'll have arguments where different

633
01:03:52,480 --> 01:03:56,320
people think the global future should  evolve in different ways, and then they

634
01:03:56,320 --> 01:04:03,280
have conflict. We want to avoid that. Maybe a good analogy here would be this.

635
01:04:03,280 --> 01:04:09,600
Suppose you are raising your own children. It might not be appropriate to have extremely

636
01:04:09,600 --> 01:04:14,560
tight goals for their own life, or also have  some sense of like, "I want my children to go out

637
01:04:14,560 --> 01:04:19,084
there in the world and have this specific impact. My son's going to become president and my daughter

638
01:04:19,084 --> 01:04:21,840
is going to become CEO of Intel. Together they're going to have

639
01:04:21,840 --> 01:04:26,640
this effect on the world." But people do have the sense—and

640
01:04:26,640 --> 01:04:32,800
I think this is appropriate—of saying, "I'm  going to give them good robust values such

641
01:04:32,800 --> 01:04:39,920
that if and when they do end up in positions of  power, they do reasonable, prosocial things."

642
01:04:39,920 --> 01:04:44,720
Maybe a similar attitude towards AI makes sense,  not in the sense of we can predict everything that

643
01:04:44,720 --> 01:04:50,320
they will do, or we have this plan about what  the world should look like in a hundred years.

644
01:04:50,320 --> 01:04:58,720
But it's quite important to give them  robust and steerable and prosocial values.

645
01:04:58,720 --> 01:05:02,080
Prosocial values? Maybe that's the wrong word.

646
01:05:02,080 --> 01:05:06,400
Are there universal values  that we can all agree on?

647
01:05:06,400 --> 01:05:12,000
I don't think so, but that doesn't prevent us  from giving our kids a good education, right?

648
01:05:12,000 --> 01:05:14,800
Like we have some sense of wanting  our children to be a certain way.

649
01:05:15,360 --> 01:05:18,320
Maybe prosocial is the wrong word. High integrity is maybe a better word.

650
01:05:18,880 --> 01:05:25,760
If there's a request or if there's a goal that  seems harmful, they will refuse to engage in it.

651
01:05:25,760 --> 01:05:32,240
Or they'll be honest, things like that. We have some sense that we can teach our

652
01:05:32,240 --> 01:05:36,000
children things like this, even if we don't  have some sense of what true morality is,

653
01:05:36,000 --> 01:05:41,600
where everybody doesn't agree on that. Maybe that's a reasonable target for AI as well.

654
01:05:41,600 --> 01:05:47,040
So we're trying to design the  future and the principles by

655
01:05:47,040 --> 01:05:51,520
which it will evolve and come into being. The first thing you're saying is, "Well,

656
01:05:51,520 --> 01:06:00,400
we try to teach our children general principles  which will promote more likely evolutions."

657
01:06:01,840 --> 01:06:04,640
Maybe we should also seek  for things to be voluntary.

658
01:06:04,640 --> 01:06:09,040
If there is change, we want it to be  voluntary rather than imposed on people.

659
01:06:09,040 --> 01:06:19,840
I think that's a very important point. That's  all good. I think this is the big or one of

660
01:06:19,840 --> 01:06:26,960
the really big human enterprises to design society  that's been ongoing for thousands of years again.

661
01:06:28,320 --> 01:06:31,920
The more things change, the  more things they stay the same.

662
01:06:31,920 --> 01:06:36,400
We still have to figure out how to be. The children will still come up with different

663
01:06:36,400 --> 01:06:43,120
values that seem strange to their parents  and their grandparents. Things will evolve.

664
01:06:43,120 --> 01:06:46,320
"The more things change, the more  they stay the same" also seems like

665
01:06:46,320 --> 01:06:49,840
a good capsule into the AI discussion. The AI discussion we were having was

666
01:06:49,840 --> 01:06:56,240
about how techniques, which were invented  even before their application to deep

667
01:06:56,240 --> 01:07:01,760
learning and backpropagation was evident,  are central to the progression of AI today.

668
01:07:01,760 --> 01:07:05,040
Maybe that's a good place  to wrap up the conversation.

669
01:07:05,040 --> 01:07:07,520
Okay. Thank you very much. Awesome. Thank you for coming on.

670
01:07:07,520 --> 01:07:08,320
My pleasure.

