1
00:00:00,240 --> 00:00:05,200
You know what's crazy? That all of this is real. Meaning what?

2
00:00:05,200 --> 00:00:10,320
Don't you think so? All this AI stuff and  all this Bay Area… that it's happening.

3
00:00:11,440 --> 00:00:16,239
Isn't it straight out of science fiction? Another thing that's crazy is how

4
00:00:16,239 --> 00:00:21,520
normal the slow takeoff feels. The idea that we'd be investing 1%

5
00:00:21,520 --> 00:00:26,880
of GDP in AI, I feel like it would have felt like  a bigger deal, whereas right now it just feels...

6
00:00:26,880 --> 00:00:32,640
We get used to things pretty fast, it turns out. But also it's kind of abstract. What does it

7
00:00:32,640 --> 00:00:37,920
mean? It means that you see it in the news,  that such and such company announced such

8
00:00:37,920 --> 00:00:45,840
and such dollar amount. That's all you see.  It's not really felt in any other way so far.

9
00:00:45,840 --> 00:00:48,000
Should we actually begin here? I think  this is an interesting discussion.

10
00:00:48,000 --> 00:00:49,920
Sure. I think your point,

11
00:00:49,920 --> 00:00:55,440
about how from the average person's point of view  nothing is that different, will continue being

12
00:00:55,440 --> 00:00:58,880
true even into the singularity. No, I don't think so.

13
00:00:58,880 --> 00:01:01,920
Okay, interesting. The thing which I was referring

14
00:01:01,920 --> 00:01:10,880
to not feeling different is, okay, such and such  company announced some difficult-to-comprehend

15
00:01:10,880 --> 00:01:15,040
dollar amount of investment. I don't think anyone knows what to do with that.

16
00:01:15,920 --> 00:01:24,160
But I think the impact of AI is going to be felt. AI is going to be diffused through the economy.

17
00:01:24,160 --> 00:01:28,560
There'll be very strong economic forces  for this, and I think the impact is

18
00:01:28,560 --> 00:01:32,640
going to be felt very strongly. When do you expect that impact?

19
00:01:32,640 --> 00:01:38,480
I think the models seem smarter than  their economic impact would imply.

20
00:01:38,480 --> 00:01:44,720
Yeah. This is one of the very confusing  things about the models right now.

21
00:01:44,720 --> 00:01:52,400
How to reconcile the fact that  they are doing so well on evals?

22
00:01:53,040 --> 00:01:57,680
You look at the evals and you go, "Those  are pretty hard evals." They are doing

23
00:01:57,680 --> 00:02:04,080
so well. But the economic impact  seems to be dramatically behind.

24
00:02:07,920 --> 00:02:12,240
It's very difficult to make sense of,  how can the model, on the one hand,

25
00:02:12,240 --> 00:02:18,880
do these amazing things, and then on the other  hand, repeat itself twice in some situation?

26
00:02:20,560 --> 00:02:24,560
An example would be, let's say you  use vibe coding to do something.

27
00:02:24,560 --> 00:02:28,240
You go to some place and then you get a bug. Then you tell the model,

28
00:02:28,240 --> 00:02:32,000
"Can you please fix the bug?" And the model says, "Oh my God,

29
00:02:32,000 --> 00:02:35,760
you're so right. I have a bug. Let me go  fix that." And it introduces a second bug.

30
00:02:36,960 --> 00:02:40,080
Then you tell it, "You have this  new second bug," and it tells you,

31
00:02:40,080 --> 00:02:43,520
"Oh my God, how could I have done it? You're so right again," and brings back

32
00:02:43,520 --> 00:02:52,000
the first bug, and you can alternate between  those. How is that possible? I'm not sure, but it

33
00:02:52,000 --> 00:03:02,080
does suggest that something strange is going on. I  have two possible explanations. The more whimsical

34
00:03:02,080 --> 00:03:07,760
explanation is that maybe RL training makes the  models a little too single-minded and narrowly

35
00:03:07,760 --> 00:03:17,680
focused, a little bit too unaware, even though  it also makes them aware in some other ways.

36
00:03:17,680 --> 00:03:25,040
Because of this, they can't do basic things.  But there is another explanation. Back when

37
00:03:25,040 --> 00:03:31,440
people were doing pre-training, the  question of what data to train on was

38
00:03:31,440 --> 00:03:41,040
answered, because that answer was everything. When you do pre-training, you need all the data.

39
00:03:41,040 --> 00:03:44,960
So you don't have to think if it's  going to be this data or that data.

40
00:03:44,960 --> 00:03:48,560
But when people do RL training,  they do need to think.

41
00:03:48,560 --> 00:03:52,080
They say, "Okay, we want to have this  kind of RL training for this thing

42
00:03:52,080 --> 00:03:58,160
and that kind of RL training for that thing." From what I hear, all the companies have teams

43
00:03:58,160 --> 00:04:02,800
that just produce new RL environments  and just add it to the training mix.

44
00:04:02,800 --> 00:04:06,080
The question is, well, what are those? There are so many degrees of freedom.

45
00:04:06,080 --> 00:04:10,080
There is such a huge variety of  RL environments you could produce.

46
00:04:12,720 --> 00:04:17,360
One thing you could do, and I think this  is something that is done inadvertently,

47
00:04:17,360 --> 00:04:24,880
is that people take inspiration from the evals. You say, "Hey, I would love our model to do

48
00:04:24,880 --> 00:04:28,720
really well when we release it. I want the evals to look great.

49
00:04:28,720 --> 00:04:32,640
What would be RL training  that could help on this task?"

50
00:04:33,840 --> 00:04:39,040
I think that is something that happens, and  it could explain a lot of what's going on.

51
00:04:39,040 --> 00:04:44,720
If you combine this with generalization  of the models actually being inadequate,

52
00:04:44,720 --> 00:04:48,640
that has the potential to explain a lot  of what we are seeing, this disconnect

53
00:04:48,640 --> 00:04:56,320
between eval performance and actual real-world  performance, which is something that we don't

54
00:04:56,320 --> 00:05:03,680
today even understand, what we mean by that. I like this idea that the real reward hacking

55
00:05:03,680 --> 00:05:07,920
is the human researchers who  are too focused on the evals.

56
00:05:09,040 --> 00:05:12,960
I think there are two ways to  understand, or to try to think about,

57
00:05:12,960 --> 00:05:18,880
what you have just pointed out. One is that if it's the case that

58
00:05:18,880 --> 00:05:23,600
simply by becoming superhuman at a coding  competition, a model will not automatically

59
00:05:23,600 --> 00:05:30,080
become more tasteful and exercise better judgment  about how to improve your codebase, well then you

60
00:05:30,080 --> 00:05:35,120
should expand the suite of environments such  that you're not just testing it on having

61
00:05:35,120 --> 00:05:38,560
the best performance in coding competition. It should also be able to make the best kind

62
00:05:38,560 --> 00:05:44,400
of application for X thing or Y thing or Z thing. Another, maybe this is what you're hinting at,

63
00:05:44,400 --> 00:05:50,320
is to say, "Why should it be the case in  the first place that becoming superhuman

64
00:05:50,320 --> 00:05:54,400
at coding competitions doesn't make you a  more tasteful programmer more generally?"

65
00:05:54,400 --> 00:05:59,280
Maybe the thing to do is not to keep  stacking up the amount and diversity

66
00:05:59,280 --> 00:06:05,200
of environments, but to figure out an approach  which lets you learn from one environment and

67
00:06:05,200 --> 00:06:12,560
improve your performance on something else. I have a human analogy which might be helpful.

68
00:06:14,160 --> 00:06:18,160
Let's take the case of competitive programming,  since you mentioned that. Suppose you have two

69
00:06:18,160 --> 00:06:24,240
students. One of them decided they want  to be the best competitive programmer, so

70
00:06:24,240 --> 00:06:31,200
they will practice 10,000 hours for that domain. They will solve all the problems, memorize all the

71
00:06:31,200 --> 00:06:39,920
proof techniques, and be very skilled at quickly  and correctly implementing all the algorithms.

72
00:06:40,640 --> 00:06:46,240
By doing so, they became one of the best. Student number two thought, "Oh,

73
00:06:46,240 --> 00:06:50,400
competitive programming is cool." Maybe they practiced for 100 hours,

74
00:06:50,400 --> 00:06:54,640
much less, and they also did really well. Which one do you think is going to do better

75
00:06:54,640 --> 00:06:56,800
in their career later on? The second.

76
00:06:56,800 --> 00:07:00,560
Right. I think that's basically what's going on. The models are much more like the

77
00:07:00,560 --> 00:07:04,400
first student, but even more. Because then we say, the model should

78
00:07:04,400 --> 00:07:10,080
be good at competitive programming so let's get  every single competitive programming problem ever.

79
00:07:10,080 --> 00:07:13,120
And then let's do some data augmentation  so we have even more competitive

80
00:07:13,120 --> 00:07:18,480
programming problems, and we train on that. Now you've got this great competitive programmer.

81
00:07:18,480 --> 00:07:27,120
With this analogy, I think it's more intuitive. Yeah, okay, if it's so well trained, all the

82
00:07:27,120 --> 00:07:32,480
different algorithms and all the different  proof techniques are right at its fingertips.

83
00:07:32,480 --> 00:07:36,320
And it's more intuitive that with this  level of preparation, it would not

84
00:07:36,320 --> 00:07:42,160
necessarily generalize to other things. But then what is the analogy for what

85
00:07:42,160 --> 00:07:48,160
the second student is doing before  they do the 100 hours of fine-tuning?

86
00:07:48,160 --> 00:07:56,480
I think they have "it." The "it"  factor. When I was an undergrad,

87
00:07:56,480 --> 00:08:01,280
I remember there was a student like this  that studied with me, so I know it exists.

88
00:08:01,840 --> 00:08:06,160
I think it's interesting to distinguish  "it" from whatever pre-training does.

89
00:08:06,160 --> 00:08:10,240
One way to understand what you just said  about not having to choose the data in

90
00:08:10,240 --> 00:08:15,040
pre-training is to say it's actually not  dissimilar to the 10,000 hours of practice.

91
00:08:15,040 --> 00:08:20,320
It's just that you get that 10,000 hours  of practice for free because it's already

92
00:08:20,320 --> 00:08:25,120
somewhere in the pre-training distribution. But maybe you're suggesting there's actually

93
00:08:25,120 --> 00:08:28,800
not that much generalization from pre-training. There's just so much data in pre-training, but

94
00:08:28,800 --> 00:08:33,360
it's not necessarily generalizing better than RL. The main strength of pre-training is

95
00:08:33,360 --> 00:08:40,159
that: A, there is so much of it, and B,  you don't have to think hard about what

96
00:08:40,159 --> 00:08:45,680
data to put into pre-training. It's very natural data, and it

97
00:08:45,680 --> 00:08:54,800
does include in it a lot of what people do:  people's thoughts and a lot of the features.

98
00:08:54,800 --> 00:09:01,120
It's like the whole world as projected by  people onto text, and pre-training tries

99
00:09:01,120 --> 00:09:08,960
to capture that using a huge amount of data. Pre-training is very difficult to reason about

100
00:09:08,960 --> 00:09:17,760
because it's so hard to understand the manner  in which the model relies on pre-training data.

101
00:09:17,760 --> 00:09:23,600
Whenever the model makes a mistake, could it be  because something by chance is not as supported

102
00:09:23,600 --> 00:09:30,720
by the pre-training data? "Support by  pre-training" is maybe a loose term.

103
00:09:30,720 --> 00:09:33,680
I don't know if I can add  anything more useful on this.

104
00:09:36,000 --> 00:09:38,400
I don't think there is a  human analog to pre-training.

105
00:09:39,440 --> 00:09:43,600
Here are analogies that people have proposed  for what the human analogy to pre-training is.

106
00:09:43,600 --> 00:09:47,920
I'm curious to get your thoughts  on why they're potentially wrong.

107
00:09:47,920 --> 00:09:54,160
One is to think about the first 18, or 15,  or 13 years of a person's life when they

108
00:09:54,160 --> 00:10:00,320
aren't necessarily economically productive,  but they are doing something that is making

109
00:10:00,320 --> 00:10:07,200
them understand the world better and so forth. The other is to think about evolution as doing

110
00:10:07,200 --> 00:10:14,480
some kind of search for 3 billion years, which  then results in a human lifetime instance.

111
00:10:14,480 --> 00:10:17,360
I'm curious if you think either of  these are analogous to pre-training.

112
00:10:18,000 --> 00:10:22,800
How would you think about what lifetime  human learning is like, if not pre-training?

113
00:10:22,800 --> 00:10:28,800
I think there are some similarities between both  of these and pre-training, and pre-training tries

114
00:10:28,800 --> 00:10:32,000
to play the role of both of these. But I think there are some

115
00:10:32,000 --> 00:10:38,240
big differences as well. The amount of pre-training data is very,

116
00:10:38,240 --> 00:10:41,520
very staggering. Yes.

117
00:10:41,520 --> 00:10:47,600
Somehow a human being, after even 15 years  with a tiny fraction of the pre-training

118
00:10:47,600 --> 00:10:50,800
data, they know much less. But whatever they do know,

119
00:10:50,800 --> 00:10:57,120
they know much more deeply somehow. Already at that age, you would not

120
00:10:57,120 --> 00:11:02,240
make mistakes that our AIs make. There is another  thing. You might say, could it be something like

121
00:11:02,240 --> 00:11:07,600
evolution? The answer is maybe. But in this case,  I think evolution might actually have an edge.

122
00:11:08,880 --> 00:11:19,680
I remember reading about this case. One way in which neuroscientists can

123
00:11:19,680 --> 00:11:25,920
learn about the brain is by studying people with  brain damage to different parts of the brain.

124
00:11:26,960 --> 00:11:30,640
Some people have the most strange symptoms  you could imagine. It's actually really,

125
00:11:30,640 --> 00:11:35,680
really interesting. One case that  comes to mind that's relevant.

126
00:11:35,680 --> 00:11:44,000
I read about this person who had some kind  of brain damage, a stroke or an accident,

127
00:11:44,000 --> 00:11:51,520
that took out his emotional processing. So he stopped feeling any emotion.

128
00:11:54,800 --> 00:11:58,240
He still remained very articulate  and he could solve little puzzles,

129
00:11:58,240 --> 00:12:03,520
and on tests he seemed to be just fine.  But he felt no emotion. He didn't feel sad,

130
00:12:03,520 --> 00:12:10,000
he didn't feel anger, he didn't feel animated. He became somehow extremely bad at making any

131
00:12:10,000 --> 00:12:12,240
decisions at all. It would take him

132
00:12:12,240 --> 00:12:17,120
hours to decide on which socks to wear. He would make very bad financial decisions.

133
00:12:23,120 --> 00:12:34,240
What does it say about the role of our built-in  emotions in making us a viable agent, essentially?

134
00:12:34,240 --> 00:12:41,680
To connect to your question about pre-training,  maybe if you are good enough at getting everything

135
00:12:41,680 --> 00:12:46,480
out of pre-training, you could get that as well. But that's the kind of thing which seems...

136
00:12:50,960 --> 00:12:56,160
Well, it may or may not be possible  to get that from pre-training.

137
00:12:56,160 --> 00:13:04,800
What is "that"? Clearly not just directly  emotion. It seems like some almost value

138
00:13:04,800 --> 00:13:12,080
function-like thing which is telling you what  the end reward for any decision should be.

139
00:13:12,080 --> 00:13:15,200
You think that doesn't sort of  implicitly come from pre-training?

140
00:13:15,200 --> 00:13:19,440
I think it could. I'm just  saying it's not 100% obvious.

141
00:13:20,400 --> 00:13:26,320
But what is that? How do you think about emotions? What is the ML analogy for emotions?

142
00:13:26,320 --> 00:13:31,440
It should be some kind of a value function thing. But I don’t think there is a great ML analogy

143
00:13:31,440 --> 00:13:36,480
because right now, value functions don't play  a very prominent role in the things people do.

144
00:13:36,480 --> 00:13:40,000
It might be worth defining for the audience what  a value function is, if you want to do that.

145
00:13:40,560 --> 00:13:50,880
Certainly, I'll be very happy to do that. When people do reinforcement learning,

146
00:13:50,880 --> 00:13:56,560
the way reinforcement learning is done  right now, how do people train those agents?

147
00:13:56,560 --> 00:14:00,160
You have your neural net and you  give it a problem, and then you

148
00:14:00,160 --> 00:14:03,440
tell the model, "Go solve it." The model takes maybe thousands,

149
00:14:03,440 --> 00:14:09,040
hundreds of thousands of actions or thoughts or  something, and then it produces a solution. The

150
00:14:09,040 --> 00:14:14,960
solution is graded. And then the score  is used to provide a training signal

151
00:14:14,960 --> 00:14:23,200
for every single action in your trajectory. That means that if you are doing something

152
00:14:23,200 --> 00:14:29,120
that goes for a long time—if you're training  a task that takes a long time to solve—it

153
00:14:29,120 --> 00:14:33,440
will do no learning at all until you  come up with the proposed solution.

154
00:14:33,440 --> 00:14:39,760
That's how reinforcement learning is done naively. That's how o1, R1 ostensibly are done.

155
00:14:40,800 --> 00:14:48,240
The value function says something like,  "Maybe I could sometimes, not always,

156
00:14:48,240 --> 00:14:52,400
tell you if you are doing well or badly." The notion of a value function is more

157
00:14:52,400 --> 00:14:56,960
useful in some domains than others. For example, when you play chess and

158
00:14:56,960 --> 00:15:01,440
you lose a piece, I messed up. You don't need to play the whole

159
00:15:01,440 --> 00:15:08,400
game to know that what I just did was bad, and  therefore whatever preceded it was also bad.

160
00:15:08,960 --> 00:15:14,800
The value function lets you short-circuit  the wait until the very end.

161
00:15:19,040 --> 00:15:23,040
Let's suppose that you are doing some kind  of a math thing or a programming thing,

162
00:15:23,040 --> 00:15:26,800
and you're trying to explore a  particular solution or direction.

163
00:15:26,800 --> 00:15:34,320
After, let's say, a thousand steps of thinking,  you concluded that this direction is unpromising.

164
00:15:34,320 --> 00:15:39,760
As soon as you conclude this, you  could already get a reward signal

165
00:15:39,760 --> 00:15:43,520
a thousand timesteps previously, when  you decided to pursue down this path.

166
00:15:43,520 --> 00:15:49,600
You say, "Next time I shouldn't pursue this  path in a similar situation," long before you

167
00:15:49,600 --> 00:15:56,480
actually came up with the proposed solution. This was in the DeepSeek R1 paper— that the

168
00:15:56,480 --> 00:16:02,640
space of trajectories is so wide that  maybe it's hard to learn a mapping

169
00:16:02,640 --> 00:16:08,800
from an intermediate trajectory and value. And also given that, in coding for example

170
00:16:08,800 --> 00:16:12,720
you'll have the wrong idea, then you'll  go back, then you'll change something.

171
00:16:12,720 --> 00:16:15,840
This sounds like such a lack  of faith in deep learning.

172
00:16:16,800 --> 00:16:23,520
Sure it might be difficult, but  nothing deep learning can't do.

173
00:16:23,520 --> 00:16:32,640
My expectation is that a value function should  be useful, and I fully expect that they will

174
00:16:32,640 --> 00:16:37,040
be used in the future, if not already. What I was alluding to with the person

175
00:16:37,040 --> 00:16:47,680
whose emotional center got damaged, it’s more  that maybe what it suggests is that the value

176
00:16:47,680 --> 00:16:55,360
function of humans is modulated by emotions in  some important way that's hardcoded by evolution.

177
00:16:55,360 --> 00:17:00,400
And maybe that is important for  people to be effective in the world.

178
00:17:00,400 --> 00:17:02,640
That's the thing I was planning on asking you.

179
00:17:02,640 --> 00:17:06,160
There's something really interesting about  emotions of the value function, which is that

180
00:17:06,160 --> 00:17:16,160
it's impressive that they have this much utility  while still being rather simple to understand.

181
00:17:16,160 --> 00:17:25,599
I have two responses. I do agree that compared to  the kind of things that we learn and the things

182
00:17:25,599 --> 00:17:30,240
we are talking about, the kind of AI we are  talking about, emotions are relatively simple.

183
00:17:31,120 --> 00:17:35,680
They might even be so simple that maybe you  could map them out in a human-understandable way.

184
00:17:35,680 --> 00:17:40,960
I think it would be cool to do. In terms of utility though,

185
00:17:40,960 --> 00:17:49,360
I think there is a thing where there is this  complexity-robustness tradeoff, where complex

186
00:17:49,360 --> 00:17:58,960
things can be very useful, but simple things are  very useful in a very broad range of situations.

187
00:18:00,240 --> 00:18:06,720
One way to interpret what we are seeing is that  we've got these emotions that evolved mostly

188
00:18:06,720 --> 00:18:13,200
from our mammal ancestors and then fine-tuned a  little bit while we were hominids, just a bit.

189
00:18:13,200 --> 00:18:19,760
We do have a decent amount of social emotions  though which mammals may lack. But they're

190
00:18:19,760 --> 00:18:24,720
not very sophisticated. And because they're  not sophisticated, they serve us so well in

191
00:18:24,720 --> 00:18:28,480
this very different world compared to the  one that we've been living in. Actually,

192
00:18:28,480 --> 00:18:32,800
they also make mistakes. For example, our  emotions… Well actually, I don’t know.

193
00:18:32,800 --> 00:18:39,760
Does hunger count as an emotion? It's debatable.  But I think, for example, our intuitive feeling

194
00:18:39,760 --> 00:18:49,120
of hunger is not succeeding in guiding us  correctly in this world with an abundance of food.

195
00:18:50,000 --> 00:18:56,240
People have been talking about scaling  data, scaling parameters, scaling compute.

196
00:18:56,240 --> 00:18:58,000
Is there a more general  way to think about scaling?

197
00:18:58,000 --> 00:19:10,480
What are the other scaling axes? Here's a perspective that I think might be true.

198
00:19:12,160 --> 00:19:16,320
The way ML used to work is that  people would just tinker with

199
00:19:16,320 --> 00:19:28,240
stuff and try to get interesting results. That's what's been going on in the past. Then

200
00:19:28,240 --> 00:19:39,120
the scaling insight arrived. Scaling laws, GPT-3,  and suddenly everyone realized we should scale.

201
00:19:40,400 --> 00:19:47,120
This is an example of how language  affects thought. "Scaling" is just

202
00:19:47,120 --> 00:19:51,120
one word, but it's such a powerful word  because it informs people what to do.

203
00:19:51,120 --> 00:19:57,280
They say, "Let's try to scale things." So you say, what are we scaling?

204
00:19:57,280 --> 00:20:02,240
Pre-training was the thing to scale. It was a particular scaling recipe.

205
00:20:02,240 --> 00:20:08,000
The big breakthrough of pre-training is  the realization that this recipe is good.

206
00:20:08,000 --> 00:20:14,400
You say, "Hey, if you mix some compute  with some data into a neural net of

207
00:20:14,400 --> 00:20:19,600
a certain size, you will get results. You will know that you'll be better if you

208
00:20:19,600 --> 00:20:26,560
just scale the recipe up." This is also great.  Companies love this because it gives you a very

209
00:20:26,560 --> 00:20:34,800
low-risk way of investing your resources. It's much harder to invest your resources

210
00:20:34,800 --> 00:20:39,760
in research. Compare that. If you research,  you need to be like, "Go forth researchers

211
00:20:39,760 --> 00:20:45,440
and research and come up with something",  versus get more data, get more compute.

212
00:20:45,440 --> 00:20:52,400
You know you'll get something from pre-training. Indeed, it looks like, based on various

213
00:20:54,640 --> 00:21:00,160
things some people say on Twitter, maybe it  appears that Gemini have found a way to get

214
00:21:00,160 --> 00:21:02,560
more out of pre-training. At some point though,

215
00:21:02,560 --> 00:21:06,480
pre-training will run out of data. The data is very clearly finite. What

216
00:21:06,480 --> 00:21:11,280
do you do next? Either you do some kind  of souped-up pre-training, a different

217
00:21:11,280 --> 00:21:15,920
recipe from the one you've done before, or  you're doing RL, or maybe something else.

218
00:21:15,920 --> 00:21:20,320
But now that compute is big, compute  is now very big, in some sense we

219
00:21:20,320 --> 00:21:24,160
are back to the age of research. Maybe here's another way to put it.

220
00:21:24,160 --> 00:21:31,280
Up until 2020, from 2012 to  2020, it was the age of research.

221
00:21:31,280 --> 00:21:35,920
Now, from 2020 to 2025, it was the  age of scaling—maybe plus or minus,

222
00:21:35,920 --> 00:21:39,440
let's add error bars to those years—because  people say, "This is amazing. You've got to

223
00:21:39,440 --> 00:21:45,280
scale more. Keep scaling." The one word:  scaling. But now the scale is so big.

224
00:21:46,320 --> 00:21:53,440
Is the belief really, "Oh, it's so big, but if you  had 100x more, everything would be so different?"

225
00:21:53,440 --> 00:21:58,000
It would be different, for sure. But is the belief that if you just

226
00:21:58,000 --> 00:22:04,560
100x the scale, everything would be transformed?  I don't think that's true. So it's back to the age

227
00:22:04,560 --> 00:22:10,400
of research again, just with big computers. That's a very interesting way to put it.

228
00:22:10,400 --> 00:22:12,560
But let me ask you the  question you just posed then.

229
00:22:12,560 --> 00:22:16,560
What are we scaling, and what  would it mean to have a recipe?

230
00:22:17,440 --> 00:22:23,680
I guess I'm not aware of a very clean  relationship that almost looks like a law

231
00:22:23,680 --> 00:22:27,920
of physics which existed in pre-training. There was a power law between data or

232
00:22:27,920 --> 00:22:33,920
compute or parameters and loss. What is the kind of relationship

233
00:22:33,920 --> 00:22:38,640
we should be seeking, and how should we think  about what this new recipe might look like?

234
00:22:40,240 --> 00:22:48,000
We've already witnessed a transition from one  type of scaling to a different type of scaling,

235
00:22:48,000 --> 00:22:56,800
from pre-training to RL. Now people are scaling  RL. Now based on what people say on Twitter,

236
00:22:56,800 --> 00:23:01,280
they spend more compute on RL than on  pre-training at this point, because RL

237
00:23:01,280 --> 00:23:07,840
can actually consume quite a bit of compute. You do very long rollouts, so it takes a lot

238
00:23:07,840 --> 00:23:11,360
of compute to produce those rollouts. Then you get a relatively small amount

239
00:23:11,360 --> 00:23:15,760
of learning per rollout, so you  really can spend a lot of compute.

240
00:23:21,680 --> 00:23:27,280
I wouldn't even call it scaling. I would say, "Hey, what are you doing?

241
00:23:27,280 --> 00:23:31,840
Is the thing you are doing the most  productive thing you could be doing?

242
00:23:31,840 --> 00:23:36,000
Can you find a more productive  way of using your compute?"

243
00:23:36,000 --> 00:23:39,520
We've discussed the value  function business earlier.

244
00:23:39,520 --> 00:23:43,920
Maybe once people get good at value  functions, they will be using their

245
00:23:44,720 --> 00:23:50,320
resources more productively. If you find a whole other way

246
00:23:50,320 --> 00:23:56,240
of training models, you could say, "Is this  scaling or is it just using your resources?"

247
00:23:56,240 --> 00:23:59,680
I think it becomes a little bit ambiguous. In the sense that, when people were in the

248
00:23:59,680 --> 00:24:03,600
age of research back then, it was,  "Let's try this and this and this.

249
00:24:03,600 --> 00:24:07,680
Let's try that and that and that. Oh, look, something interesting is happening."

250
00:24:07,680 --> 00:24:12,480
I think there will be a return to that. If we're back in the era of research,

251
00:24:12,480 --> 00:24:17,040
stepping back, what is the part of the  recipe that we need to think most about?

252
00:24:17,040 --> 00:24:21,200
When you say value function, people  are already trying the current recipe,

253
00:24:21,200 --> 00:24:24,720
but then having LLM-as-a-Judge and so forth. You could say that's a value function,

254
00:24:24,720 --> 00:24:26,880
but it sounds like you have something  much more fundamental in mind.

255
00:24:29,920 --> 00:24:35,920
Should we even rethink pre-training at all and not  just add more steps to the end of that process?

256
00:24:38,240 --> 00:24:41,600
The discussion about value function,  I think it was interesting.

257
00:24:41,600 --> 00:24:48,320
I want to emphasize that I think the value  function is something that's going to make RL more

258
00:24:48,320 --> 00:24:55,440
efficient, and I think that makes a difference. But I think anything you can do with a value

259
00:24:55,440 --> 00:25:02,080
function, you can do without, just more slowly. The thing which I think is the most fundamental

260
00:25:02,080 --> 00:25:08,560
is that these models somehow just generalize  dramatically worse than people. It's super

261
00:25:08,560 --> 00:25:18,480
obvious. That seems like a very fundamental thing. So this is the crux: generalization. There are two

262
00:25:18,480 --> 00:25:24,160
sub-questions. There's one which is about sample  efficiency: why should it take so much more data

263
00:25:24,160 --> 00:25:29,520
for these models to learn than humans? There's  a second question. Even separate from the amount

264
00:25:29,520 --> 00:25:35,840
of data it takes, why is it so hard to teach  the thing we want to a model than to a human?

265
00:25:37,360 --> 00:25:43,920
For a human, we don't necessarily need a  verifiable reward to be able to… You're probably

266
00:25:43,920 --> 00:25:48,560
mentoring a bunch of researchers right now, and  you're talking with them, you're showing them your

267
00:25:48,560 --> 00:25:52,560
code, and you're showing them how you think. From that, they're picking up your way of

268
00:25:52,560 --> 00:25:56,800
thinking and how they should do research. You don’t have to set a verifiable reward for

269
00:25:56,800 --> 00:25:59,680
them that's like, "Okay, this is the next part of  the curriculum, and now this is the next part of

270
00:25:59,680 --> 00:26:06,080
your curriculum. Oh, this training was unstable."  There's not this schleppy, bespoke process.

271
00:26:06,640 --> 00:26:10,160
Perhaps these two issues are actually  related in some way, but I'd be curious

272
00:26:10,160 --> 00:26:15,520
to explore this second thing, which is more  like continual learning, and this first thing,

273
00:26:15,520 --> 00:26:22,720
which feels just like sample efficiency. You could actually wonder that one possible

274
00:26:22,720 --> 00:26:30,720
explanation for the human sample efficiency  that needs to be considered is evolution.

275
00:26:31,760 --> 00:26:38,880
Evolution has given us a small amount  of the most useful information possible.

276
00:26:38,880 --> 00:26:45,040
For things like vision, hearing, and  locomotion, I think there's a pretty

277
00:26:45,040 --> 00:26:55,120
strong case that evolution has given us a lot. For example, human dexterity far exceeds… I mean

278
00:26:55,120 --> 00:27:00,480
robots can become dexterous too if you subject  them to a huge amount of training in simulation.

279
00:27:00,480 --> 00:27:04,560
But to train a robot in the real world  to quickly pick up a new skill like

280
00:27:04,560 --> 00:27:10,560
a person does seems very out of reach. Here you could say, "Oh yeah, locomotion.

281
00:27:10,560 --> 00:27:14,640
All our ancestors needed  great locomotion, squirrels.

282
00:27:15,920 --> 00:27:19,760
So with locomotion, maybe we've  got some unbelievable prior."

283
00:27:19,760 --> 00:27:24,400
You could make the same case for vision. I believe Yann LeCun made the point that

284
00:27:25,440 --> 00:27:30,800
children learn to drive after 10  hours of practice, which is true.

285
00:27:30,800 --> 00:27:35,280
But our vision is so good. At least for me,

286
00:27:35,280 --> 00:27:41,120
I remember myself being a five-year-old. I was very excited about cars back then.

287
00:27:41,120 --> 00:27:47,200
I'm pretty sure my car recognition was more than  adequate for driving already as a five-year-old.

288
00:27:47,200 --> 00:27:49,040
You don't get to see that  much data as a five-year-old.

289
00:27:49,040 --> 00:27:53,200
You spend most of your time in your parents'  house, so you have very low data diversity.

290
00:27:53,200 --> 00:28:00,400
But you could say maybe that's evolution too. But in language and math and coding, probably not.

291
00:28:00,400 --> 00:28:04,720
It still seems better than models. Obviously, models are better than the average

292
00:28:04,720 --> 00:28:07,760
human at language, math, and coding. But are they better than

293
00:28:07,760 --> 00:28:12,160
the average human at learning? Oh yeah. Oh yeah, absolutely. What I meant

294
00:28:12,160 --> 00:28:18,320
to say is that language, math, and coding—and  especially math and coding—suggests that whatever

295
00:28:18,320 --> 00:28:25,920
it is that makes people good at learning is  probably not so much a complicated prior,

296
00:28:25,920 --> 00:28:30,880
but something more, some fundamental thing. I'm not sure I understood. Why

297
00:28:30,880 --> 00:28:35,120
should that be the case? So consider a skill in which

298
00:28:35,120 --> 00:28:45,120
people exhibit some kind of great reliability. If the skill is one that was very useful to our

299
00:28:45,120 --> 00:28:52,240
ancestors for many millions of years, hundreds  of millions of years, you could argue that maybe

300
00:28:52,240 --> 00:29:00,720
humans are good at it because of evolution,  because we have a prior, an evolutionary prior

301
00:29:00,720 --> 00:29:06,400
that's encoded in some very non-obvious  way that somehow makes us so good at it.

302
00:29:07,200 --> 00:29:16,560
But if people exhibit great ability, reliability,  robustness, and ability to learn in a domain that

303
00:29:16,560 --> 00:29:23,760
really did not exist until recently, then  this is more an indication that people

304
00:29:23,760 --> 00:29:34,800
might have just better machine learning, period. How should we think about what that is? What is

305
00:29:34,800 --> 00:29:41,200
the ML analogy? There are a couple of interesting  things about it. It takes fewer samples. It's

306
00:29:41,200 --> 00:29:47,200
more unsupervised. A child learning to drive a  car… Children are not learning to drive a car.

307
00:29:47,200 --> 00:29:55,600
A teenager learning how to drive a car is not  exactly getting some prebuilt, verifiable reward.

308
00:29:56,400 --> 00:30:01,760
It comes from their interaction with  the machine and with the environment.

309
00:30:02,720 --> 00:30:07,440
It takes much fewer samples. It seems  more unsupervised. It seems more robust?

310
00:30:07,440 --> 00:30:12,400
Much more robust. The robustness  of people is really staggering.

311
00:30:14,160 --> 00:30:18,320
Do you have a unified way of thinking about  why all these things are happening at once?

312
00:30:18,320 --> 00:30:23,920
What is the ML analogy that could  realize something like this?

313
00:30:26,320 --> 00:30:33,680
One of the things that you've been asking about is  how can the teenage driver self-correct and learn

314
00:30:33,680 --> 00:30:41,600
from their experience without an external teacher? The answer is that they have their value function.

315
00:30:41,600 --> 00:30:46,800
They have a general sense which is also,  by the way, extremely robust in people.

316
00:30:50,480 --> 00:30:56,080
Whatever the human value function is,  with a few exceptions around addiction,

317
00:30:56,080 --> 00:31:00,800
it's actually very, very robust. So for something like a teenager

318
00:31:00,800 --> 00:31:07,520
that's learning to drive, they start to drive, and  they already have a sense of how they're driving

319
00:31:07,520 --> 00:31:13,200
immediately, how badly they are, how unconfident.  And then they see, "Okay." And then, of course,

320
00:31:13,200 --> 00:31:17,520
the learning speed of any teenager is so fast. After 10 hours, you're good to go.

321
00:31:17,520 --> 00:31:19,760
It seems like humans have some  solution, but I'm curious about

322
00:31:20,800 --> 00:31:24,960
how they are doing it and why is it so hard? How do we need to reconceptualize the way

323
00:31:24,960 --> 00:31:27,520
we're training models to make  something like this possible?

324
00:31:28,720 --> 00:31:37,200
That is a great question to ask, and it's  a question I have a lot of opinions about.

325
00:31:37,200 --> 00:31:43,200
But unfortunately, we live in a world where  not all machine learning ideas are discussed

326
00:31:43,200 --> 00:31:49,360
freely, and this is one of them. There's probably a way to do it.

327
00:31:49,360 --> 00:31:54,240
I think it can be done. The fact that people are like that,

328
00:31:54,240 --> 00:31:57,840
I think it's a proof that it can be done. There may be another blocker though,

329
00:31:57,840 --> 00:32:07,760
which is that there is a possibility that the  human neurons do more compute than we think.

330
00:32:07,760 --> 00:32:13,760
If that is true, and if that plays an important  role, then things might be more difficult.

331
00:32:13,760 --> 00:32:20,080
But regardless, I do think it points to  the existence of some machine learning

332
00:32:20,080 --> 00:32:25,840
principle that I have opinions on. But unfortunately, circumstances

333
00:32:25,840 --> 00:32:31,680
make it hard to discuss in detail. Nobody listens to this podcast, Ilya.

334
00:32:32,560 --> 00:35:53,360
I'm curious. If you say we are back in an era  of research, you were there from 2012 to 2020.

335
00:35:55,840 --> 00:36:00,640
What is the vibe now going to be if  we go back to the era of research?

336
00:36:00,640 --> 00:36:05,920
For example, even after AlexNet, the  amount of compute that was used to

337
00:36:05,920 --> 00:36:12,240
run experiments kept increasing, and the  size of frontier systems kept increasing.

338
00:36:13,440 --> 00:36:18,640
Do you think now that this era of research will  still require tremendous amounts of compute?

339
00:36:19,760 --> 00:36:24,400
Do you think it will require going back  into the archives and reading old papers?

340
00:36:28,000 --> 00:36:34,960
You were at Google and OpenAI and Stanford, these  places, when there was more of a vibe of research?

341
00:36:34,960 --> 00:36:38,720
What kind of things should we  be expecting in the community?

342
00:36:40,160 --> 00:36:49,600
One consequence of the age of scaling is that  scaling sucked out all the air in the room.

343
00:36:53,120 --> 00:36:59,680
Because scaling sucked out all the air in the  room, everyone started to do the same thing.

344
00:36:59,680 --> 00:37:05,840
We got to the point where we are  in a world where there are more

345
00:37:05,840 --> 00:37:11,440
companies than ideas by quite a bit. Actually on that, there is this Silicon

346
00:37:11,440 --> 00:37:18,480
Valley saying that says that ideas  are cheap, execution is everything.

347
00:37:18,480 --> 00:37:25,280
People say that a lot, and there is truth to that. But then I saw someone say on Twitter

348
00:37:25,280 --> 00:37:30,880
something like, "If ideas are so cheap,  how come no one's having any ideas?"

349
00:37:30,880 --> 00:37:37,840
And I think it's true too. If you think about research progress in terms

350
00:37:37,840 --> 00:37:47,200
of bottlenecks, there are several bottlenecks. One of them is ideas, and one of them is your

351
00:37:47,200 --> 00:37:52,240
ability to bring them to life, which  might be compute but also engineering.

352
00:37:52,240 --> 00:37:56,880
If you go back to the '90s, let's say,  you had people who had pretty good ideas,

353
00:37:56,880 --> 00:38:01,120
and if they had much larger computers, maybe they  could demonstrate that their ideas were viable.

354
00:38:01,120 --> 00:38:05,200
But they could not, so they could only  have a very, very small demonstration

355
00:38:05,200 --> 00:38:10,480
that did not convince anyone. So the  bottleneck was compute. Then in the

356
00:38:10,480 --> 00:38:17,120
age of scaling, compute has increased a lot. Of course, there is a question of how much

357
00:38:17,120 --> 00:38:26,640
compute is needed, but compute is large. Compute is large enough such that it's not

358
00:38:26,640 --> 00:38:33,520
obvious that you need that much more  compute to prove some idea. I'll give

359
00:38:33,520 --> 00:38:40,640
you an analogy. AlexNet was built on two GPUs. That was the total amount of compute used for it.

360
00:38:40,640 --> 00:38:48,720
The transformer was built on 8 to 64 GPUs. No single transformer paper experiment used

361
00:38:48,720 --> 00:38:57,200
more than 64 GPUs of 2017, which would be  like, what, two GPUs of today? The ResNet,

362
00:38:57,200 --> 00:39:08,000
right? You could argue that the o1 reasoning was  not the most compute-heavy thing in the world.

363
00:39:08,000 --> 00:39:17,200
So for research, you definitely need  some amount of compute, but it's far

364
00:39:17,200 --> 00:39:22,160
from obvious that you need the absolutely  largest amount of compute ever for research.

365
00:39:22,160 --> 00:39:28,800
You might argue, and I think it is true, that  if you want to build the absolutely best system

366
00:39:31,360 --> 00:39:35,360
then it helps to have much more compute. Especially if everyone is within the same

367
00:39:35,360 --> 00:39:42,000
paradigm, then compute becomes  one of the big differentiators.

368
00:39:46,400 --> 00:39:48,400
I'm asking you for the history,  because you were actually there.

369
00:39:48,400 --> 00:39:51,680
I'm not sure what actually happened. It sounds like it was possible to develop

370
00:39:51,680 --> 00:39:56,480
these ideas using minimal amounts of compute. But the transformer didn't

371
00:39:56,480 --> 00:39:59,840
immediately become famous. It became the thing everybody started

372
00:39:59,840 --> 00:40:04,320
doing and then started experimenting on top of  and building on top of because it was validated

373
00:40:04,320 --> 00:40:07,360
at higher and higher levels of compute. Correct.

374
00:40:07,360 --> 00:40:13,840
And if you at SSI have 50 different ideas, how  will you know which one is the next transformer

375
00:40:13,840 --> 00:40:21,920
and which one is brittle, without having the  kinds of compute that other frontier labs have?

376
00:40:22,960 --> 00:40:30,320
I can comment on that. The short  comment is that you mentioned SSI.

377
00:40:30,320 --> 00:40:40,640
Specifically for us, the amount of compute  that SSI has for research is really not that

378
00:40:40,640 --> 00:40:45,200
small. I want to explain why. Simple math  can explain why the amount of compute that

379
00:40:45,200 --> 00:40:58,880
we have is comparable for research than one might  think. I'll explain. SSI has raised $3 billion,

380
00:40:58,880 --> 00:41:05,440
which is a lot by any absolute sense. But you could say, "Look at the

381
00:41:05,440 --> 00:41:13,920
other companies raising much more." But a lot of their compute goes for inference.

382
00:41:13,920 --> 00:41:20,160
These big numbers, these big loans, it's  earmarked for inference. That's number one.

383
00:41:20,160 --> 00:41:25,120
Number two, if you want to have a product  on which you do inference, you need to

384
00:41:25,120 --> 00:41:31,120
have a big staff of engineers, salespeople. A lot of the research needs to be dedicated to

385
00:41:31,120 --> 00:41:37,440
producing all kinds of product-related features. So then when you look at what's actually left for

386
00:41:37,440 --> 00:41:45,760
research, the difference becomes a lot smaller. The other thing is, if you are doing something

387
00:41:45,760 --> 00:41:51,040
different, do you really need the  absolute maximal scale to prove it?

388
00:41:51,040 --> 00:41:58,080
I don't think that's true at all. I think that in our case, we have sufficient

389
00:41:58,080 --> 00:42:02,320
compute to prove, to convince ourselves and  anyone else, that what we are doing is correct.

390
00:42:02,880 --> 00:42:08,320
There have been public estimates that companies  like OpenAI spend on the order of $5-6 billion

391
00:42:08,320 --> 00:42:13,680
a year just so far, on experiments. This is separate from the amount of

392
00:42:13,680 --> 00:42:18,720
money they're spending on inference and so forth. So it seems like they're spending more a year

393
00:42:18,720 --> 00:42:22,960
running research experiments than  you guys have in total funding.

394
00:42:22,960 --> 00:42:26,880
I think it's a question of what you do with it. It's a question of what you do with it.

395
00:42:29,840 --> 00:42:35,120
In their case, in the case of others, there  is a lot more demand on the training compute.

396
00:42:35,120 --> 00:42:41,120
There’s a lot more different work streams, there  are different modalities, there is just more

397
00:42:41,120 --> 00:42:46,320
stuff. So it becomes fragmented. How will SSI make money?

398
00:42:48,480 --> 00:42:55,680
My answer to this question is something like this. Right now, we just focus on the research, and then

399
00:42:55,680 --> 00:43:01,360
the answer to that question will reveal itself. I think there will be lots of possible answers.

400
00:43:01,360 --> 00:43:05,040
Is SSI's plan still to straight  shot superintelligence?

401
00:43:05,040 --> 00:43:11,280
Maybe. I think that there is merit to it. I think there's a lot of merit because

402
00:43:11,280 --> 00:43:17,840
it's very nice to not be affected by  the day-to-day market competition.

403
00:43:17,840 --> 00:43:25,200
But I think there are two reasons  that may cause us to change the plan.

404
00:43:25,200 --> 00:43:31,360
One is pragmatic, if timelines turned  out to be long, which they might.

405
00:43:31,360 --> 00:43:38,400
Second, I think there is a lot  of value in the best and most

406
00:43:38,400 --> 00:43:46,480
powerful AI being out there impacting the world. I think this is a meaningfully valuable thing.

407
00:43:46,480 --> 00:43:49,440
So then why is your default plan  to straight shot superintelligence?

408
00:43:49,440 --> 00:43:54,880
Because it sounds like OpenAI, Anthropic, all  these other companies, their explicit thinking

409
00:43:54,880 --> 00:44:01,040
is, "Look, we have weaker and weaker intelligences  that the public can get used to and prepare for."

410
00:44:01,040 --> 00:44:06,400
Why is it potentially better to  build a superintelligence directly?

411
00:44:06,400 --> 00:44:14,000
I'll make the case for and against. The case for is that one of the challenges

412
00:44:14,000 --> 00:44:20,320
that people face when they're in the market is  that they have to participate in the rat race.

413
00:44:20,320 --> 00:44:24,720
The rat race is quite difficult in  that it exposes you to difficult

414
00:44:24,720 --> 00:44:32,000
trade-offs which you need to make. It is nice to say, "We'll insulate ourselves

415
00:44:32,000 --> 00:44:38,160
from all this and just focus on the research and  come out only when we are ready, and not before."

416
00:44:38,160 --> 00:44:43,600
But the counterpoint is valid too,  and those are opposing forces.

417
00:44:43,600 --> 00:44:50,880
The counterpoint is, "Hey, it is useful  for the world to see powerful AI.

418
00:44:50,880 --> 00:44:53,600
It is useful for the world to  see powerful AI because that's

419
00:44:53,600 --> 00:44:56,800
the only way you can communicate it." Well, I guess not even just that you can

420
00:44:56,800 --> 00:44:59,760
communicate the idea— Communicate the AI,

421
00:44:59,760 --> 00:45:04,080
not the idea. Communicate the AI. What do you mean, "communicate the AI"?

422
00:45:04,640 --> 00:45:10,080
Let's suppose you write an essay about AI, and  the essay says, "AI is going to be this, and AI is

423
00:45:10,080 --> 00:45:13,760
going to be that, and it's going to be this." You read it and you say, "Okay,

424
00:45:13,760 --> 00:45:18,320
this is an interesting essay." Now suppose you see an AI doing this,

425
00:45:18,320 --> 00:45:27,920
an AI doing that. It is incomparable. Basically  I think that there is a big benefit from AI

426
00:45:27,920 --> 00:45:35,600
being in the public, and that would be a  reason for us to not be quite straight shot.

427
00:45:36,320 --> 00:45:40,320
I guess it's not even that, but I do  think that is an important part of it.

428
00:45:40,320 --> 00:45:45,360
The other big thing is that I can't think of  another discipline in human engineering and

429
00:45:45,360 --> 00:45:53,280
research where the end artifact was made  safer mostly through just thinking about

430
00:45:53,280 --> 00:45:58,000
how to make it safe, as opposed to,  why airplane crashes per mile are so

431
00:45:58,000 --> 00:46:02,560
much lower today than they were decades ago. Why is it so much harder to find a bug in Linux

432
00:46:02,560 --> 00:46:06,320
than it would have been decades ago? I think it's mostly because these

433
00:46:06,320 --> 00:46:11,040
systems were deployed to the world. You noticed failures, those failures

434
00:46:11,040 --> 00:46:17,280
were corrected and the systems became more robust. I'm not sure why AGI and superhuman intelligence

435
00:46:17,280 --> 00:46:23,600
would be any different, especially given—and I  hope we're going to get to this—it seems like

436
00:46:23,600 --> 00:46:29,760
the harms of superintelligence are not just about  having some malevolent paper clipper out there.

437
00:46:29,760 --> 00:46:34,640
But this is a really powerful thing and we don't  even know how to conceptualize how people interact

438
00:46:34,640 --> 00:46:39,760
with it, what people will do with it. Having gradual access to it seems like a

439
00:46:40,880 --> 00:46:45,440
better way to maybe spread out the impact  of it and to help people prepare for it.

440
00:46:45,440 --> 00:46:52,720
Well I think on this point, even in the straight  shot scenario, you would still do a gradual

441
00:46:52,720 --> 00:47:01,760
release of it, that’s how I would imagine it. Gradualism would be an inherent

442
00:47:01,760 --> 00:47:04,560
component of any plan. It's just a question of what is the first

443
00:47:04,560 --> 00:47:11,680
thing that you get out of the door. That's number  one. Number two, I believe you have advocated

444
00:47:11,680 --> 00:47:17,680
for continual learning more than other people,  and I actually think that this is an important

445
00:47:17,680 --> 00:47:29,600
and correct thing. Here is why. I'll give you  another example of how language affects thinking.

446
00:47:29,600 --> 00:47:35,840
In this case, it will be two words that  have shaped everyone's thinking, I maintain.

447
00:47:37,520 --> 00:47:48,400
First word: AGI. Second word: pre-training.  Let me explain. The term AGI, why does this

448
00:47:48,400 --> 00:47:55,120
term exist? It's a very particular term. Why  does it exist? There's a reason. The reason

449
00:47:55,120 --> 00:48:02,640
that the term AGI exists is, in my opinion, not  so much because it's a very important, essential

450
00:48:02,640 --> 00:48:14,160
descriptor of some end state of intelligence, but  because it is a reaction to a different term that

451
00:48:14,160 --> 00:48:19,360
existed, and the term is narrow AI. If you go back to ancient history

452
00:48:19,360 --> 00:48:25,600
of gameplay and AI, of checkers AI, chess  AI, computer games AI, everyone would say,

453
00:48:25,600 --> 00:48:29,600
look at this narrow intelligence. Sure, the chess AI can beat Kasparov,

454
00:48:29,600 --> 00:48:34,720
but it can't do anything else. It is so narrow, artificial narrow intelligence.

455
00:48:34,720 --> 00:48:41,280
So in response, as a reaction to this,  some people said, this is not good. It

456
00:48:41,280 --> 00:48:50,800
is so narrow. What we need is general AI,  an AI that can just do all the things.

457
00:48:53,040 --> 00:48:59,360
That term just got a lot of traction. The second thing that got a lot of traction

458
00:48:59,360 --> 00:49:03,120
is pre-training, specifically  the recipe of pre-training.

459
00:49:03,120 --> 00:49:12,960
I think the way people do RL now is maybe  undoing the conceptual imprint of pre-training.

460
00:49:12,960 --> 00:49:17,520
But pre-training had this property. You  do more pre-training and the model gets

461
00:49:17,520 --> 00:49:29,760
better at everything, more or less uniformly.  General AI. Pre-training gives AGI. But the

462
00:49:29,760 --> 00:49:36,400
thing that happened with AGI and pre-training  is that in some sense they overshot the target.

463
00:49:38,160 --> 00:49:43,360
If you think about the term "AGI",  especially in the context of pre-training,

464
00:49:43,360 --> 00:49:53,760
you will realize that a human being is not an AGI. Yes, there is definitely a foundation of skills,

465
00:49:53,760 --> 00:50:00,080
but a human being lacks a  huge amount of knowledge.

466
00:50:00,080 --> 00:50:06,720
Instead, we rely on continual learning. So when you think about, "Okay,

467
00:50:06,720 --> 00:50:12,240
so let's suppose that we achieve success and we  produce some kind of safe superintelligence."

468
00:50:12,240 --> 00:50:16,320
The question is, how do you define it? Where on the curve of continual

469
00:50:16,320 --> 00:50:20,640
learning is it going to be? I produce a superintelligent

470
00:50:20,640 --> 00:50:25,200
15-year-old that's very eager to go. They don't know very much at all,

471
00:50:25,200 --> 00:50:29,280
a great student, very eager. You go and be a programmer,

472
00:50:29,280 --> 00:50:34,720
you go and be a doctor, go and learn. So you could imagine that the deployment

473
00:50:34,720 --> 00:50:38,880
itself will involve some kind of  a learning trial-and-error period.

474
00:50:38,880 --> 00:50:43,520
It's a process, as opposed to  you dropping the finished thing.

475
00:50:44,240 --> 00:50:51,120
I see. You're suggesting that the thing  you're pointing out with superintelligence

476
00:50:51,120 --> 00:50:58,560
is not some finished mind which knows how  to do every single job in the economy.

477
00:50:58,560 --> 00:51:05,120
Because the way, say, the original OpenAI charter  or whatever defines AGI is like, it can do every

478
00:51:05,120 --> 00:51:11,680
single job, every single thing a human can do. You're proposing instead a mind which can

479
00:51:11,680 --> 00:51:15,520
learn to do every single job,  and that is superintelligence.

480
00:51:15,520 --> 00:51:19,840
Yes. But once you have the learning algorithm,

481
00:51:19,840 --> 00:51:25,200
it gets deployed into the world the same way  a human laborer might join an organization.

482
00:51:25,200 --> 00:51:27,360
Exactly. It seems like one of these two things

483
00:51:27,360 --> 00:51:35,440
might happen, maybe neither of these happens. One, this super-efficient learning algorithm

484
00:51:35,440 --> 00:51:40,400
becomes superhuman, becomes as good  as you and potentially even better,

485
00:51:40,400 --> 00:51:45,920
at the task of ML research. As a result the algorithm

486
00:51:45,920 --> 00:51:50,560
itself becomes more and more superhuman. The other is, even if that doesn't happen,

487
00:51:50,560 --> 00:51:56,800
if you have a single model—this is explicitly  your vision—where instances of a model

488
00:51:56,800 --> 00:52:00,800
which are deployed through the economy doing  different jobs, learning how to do those jobs,

489
00:52:00,800 --> 00:52:05,520
continually learning on the job, picking up  all the skills that any human could pick up,

490
00:52:05,520 --> 00:52:10,480
but picking them all up at the same time,  and then amalgamating their learnings,

491
00:52:10,480 --> 00:52:15,440
you basically have a model which functionally  becomes superintelligent even without any sort

492
00:52:15,440 --> 00:52:20,560
of recursive self-improvement in software. Because you now have one model that can do

493
00:52:20,560 --> 00:52:25,040
every single job in the economy and humans  can't merge our minds in the same way.

494
00:52:25,040 --> 00:52:28,640
So do you expect some sort of intelligence  explosion from broad deployment?

495
00:52:28,640 --> 00:52:37,280
I think that it is likely that we  will have rapid economic growth.

496
00:52:37,280 --> 00:52:46,640
I think with broad deployment, there are two  arguments you could make which are conflicting.

497
00:52:46,640 --> 00:52:59,120
One is that once indeed you get to a point where  you have an AI that can learn to do things quickly

498
00:52:59,120 --> 00:53:07,280
and you have many of them, then there will be  a strong force to deploy them in the economy

499
00:53:07,280 --> 00:53:13,360
unless there will be some kind of a regulation  that stops it, which by the way there might be.

500
00:53:13,360 --> 00:53:19,120
But the idea of very rapid  economic growth for some time,

501
00:53:19,120 --> 00:53:25,440
I think it’s very possible from broad deployment. The question is how rapid it's going to be.

502
00:53:25,440 --> 00:53:30,720
I think this is hard to know because on the  one hand you have this very efficient worker.

503
00:53:30,720 --> 00:53:36,160
On the other hand, the world is just  really big and there's a lot of stuff,

504
00:53:36,160 --> 00:53:41,600
and that stuff moves at a different speed. But then on the other hand, now the AI could…

505
00:53:41,600 --> 00:53:47,920
So I think very rapid economic growth is possible. We will see all kinds of things like different

506
00:53:47,920 --> 00:53:51,280
countries with different rules and the  ones which have the friendlier rules, the

507
00:53:51,280 --> 00:55:10,320
economic growth will be faster. Hard to predict. It seems to me that this is a very precarious

508
00:55:10,320 --> 00:55:14,080
situation to be in. In the limit,

509
00:55:14,080 --> 00:55:17,760
we know that this should be possible. If you have something that is as good

510
00:55:17,760 --> 00:55:24,400
as a human at learning, but which can merge its  brains—merge different instances in a way that

511
00:55:24,400 --> 00:55:28,320
humans can't merge—already, this seems like  a thing that should physically be possible.

512
00:55:28,320 --> 00:55:30,800
Humans are possible, digital  computers are possible.

513
00:55:30,800 --> 00:55:33,440
You just need both of those  combined to produce this thing.

514
00:55:33,440 --> 00:55:40,080
It also seems this kind of  thing is extremely powerful.

515
00:55:41,440 --> 00:55:45,680
Economic growth is one way to put it. A Dyson sphere is a lot of economic growth.

516
00:55:45,680 --> 00:55:50,480
But another way to put it is that you will have,  in potentially a very short period of time...

517
00:55:52,080 --> 00:55:55,200
You hire people at SSI, and in six  months, they're net productive, probably.

518
00:55:56,000 --> 00:56:00,640
A human learns really fast, and this thing  is becoming smarter and smarter very fast.

519
00:56:01,200 --> 00:56:05,840
How do you think about making that go well? Why is SSI positioned to do that well?

520
00:56:05,840 --> 00:56:08,160
What is SSI's plan there, is  basically what I'm trying to ask.

521
00:56:12,160 --> 00:56:22,880
One of the ways in which my thinking has been  changing is that I now place more importance on

522
00:56:22,880 --> 00:56:34,960
AI being deployed incrementally and in advance. One very difficult thing about AI is that we are

523
00:56:34,960 --> 00:56:42,480
talking about systems that don't yet  exist and it's hard to imagine them.

524
00:56:43,600 --> 00:56:52,080
I think that one of the things that's happening is  that in practice, it's very hard to feel the AGI.

525
00:56:52,080 --> 00:57:01,440
It's very hard to feel the AGI. We can talk about it, but imagine

526
00:57:01,440 --> 00:57:07,840
having a conversation about how it is  like to be old when you're old and frail.

527
00:57:07,840 --> 00:57:12,640
You can have a conversation, you can try to  imagine it, but it's just hard, and you come

528
00:57:12,640 --> 00:57:22,400
back to reality where that's not the case. I think that a lot of the issues around AGI

529
00:57:22,400 --> 00:57:30,960
and its future power stem from the fact  that it's very difficult to imagine.

530
00:57:30,960 --> 00:57:37,680
Future AI is going to be different. It's going  to be powerful. Indeed, the whole problem,

531
00:57:37,680 --> 00:57:43,360
what is the problem of AI and AGI? The whole problem is the power.

532
00:57:43,360 --> 00:57:48,320
The whole problem is the power. When the power is really big,

533
00:57:48,320 --> 00:57:53,120
what's going to happen? One of the ways in which I've

534
00:57:53,120 --> 00:58:02,320
changed my mind over the past year—and that  change of mind, I'll hedge a little bit, may

535
00:58:02,320 --> 00:58:12,960
back-propagate into the plans of our company—is  that if it's hard to imagine, what do you do?

536
00:58:12,960 --> 00:58:16,560
You’ve got to be showing the thing. You’ve got to be showing the thing.

537
00:58:16,560 --> 00:58:24,400
I maintain that most people who work on AI also  can't imagine it because it's too different from

538
00:58:24,400 --> 00:58:31,440
what people see on a day-to-day basis. I do maintain, here's something which

539
00:58:31,440 --> 00:58:40,960
I predict will happen. This is a prediction.  I maintain that as AI becomes more powerful,

540
00:58:40,960 --> 00:58:48,320
people will change their behaviors. We will see all kinds of unprecedented

541
00:58:48,320 --> 00:58:56,720
things which are not happening right now. I’ll  give some examples. I think for better or worse,

542
00:58:57,440 --> 00:59:03,120
the frontier companies will play a very important  role in what happens, as will the government.

543
00:59:03,120 --> 00:59:06,800
The kind of things that I think  you'll see, which you see the

544
00:59:06,800 --> 00:59:15,920
beginnings of, are companies that are fierce  competitors starting to collaborate on AI safety.

545
00:59:15,920 --> 00:59:22,880
You may have seen OpenAI and Anthropic doing  a first small step, but that did not exist.

546
00:59:22,880 --> 00:59:27,360
That's something which I predicted in  one of my talks about three years ago,

547
00:59:27,360 --> 00:59:30,960
that such a thing will happen. I also maintain that as AI continues

548
00:59:30,960 --> 00:59:38,320
to become more powerful, more visibly  powerful, there will also be a desire from

549
00:59:38,320 --> 00:59:46,160
governments and the public to do something. I think this is a very important force,

550
00:59:46,160 --> 00:59:51,600
of showing the AI. That's number one.  Number two, okay, so the AI is being

551
00:59:51,600 --> 00:59:59,600
built. What needs to be done? One thing that  I maintain that will happen is that right now,

552
00:59:59,600 --> 01:00:06,560
people who are working on AI, I maintain that the  AI doesn't feel powerful because of its mistakes.

553
01:00:06,560 --> 01:00:10,880
I do think that at some point the AI  will start to feel powerful actually.

554
01:00:10,880 --> 01:00:18,160
I think when that happens, we will see a big  change in the way all AI companies approach

555
01:00:18,160 --> 01:00:25,680
safety. They'll become much more paranoid.  I say this as a prediction that we will

556
01:00:25,680 --> 01:00:30,320
see happen. We'll see if I'm right. But I think  this is something that will happen because they

557
01:00:30,320 --> 01:00:34,160
will see the AI becoming more powerful. Everything that's happening right now,

558
01:00:34,160 --> 01:00:42,080
I maintain, is because people look at today's  AI and it's hard to imagine the future AI.

559
01:00:42,640 --> 01:00:49,520
There is a third thing which needs to happen. I'm talking about it in broader terms,

560
01:00:49,520 --> 01:00:54,640
not just from the perspective of SSI  because you asked me about our company.

561
01:00:54,640 --> 01:00:58,400
The question is, what should  the companies aspire to build?

562
01:00:58,400 --> 01:01:02,160
What should they aspire to build? There has been one big idea that

563
01:01:04,000 --> 01:01:11,120
everyone has been locked into, which is  the self-improving AI. Why did it happen?

564
01:01:11,120 --> 01:01:17,200
Because there are fewer ideas than companies. But I maintain that there is something that's

565
01:01:17,200 --> 01:01:21,360
better to build, and I think  that everyone will want that.

566
01:01:21,360 --> 01:01:28,880
It's the AI that's robustly aligned to  care about sentient life specifically.

567
01:01:29,680 --> 01:01:35,280
I think in particular, there's a case to  be made that it will be easier to build

568
01:01:35,280 --> 01:01:40,640
an AI that cares about sentient life than  an AI that cares about human life alone,

569
01:01:40,640 --> 01:01:46,080
because the AI itself will be sentient. And if you think about things like mirror

570
01:01:46,080 --> 01:01:53,200
neurons and human empathy for animals, which you  might argue it's not big enough, but it exists.

571
01:01:53,200 --> 01:01:58,240
I think it's an emergent property from  the fact that we model others with the

572
01:01:58,240 --> 01:02:03,680
same circuit that we use to model ourselves,  because that's the most efficient thing to do.

573
01:02:03,680 --> 01:02:08,880
So even if you got an AI to care about  sentient beings—and it's not actually

574
01:02:08,880 --> 01:02:10,720
clear to me that that's what you  should try to do if you solved

575
01:02:10,720 --> 01:02:16,480
alignment—it would still be the case  that most sentient beings will be AIs.

576
01:02:16,480 --> 01:02:19,360
There will be trillions,  eventually quadrillions, of AIs.

577
01:02:19,360 --> 01:02:22,400
Humans will be a very small  fraction of sentient beings.

578
01:02:23,280 --> 01:02:32,480
So it's not clear to me if the goal is some kind  of human control over this future civilization,

579
01:02:32,480 --> 01:02:39,680
that this is the best criterion. It's true. It's possible it's not

580
01:02:39,680 --> 01:02:53,120
the best criterion. I'll say two things. Number  one, care for sentient life, I think there is

581
01:02:53,120 --> 01:03:01,120
merit to it. It should be considered. I think it  would be helpful if there was some kind of short

582
01:03:01,120 --> 01:03:10,240
list of ideas that the companies, when they are  in this situation, could use. That’s number two.

583
01:03:10,240 --> 01:03:16,480
Number three, I think it would be really  materially helpful if the power of the

584
01:03:16,480 --> 01:03:23,680
most powerful superintelligence was somehow capped  because it would address a lot of these concerns.

585
01:03:23,680 --> 01:03:29,680
The question of how to do it, I'm not sure, but I  think that would be materially helpful when you're

586
01:03:29,680 --> 01:03:35,360
talking about really, really powerful systems. Before we continue the alignment discussion,

587
01:03:35,360 --> 01:03:38,560
I want to double-click on that. How much room is there at the top?

588
01:03:38,560 --> 01:03:44,400
How do you think about superintelligence? Do you think, using this learning efficiency idea,

589
01:03:44,400 --> 01:03:48,880
maybe it is just extremely fast at  learning new skills or new knowledge?

590
01:03:48,880 --> 01:03:54,320
Does it just have a bigger pool of strategies? Is there a single cohesive "it" in the

591
01:03:54,320 --> 01:04:01,600
center that's more powerful or bigger? If so, do you imagine that this will be

592
01:04:01,600 --> 01:04:05,120
sort of godlike in comparison to the rest of human  civilization, or does it just feel like another

593
01:04:05,120 --> 01:04:10,000
agent, or another cluster of agents? This is an area where different

594
01:04:10,000 --> 01:04:14,800
people have different intuitions. I think it will be very powerful, for sure.

595
01:04:16,240 --> 01:04:23,200
What I think is most likely to happen  is that there will be multiple such

596
01:04:23,200 --> 01:04:33,280
AIs being created roughly at the same time. I think that if the cluster is big enough—like

597
01:04:33,280 --> 01:04:39,040
if the cluster is literally continent-sized—that  thing could be really powerful, indeed.

598
01:04:39,040 --> 01:04:44,720
If you literally have a continent-sized  cluster, those AIs can be very powerful.

599
01:04:46,640 --> 01:04:51,680
All I can tell you is that if you're  talking about extremely powerful AIs,

600
01:04:51,680 --> 01:04:59,760
truly dramatically powerful, it would be nice if  they could be restrained in some ways or if there

601
01:04:59,760 --> 01:05:11,200
were some kind of agreement or something. What is the concern of superintelligence?

602
01:05:11,200 --> 01:05:16,800
What is one way to explain the concern? If you imagine a system that is sufficiently

603
01:05:16,800 --> 01:05:23,440
powerful, really sufficiently powerful—and you  could say you need to do something sensible like

604
01:05:23,440 --> 01:05:29,440
care for sentient life in a very single-minded  way—we might not like the results. That's really

605
01:05:29,440 --> 01:05:35,840
what it is. Maybe, by the way, the answer is  that you do not build an RL agent in the usual

606
01:05:35,840 --> 01:05:42,800
sense. I'll point several things out. I  think human beings are semi-RL agents.

607
01:05:43,600 --> 01:05:48,160
We pursue a reward, and then the emotions  or whatever make us tire out of the

608
01:05:48,160 --> 01:05:55,760
reward and we pursue a different reward. The market is a very short-sighted kind of

609
01:05:55,760 --> 01:05:59,600
agent. Evolution is the same. Evolution  is very intelligent in some ways,

610
01:05:59,600 --> 01:06:03,040
but very dumb in other ways. The government has been designed

611
01:06:03,040 --> 01:06:08,320
to be a never-ending fight between  three parts, which has an effect.

612
01:06:08,320 --> 01:06:13,120
So I think things like this. Another thing that makes this discussion

613
01:06:13,120 --> 01:06:19,600
difficult is that we are talking about systems  that don't exist, that we don't know how to build.

614
01:06:19,600 --> 01:06:21,760
That’s the other thing and  that’s actually my belief.

615
01:06:21,760 --> 01:06:26,560
I think what people are doing right now  will go some distance and then peter out.

616
01:06:26,560 --> 01:06:30,080
It will continue to improve,  but it will also not be "it".

617
01:06:30,080 --> 01:06:38,960
The "It" we don't know how to build, and  a lot hinges on understanding reliable

618
01:06:38,960 --> 01:06:47,120
generalization. I’ll say another thing.  One of the things that you could say about

619
01:06:47,120 --> 01:06:55,760
what causes alignment to be difficult is that  your ability to learn human values is fragile.

620
01:06:55,760 --> 01:07:00,080
Then your ability to optimize them is fragile. You actually learn to optimize them.

621
01:07:00,080 --> 01:07:06,640
And can't you say, "Are these not all  instances of unreliable generalization?"

622
01:07:06,640 --> 01:07:10,080
Why is it that human beings appear  to generalize so much better?

623
01:07:10,080 --> 01:07:13,360
What if generalization was much better? What would happen in this case? What would

624
01:07:13,360 --> 01:07:18,560
be the effect? But those questions  are right now still unanswerable.

625
01:07:19,200 --> 01:07:24,640
How does one think about what  AI going well looks like?

626
01:07:24,640 --> 01:07:28,480
You've scoped out how AI might evolve. We'll have these sort of continual

627
01:07:28,480 --> 01:07:33,600
learning agents. AI will be very powerful.  Maybe there will be many different AIs.

628
01:07:33,600 --> 01:07:40,800
How do you think about lots of continent-sized  compute intelligences going around? How dangerous

629
01:07:40,800 --> 01:07:49,840
is that? How do we make that less dangerous? And how do we do that in a way that protects an

630
01:07:49,840 --> 01:07:56,240
equilibrium where there might be misaligned  AIs out there and bad actors out there?

631
01:07:56,240 --> 01:08:00,960
Here’s one reason why I liked "AI  that cares for sentient life".

632
01:08:00,960 --> 01:08:09,520
We can debate on whether it's good or bad. But if the first N of these dramatic

633
01:08:09,520 --> 01:08:17,520
systems do care for, love, humanity  or something, care for sentient life,

634
01:08:17,520 --> 01:08:23,840
obviously this also needs to be achieved. This  needs to be achieved. So if this is achieved

635
01:08:23,840 --> 01:08:32,960
by the first N of those systems, then I can  see it go well, at least for quite some time.

636
01:08:32,960 --> 01:08:36,560
Then there is the question of  what happens in the long run.

637
01:08:36,560 --> 01:08:44,960
How do you achieve a long-run equilibrium? I think that there, there is an answer as well.

638
01:08:44,960 --> 01:08:49,200
I don't like this answer, but  it needs to be considered.

639
01:08:51,760 --> 01:08:57,120
In the long run, you might say, "Okay, if  you have a world where powerful AIs exist,

640
01:08:57,120 --> 01:09:01,200
in the short term, you could say  you have universal high income.

641
01:09:01,200 --> 01:09:04,880
You have universal high income  and we're all doing well."

642
01:09:04,880 --> 01:09:11,439
But what do the Buddhists say? "Change is the  only constant." Things change. There is some

643
01:09:11,439 --> 01:09:18,160
kind of government, political structure thing, and  it changes because these things have a shelf life.

644
01:09:18,720 --> 01:09:22,560
Some new government thing comes up and  it functions, and then after some time

645
01:09:22,560 --> 01:09:25,600
it stops functioning. That's something that

646
01:09:25,600 --> 01:09:32,240
we see happening all the time. So I think for the long-run equilibrium,

647
01:09:32,240 --> 01:09:38,800
one approach is that you could say maybe every  person will have an AI that will do their bidding,

648
01:09:38,800 --> 01:09:41,040
and that's good. If that could be

649
01:09:41,040 --> 01:09:47,040
maintained indefinitely, that's true. But the downside with that is then the AI

650
01:09:47,040 --> 01:09:55,840
goes and earns money for the person and advocates  for their needs in the political sphere, and maybe

651
01:09:55,840 --> 01:09:59,520
then writes a little report saying, "Okay,  here's what I've done, here's the situation,"

652
01:09:59,520 --> 01:10:05,760
and the person says, "Great, keep it up." But the person is no longer a participant.

653
01:10:05,760 --> 01:10:08,720
Then you can say that's a  precarious place to be in.

654
01:10:10,480 --> 01:10:16,880
I'm going to preface by saying I don't  like this solution, but it is a solution.

655
01:10:19,040 --> 01:10:23,680
The solution is if people become  part-AI with some kind of Neuralink++.

656
01:10:23,680 --> 01:10:27,920
Because what will happen as a result is  that now the AI understands something,

657
01:10:27,920 --> 01:10:34,160
and we understand it too, because now the  understanding is transmitted wholesale.

658
01:10:34,160 --> 01:10:41,920
So now if the AI is in some situation, you  are involved in that situation yourself fully.

659
01:10:41,920 --> 01:10:49,840
I think this is the answer to the equilibrium. I wonder if the fact that emotions which were

660
01:10:49,840 --> 01:10:56,160
developed millions—or in many cases, billions—of  years ago in a totally different environment are

661
01:10:56,160 --> 01:11:03,520
still guiding our actions so strongly  is an example of alignment success.

662
01:11:03,520 --> 01:11:11,520
To spell out what I mean—I don’t know  whether it’s more accurate to call it

663
01:11:11,520 --> 01:11:15,760
a value function or reward function—but the  brainstem has a directive where it's saying,

664
01:11:15,760 --> 01:11:19,600
"Mate with somebody who's more successful." The cortex is the part that understands

665
01:11:19,600 --> 01:11:25,200
what success means in the modern context. But the brainstem is able to align the cortex

666
01:11:25,200 --> 01:11:29,840
and say, "However you recognize success to be—and  I’m not smart enough to understand what that is—

667
01:11:29,840 --> 01:11:36,560
you're still going to pursue this directive." I think there's a more general point.

668
01:11:36,560 --> 01:11:46,960
I think it's actually really mysterious  how evolution encodes high-level desires.

669
01:11:46,960 --> 01:11:51,920
It's pretty easy to understand how  evolution would endow us with the

670
01:11:51,920 --> 01:11:58,400
desire for food that smells good because smell  is a chemical, so just pursue that chemical.

671
01:11:58,400 --> 01:12:02,800
It's very easy to imagine  evolution doing that thing.

672
01:12:02,800 --> 01:12:08,880
But evolution also has endowed  us with all these social desires.

673
01:12:08,880 --> 01:12:12,720
We really care about being  seen positively by society.

674
01:12:12,720 --> 01:12:19,760
We care about being in good standing. All these social intuitions that we have,

675
01:12:19,760 --> 01:12:26,240
I feel strongly that they're baked in. I don't know how evolution did it

676
01:12:26,240 --> 01:12:29,840
because it's a high-level concept  that's represented in the brain.

677
01:12:31,360 --> 01:12:40,880
Let’s say you care about some social thing,  it's not a low-level signal like smell.

678
01:12:40,880 --> 01:12:46,320
It's not something for which there is a sensor. The brain needs to do a lot of processing to

679
01:12:46,320 --> 01:12:51,440
piece together lots of bits of information  to understand what's going on socially.

680
01:12:51,440 --> 01:12:56,640
Somehow evolution said, "That's what you should  care about." How did it do it? It did it quickly,

681
01:12:56,640 --> 01:13:04,400
too. All these sophisticated social things that we  care about, I think they evolved pretty recently.

682
01:13:04,400 --> 01:13:08,560
Evolution had an easy time  hard-coding this high-level desire.

683
01:13:12,000 --> 01:13:16,000
I'm unaware of a good  hypothesis for how it's done.

684
01:13:16,000 --> 01:13:23,920
I had some ideas I was kicking around,  but none of them are satisfying.

685
01:13:24,560 --> 01:13:29,680
What's especially impressive is it was desire  that you learned in your lifetime, it makes sense

686
01:13:29,680 --> 01:13:32,560
because your brain is intelligent. It makes sense why you would

687
01:13:32,560 --> 01:13:38,880
be able to learn intelligent desires. Maybe this is not your point, but one way

688
01:13:38,880 --> 01:13:44,240
to understand it is that the desire is built into  the genome, and the genome is not intelligent.

689
01:13:44,240 --> 01:13:50,160
But you're somehow able to describe this feature. It's not even clear how you define that feature,

690
01:13:50,160 --> 01:13:55,520
and you can build it into the genes. Essentially, or maybe I'll put it differently.

691
01:13:55,520 --> 01:14:01,280
If you think about the tools that  are available to the genome, it says,

692
01:14:01,280 --> 01:14:05,760
"Okay, here's a recipe for building a brain." You could say, "Here is a recipe for connecting

693
01:14:05,760 --> 01:14:10,560
the dopamine neurons to the smell sensor." And if the smell is a certain kind

694
01:14:10,560 --> 01:14:15,680
of good smell, you want to eat that. I could imagine the genome doing that.

695
01:14:15,680 --> 01:14:21,120
I'm claiming that it is harder to imagine. It's harder to imagine the genome saying

696
01:14:21,120 --> 01:14:28,080
you should care about some complicated computation  that your entire brain, a big chunk of your brain,

697
01:14:28,080 --> 01:14:33,760
does. That's all I'm claiming. I can tell  you a speculation of how it could be done.

698
01:14:33,760 --> 01:14:37,760
Let me offer a speculation, and I'll explain  why the speculation is probably false.

699
01:14:37,760 --> 01:14:52,080
So the brain has brain regions. We have  our cortex. It has all those brain regions.

700
01:14:52,080 --> 01:14:57,040
The cortex is uniform, but the brain  regions and the neurons in the cortex

701
01:14:57,040 --> 01:15:01,120
kind of speak to their neighbors mostly. That explains why you get brain regions.

702
01:15:01,120 --> 01:15:04,640
Because if you want to do some kind of  speech processing, all the neurons that

703
01:15:04,640 --> 01:15:08,000
do speech need to talk to each other. And because neurons can only speak to

704
01:15:08,000 --> 01:15:11,520
their nearby neighbors, for the  most part, it has to be a region.

705
01:15:11,520 --> 01:15:15,280
All the regions are mostly located in  the same place from person to person.

706
01:15:15,280 --> 01:15:21,360
So maybe evolution hard-coded  literally a location on the brain.

707
01:15:21,360 --> 01:15:27,920
So it says, "Oh, when the GPS coordinates  of the brain such and such, when that fires,

708
01:15:27,920 --> 01:15:30,720
that's what you should care about." Maybe that's what evolution did because

709
01:15:30,720 --> 01:15:36,000
that would be within the toolkit of evolution. Yeah, although there are examples where,

710
01:15:36,000 --> 01:15:44,160
for example, people who are born blind have that  area of their cortex adopted by another sense.

711
01:15:44,960 --> 01:15:53,200
I have no idea, but I'd be surprised if the  desires or the reward functions which require a

712
01:15:53,200 --> 01:15:58,720
visual signal no longer worked for people who have  their different areas of their cortex co-opted.

713
01:15:58,720 --> 01:16:05,680
For example, if you no longer have vision, can  you still feel the sense that I want people

714
01:16:05,680 --> 01:16:10,000
around me to like me and so forth, which  usually there are also visual cues for.

715
01:16:10,000 --> 01:16:14,320
I fully agree with that. I think there's an  even stronger counterargument to this theory.

716
01:16:16,880 --> 01:16:22,560
There are people who get half of  their brains removed in childhood,

717
01:16:23,360 --> 01:16:27,760
and they still have all their brain regions. But they all somehow move to just one hemisphere,

718
01:16:27,760 --> 01:16:32,240
which suggests that the brain regions,  their location is not fixed and so

719
01:16:32,240 --> 01:16:34,240
that theory is not true. It would have been cool

720
01:16:34,240 --> 01:16:37,680
if it was true, but it's not. So I think that's a mystery.

721
01:16:37,680 --> 01:16:43,600
But it's an interesting mystery. The fact is  that somehow evolution was able to endow us

722
01:16:43,600 --> 01:16:49,840
to care about social stuff very, very reliably. Even people who have all kinds of strange mental

723
01:16:49,840 --> 01:16:54,240
conditions and deficiencies and emotional  problems tend to care about this also.

724
01:18:13,360 --> 01:18:18,080
What is SSI planning on doing differently? Presumably your plan is to be one of the

725
01:18:18,080 --> 01:18:27,120
frontier companies when this time arrives. Presumably you started SSI because you're like,

726
01:18:27,120 --> 01:18:30,720
"I think I have a way of approaching how  to do this safely in a way that the other

727
01:18:30,720 --> 01:18:37,760
companies don't." What is that difference? The way I would describe it is that there

728
01:18:37,760 --> 01:18:43,040
are some ideas that I think are promising and  I want to investigate them and see if they

729
01:18:43,040 --> 01:18:48,960
are indeed promising or not. It's really that  simple. It's an attempt. If the ideas turn out

730
01:18:48,960 --> 01:19:01,040
to be correct—these ideas that we discussed  around understanding generalization—then I

731
01:19:01,040 --> 01:19:05,200
think we will have something worthy. Will they turn out to be correct? We

732
01:19:05,200 --> 01:19:10,720
are doing research. We are squarely an "age of  research" company. We are making progress. We've

733
01:19:10,720 --> 01:19:14,720
actually made quite good progress over the past  year, but we need to keep making more progress,

734
01:19:14,720 --> 01:19:25,920
more research. That's how I see it. I see it  as an attempt to be a voice and a participant.

735
01:19:29,840 --> 01:19:37,440
Your cofounder and previous CEO left to go to  Meta recently, and people have asked, "Well,

736
01:19:37,440 --> 01:19:40,960
if there were a lot of breakthroughs being  made, that seems like a thing that should

737
01:19:40,960 --> 01:19:49,200
have been unlikely." I wonder how you respond. For this, I will simply remind a few facts that

738
01:19:49,200 --> 01:19:52,640
may have been forgotten. I think these facts which

739
01:19:52,640 --> 01:19:59,200
provide the context explain the situation. The context was that we were fundraising at

740
01:19:59,200 --> 01:20:10,720
a $32 billion valuation, and then Meta came  in and offered to acquire us, and I said no.

741
01:20:10,720 --> 01:20:19,920
But my former cofounder in some sense said yes. As a result, he also was able to enjoy a lot of

742
01:20:19,920 --> 01:20:24,880
near-term liquidity, and he was the  only person from SSI to join Meta.

743
01:20:25,440 --> 01:20:31,200
It sounds like SSI's plan is to be a company  that is at the frontier when you get to this

744
01:20:31,200 --> 01:20:35,600
very important period in human history  where you have superhuman intelligence.

745
01:20:35,600 --> 01:20:39,360
You have these ideas about how to  make superhuman intelligence go well.

746
01:20:39,360 --> 01:20:42,480
But other companies will  be trying their own ideas.

747
01:20:42,480 --> 01:20:48,400
What distinguishes SSI's approach  to making superintelligence go well?

748
01:20:48,400 --> 01:20:54,080
The main thing that distinguishes  SSI is its technical approach.

749
01:20:54,880 --> 01:21:01,520
We have a different technical approach that  I think is worthy and we are pursuing it.

750
01:21:01,520 --> 01:21:06,160
I maintain that in the end there  will be a convergence of strategies.

751
01:21:06,160 --> 01:21:14,960
I think there will be a convergence of strategies  where at some point, as AI becomes more powerful,

752
01:21:14,960 --> 01:21:19,520
it's going to become more or less clearer  to everyone what the strategy should be.

753
01:21:19,520 --> 01:21:24,800
It should be something like, you need to find  some way to talk to each other and you want

754
01:21:24,800 --> 01:21:37,920
your first actual real superintelligent AI to  be aligned and somehow care for sentient life,

755
01:21:37,920 --> 01:21:42,560
care for people, democratic, one  of those, some combination thereof.

756
01:21:42,560 --> 01:21:50,400
I think this is the condition  that everyone should strive for.

757
01:21:50,400 --> 01:21:57,040
That's what SSI is striving for. I think that this time, if not already,

758
01:21:57,040 --> 01:22:00,320
all the other companies will realize that  they're striving towards the same thing.

759
01:22:00,320 --> 01:22:03,920
We'll see. I think that the world will  truly change as AI becomes more powerful.

760
01:22:07,920 --> 01:22:11,680
I think things will be really different and  people will be acting really differently.

761
01:22:12,480 --> 01:22:16,560
Speaking of forecasts, what are your  forecasts to this system you're describing,

762
01:22:16,560 --> 01:22:23,840
which can learn as well as a human and  subsequently, as a result, become superhuman?

763
01:22:23,840 --> 01:22:27,600
I think like 5 to 20. 5 to 20 years?

764
01:22:27,600 --> 01:22:29,760
Mhm. I just want

765
01:22:29,760 --> 01:22:35,680
to unroll how you might see the world coming. It's like, we have a couple more years where

766
01:22:35,680 --> 01:22:40,000
these other companies are continuing  the current approach and it stalls out.

767
01:22:40,000 --> 01:22:44,720
"Stalls out" here meaning they earn no more  than low hundreds of billions in revenue?

768
01:22:44,720 --> 01:22:57,200
How do you think about what stalling out means? I think stalling out will look like…it will

769
01:22:57,200 --> 01:23:00,788
all look very similar among  all the different companies.

770
01:23:00,788 --> 01:23:02,640
It could be something like this. I'm not sure because I think

771
01:23:05,360 --> 01:23:10,320
even with stalling out, I think these  companies could make a stupendous revenue.

772
01:23:10,320 --> 01:23:15,280
Maybe not profits because they will need  to work hard to differentiate each other

773
01:23:15,280 --> 01:23:22,880
from themselves, but revenue definitely. But something in your model implies that

774
01:23:23,760 --> 01:23:27,920
when the correct solution does emerge, there  will be convergence between all the companies.

775
01:23:27,920 --> 01:23:31,440
I'm curious why you think that's the case. I was talking more about convergence

776
01:23:31,440 --> 01:23:34,720
on their alignment strategies. I think eventual convergence on

777
01:23:34,720 --> 01:23:38,800
the technical approach is probably going  to happen as well, but I was alluding

778
01:23:38,800 --> 01:23:43,680
to convergence to the alignment strategies. What exactly is the thing that should be done?

779
01:23:43,680 --> 01:23:46,880
I just want to better understand  how you see the future unrolling.

780
01:23:46,880 --> 01:23:50,320
Currently, we have these different companies, and  you expect their approach to continue generating

781
01:23:50,320 --> 01:23:56,560
revenue but not get to this human-like learner. So now we have these different forks of companies.

782
01:23:56,560 --> 01:23:59,520
We have you, we have Thinking Machines,  there's a bunch of other labs.

783
01:24:00,160 --> 01:24:03,120
Maybe one of them figures  out the correct approach.

784
01:24:03,120 --> 01:24:07,600
But then the release of their product makes  it clear to other people how to do this thing.

785
01:24:07,600 --> 01:24:11,920
I think it won't be clear how to do it, but  it will be clear that something different is

786
01:24:11,920 --> 01:24:17,600
possible, and that is information. People will then be trying

787
01:24:17,600 --> 01:24:26,560
to figure out how that works. I do think though that one of the things not

788
01:24:26,560 --> 01:24:34,320
addressed here, not discussed, is that with each  increase in the AI's capabilities, I think there

789
01:24:34,320 --> 01:24:40,960
will be some kind of changes, but I don't know  exactly which ones, in how things are being done.

790
01:24:42,960 --> 01:24:47,280
I think it's going to be important, yet  I can't spell out what that is exactly.

791
01:24:50,320 --> 01:24:55,360
By default, you would expect the company that  has that model to be getting all these gains

792
01:24:55,360 --> 01:25:02,480
because they have the model that has the skills  and knowledge that it's building up in the world.

793
01:25:02,480 --> 01:25:05,760
What is the reason to think that the benefits  of that would be widely distributed and not

794
01:25:05,760 --> 01:25:11,200
just end up at whatever model company gets  this continuous learning loop going first?

795
01:25:14,880 --> 01:25:25,680
Here is what I think is going to happen. Number one, let's look at how things have

796
01:25:25,680 --> 01:25:32,000
gone so far with the AIs of the past. One company produced an advance and the

797
01:25:32,000 --> 01:25:40,400
other company scrambled and produced some similar  things after some amount of time and they started

798
01:25:40,400 --> 01:25:48,640
to compete in the market and push the prices down. So I think from the market perspective,

799
01:25:48,640 --> 01:25:54,800
something similar will happen there as well. We are talking about the good world, by the way.

800
01:25:56,640 --> 01:26:08,000
What's the good world? It’s where we have these  powerful human-like learners that are also… By

801
01:26:08,000 --> 01:26:13,760
the way, maybe there's another thing we haven't  discussed on the spec of the superintelligent

802
01:26:13,760 --> 01:26:20,160
AI that I think is worth considering. It’s that you make it narrow, it can

803
01:26:20,160 --> 01:26:24,320
be useful and narrow at the same time. You can have lots of narrow superintelligent AIs.

804
01:26:24,320 --> 01:26:31,600
But suppose you have many of them and you  have some company that's producing a lot of

805
01:26:32,720 --> 01:26:35,440
profits from it. Then you have another

806
01:26:35,440 --> 01:26:40,240
company that comes in and starts to compete. The way the competition is going to work is

807
01:26:40,240 --> 01:26:52,800
through specialization. Competition loves  specialization. You see it in the market,

808
01:26:52,800 --> 01:26:55,280
you see it in evolution as well. You're going to have lots of different

809
01:26:55,280 --> 01:26:59,520
niches and you're going to have lots of different  companies who are occupying different niches.

810
01:26:59,520 --> 01:27:08,640
In this world we might say one AI company  is really quite a bit better at some area

811
01:27:08,640 --> 01:27:13,360
of really complicated economic activity and a  different company is better at another area.

812
01:27:13,360 --> 01:27:15,200
And the third company is  really good at litigation.

813
01:27:15,200 --> 01:27:19,840
Isn't this contradicted by what human-like  learning implies? It’s that it can learn…

814
01:27:19,840 --> 01:27:25,120
It can, but you have accumulated  learning. You have a big investment.

815
01:27:25,120 --> 01:27:30,960
You spent a lot of compute to become really,  really good, really phenomenal at this thing.

816
01:27:30,960 --> 01:27:34,080
Someone else spent a huge amount  of compute and a huge amount of

817
01:27:34,080 --> 01:27:38,000
experience to get really good at some other thing. You apply a lot of human learning to get there,

818
01:27:38,000 --> 01:27:44,160
but now you are at this high point where  someone else would say, "Look, I don't want

819
01:27:44,160 --> 01:27:47,600
to start learning what you've learned." I guess that would require many different

820
01:27:47,600 --> 01:27:53,360
companies to begin at the human-like continual  learning agent at the same time so that they

821
01:27:53,360 --> 01:27:58,000
can start their different tree  search in different branches.

822
01:27:58,000 --> 01:28:07,280
But if one company gets that agent first, or gets  that learner first, it does then seem like… Well,

823
01:28:09,120 --> 01:28:15,760
if you just think about every single job in  the economy, having an instance learning each

824
01:28:15,760 --> 01:28:21,200
one seems tractable for a company. That's a valid argument. My strong

825
01:28:21,200 --> 01:28:28,720
intuition is that it's not how it's going to go. The argument says it will go this way, but my

826
01:28:28,720 --> 01:28:36,240
strong intuition is that it will not go this way. In theory, there is no difference between theory

827
01:28:36,240 --> 01:28:39,440
and practice. In practice, there is. I  think that's going to be one of those.

828
01:28:39,440 --> 01:28:44,000
A lot of people's models of recursive  self-improvement literally, explicitly state

829
01:28:44,000 --> 01:28:49,360
we will have a million Ilyas in a server that are  coming up with different ideas, and this will lead

830
01:28:49,360 --> 01:28:52,800
to a superintelligence emerging very fast. Do you have some intuition about how

831
01:28:52,800 --> 01:29:00,800
parallelizable the thing you are doing is? What are the gains from making copies of Ilya?

832
01:29:00,800 --> 01:29:09,440
I don’t know. I think there'll definitely be  diminishing returns because you want people

833
01:29:09,440 --> 01:29:14,320
who think differently rather than the same. If there were literal copies of me, I'm not sure

834
01:29:14,320 --> 01:29:21,920
how much more incremental value you'd get. People who think differently,

835
01:29:21,920 --> 01:29:25,360
that's what you want. Why is it that if you look

836
01:29:25,360 --> 01:29:30,800
at different models, even released by totally  different companies trained on potentially

837
01:29:30,800 --> 01:29:35,920
non-overlapping datasets, it's actually  crazy how similar LLMs are to each other?

838
01:29:35,920 --> 01:29:39,520
Maybe the datasets are not as  non-overlapping as it seems.

839
01:29:39,520 --> 01:29:44,080
But there’s some sense in which even  if an individual human might be less

840
01:29:44,080 --> 01:29:46,960
productive than the future AI, maybe there’s  something to the fact that human teams have

841
01:29:46,960 --> 01:29:53,600
more diversity than teams of AIs might have. How do we elicit meaningful diversity among AIs?

842
01:29:53,600 --> 01:29:56,720
I think just raising the temperature  just results in gibberish.

843
01:29:56,720 --> 01:30:01,360
You want something more like different scientists  have different prejudices or different ideas.

844
01:30:01,360 --> 01:30:04,720
How do you get that kind of  diversity among AI agents?

845
01:30:04,720 --> 01:30:10,720
So the reason there has been no diversity,  I believe, is because of pre-training.

846
01:30:10,720 --> 01:30:16,800
All the pre-trained models are pretty much the  same because they pre-train on the same data.

847
01:30:16,800 --> 01:30:20,960
Now RL and post-training is where  some differentiation starts to emerge

848
01:30:20,960 --> 01:30:24,960
because different people come  up with different RL training.

849
01:30:26,400 --> 01:30:31,520
I've heard you hint in the past  about self-play as a way to either

850
01:30:31,520 --> 01:30:38,000
get data or match agents to other agents of  equivalent intelligence to kick off learning.

851
01:30:38,000 --> 01:30:46,800
How should we think about why there are no public  proposals of this kind of thing working with LLMs?

852
01:30:46,800 --> 01:30:52,320
I would say there are two things to say. The reason why I thought self-play was

853
01:30:52,320 --> 01:31:00,800
interesting is because it offered a way to  create models using compute only, without data.

854
01:31:00,800 --> 01:31:06,320
If you think that data is the ultimate bottleneck,  then using compute only is very interesting.

855
01:31:06,320 --> 01:31:15,760
So that's what makes it interesting. The thing is that self-play, at least the

856
01:31:15,760 --> 01:31:21,520
way it was done in the past—when you have agents  which somehow compete with each other—it's only

857
01:31:21,520 --> 01:31:29,040
good for developing a certain set of skills. It  is too narrow. It's only good for negotiation,

858
01:31:29,040 --> 01:31:35,040
conflict, certain social skills,  strategizing, that kind of stuff.

859
01:31:35,040 --> 01:31:38,800
If you care about those skills,  then self-play will be useful.

860
01:31:39,360 --> 01:31:46,880
Actually, I think that self-play did find  a home, but just in a different form.

861
01:31:48,080 --> 01:31:55,680
So things like debate, prover-verifier, you  have some kind of an LLM-as-a-Judge which is

862
01:31:55,680 --> 01:32:00,080
also incentivized to find mistakes in your work. You could say this is not exactly self-play,

863
01:32:00,080 --> 01:32:04,720
but this is a related adversarial  setup that people are doing, I believe.

864
01:32:04,720 --> 01:32:12,800
Really self-play is a special case of  more general competition between agents.

865
01:32:13,600 --> 01:32:16,560
The natural response to competition  is to try to be different.

866
01:32:16,560 --> 01:32:21,360
So if you were to put multiple agents together  and you tell them, "You all need to work on some

867
01:32:21,360 --> 01:32:26,560
problem and you are an agent and you're inspecting  what everyone else is working," they’re going to

868
01:32:26,560 --> 01:32:31,520
say, "Well, if they're already taking this  approach, it's not clear I should pursue it.

869
01:32:31,520 --> 01:32:36,000
I should pursue something differentiated." So I  think something like this could also create an

870
01:32:36,000 --> 01:32:44,400
incentive for a diversity of approaches. Final question: What is research taste?

871
01:32:44,400 --> 01:32:51,040
You're obviously the person in  the world who is considered to

872
01:32:51,040 --> 01:33:01,280
have the best taste in doing research in AI. You were the co-author on the biggest things

873
01:33:01,280 --> 01:33:05,040
that have happened in the history of deep  learning, from AlexNet to GPT-3 to so on.

874
01:33:05,040 --> 01:33:11,120
What is it, how do you characterize  how you come up with these ideas?

875
01:33:11,120 --> 01:33:18,720
I can comment on this for myself. I think different people do it differently.

876
01:33:18,720 --> 01:33:29,680
One thing that guides me personally is an  aesthetic of how AI should be, by thinking

877
01:33:29,680 --> 01:33:35,520
about how people are, but thinking correctly. It's very easy to think about how people are

878
01:33:35,520 --> 01:33:40,320
incorrectly, but what does it mean to think  about people correctly? I'll give you some

879
01:33:40,320 --> 01:33:48,080
examples. The idea of the artificial neuron  is directly inspired by the brain, and it's a

880
01:33:48,080 --> 01:33:52,720
great idea. Why? Because you say the brain has  all these different organs, it has the folds,

881
01:33:52,720 --> 01:33:56,400
but the folds probably don't matter. Why do we think that the neurons matter?

882
01:33:56,400 --> 01:34:01,280
Because there are many of them. It kind of feels right, so you want the neuron.

883
01:34:01,280 --> 01:34:09,600
You want some local learning rule that will  change the connections between the neurons.

884
01:34:10,240 --> 01:34:15,600
It feels plausible that the brain does it. The idea of the distributed representation.

885
01:34:15,600 --> 01:34:19,920
The idea that the brain responds  to experience therefore our neural

886
01:34:19,920 --> 01:34:23,200
net should learn from experience. The brain learns from experience,

887
01:34:24,480 --> 01:34:29,280
the neural net should learn from experience. You kind of ask yourself, is something fundamental

888
01:34:29,280 --> 01:34:35,600
or not fundamental? How things should be.  I think that's been guiding me a fair bit,

889
01:34:35,600 --> 01:34:41,440
thinking from multiple angles and looking  for almost beauty, beauty and simplicity.

890
01:34:41,440 --> 01:34:46,400
Ugliness, there's no room for ugliness. It's beauty, simplicity, elegance,

891
01:34:46,400 --> 01:34:49,360
correct inspiration from the brain. All of those things need to

892
01:34:49,360 --> 01:34:53,120
be present at the same time. The more they are present, the

893
01:34:53,120 --> 01:34:58,640
more confident you can be in a top-down belief. The top-down belief is the thing that sustains

894
01:34:58,640 --> 01:35:04,560
you when the experiments contradict you. Because if you trust the data all the time,

895
01:35:04,560 --> 01:35:07,520
well sometimes you can be doing the  correct thing but there's a bug.

896
01:35:07,520 --> 01:35:11,040
But you don't know that there is a bug. How can you tell that there is a bug?

897
01:35:11,040 --> 01:35:14,960
How do you know if you should keep debugging or  you conclude it's the wrong direction? It's the

898
01:35:14,960 --> 01:35:20,720
top-down. You can say things have to be this way. Something like this has to work,

899
01:35:20,720 --> 01:35:24,880
therefore we’ve got to keep going. That's the top-down, and it's based on this

900
01:35:25,520 --> 01:35:31,600
multifaceted beauty and inspiration by the brain. Alright, we'll leave it there.

901
01:35:31,600 --> 01:35:34,960
Thank you so much. Ilya, thank you so much.

902
01:35:34,960 --> 01:35:36,480
Alright. Appreciate it. That was great.

903
01:35:36,480 --> 01:35:39,040
Yeah, I enjoyed it. Yes, me too.

