1
00:00:46,920 --> 00:00:53,840
Today I have the honor of chatting with Jeff  Dean and Noam Shazeer. Jeff is Google's Chief

2
00:00:53,840 --> 00:00:58,920
Scientist, and through his 25 years at the  company, he has worked on basically the most

3
00:00:58,920 --> 00:01:05,280
transformative systems in modern computing: from  MapReduce, BigTable, Tensorflow, AlphaChip –

4
00:01:05,280 --> 00:01:11,320
genuinely, the list doesn't end – Gemini now. And Noam is the single person most responsible

5
00:01:11,320 --> 00:01:19,320
for the current AI revolution. He has been  the inventor or co-inventor of all the main

6
00:01:19,320 --> 00:01:24,120
architectures and techniques that are used  for modern LLMs: from the Transformer itself,

7
00:01:24,120 --> 00:01:30,160
to Mixture of Experts, to Mesh Tensorflow, to  many other things. And they are two of the three

8
00:01:30,160 --> 00:01:35,280
co-leads of Gemini at Google DeepMind.  Awesome. Thanks so much for coming on.

9
00:01:35,280 --> 00:01:39,960
Thank you. Super excited to be here. Okay, first question. Both of you

10
00:01:39,960 --> 00:01:45,600
have been at Google for 25, or close to 25,  years. At some point early on in the company,

11
00:01:45,600 --> 00:01:51,360
you probably understood how everything worked.  When did that stop being the case? Do you feel

12
00:01:51,360 --> 00:02:00,040
like there was a clear moment that happened? I joined, this was like, end of 2000, and they

13
00:02:00,040 --> 00:02:06,120
had this thing: everybody gets a mentor. I knew  nothing. I would just ask my mentor everything,

14
00:02:06,120 --> 00:02:11,160
and my mentor knew everything. It  turned out my mentor was Jeff.

15
00:02:11,160 --> 00:02:14,160
It was not the case that everyone at  Google knew everything. It was just the

16
00:02:14,160 --> 00:02:18,600
case that Jeff knew everything because  he had basically written everything.

17
00:02:18,600 --> 00:02:25,760
You're very kind. I think as companies grow, you  kind of go through these phases. When I joined,

18
00:02:25,760 --> 00:02:30,960
we were 25 people, 26 people, something like that.  So you eventually you learned everyone's name,

19
00:02:30,960 --> 00:02:35,000
and even though we were growing, you kept  track of all the people who were joining.

20
00:02:35,000 --> 00:02:39,520
At some point, you lose track of everyone's  name in the company, but you still know everyone

21
00:02:39,520 --> 00:02:46,640
working on software engineering things. Then  you lose track of all the names of people in

22
00:02:46,640 --> 00:02:50,960
the software engineering group, but you at  least know all the different projects that

23
00:02:50,960 --> 00:02:56,160
everyone's working on. Then at some point, the  company gets big enough that you get an email

24
00:02:56,160 --> 00:02:59,840
that Project Platypus is launching on Friday, and  you're like, "What the heck is Project Platypus?"

25
00:03:00,800 --> 00:03:04,240
Usually it's a very good surprise.  You're like, "Wow, Project

26
00:03:04,240 --> 00:03:10,560
Platypus!" I had no idea we were doing that. But I think it is good to keep track of what's

27
00:03:10,560 --> 00:03:14,000
going on in the company, even at a very high  level, even if you don't know every last detail.

28
00:03:14,000 --> 00:03:19,080
And it's good to know lots of people throughout  the company so that you can go ask someone for

29
00:03:19,080 --> 00:03:24,520
more details or figure out who to talk to. With  one level of indirection, you can usually find

30
00:03:24,520 --> 00:03:29,520
the right person in the company if you have a good  network of people that you've built up over time.

31
00:03:29,520 --> 00:03:35,840
How did Google recruit you, by the way? I kind of reached out to them, actually.

32
00:03:35,840 --> 00:03:44,440
And Noam, how did you get recruited? I actually saw Google at a job fair in 1999,

33
00:03:44,440 --> 00:03:49,520
and I assumed that it was already this huge  company, that there was no point in joining,

34
00:03:49,520 --> 00:03:53,080
because everyone I knew used Google.  I guess that was because I was a grad

35
00:03:53,080 --> 00:03:57,400
student at Berkeley at the time. I guess I've  dropped out of grad programs a few times.

36
00:03:59,360 --> 00:04:07,040
It turns out that actually it wasn't really that  large. It turns out that I did not apply in 1999,

37
00:04:07,040 --> 00:04:12,880
but just kind of sent them a resume on a whim in  2000, because I figured it was my favorite search

38
00:04:12,880 --> 00:04:19,399
engine, and figured I should apply to multiple  places for a job. But then it turned out to be

39
00:04:22,000 --> 00:04:27,280
really fun, it looked like a bunch of smart people  doing good stuff. They had this really nice crayon

40
00:04:27,280 --> 00:04:35,400
chart on the wall of the daily number of search  queries that somebody had just been maintaining.

41
00:04:37,200 --> 00:04:42,480
It looked very exponential. I thought, "These guys  are going to be very successful, and it looks like

42
00:04:42,480 --> 00:04:48,520
they have a lot of good problems to work on." So  I was like, "Okay, maybe I'll go work there for

43
00:04:48,520 --> 00:04:53,160
a little while and then have enough money to just  go work on AI for as long as I want after that."

44
00:04:53,160 --> 00:04:58,200
Yeah, yeah. In a way you did that, right? Yeah, it totally worked out

45
00:04:58,200 --> 00:05:02,720
exactly according to plan. You were thinking about AI in 1999?

46
00:05:02,720 --> 00:05:11,320
Yeah, this was like 2000. Yeah, I remember in  grad school, a friend of mine at the time had

47
00:05:11,840 --> 00:05:18,520
told me that his New Year's resolution for 2000  was to live to see the year 3000, and that he

48
00:05:18,520 --> 00:05:24,880
was going to achieve this by inventing AI. I  was like, "Oh, that sounds like a good idea."

49
00:05:28,160 --> 00:05:34,600
I didn't get the idea at the time that you could  go do it at a big company. But I figured, "Hey,

50
00:05:34,600 --> 00:05:39,480
a bunch of people seem to be making a ton of money  at startups. Maybe I'll just make some money,

51
00:05:39,480 --> 00:05:47,880
and then I'll have enough to live on and just  work on AI research for a long time." But yeah,

52
00:05:47,880 --> 00:05:51,800
it actually turned out that Google  was a terrific place to work on AI.

53
00:05:51,800 --> 00:05:56,280
One of the things I like about Google is our  ambition has always been sort of something

54
00:05:56,280 --> 00:06:01,320
that would require pretty advanced AI. Because  I think organizing the world's information and

55
00:06:01,320 --> 00:06:06,400
making it universally accessible and useful,  actually there is a really broad mandate in

56
00:06:06,400 --> 00:06:10,720
there. It's not like the company was going  to do this one little thing and stay doing

57
00:06:10,720 --> 00:06:17,840
that. And also you could see that what we  were doing initially was in that direction,

58
00:06:17,840 --> 00:06:21,880
but you could do so much more in that direction. How has Moore's Law over the last two or three

59
00:06:21,880 --> 00:06:27,360
decades changed the kinds of considerations you  have to take on board when you design new systems,

60
00:06:27,360 --> 00:06:31,640
when you figure out what projects  are feasible? What are still the

61
00:06:31,640 --> 00:06:34,960
limitations? What are things you can now  do that you obviously couldn't do before?

62
00:06:34,960 --> 00:06:40,160
I think of it as actually changing quite a bit  in the last couple of decades. Two decades ago

63
00:06:40,160 --> 00:06:46,000
to one decade ago, it was awesome because  you just wait, and like 18 months later,

64
00:06:46,000 --> 00:06:52,360
you get much faster hardware, and you don't have  to do anything. And then more recently, I feel

65
00:06:52,360 --> 00:06:59,960
like the general-purpose CPU-based machine scaling  has not been as good, like the fabrication process

66
00:06:59,960 --> 00:07:06,920
improvements are now taking three years instead of  every two years. The architectural improvements in

67
00:07:06,920 --> 00:07:15,120
multi-core processors and so on are not giving you  the same boost that we were getting 20 to 10 years

68
00:07:15,120 --> 00:07:22,640
ago. But I think at the same time, we're seeing  much more specialized computational devices,

69
00:07:22,640 --> 00:07:29,040
like machine learning accelerators, TPUs,  and very ML-focused GPUs, more recently,

70
00:07:30,200 --> 00:07:37,240
are making it so that we can actually get really  high performance and good efficiency out of the

71
00:07:37,240 --> 00:07:43,480
more modern kinds of computations we want to run  that are different than a twisty pile of C++ code

72
00:07:43,480 --> 00:07:50,160
trying to run Microsoft Office or something. It feels like the algorithms are following the

73
00:07:50,160 --> 00:07:58,400
hardware. Basically, what's happened is that  at this point, arithmetic is very, very cheap,

74
00:07:58,400 --> 00:08:06,040
and moving data around is comparatively much  more expensive. So pretty much all of deep

75
00:08:06,040 --> 00:08:13,320
learning has taken off roughly because of that.  You can build it out of matrix multiplications

76
00:08:13,320 --> 00:08:22,240
that are N cubed operations and N squared  bytes of data communication basically.

77
00:08:22,240 --> 00:08:27,320
Well, I would say that the pivot  to hardware oriented around that

78
00:08:27,320 --> 00:08:29,440
was an important transition,  because before that, we had CPUs

79
00:08:30,600 --> 00:08:37,720
and GPUs that were not especially well-suited for  deep learning. And then we started to build TPUs

80
00:08:37,720 --> 00:08:43,919
at Google that were really just reduced-precision  linear algebra machines, and then once you

81
00:08:43,919 --> 00:08:52,200
have that then you want to exploit it. It seems like it's all about identifying

82
00:08:52,200 --> 00:08:58,040
opportunity costs. Like, okay, this is something  like Larry Page, I think, used to always say:

83
00:08:58,040 --> 00:09:03,680
"Our second biggest cost is taxes, and our biggest  cost is opportunity costs." If he didn't say that,

84
00:09:03,680 --> 00:09:11,440
then I've been misquoting him for years. But basically it’s like, what is the opportunity

85
00:09:11,440 --> 00:09:18,000
that you have that you're missing out on? In  this case, I guess it was that you've got all

86
00:09:18,000 --> 00:09:24,240
of this chip area, and you're putting a very  small number of arithmetic units on it. Fill

87
00:09:24,240 --> 00:09:30,840
the thing up with arithmetic units! You could have  orders of magnitude more arithmetic getting done.

88
00:09:30,840 --> 00:09:35,240
Now, what else has to change? Okay, the  algorithms and the data flow and everything else.

89
00:09:35,240 --> 00:09:38,280
And, oh, by the way, the arithmetic can  be really low precision, so then you

90
00:09:38,280 --> 00:09:42,840
can squeeze even more multiplier units in. Noam, I want to follow up on what you said,

91
00:09:42,840 --> 00:09:46,480
that the algorithms have been following the  hardware. If you imagine a counterfactual

92
00:09:46,480 --> 00:09:51,960
world where, suppose that the cost of  memory had declined more than arithmetic,

93
00:09:51,960 --> 00:09:58,880
or just invert the dynamic you saw. Okay, data flow is extremely cheap,

94
00:09:58,880 --> 00:10:03,520
and arithmetic is not. What would AI look like today?

95
00:10:04,080 --> 00:10:06,880
You'd have a lot more lookups  into very large memories.

96
00:10:08,280 --> 00:10:17,320
Yeah, it might look more like AI looked like 20  years ago but in the opposite direction. I'm not

97
00:10:17,320 --> 00:10:27,200
sure. I guess I joined Google Brain in 2012. I  left Google for a few years, happened to go back

98
00:10:27,200 --> 00:10:35,920
for lunch to visit my wife, and we happened to sit  down next to Jeff and the early Google Brain team.

99
00:10:35,920 --> 00:10:40,080
I thought, "Wow, that's a smart group of people." I think I said, "You should think about

100
00:10:40,080 --> 00:10:42,720
deep neural nets. We're making  some pretty good progress there."

101
00:10:42,720 --> 00:10:47,480
"That sounds fun." Okay, so I jumped back in…  I wooed him back, it was great.

102
00:10:47,480 --> 00:10:54,160
..to join Jeff, that was like 2012. I seem  to join Google every 12 years: I rejoined

103
00:10:54,160 --> 00:10:59,240
Google in 2000, 2012, and 2024. What's going to happen in 2036?

104
00:10:59,840 --> 00:11:05,640
I don't know. I guess we shall see. What are the trade-offs that you're

105
00:11:05,640 --> 00:11:12,440
considering changing for future versions of TPU to  integrate how you're thinking about algorithms?

106
00:11:13,400 --> 00:11:20,160
I think one general trend is we're getting  better at quantizing or having much more

107
00:11:20,160 --> 00:11:27,080
reduced precision models. We started with TPUv1,  and we weren't even quite sure we could quantize

108
00:11:27,080 --> 00:11:31,920
and model for serving with eight-bit integers.  But we sort of had some early evidence that

109
00:11:31,920 --> 00:11:36,640
seemed like it might be possible. So we're like,  "Great, let's build the whole chip around that."

110
00:11:36,640 --> 00:11:42,400
And then over time, I think you've seen people  able to use much lower precision for training

111
00:11:42,400 --> 00:11:48,960
as well. But also the inference precision has  gone. People are now using INT4 or FP4, which

112
00:11:48,960 --> 00:11:54,280
sounded like, if you said to someone like we're  going to use FP4, like a supercomputing floating

113
00:11:54,280 --> 00:11:59,240
point person 20 years ago, they'd be like, "What?  That's crazy. We like 64 bits in our floats."

114
00:12:00,840 --> 00:12:07,320
Or even below that, some people are quantizing  models to two bits or one bit, and I think

115
00:12:07,320 --> 00:12:10,760
that's a trend that definitely – One bit? Just like a zero-or-one?

116
00:12:10,760 --> 00:12:16,960
Yeah, just a 0-1. And then you have a sign  bit for a group of bits or something.

117
00:12:16,960 --> 00:12:26,480
It really has to be a co-design thing because,  if the algorithm designer doesn't realize

118
00:12:26,480 --> 00:12:34,600
that you can get greatly improved performance,  throughput, with the lower precision, of course,

119
00:12:34,600 --> 00:12:38,960
the algorithm designer is going to say, "Of  course, I don't want low precision. That

120
00:12:38,960 --> 00:12:47,440
introduces risk." And then it adds irritation. Then if you ask the chip designer, "Okay,

121
00:12:48,760 --> 00:12:53,240
what do you want to build?" And then they'll ask  the person who's writing the algorithms today,

122
00:12:53,240 --> 00:12:59,520
who's going to say, "No, I don't like  quantization. It's irritating." So you actually

123
00:12:59,520 --> 00:13:06,440
need to basically see the whole picture and figure  out, "Oh, wait a minute, we can increase our

124
00:13:06,440 --> 00:13:13,360
throughput-to-cost ratio by a lot by quantizing." Then you're like, yes, quantization is irritating,

125
00:13:13,360 --> 00:13:17,440
but your model is going to be three times  faster, so you're going to have to deal.

126
00:13:18,120 --> 00:13:24,320
Through your careers, at various times,  you’ve worked on things that have an

127
00:13:24,320 --> 00:13:31,200
uncanny resemblance to what we're actually  using now for generative AI. In 1990, Jeff,

128
00:13:31,200 --> 00:13:38,840
your senior thesis was about backpropagation.  And in 2007- this is the thing that I didn’t

129
00:13:38,840 --> 00:13:43,600
realise until I was prepping for this episode  – in 2007 you guys trained a two trillion

130
00:13:43,600 --> 00:13:50,560
token N-gram model for language modeling. Just walk me through when you were developing that

131
00:13:50,560 --> 00:13:56,400
model. Was this kind of thing in your head? What  did you think you guys were doing at the time?

132
00:13:58,360 --> 00:14:03,040
Let me start with the undergrad thesis. I got  introduced to neural nets in one section of one

133
00:14:03,040 --> 00:14:08,720
class on parallel computing that I was taking  in my senior year. I needed to do a thesis to

134
00:14:08,720 --> 00:14:12,760
graduate, an honors thesis. So I approached  the professor and I said, "Oh, it'd be really

135
00:14:12,760 --> 00:14:18,360
fun to do something around neural nets." So, he and I decided I would implement

136
00:14:18,360 --> 00:14:24,520
a couple of different ways of parallelizing  backpropagation training for neural nets in 1990.

137
00:14:24,520 --> 00:14:29,600
I called them something funny in my thesis, like  "pattern partitioning" or something. But really,

138
00:14:29,600 --> 00:14:38,320
I implemented a model parallelism and data  parallelism on a 32-processor Hypercube machine.

139
00:14:38,320 --> 00:14:43,360
In one, you split all the examples into  different batches, and every CPU has a copy

140
00:14:43,360 --> 00:14:49,800
of the model. In the other one, you pipeline  a bunch of examples along to processors that

141
00:14:49,800 --> 00:14:57,480
have different parts of the model. I compared  and contrasted them, and it was interesting.

142
00:14:57,480 --> 00:15:00,960
I was really excited about the abstraction  because it felt like neural nets were the right

143
00:15:00,960 --> 00:15:06,600
abstraction. They could solve tiny toy problems  that no other approach could solve at the time.

144
00:15:08,160 --> 00:15:15,120
I thought, naive me, that 32 processors would  be able to train really awesome neural nets.

145
00:15:15,120 --> 00:15:19,240
But it turned out we needed about a million  times more compute before they really started

146
00:15:19,240 --> 00:15:26,640
to work for real problems, but then starting  in the late 2008, 2009, 2010 timeframe,

147
00:15:26,640 --> 00:15:31,960
we started to have enough compute, thanks  to Moore's law, to actually make neural

148
00:15:31,960 --> 00:15:36,960
nets work for real things. That was kind of  when I re-entered, looking at neural nets.

149
00:15:36,960 --> 00:15:41,120
But prior to that, in 2007... Sorry, actually could I ask about this?

150
00:15:41,120 --> 00:15:43,280
Oh yeah, sure. First of all,

151
00:15:43,280 --> 00:15:49,080
unlike other artifacts of academia, it's actually  like four pages, and you can just read it.

152
00:15:49,080 --> 00:15:56,800
It was four pages and then 30 pages of C code. But it's just a well-produced artifact. Tell

153
00:15:56,800 --> 00:16:01,520
me about how the 2007 paper came together. Oh yeah, so that, we had a machine translation

154
00:16:01,520 --> 00:16:07,640
research team at Google led by Franz Och,  who had joined Google maybe a year before,

155
00:16:07,640 --> 00:16:15,760
and a bunch of other people. Every year they  competed in a DARPA contest on translating

156
00:16:15,760 --> 00:16:19,360
a couple of different languages to English, I  think, Chinese to English and Arabic to English.

157
00:16:23,280 --> 00:16:30,920
The Google team had submitted an entry, and the  way this works is you get 500 sentences on Monday,

158
00:16:30,920 --> 00:16:39,160
and you have to submit the answer on Friday. I  saw the results of this, and we'd won the contest

159
00:16:41,640 --> 00:16:46,480
by a pretty substantial margin measured in Bleu  score, which is a measure of translation quality.

160
00:16:46,480 --> 00:16:51,760
So I reached out to Franz, the head of this  winning team. I'm like, "This is great,

161
00:16:51,760 --> 00:16:55,280
when are we going to launch it?" And he's like,  "Oh, well, we can't launch this. It's not really

162
00:16:55,280 --> 00:17:03,520
very practical because it takes 12 hours to  translate a sentence." I'm like, "Well, that

163
00:17:03,520 --> 00:17:12,119
seems like a long time. How could we fix that?" It turned out they'd not really designed it for

164
00:17:12,119 --> 00:17:20,079
high throughput, obviously. It was doing  100,000 disk seeks in a large language

165
00:17:20,079 --> 00:17:29,080
model that they sort of computed statistics  over – I wouldn't say "trained" really – for

166
00:17:29,080 --> 00:17:32,760
each word that it wanted to translate. Obviously, doing 100,000 disk seeks is

167
00:17:32,760 --> 00:17:38,000
not super speedy. But I said, "Okay, well, let's  dive into this." So I spent about two or three

168
00:17:38,000 --> 00:17:45,440
months with them, designing an in-memory  compressed representation of N-gram data.

169
00:17:45,440 --> 00:17:51,480
We were using- an N-gram is basically statistics  for how often every N-word sequence occurs in a

170
00:17:51,480 --> 00:17:57,680
large corpus, so you basically have, in this case,  we had 2 trillion words. Most N-gram models of the

171
00:17:57,680 --> 00:18:03,160
day were using two-grams or maybe three-grams,  but we decided we would use five-grams.

172
00:18:03,160 --> 00:18:08,840
So, how often every five-word sequence occurs in  basically as much of the web as we could process

173
00:18:09,600 --> 00:18:17,200
in that day. Then you have a data structure that  says, "Okay, 'I really like this restaurant'

174
00:18:17,200 --> 00:18:25,080
occurs 17 times in the web, or something. And so I built a data structure that would let

175
00:18:25,080 --> 00:18:31,840
you store all those in memory on 200 machines  and then have sort of a batched API where you

176
00:18:31,840 --> 00:18:37,640
could say, "Here are the 100,000 things I need  to look up in this round for this word," and

177
00:18:37,640 --> 00:18:43,560
we'd give you them all back in parallel.  That enabled us to go from taking a night

178
00:18:43,560 --> 00:18:47,600
to translate a sentence to basically doing  something in 100 milliseconds or something.

179
00:18:47,600 --> 00:18:58,960
There's this list of Jeff Dean facts, like Chuck  Norris facts. For example, that “for Jeff Dean,

180
00:18:58,960 --> 00:19:05,880
NP equals "no problemo."” One of them, it's  funny because now that I hear you say it,

181
00:19:06,400 --> 00:19:11,280
actually, it's kind of true. One of them is, "The  speed of light was 35 miles an hour until Jeff

182
00:19:11,280 --> 00:19:17,200
Dean decided to optimize it over a weekend."  Just going from 12 hours to 100 milliseconds,

183
00:19:18,640 --> 00:19:24,080
I got to do the orders of magnitude there. All of these are very flattering. They're

184
00:19:24,080 --> 00:19:28,720
pretty funny. They're like an April  Fool's joke gone awry by my colleagues.

185
00:19:31,840 --> 00:19:38,320
Obviously, in retrospect, this idea that you  can develop a latent representation of the

186
00:19:38,320 --> 00:19:44,280
entire internet through just considering the  relationships between words is like: yeah,

187
00:19:44,280 --> 00:19:50,440
this is large language models. This is Gemini.  At the time, was it just a translation idea,

188
00:19:50,440 --> 00:19:54,840
or did you see that as being the beginning  of a different kind of paradigm?

189
00:19:54,840 --> 00:20:00,280
I think once we built that for translation,  the serving of large language models started

190
00:20:00,280 --> 00:20:04,480
to be used for other things, like  completion... you start to type,

191
00:20:04,480 --> 00:20:12,160
and it suggests what completions make sense. So it was definitely the start of a lot of uses of

192
00:20:12,160 --> 00:20:17,400
language models in Google. And Noam has worked on  a number of other things at Google, like spelling

193
00:20:17,400 --> 00:20:24,760
correction systems that use language models. That was like 2000, 2001, and I think

194
00:20:24,760 --> 00:20:29,280
it was all in-memory on one machine. Yeah, I think it was one machine. His spelling

195
00:20:29,280 --> 00:20:34,760
correction system he built in 2001 was amazing.  He sent out this demo link to the whole company.

196
00:20:35,720 --> 00:20:39,880
I just tried every butchered spelling  of every few-word query I could get,

197
00:20:39,880 --> 00:20:44,400
like “scrumbled uggs Bundict"— I remember that one, yeah yeah.

198
00:20:44,400 --> 00:20:48,160
—instead of “scrambled eggs benedict”,  and it just nailed it every time.

199
00:20:48,160 --> 00:20:52,480
Yeah, I guess that was language modeling. But at the time, when you were developing

200
00:20:52,480 --> 00:20:58,960
these systems, did you have this sense of, “look,  you make these things more and more sophisticated,

201
00:20:58,960 --> 00:21:03,240
don't consider five words, consider  100 words, 1,000 words, then the

202
00:21:03,240 --> 00:21:08,280
latent representation is intelligence”.  Basically when did that insight hit?

203
00:21:08,280 --> 00:21:14,720
Not really. I don't think I ever felt  like, okay, N-gram models are going to–

204
00:21:15,960 --> 00:21:19,600
–sweep the world– –yeah: “be” artificial intelligence.

205
00:21:20,400 --> 00:21:27,960
I think at the time, a lot of people were excited  about Bayesian networks. That seemed exciting.

206
00:21:27,960 --> 00:21:35,760
Definitely seeing those early neural  language models, both the magic in that,

207
00:21:35,760 --> 00:21:42,880
“okay, this is doing something extremely cool”  and also, it just struck me as the best problem

208
00:21:42,880 --> 00:21:49,920
in the world in that for one, it is very,  very simple to state: give me a probability

209
00:21:49,920 --> 00:21:56,880
distribution over the next word. Also, there's  roughly infinite training data out there. There's

210
00:21:56,880 --> 00:22:05,014
the text of the web; you have trillions of  training examples of unsupervised data.

211
00:22:05,014 --> 00:22:06,400
Yeah, or self-supervised. Self-supervised, yeah.

212
00:22:06,400 --> 00:22:10,240
It's nice because you then have the right  answer, and then you can train on all but

213
00:22:10,240 --> 00:22:17,640
the current word and try to predict the current  word. It's this amazing ability to just learn

214
00:22:17,640 --> 00:22:21,000
from observations of the world. And then it's AI complete. If you

215
00:22:21,000 --> 00:22:25,560
can do a great job of that, then  you can pretty much do anything.

216
00:23:44,720 --> 00:23:49,600
There's this interesting discussion in the history  of science about whether ideas are just in the

217
00:23:49,600 --> 00:23:56,440
air and there's a sort of inevitability to big  ideas, or whether they're sort of plucked out of

218
00:23:56,440 --> 00:24:02,200
some tangential direction. In this case, this way  in which we're laying it out very logically, does

219
00:24:02,200 --> 00:24:07,800
that imply basically, how inevitable does this... It does feel like it's in the air. There were

220
00:24:07,800 --> 00:24:16,120
definitely some, there was like the neural Turing  machine, a bunch of ideas around attention,

221
00:24:17,320 --> 00:24:28,360
like having these key-value stores that could  be useful in neural networks to focus on things.

222
00:24:30,000 --> 00:24:37,520
I think in some sense, it was in the air, and  in some sense, you need some group to go do it.

223
00:24:37,520 --> 00:24:43,880
I like to think of a lot of ideas as being  partially in the air, where there are a few

224
00:24:43,880 --> 00:24:49,280
different, maybe separate research ideas that  one is squinting at when you’re trying to solve

225
00:24:49,280 --> 00:24:54,360
a new problem. You draw on those for some  inspiration, and then there's some aspect

226
00:24:54,360 --> 00:24:58,320
that is not solved, and you need to figure  out how to solve that. The combination of

227
00:24:59,840 --> 00:25:05,440
some morphing of the things that already exist  and some new things lead to some new breakthrough

228
00:25:05,440 --> 00:25:11,360
or new research result that didn't exist before. Are there key moments that stand out to you where

229
00:25:11,360 --> 00:25:18,960
you're looking at a research area, you come up  with this idea, and you have this feeling of,

230
00:25:18,960 --> 00:25:27,200
"Holy shit, I can't believe that worked?"  One thing I remember was in the early days of

231
00:25:27,200 --> 00:25:31,320
the Brain team. We were focused on “let’s see if  we could build some infrastructure that lets us

232
00:25:31,320 --> 00:25:36,520
train really, really big neural nets”. At that  time, we didn't have GPUs in our data centers;

233
00:25:36,520 --> 00:25:41,440
we just had CPUs. But we know how  to make lots of CPUs work together.

234
00:25:41,440 --> 00:25:47,680
So we built a system that enabled us to train  pretty large neural nets through both model

235
00:25:47,680 --> 00:25:55,640
and data parallelism. We had a system for  unsupervised learning on 10 million randomly

236
00:25:55,640 --> 00:26:05,520
selected YouTube frames. It was a spatially local  representation, so it would build up unsupervised

237
00:26:05,520 --> 00:26:11,120
representations based on trying to reconstruct  the thing from the high-level representations.

238
00:26:12,200 --> 00:26:20,480
We got that working and training on 2,000  computers using 16,000 cores. After a little

239
00:26:20,480 --> 00:26:26,560
while, that model was actually able to build a  representation at the highest level where one

240
00:26:26,560 --> 00:26:35,680
neuron would get excited by images of cats. It  had never been told what a cat was, but it had

241
00:26:35,680 --> 00:26:42,520
seen enough examples of them in the training data  of head-on facial views of cats that that neuron

242
00:26:42,520 --> 00:26:47,840
would turn on for that and not for much else. Similarly, you'd have other ones for human faces

243
00:26:47,840 --> 00:26:53,200
and backs of pedestrians, and this kind of  thing. That was kind of cool because it's

244
00:26:53,840 --> 00:26:59,320
from unsupervised learning principles, building  up these really high-level representations. Then

245
00:26:59,320 --> 00:27:06,920
we were able to get very good results on the  supervised ImageNet 20,000 category challenge

246
00:27:06,920 --> 00:27:12,200
that advanced the state of the art by 60% relative  improvement, which was quite good at the time.

247
00:27:13,320 --> 00:27:19,520
That neural net was probably 50x bigger than one  that had been trained previously, and it got good

248
00:27:19,520 --> 00:27:23,560
results. So that sort of said to me, "Hey,  actually scaling up neural nets seems like,

249
00:27:24,520 --> 00:27:28,160
I thought it would be a good idea and it seems  to be, so we should keep pushing on that."

250
00:27:29,760 --> 00:27:35,360
These examples illustrate how these AI systems  fit into what you were just mentioning:

251
00:27:35,360 --> 00:27:42,040
that Google is fundamentally a company that  organizes information. AI, in this context,

252
00:27:42,040 --> 00:27:49,080
is finding relationships between information,  between concepts, to help get ideas to you faster,

253
00:27:49,080 --> 00:27:54,800
information you want to you faster. Now we're moving with current AI models.

254
00:27:54,800 --> 00:27:59,440
Obviously, you can use BERT in Google  Search and you can ask these questions.

255
00:27:59,440 --> 00:28:06,480
They are still good at information retrieval,  but more fundamentally, they can write your

256
00:28:06,480 --> 00:28:15,840
entire code base for you and do actual work,  which goes beyond just information retrieval.

257
00:28:15,840 --> 00:28:21,920
So how are you thinking about that? Is  Google still an information retrieval

258
00:28:21,920 --> 00:28:26,360
company if you're building an AGI?  An AGI can do information retrieval,

259
00:28:26,360 --> 00:28:30,040
but it can do many other things as well. I think we're an "organize the world's

260
00:28:30,040 --> 00:28:34,040
information" company, and that's broader  than information retrieval. Maybe:

261
00:28:34,040 --> 00:28:39,320
“organizing and creating new information  from some guidance you give it”.

262
00:28:39,320 --> 00:28:45,320
"Can you help me write a letter to my veterinarian  about my dog? It's got these symptoms," and it'll

263
00:28:45,320 --> 00:28:50,120
draft that. Or, "Can you feed in this  video, and can you produce a summary of

264
00:28:50,120 --> 00:28:57,840
what's happening in the video every few minutes?" I think our multimodal capabilities are showing

265
00:28:57,840 --> 00:29:02,120
that it's more than just text. It's about  understanding the world in all the different

266
00:29:02,120 --> 00:29:11,680
modalities that information exists in, both  human ones but also non-human-oriented ones,

267
00:29:11,680 --> 00:29:18,760
like weird lidar sensors on autonomous vehicles,  or genomic information, or health information.

268
00:29:18,760 --> 00:29:25,680
And then, how do you extract and transform that  into useful insights for people and make use

269
00:29:25,680 --> 00:29:30,840
of that in helping them do all kinds of things  they want to do? Sometimes it's, "I want to be

270
00:29:30,840 --> 00:29:36,400
entertained by chatting with a chatbot." Sometimes  it's, "I want answers to this really complicated

271
00:29:36,400 --> 00:29:42,840
question, there is no single source to retrieve  from." You need to pull information from 100 web

272
00:29:42,840 --> 00:29:49,520
pages, figure out what's going on, and make an  organized, synthesized version of that data.

273
00:29:50,120 --> 00:29:53,680
Then dealing with multimodal things  or coding-related problems. I think

274
00:29:53,680 --> 00:29:58,040
it's super exciting what these models are  capable of, and they're improving fast, so

275
00:29:58,560 --> 00:30:02,480
I'm excited to see where we go. I am also excited to see where we go.

276
00:30:03,360 --> 00:30:14,960
I think definitely organizing information  is clearly a trillion-dollar opportunity,

277
00:30:14,960 --> 00:30:21,040
but a trillion dollars is not cool anymore.  What's cool is a quadrillion dollars.

278
00:30:22,000 --> 00:30:26,720
Obviously the idea is not to just  pile up some giant pile of money,

279
00:30:26,720 --> 00:30:35,520
but it's to create value in the world, and so much  more value can be created when these systems can

280
00:30:35,520 --> 00:30:41,800
actually go and do something for you, write your  code, or figure out problems that you wouldn't

281
00:30:41,800 --> 00:30:48,640
have been able to figure out yourself. To do that at scale, we're going to have

282
00:30:48,640 --> 00:30:56,840
to be very, very flexible and dynamic as we  improve the capabilities of these models.

283
00:30:56,840 --> 00:31:02,320
Yeah, I'm pretty excited about a lot of  fundamental research questions that come

284
00:31:02,320 --> 00:31:06,480
about because you see something that we're  doing could be substantially improved if

285
00:31:06,480 --> 00:31:12,320
we tried this approach or things in this rough  direction. Maybe that'll work, maybe it won't.

286
00:31:12,320 --> 00:31:18,680
But I also think there's value in seeing what  we could achieve for end-users and then how

287
00:31:18,680 --> 00:31:25,200
can we work backwards from that to actually build  systems that are able to do that. As one example:

288
00:31:25,200 --> 00:31:28,120
organizing information, that  should mean any information

289
00:31:28,120 --> 00:31:32,160
in the world should be usable by anyone,  regardless of what language they speak.

290
00:31:32,160 --> 00:31:38,520
And that I think we've done some amount  of, but it's not nearly the full vision of,

291
00:31:38,520 --> 00:31:43,240
"No matter what language you speak, out of  thousands of languages, we can make any piece

292
00:31:43,240 --> 00:31:51,360
of content available to you and make it usable by  you. Any video could be watched in any language."

293
00:31:51,360 --> 00:31:56,560
I think that would be pretty awesome. We're not  quite there yet, but that's definitely things

294
00:31:56,560 --> 00:32:02,200
I see on the horizon that should be possible. Speaking of different architectures you might try,

295
00:32:02,200 --> 00:32:07,680
I know one thing you're working on right now is  longer context. If you think of Google Search,

296
00:32:07,680 --> 00:32:13,760
it's got the entire index of the internet in  its context, but it's a very shallow search.

297
00:32:13,760 --> 00:32:20,680
And then obviously language models have limited  context right now, but they can really think.

298
00:32:20,680 --> 00:32:25,200
It's like dark magic, in-context learning.  It can really think about what it’s seeing.

299
00:32:26,280 --> 00:32:28,960
How do you think about what it would  be like to merge something like Google

300
00:32:28,960 --> 00:32:33,680
Search and something like in-context learning? Yeah, I'll take a first stab at it because – I've

301
00:32:33,680 --> 00:32:40,160
thought about this for a bit. One of the things  you see with these models is they're quite good,

302
00:32:40,160 --> 00:32:46,920
but they do hallucinate and have factuality issues  sometimes. Part of that is you've trained on, say,

303
00:32:46,920 --> 00:32:51,720
tens of trillions of tokens, and you've  stirred all that together in your tens

304
00:32:51,720 --> 00:32:56,640
or hundreds of billions of parameters. But it's all a bit squishy because you've

305
00:32:58,120 --> 00:33:06,160
churned all these tokens together. The model  has a reasonably clear view of that data,

306
00:33:06,160 --> 00:33:10,240
but it sometimes gets confused and will  give the wrong date for something.

307
00:33:10,920 --> 00:33:15,160
Whereas information in the context  window, in the input of the model,

308
00:33:15,160 --> 00:33:20,040
is really sharp and clear because we have this  really nice attention mechanism in transformers.

309
00:33:20,040 --> 00:33:25,840
The model can pay attention to things, and it  knows the exact text or the exact frames of the

310
00:33:25,840 --> 00:33:34,400
video or audio or whatever that it's processing. Right now, we have models that can deal with

311
00:33:34,400 --> 00:33:41,840
millions of tokens of context, which is quite a  lot. It's hundreds of pages of PDF, or 50 research

312
00:33:41,840 --> 00:33:48,800
papers, or hours of video, or tens of hours  of audio, or some combination of those things,

313
00:33:48,800 --> 00:33:56,040
which is pretty cool. But it would be really nice  if the model could attend to trillions of tokens.

314
00:33:56,040 --> 00:34:00,280
Could it attend to the entire internet and  find the right stuff for you? Could it attend

315
00:34:00,280 --> 00:34:06,480
to all your personal information for you?  I would love a model that has access to all

316
00:34:06,480 --> 00:34:13,120
my emails, all my documents, and all my photos. When I ask it to do something, it can sort of make

317
00:34:13,120 --> 00:34:19,280
use of that, with my permission, to help solve  what it is I'm wanting it to do. But that's going

318
00:34:19,280 --> 00:34:26,480
to be a big computational challenge because the  naive attention algorithm is quadratic. You can

319
00:34:26,480 --> 00:34:31,440
barely make it work on a fair bit of hardware for  millions of tokens, but there's no hope of making

320
00:34:31,440 --> 00:34:36,320
that just naively go to trillions of tokens. So, we need a whole bunch of interesting

321
00:34:36,320 --> 00:34:42,360
algorithmic approximations to what you  would really want: a way for the model

322
00:34:42,360 --> 00:34:50,440
to attend conceptually to lots and lots  more tokens, trillions of tokens. Maybe

323
00:34:50,440 --> 00:34:55,120
we can put all of the Google code base  in context for every Google developer,

324
00:34:55,120 --> 00:35:00,920
all the world's source code in context for any  open-source developer. That would be amazing.

325
00:35:00,920 --> 00:35:11,800
It would be incredible. The beautiful  thing about model parameters is they are

326
00:35:11,800 --> 00:35:17,960
quite memory-efficient at memorizing facts.  You can probably memorize on the order of

327
00:35:18,480 --> 00:35:24,960
one fact or something per model parameter. Whereas if you have some token in context,

328
00:35:24,960 --> 00:35:30,520
there are lots of keys and values at  every layer. It could be a kilobyte,

329
00:35:30,520 --> 00:35:37,480
a megabyte of memory per token. You take a word and you blow it up

330
00:35:37,480 --> 00:35:43,960
to 10 kilobytes or something. Yes. There's actually a lot of

331
00:35:43,960 --> 00:35:50,640
innovation going on around, okay, A, how  do you minimize that? And B, what words

332
00:35:50,640 --> 00:35:56,840
do you need to have there? Are there better  ways of accessing bits of that information?

333
00:35:58,320 --> 00:36:02,920
Jeff seems like the right person to  figure this out. Okay, what does our

334
00:36:02,920 --> 00:36:12,920
memory hierarchy look like from the SRAM all  the way up to data center worldwide level?

335
00:36:12,920 --> 00:36:17,560
I want to talk more about the thing you mentioned  about: look, Google is a company with lots of

336
00:36:17,560 --> 00:36:23,920
code and lots of examples. If you just think  about that one use case and what that implies,

337
00:36:23,920 --> 00:36:29,880
so you've got the Google monorepo. Maybe you  figure out the long context thing, you can put

338
00:36:29,880 --> 00:36:38,520
the whole thing in context, or you fine-tune  on it. Why hasn't this been already done?

339
00:36:39,440 --> 00:36:45,320
You can imagine the amount of code  that Google has proprietary access to,

340
00:36:46,280 --> 00:36:49,760
even if you're just using it internally to make  your developers more efficient and productive.

341
00:36:49,760 --> 00:36:56,600
To be clear, we have actually already done  further training on a Gemini model on our

342
00:36:56,600 --> 00:37:00,880
internal code base for our internal developers.  But that's different than attending to all of

343
00:37:00,880 --> 00:37:06,520
it because it sort of stirs together the  code base into a bunch of parameters, and I

344
00:37:06,520 --> 00:37:15,480
think having it in context makes things clearer. But even the further trained model internally is

345
00:37:15,480 --> 00:37:21,320
incredibly useful. Sundar, I think, has said that  25% of the characters that we're checking into our

346
00:37:21,320 --> 00:37:28,400
code base these days are generated by our AI-based  coding models with kind of human oversight.

347
00:37:28,400 --> 00:37:32,600
How do you imagine, in the next year or two, based  on the capabilities you see around the horizon,

348
00:37:32,600 --> 00:37:39,120
your own personal work? What will it be like to  be a researcher at Google? You have a new idea

349
00:37:39,120 --> 00:37:43,160
or something. With the way in which you're  interacting with these models in a year,

350
00:37:43,160 --> 00:37:47,680
what does that look like? Well, I assume we will have

351
00:37:47,680 --> 00:37:55,160
these models a lot better and hopefully  be able to be much, much more productive.

352
00:37:55,160 --> 00:38:01,920
Yeah, in addition to kind of research-y context,  anytime you're seeing these models used,

353
00:38:01,920 --> 00:38:05,120
I think they're able to make software  developers more productive because they

354
00:38:05,120 --> 00:38:13,520
can kind of take a high-level spec or sentence  description of what you want done and give a

355
00:38:13,520 --> 00:38:20,560
pretty reasonable first cut at that. From  a research perspective, maybe you can say,

356
00:38:20,560 --> 00:38:28,520
"I'd really like you to explore this kind of idea  similar to the one in this paper, but maybe let's

357
00:38:28,520 --> 00:38:33,360
try making it convolutional or something." If you could do that and have the system

358
00:38:33,360 --> 00:38:38,040
automatically generate a bunch of experimental  code, and maybe you look at it and you're like,

359
00:38:38,040 --> 00:38:44,040
"Yeah, that looks good, run that." That  seems like a nice dream direction to go in.

360
00:38:44,040 --> 00:38:48,400
It seems plausible in the next year or two years  that you might make a lot of progress on that.

361
00:38:48,400 --> 00:38:54,360
It seems under-hyped because you could  have literally millions of extra employees,

362
00:38:54,880 --> 00:38:58,800
and you can immediately check their output,  the employees can check each other's output,

363
00:38:58,800 --> 00:39:02,800
hey immediately stream tokens. Sorry, I didn't mean to underhype

364
00:39:02,800 --> 00:39:11,520
it. I think it's super exciting. I just don't  like to hype things that aren't done yet.

365
00:39:14,320 --> 00:39:18,160
I do want to play with this idea more because  it seems like a big deal if you have something

366
00:39:18,160 --> 00:39:22,000
kind of like an autonomous software engineer,  especially from the perspective of a researcher

367
00:39:22,000 --> 00:39:31,600
who's like, "I want to build the system." Okay,  so let's just play with this idea. As somebody who

368
00:39:31,600 --> 00:39:37,480
has worked on developing transformative systems  through your careers, the idea that instead of

369
00:39:37,480 --> 00:39:42,800
having to code something like whatever today's  equivalent of MapReduce is or Tensorflow is,

370
00:39:42,800 --> 00:39:49,960
just like, "Here's how I want a distributed  AI library to look. Write it up for me."

371
00:39:50,520 --> 00:39:53,600
Do you imagine you could be 10x more  productive? 100x more productive?

372
00:39:53,600 --> 00:39:57,680
I was pretty impressed. I think it was on  Reddit that I saw we have a new experimental

373
00:39:57,680 --> 00:40:04,480
coding model that's much better at coding and math  and so on. Someone external tried it, and they

374
00:40:04,480 --> 00:40:13,760
basically prompted it and said, "I'd like you to  implement a SQL processing database system with no

375
00:40:13,760 --> 00:40:22,560
external dependencies, and please do that in C." From what the person said, it actually did

376
00:40:22,560 --> 00:40:27,320
a quite good job. It generated a  SQL parser and a tokenizer and a

377
00:40:27,880 --> 00:40:33,160
query planning system and some storage format  for the data on disk and actually was able to

378
00:40:33,160 --> 00:40:40,880
handle simple queries. From that prompt, which  is like a paragraph of text or something, to

379
00:40:40,880 --> 00:40:48,920
get even an initial cut at that seems like a big  boost in productivity for software developers.

380
00:40:48,920 --> 00:40:55,120
I think you might end up with other kinds of  systems that maybe don't try to do that in a

381
00:40:55,120 --> 00:41:03,400
single semi-interactive, "respond in 40 seconds"  kind of thing but might go off for 10 minutes and

382
00:41:03,400 --> 00:41:08,760
might interrupt you after five minutes saying,  "I've done a lot of this, but now I need to get

383
00:41:08,760 --> 00:41:15,400
some input. Do you care about handling video or  just images or something?" That seems like you'll

384
00:41:15,400 --> 00:41:23,280
need ways of managing the workflow if you have  a lot of these background activities happening.

385
00:41:23,800 --> 00:41:27,640
Can you talk more about that? What  interface do you imagine we might need

386
00:41:27,640 --> 00:41:33,480
if you could literally have millions of employees  you could spin up, hundreds of thousands of

387
00:41:33,480 --> 00:41:39,640
employees you could spin up on command, who  are able to type incredibly fast, and who-

388
00:41:40,680 --> 00:41:47,280
It's almost like you go from 1930s trading of  tickets or something to now modern Jane Street

389
00:41:47,280 --> 00:41:51,800
or something. You need some interface to keep  track of all this that's going on, for the AIs

390
00:41:51,800 --> 00:41:58,240
to integrate into this big monorepo and leverage  their own strengths, for humans to keep track of

391
00:41:58,240 --> 00:42:05,200
what's happening. Basically what is it like to be  Jeff or Noam in three years working day-to-day?

392
00:42:05,200 --> 00:42:11,640
It might be kind of similar to what we have now  because we already have sort of parallelization

393
00:42:11,640 --> 00:42:18,760
as a major issue. We have lots and lots of really,  really brilliant machine learning researchers, and

394
00:42:18,760 --> 00:42:26,480
we want them to all work together and build AI. So actually, the parallelization among people

395
00:42:26,480 --> 00:42:37,440
might be similar to parallelization among  machines. I think definitely it should be good

396
00:42:37,440 --> 00:42:44,440
for things that require a lot of exploration,  like, "Come up with the next breakthrough."

397
00:42:47,400 --> 00:42:52,360
If you have a brilliant idea that is  just certain to work in the ML domain,

398
00:42:52,360 --> 00:43:01,000
then it has a 2% chance of working if  you're brilliant. Mostly these things fail,

399
00:43:01,000 --> 00:43:08,120
but if you try 100 things or 1,000 things or a  million things, then you might hit on something

400
00:43:09,080 --> 00:43:17,480
amazing. We have plenty of compute. Like modern  top labs these days have probably a million times

401
00:43:17,480 --> 00:43:22,800
as much compute as it took to train Transformer. Yeah, actually, so that's a really interesting

402
00:43:22,800 --> 00:43:29,520
idea. Suppose in the world today there are  on the order of 10,000 AI researchers in this

403
00:43:29,520 --> 00:43:32,640
community coming up with a breakthrough- Probably more than that. There were

404
00:43:32,640 --> 00:43:36,760
15,000 at NeurIPS last week. Wow.

405
00:43:36,760 --> 00:43:39,720
100,000, I don't know. Yeah, maybe. Sorry.

406
00:43:39,720 --> 00:43:45,960
No, no, it's good to have the correct order  of magnitude. The odds that this community

407
00:43:45,960 --> 00:43:49,800
every year comes up with a breakthrough on  the scale of a Transformer is, let's say,

408
00:43:49,800 --> 00:43:56,040
10%. Now suppose this community is  a thousand times bigger, and it is,

409
00:43:56,040 --> 00:43:59,880
in some sense, like this sort of parallel search  of better architectures, better techniques.

410
00:43:59,880 --> 00:44:02,280
Do we just get like- A breakthrough a day?

411
00:44:02,280 --> 00:44:08,480
-breakthroughs every year or every day? Maybe. Sounds potentially good.

412
00:44:09,840 --> 00:44:16,440
But does that feel like what ML research is like?  If you are able to try all these experiments…

413
00:44:16,440 --> 00:44:23,400
It's a good question, because I don't know  that folks haven't been doing that as much.

414
00:44:23,960 --> 00:44:29,600
We definitely have lots of great ideas  coming along. Everyone seems to want to

415
00:44:29,600 --> 00:44:35,160
run their experiment at maximum scale,  but I think that's a human problem.

416
00:44:36,360 --> 00:44:42,760
It's very helpful to have a 1/1000th scale  problem and then vet 100,000 ideas on that,

417
00:44:42,760 --> 00:45:53,920
and then scale up the ones that seem promising. So, one thing the world might not be taking

418
00:45:53,920 --> 00:46:01,800
seriously: people are aware that it's  exponentially harder to make a model that's 100x

419
00:46:01,800 --> 00:46:06,080
bigger. It's 100x more compute, right? So people  are worried that it's an exponentially harder

420
00:46:06,080 --> 00:46:11,280
problem to go from Gemini 2 to 3, or so forth. But maybe people aren't aware of this other

421
00:46:11,280 --> 00:46:17,400
trend where Gemini 3 is coming up with all these  different architectural ideas, trying them out,

422
00:46:17,400 --> 00:46:22,320
and you see what works, and you're constantly  coming up with algorithmic progress that makes

423
00:46:22,320 --> 00:46:26,600
training the next one easier and easier.  How far could you take that feedback loop?

424
00:46:26,600 --> 00:46:30,680
I think one thing people should be aware  of is that the improvements from generation

425
00:46:30,680 --> 00:46:37,200
to generation of these models often are  partially driven by hardware and larger scale,

426
00:46:37,200 --> 00:46:43,360
but equally and perhaps even more so driven  by major algorithmic improvements and major

427
00:46:43,360 --> 00:46:48,280
changes in the model architecture, the training  data mix, and so on, that really makes the model

428
00:46:48,280 --> 00:46:54,120
better per flop that is applied to the model,  so I think that's a good realization. Then

429
00:46:54,120 --> 00:46:58,800
I think if we have automated exploration  of ideas, we'll be able to vet a lot more

430
00:46:58,800 --> 00:47:06,760
ideas and bring them into the actual production  training for next generations of these models.

431
00:47:06,760 --> 00:47:10,640
That's going to be really helpful because  that's sort of what we're currently doing

432
00:47:10,640 --> 00:47:16,360
with a lot of brilliant machine learning  researchers: looking at lots of ideas,

433
00:47:16,360 --> 00:47:22,000
winnowing ones that seem to work well at small  scale, seeing if they work well at medium scale,

434
00:47:22,000 --> 00:47:27,640
bringing them into larger scale experiments, and  then settling on adding a whole bunch of new and

435
00:47:27,640 --> 00:47:37,560
interesting things to the final model recipe.  If we can do that 100 times faster through those

436
00:47:37,560 --> 00:47:43,160
machine learning researchers just gently steering  a more automated search process, rather than

437
00:47:43,160 --> 00:47:47,000
hand-babysitting lots of experiments themselves,  that's going to be really, really good.

438
00:47:47,000 --> 00:47:53,160
The one thing that doesn't speed up is experiments  at the largest scale. You still end up doing these

439
00:47:53,160 --> 00:48:00,120
N = 1 experiments. Really, you just try to  put a bunch of brilliant people in the room,

440
00:48:00,120 --> 00:48:05,910
have them stare at the thing, and figure out  why this is working, why this is not working.

441
00:48:05,910 --> 00:48:08,560
For that, more hardware is a good  solution. And better hardware.

442
00:48:08,560 --> 00:48:18,880
Yes, we're counting on you. So, naively, there's this software,

443
00:48:19,640 --> 00:48:24,920
there's this algorithmic side improvement that  future AI can make. There's also the stuff

444
00:48:24,920 --> 00:48:30,520
you're working on. I'll let you describe it. But if you get into a situation where just from

445
00:48:30,520 --> 00:48:37,840
a software level, you can be making better and  better chips in a matter of weeks and months,

446
00:48:37,840 --> 00:48:44,400
and better AIs can presumably do that better,  how does this feedback loop not just end up in,

447
00:48:45,800 --> 00:48:53,680
Gemini 3 taking two years, then Gemini 4 is-  or the equivalent level jump is now six months,

448
00:48:53,680 --> 00:49:01,120
then level five is three months, then one month?  You get to superhuman intelligence much more

449
00:49:01,120 --> 00:49:07,200
rapidly than you might naively think, because  of this software, both on the hardware side and

450
00:49:07,200 --> 00:49:12,640
from the algorithmic side improvements. I've been pretty excited lately about how

451
00:49:12,640 --> 00:49:19,000
we could dramatically speed up the chip design  process. As we were talking earlier, the current

452
00:49:19,000 --> 00:49:24,600
way in which you design a chip takes you roughly  18 months to go from "we should build a chip" to

453
00:49:24,600 --> 00:49:32,600
something that you then hand over to TSMC and then  TSMC takes four months to fab it, and then you get

454
00:49:32,600 --> 00:49:38,040
it back and you put it in your data centers. So that's a pretty lengthy cycle, and the fab

455
00:49:38,040 --> 00:49:45,120
time in there is a pretty small portion of it  today. But if you could make that the dominant

456
00:49:45,120 --> 00:49:54,560
portion, so that instead of taking 12 to 18  months to design the chip with 150 people,

457
00:49:54,560 --> 00:50:00,440
you could shrink that to a few people  with a much more automated search process,

458
00:50:00,440 --> 00:50:05,520
exploring the whole design space of chips and  getting feedback from all aspects of the chip

459
00:50:05,520 --> 00:50:11,160
design process for the kind of choices that the  system is trying to explore at the high level,

460
00:50:11,760 --> 00:50:19,320
then I think you could get perhaps much more  exploration and more rapid design of something

461
00:50:19,320 --> 00:50:23,760
that you actually want to give to a fab. That would be great because you can shrink

462
00:50:23,760 --> 00:50:28,600
fab time, you can shrink the deployment time  by designing the hardware in the right way,

463
00:50:28,600 --> 00:50:35,280
so that you just get the chips back and you  just plug them into some system. And that

464
00:50:35,280 --> 00:50:41,960
will then enable a lot more specialization, it  will enable a shorter timeframe for the hardware

465
00:50:41,960 --> 00:50:45,960
design so that you don't have to look out quite  as far into what kind of ML algorithms would be

466
00:50:45,960 --> 00:50:51,960
interesting. Instead, it's like you're looking  at six to nine months from now, what should it

467
00:50:51,960 --> 00:50:57,320
be? Rather than two, two and a half years. That would be pretty cool. I do think that

468
00:50:57,880 --> 00:51:02,400
fabrication time, if that's in your inner  loop of improvement, you're going to like...

469
00:51:02,400 --> 00:51:05,040
How long is it?  The leading edge nodes,

470
00:51:05,040 --> 00:51:09,560
unfortunately, are taking longer and longer  because they have more metal layers than previous,

471
00:51:09,560 --> 00:51:15,280
older nodes. So that tends to make it  take anywhere from three to five months.

472
00:51:15,280 --> 00:51:20,040
Okay, but that's how long training runs take  anyways, right? So you could potentially do

473
00:51:20,040 --> 00:51:22,200
both at the same time. Potentially.

474
00:51:22,200 --> 00:51:24,880
Okay, so I guess you can't get sooner  than three to five months. But the idea

475
00:51:24,880 --> 00:51:30,440
that you could get- but also, yeah, you're  rapidly developing new algorithmic ideas.

476
00:51:31,080 --> 00:51:33,560
That can move fast. That can move fast, that can run on

477
00:51:33,560 --> 00:51:41,120
existing chips and explore lots of cool ideas. So, isn't that a situation in which you're... I

478
00:51:41,120 --> 00:51:45,920
think people sort of expect like, ah,  there's going to be a sigmoid. Again,

479
00:51:45,920 --> 00:51:50,160
this is not a sure thing. But just like, is this  a possibility? The idea that you have sort of an

480
00:51:50,160 --> 00:51:56,400
explosion of capabilities very rapidly towards  the tail end of human intelligence that gets

481
00:51:57,640 --> 00:51:59,960
smarter and smarter at a  more and more rapid rate?

482
00:51:59,960 --> 00:52:03,080
Quite possibly. Yeah. I like to think of it

483
00:52:03,080 --> 00:52:11,080
like this. Right now, we have models that can take  a pretty complicated problem and can break it down

484
00:52:11,080 --> 00:52:16,600
internally in the model into a bunch of steps,  can sort of puzzle together the solutions for

485
00:52:16,600 --> 00:52:21,320
those steps, and can often give you a solution  to the entire problem that you're asking.

486
00:52:22,000 --> 00:52:29,040
But it isn't super reliable, and it's good at  breaking things down into five to ten steps,

487
00:52:29,040 --> 00:52:35,400
not 100 to 1,000 steps. So if you could go  from, yeah, 80% of the time it can give you

488
00:52:35,400 --> 00:52:42,240
a perfect answer to something that's ten steps  long to something that 90% of the time can give

489
00:52:42,240 --> 00:52:48,840
you a perfect answer to something that's 100 to  1,000 steps of sub-problem long, that would be

490
00:52:48,840 --> 00:52:54,360
an amazing improvement in the capability of these  models. We're not there yet, but I think that's

491
00:52:54,360 --> 00:52:59,080
what we're aspirationally trying to get to. We don't need new hardware for that,

492
00:52:59,080 --> 00:53:06,320
but we'll take it. Never look new hardware in the mouth.

493
00:53:06,320 --> 00:53:16,240
One of the big areas of improvement in  the near future is inference time compute,

494
00:53:16,240 --> 00:53:24,200
applying more compute at inference time. I  guess the way I like to describe it is that

495
00:53:26,800 --> 00:53:37,560
even a giant language model, even if you’re doing  a trillion operations per token, which is more

496
00:53:37,560 --> 00:53:44,160
than most people are doing these days, operations  cost something like 10 to the negative $18. And

497
00:53:44,160 --> 00:53:51,920
so you're getting a million tokens to the dollar. I mean compare that to a relatively cheap pastime:

498
00:53:52,840 --> 00:53:56,600
you go out and buy a paper book and  read it, you're paying 10,000 tokens

499
00:53:56,600 --> 00:54:05,440
to the dollar. Talking to a language model is  like 100 times cheaper than reading a paperback.

500
00:54:05,440 --> 00:54:11,640
So there is a huge amount of headroom there  to say, okay, if we can make this thing more

501
00:54:11,640 --> 00:54:19,000
expensive but smarter, because we're  100x cheaper than reading a paperback,

502
00:54:19,000 --> 00:54:24,680
we're 10,000 times cheaper than talking  to a customer support agent, or a million

503
00:54:24,680 --> 00:54:30,960
times or more cheaper than hiring a software  engineer or talking to your doctor or lawyer.

504
00:54:30,960 --> 00:54:41,600
Can we add computation and make it smarter? I think a lot of the takeoff that we're going

505
00:54:41,600 --> 00:54:50,920
to see in the very near future is of this form.  We've been exploiting and improving pre-training

506
00:54:50,920 --> 00:54:56,400
a lot in the past, and post-training, and those  things will continue to improve. But taking

507
00:54:56,400 --> 00:55:04,520
advantage of "think harder" at inference  time is just going to be an explosion.

508
00:55:04,520 --> 00:55:10,320
Yeah, and an aspect of inference time is I think  you want the system to be actively exploring a

509
00:55:10,320 --> 00:55:14,840
bunch of different potential solutions.  Maybe it does some searches on its own,

510
00:55:14,840 --> 00:55:19,080
gets some information back, consumes that  information, and figures out, oh, now I

511
00:55:19,080 --> 00:55:26,520
would really like to know more about this thing.  So now it iteratively explores how to best solve

512
00:55:26,520 --> 00:55:33,720
the high-level problem you pose to this system. And I think having a dial where you can make the

513
00:55:33,720 --> 00:55:39,840
model give you better answers with more inference  time compute seems like we have a bunch of

514
00:55:39,840 --> 00:55:45,400
techniques now that can kind of do that. The more  you crank up the dial, the more it costs you in

515
00:55:45,400 --> 00:55:51,960
terms of compute, but the better the answers get. That seems like a nice trade-off to have,

516
00:55:51,960 --> 00:55:56,480
because sometimes you want to think really  hard because it's a super important problem.

517
00:55:56,480 --> 00:56:01,680
Sometimes you probably don't want to spend  enormous amounts of compute to compute “what's

518
00:56:01,680 --> 00:56:07,520
the answer to one plus one”. Maybe the system – Shouldn’t decide to come up with new

519
00:56:07,520 --> 00:56:09,760
axioms of set theory or whatever! – should decide to use a calculator

520
00:56:09,760 --> 00:56:15,320
tool instead of a very large language model. Interesting. So are there any impediments

521
00:56:15,320 --> 00:56:21,080
to taking inference time, like having  some way in which you can just linearly

522
00:56:21,080 --> 00:56:26,360
scale up inference time compute? Or is this  basically a problem that's sort of solved,

523
00:56:26,360 --> 00:56:32,360
and we know how to throw 100x compute, 1000x  compute, and get correspondingly better results?

524
00:56:33,120 --> 00:56:40,640
We're working out the algorithms as we speak. So  I believe we'll see better and better solutions to

525
00:56:40,640 --> 00:56:48,880
this as these many more than 10,000 researchers  are hacking at it, many of them at Google.

526
00:56:48,880 --> 00:56:55,080
I think we do see some examples in our own  experimental work of things where if you apply

527
00:56:55,080 --> 00:57:02,480
more inference time compute, the answers are  better than if you just apply 10x, you can get

528
00:57:02,480 --> 00:57:08,400
better answers than x amount of computed inference  time. And that seems useful and important.

529
00:57:08,400 --> 00:57:15,560
But I think what we would like is when you apply  10x to get even a bigger improvement in the

530
00:57:15,560 --> 00:57:20,520
quality of the answers than we're getting today.  And so that's about designing new algorithms,

531
00:57:20,520 --> 00:57:27,000
trying new approaches, figuring out how best to  spend that 10x instead of x to improve things.

532
00:57:27,000 --> 00:57:30,000
Does it look more like search, or does  it look more like just keeping going in

533
00:57:30,000 --> 00:57:37,440
the linear direction for a longer time? I really like Rich Sutton's paper that he

534
00:57:37,440 --> 00:57:42,480
wrote about the Bitter Lesson and the Bitter  Lesson effectively is this nice one-page paper

535
00:57:42,480 --> 00:57:49,800
but the essence of it is you can try lots of  approaches, but the two techniques that are

536
00:57:49,800 --> 00:57:56,760
incredibly effective are learning and search. You can apply and scale those algorithmically

537
00:57:56,760 --> 00:58:01,400
or computationally, and you often will  then get better results than any other

538
00:58:01,400 --> 00:58:04,680
kind of approach you can apply it to  a pretty broad variety of problems.

539
00:58:06,400 --> 00:58:11,680
Search has got to be part of the solution to  spending more inference time. Maybe you explore

540
00:58:11,680 --> 00:58:16,240
a few different ways of solving this problem,  and that one didn't work, but this one worked

541
00:58:16,240 --> 00:58:22,600
better. I'm going to explore that a bit more. How does this change your plans for future data

542
00:58:22,600 --> 00:58:31,480
center planning and so forth? Where can this  kind of search be done asynchronously? Does

543
00:58:31,480 --> 00:58:35,960
it have to be online, offline? How  does that change how big of a campus

544
00:58:35,960 --> 00:58:43,400
you need and those kinds of considerations? One general trend is it's clear that inference

545
00:58:43,400 --> 00:58:48,160
time compute, you have a model that's pretty much  already trained and you want to do inference on,

546
00:58:48,160 --> 00:58:53,840
it is going to be a growing and important  class of computation. Maybe you want to

547
00:58:53,840 --> 00:59:01,640
specialize hardware more around that. Actually, the first TPU was specialized for

548
00:59:01,640 --> 00:59:06,320
inference and wasn't really designed for training.  Then subsequent TPUs were really designed more

549
00:59:06,320 --> 00:59:14,200
around training and also for inference. But it may be that when you have something

550
00:59:14,200 --> 00:59:18,080
where you really want to crank up the amount of  compute you use at inference time, that even more

551
00:59:18,080 --> 00:59:21,840
specialized solutions will make a lot of sense. Does that mean you can accommodate more

552
00:59:21,840 --> 00:59:25,040
asynchronous training?  Training? Or inference?

553
00:59:25,040 --> 00:59:29,600
Or just you can have the different data  centers don't need to talk to each other,

554
00:59:29,600 --> 00:59:37,080
you can just have them do a bunch of... I like to think of it as, is the inference that

555
00:59:37,080 --> 00:59:42,360
you're trying to do latency-sensitive? Like a user  is actively waiting for it, or is it a background

556
00:59:42,360 --> 00:59:48,760
thing? Maybe I have some inference tasks that I'm  trying to run over a whole batch of data, but it's

557
00:59:48,760 --> 00:59:54,400
not for a particular user. It's just I want to  run inference on it and extract some information.

558
00:59:54,400 --> 00:59:59,000
There's probably a bunch of things that we  don't really have very much of right now,

559
00:59:59,000 --> 01:00:04,040
but you're seeing inklings of it in our  deep research tool that we just released,

560
01:00:04,920 --> 01:00:11,160
like a week ago. You can give it a pretty  complicated, high-level task like, "Hey,

561
01:00:11,160 --> 01:00:15,960
can you go off and research the history of  renewable energy and all the trends in costs for

562
01:00:15,960 --> 01:00:20,760
wind and solar and other kinds of techniques, and  put it in a table and give me a full eight-page

563
01:00:20,760 --> 01:00:26,040
report?" And it will come back with an eight-page  report with like 50 entries in the bibliography.

564
01:00:26,040 --> 01:00:30,120
It's pretty remarkable. But you're not  actively waiting for that for one second.

565
01:00:30,120 --> 01:00:35,640
It takes like a minute or two to go do that. And I think there's going to be a fair bit of

566
01:00:35,640 --> 01:00:41,960
that kind of compute, and that's the kind of thing  where you have some UI questions around. Okay,

567
01:00:41,960 --> 01:00:47,240
if you're going to have a user with 20 of these  kind of asynchronous tasks in the background

568
01:00:47,240 --> 01:00:52,440
happening, and maybe each one of them needs  to get more information from the user, like,

569
01:00:52,440 --> 01:00:59,760
"I found your flights to Berlin, but there's no  non-stop ones. Are you okay with a non-stop one?"

570
01:00:59,760 --> 01:01:05,360
How does that flow work when you kind of need a  bit more information, and then you want to put

571
01:01:05,360 --> 01:01:09,280
it back in the background for it to continue  doing, you know, finding the hotels in Berlin

572
01:01:09,280 --> 01:01:15,280
or whatever? I think it's going to be pretty  interesting, and inference will be useful.

573
01:01:15,280 --> 01:01:20,240
Inference will be useful. There's also a compute  efficiency in inference that you don't have in

574
01:01:20,240 --> 01:01:28,160
training. In general, transformers can use the  sequence length as a batch during training,

575
01:01:28,160 --> 01:01:33,000
but they can't really in inference, because  when you're generating one token at a time,

576
01:01:33,000 --> 01:01:41,200
so there may be different hardware and  inference algorithms that we design for the

577
01:01:41,200 --> 01:01:46,400
purposes of being efficient at inference. Yeah, as a good example of an algorithmic

578
01:01:46,400 --> 01:01:53,400
improvement is the use of drafter models. So you  have a really small language model that you do

579
01:01:53,400 --> 01:01:58,560
one token at a time when you're decoding,  and it predicts four tokens. Then you give

580
01:01:58,560 --> 01:02:03,360
that to the big model and you say, "Okay,  here are the four tokens the little model

581
01:02:03,360 --> 01:02:08,680
came up with. Check which ones you agree with." If you agree with the first three, then you just

582
01:02:08,680 --> 01:02:16,160
advance. Then you've basically been able to do a  four-token width parallel computation instead of

583
01:02:16,160 --> 01:02:21,280
a one-token width computation in the big model.  Those are the kinds of things that people are

584
01:02:21,280 --> 01:02:27,920
looking at to improve inference efficiency, so you  don't have this single-token decode bottleneck.

585
01:02:27,920 --> 01:02:30,626
Right, basically the big model's  being used as a verifier.

586
01:02:30,626 --> 01:02:32,560
Right, “can you verify”, yeah. [inaudible] generator

587
01:02:32,560 --> 01:02:35,520
and verification you can do. Right. "Hello, how are you?" That sounds

588
01:02:35,520 --> 01:02:42,840
great to me. I'm going to advance past that. So, a big discussion has been about how we're

589
01:02:42,840 --> 01:02:48,080
already tapping out nuclear power plants in  terms of delivering power into one single

590
01:02:48,080 --> 01:02:57,080
campus. Do we have to have just two gigawatts  in one place, five gigawatts in one place,

591
01:02:57,080 --> 01:03:04,000
or can it be more distributed and still  be able to train a model? Does this new

592
01:03:04,000 --> 01:03:10,240
regime of inference scaling make different  considerations there plausible? How are you

593
01:03:10,240 --> 01:03:15,880
thinking about multi-data center training now? We're already doing it. We're pro multi-data

594
01:03:15,880 --> 01:03:20,480
center training. I think in the Gemini  1.5 tech report, we said we used multiple

595
01:03:20,480 --> 01:03:25,760
metro areas and trained with some of the  compute in each place. And then a pretty

596
01:03:27,280 --> 01:03:31,920
long latency but high bandwidth connection  between those data centers, and that works fine.

597
01:03:33,160 --> 01:03:37,920
Training is kind of interesting because  each step in a training process is usually,

598
01:03:37,920 --> 01:03:41,520
for a large model, is usually a few  seconds or something, at least. So,

599
01:03:41,520 --> 01:03:47,160
the latency of it being 50 milliseconds  away doesn't matter that much.

600
01:03:47,160 --> 01:03:49,200
Just the bandwidth. Yeah, just bandwidth.

601
01:03:49,200 --> 01:03:55,880
As long as you can sync all of the parameters  of the model across the different data centers

602
01:03:55,880 --> 01:04:01,720
and then accumulate all the gradients, in the  time it takes to do one step, you're pretty good.

603
01:04:01,720 --> 01:04:07,600
And then we have a bunch of work, even from  early Brain days, when we were using CPU

604
01:04:07,600 --> 01:04:14,560
machines and they were really slow. We needed to  do asynchronous training to help scale, where each

605
01:04:14,560 --> 01:04:20,240
copy of the model would do some local computation,  send gradient updates to a centralized system,

606
01:04:20,240 --> 01:04:25,720
and then apply them asynchronously. Another copy  of the model would be doing the same thing.

607
01:04:26,680 --> 01:04:32,080
It makes your model parameters wiggle around  a bit, and it makes people uncomfortable with

608
01:04:32,080 --> 01:04:35,960
the theoretical guarantees, but it  actually seems to work in practice.

609
01:04:35,960 --> 01:04:41,840
It was so pleasant to go from asynchronous  to synchronous because your experiments are

610
01:04:41,840 --> 01:04:50,480
now replicable, rather than your results  depend on whether there was a web crawler

611
01:04:50,480 --> 01:04:59,240
running on the same machine. So, I am  so much happier running on TPU pods.

612
01:04:59,240 --> 01:05:01,429
I love asynchrony. It just  lets you scale so much more.

613
01:05:01,429 --> 01:05:07,640
With these two iPhones and an Xbox or whatever. Yeah, what if we could give you asynchronous but

614
01:05:07,640 --> 01:05:10,280
replicable results? Ooh.

615
01:05:10,280 --> 01:05:17,400
So, one way to do that is you effectively record  the sequence of operations, like which gradient

616
01:05:17,400 --> 01:05:22,680
update happened and when and on which batch of  data. You don't necessarily record the actual

617
01:05:22,680 --> 01:05:29,600
gradient update in a log or something, but you  could replay that log of operations so that you

618
01:05:29,600 --> 01:05:37,040
get repeatability. Then I think you'd be happy. Possibly. At least you could debug what happened,

619
01:05:37,040 --> 01:05:43,800
but you wouldn't be able to necessarily compare  two training runs. Because, okay, I made one

620
01:05:43,800 --> 01:05:50,150
change in the hyperparameter, but also I had a- Web crawler.

621
01:05:50,150 --> 01:05:59,120
-web crawler messing up, and there were a lot of  people streaming the Super Bowl at the same time.

622
01:05:59,120 --> 01:06:06,120
The thing that led us to go from asynchronous  training on CPUs to fully synchronous training

623
01:06:06,120 --> 01:06:11,240
is the fact that we have these super  fast TPU hardware chips and pods,

624
01:06:11,240 --> 01:06:15,960
which have incredible amounts of bandwidth between  the chips in a pod. Then, scaling beyond that,

625
01:06:15,960 --> 01:06:21,200
we have really good data center networks and  even cross-metro area networks that enable

626
01:06:21,200 --> 01:06:26,920
us to scale to many, many pods in multiple  metro areas for our largest training runs.

627
01:06:26,920 --> 01:06:30,720
We can do that fully synchronously. As Noam said, as long as the gradient

628
01:06:30,720 --> 01:06:36,920
accumulation and communication of the parameters  across metro areas happens fast enough relative

629
01:06:36,920 --> 01:06:44,560
to the step time, you're golden. You don't  really care. But I think as you scale up,

630
01:06:44,560 --> 01:06:51,480
there may be a push to have a bit more asynchrony  in our systems than we have now because we can

631
01:06:51,480 --> 01:06:58,080
make it work, our ML researchers have been really  happy how far we've been able to push synchronous

632
01:06:58,080 --> 01:07:03,560
training because it is an easier mental model to  understand. You just have your algorithm sort of

633
01:07:03,560 --> 01:07:08,400
fighting you, rather than the asynchrony  and the algorithm kind of battling you.

634
01:07:08,400 --> 01:07:15,880
As you scale up, there are more things  fighting you. That's the problem with scaling,

635
01:07:15,880 --> 01:07:23,440
that you don't always know what it is that's  fighting you. Is it the fact that you've pushed

636
01:07:23,440 --> 01:07:27,760
quantization a little too far in some  place or another? Or is it your data?

637
01:07:28,600 --> 01:07:36,080
Maybe it's your adversarial machine MUQQ17 that  is setting the seventh bit of your exponent

638
01:07:36,080 --> 01:07:40,200
and all your gradients or something. Right. And all of these things just make

639
01:07:40,200 --> 01:07:45,240
the model slightly worse, so you don't  even know that the thing is going on.

640
01:07:46,600 --> 01:07:50,280
That's actually a bit of a problem with neural  nets, is they're so tolerant of noise. You can

641
01:07:50,280 --> 01:07:55,560
have things set up kind of wrong in a  lot of ways, and they just figure out

642
01:07:55,560 --> 01:08:00,040
ways to work around that or learn. You could have bugs in your code. Most

643
01:08:00,040 --> 01:08:05,480
of the time that does nothing. Some of the time it  makes your model worse. Some of the time it makes

644
01:08:05,480 --> 01:08:10,840
your model better. Then you discover something  new because you never tried this bug at scale

645
01:08:10,840 --> 01:08:19,160
before because you didn't have the budget for it. What practically does it look like to debug or

646
01:08:19,160 --> 01:08:23,960
decode? You've got these things, some of which are  making the model better, some of which are making

647
01:08:23,960 --> 01:08:32,439
it worse. When you go into work tomorrow, how do  you figure out what the most salient inputs are?

648
01:08:33,319 --> 01:08:42,160
At small scale, you do lots of experiments.  There's one part of the research that involves,

649
01:08:42,160 --> 01:08:48,120
okay, I want to invent these improvements  or breakthroughs in isolation. In which

650
01:08:48,120 --> 01:08:54,359
case you want a nice simple code base that  you can fork and hack, and some baselines.

651
01:08:55,840 --> 01:09:02,120
My dream is I wake up in the morning,  come up with an idea, hack it up in a day,

652
01:09:02,120 --> 01:09:08,160
run some experiments, get some initial results  in a day. Like okay this looks promising, these

653
01:09:08,160 --> 01:09:16,000
things worked, and these things didn't work. I think that is very achievable because-

654
01:09:16,000 --> 01:09:18,080
At small scale. At small scale, as long as you

655
01:09:18,080 --> 01:09:23,319
keep a nice experimental code base.  Maybe an experiment takes an hour

656
01:09:23,319 --> 01:09:30,000
to run or two hours, not two weeks.  It’s great. So there's that part of the research,

657
01:09:30,000 --> 01:09:35,120
and then there's some amount of scaling up. Then  you have the part which is integrating, where

658
01:09:35,120 --> 01:09:41,800
you want to stack all the improvements on top of  each other and see if they work at large scale,

659
01:09:41,800 --> 01:09:45,720
and see if they work all in conjunction. Right, how do they interact? Right,

660
01:09:45,720 --> 01:09:50,800
you think maybe they're independent, but actually  maybe there's some funny interaction between

661
01:09:50,800 --> 01:09:59,400
improving the way in which we handle video  data input and the way in which we update

662
01:09:59,400 --> 01:10:04,120
the model parameters. Maybe that interacts  more for video data than some other thing.

663
01:10:04,840 --> 01:10:09,400
There are all kinds of interactions that can  happen that you maybe don't anticipate. So

664
01:10:09,400 --> 01:10:13,400
you want to run these experiments where you're  then putting a bunch of things together and then

665
01:10:13,400 --> 01:10:19,720
periodically making sure that all the things  you think are good are good together. If not,

666
01:10:19,720 --> 01:10:26,640
understanding why they're not playing nicely. Two questions. One, how often does it end up

667
01:10:26,640 --> 01:10:32,000
being the case that things don't stack up  well together? Is it like a rare thing or

668
01:10:32,000 --> 01:10:36,720
does it happen all the time? It happens 50% of the time.

669
01:10:36,720 --> 01:10:42,160
Yeah, I mean, I think most things you don't  even try to stack because the initial experiment

670
01:10:42,160 --> 01:10:45,960
didn't work that well, or it showed results  that aren't that promising relative to the

671
01:10:45,960 --> 01:10:52,080
baseline. Then you sort of take those things  and you try to scale them up individually.

672
01:10:52,080 --> 01:10:56,160
Then you're like, "Oh yeah, these ones seem  really promising." So I'm going to now include

673
01:10:56,160 --> 01:11:02,240
them in something that I'm going to now bundle  together and try to advance and combine with

674
01:11:02,240 --> 01:11:05,920
other things that seem promising. Then you  run the experiments and then you're like,

675
01:11:05,920 --> 01:11:09,560
"Oh, well, they didn't really work  that well. Let's try to debug why."

676
01:11:09,560 --> 01:11:15,760
And then there are trade offs, because you want to  keep your integrated system as clean as you can,

677
01:11:15,760 --> 01:11:18,300
because complexity – Codebase-wise.

678
01:11:18,300 --> 01:11:24,080
– yeah codebase and algorithmically.  Complexity hurts, complexity makes

679
01:11:24,080 --> 01:11:28,560
things slower, introduces more risk. And then at the same time you want it

680
01:11:28,560 --> 01:11:36,680
to be as good as possible. And of course, every  individual researcher wants his inventions to go

681
01:11:36,680 --> 01:11:46,280
into it. So there are definitely challenges there,  but we've been working together quite well.

682
01:12:40,880 --> 01:12:47,680
Okay, so then going back to the whole dynamic “you  find better and better algorithmic improvements

683
01:12:47,680 --> 01:12:52,320
and the models get better and better over time”,  even if you take the hardware part out of it.

684
01:12:52,320 --> 01:12:57,680
Should the world be thinking more about, and  should you guys be thinking more about this?

685
01:12:57,680 --> 01:13:03,080
There's one world where AI is a thing that takes  two decades to slowly get better over time and

686
01:13:03,080 --> 01:13:09,560
you can sort of refine things over. If you've kind  of messed something up, you fix it, and it's not

687
01:13:09,560 --> 01:13:13,480
that big a deal, right? It's like not that much  better than the previous version you released.

688
01:13:13,480 --> 01:13:21,800
There's another world where you have this big  feedback loop, which means that the two years

689
01:13:21,800 --> 01:13:26,320
between Gemini 4 and Gemini 5 are the most  important years in human history. Because

690
01:13:26,320 --> 01:13:33,120
you go from a pretty good ML researcher  to superhuman intelligence because of

691
01:13:33,120 --> 01:13:37,240
this feedback loop. To the extent that you  think that the second world is plausible,

692
01:13:37,240 --> 01:13:42,840
how does that change how you sort of approach  these greater and greater levels of intelligence?

693
01:13:42,840 --> 01:13:48,080
I've stopped cleaning my garage because  I'm waiting for the robots. So probably

694
01:13:48,080 --> 01:13:52,960
I'm more in the second camp of what we're  going to see, a lot of acceleration.

695
01:13:52,960 --> 01:13:57,800
Yeah, I mean, I think it's super important to  understand what's going on and what the trends

696
01:13:57,800 --> 01:14:02,440
are. And I think right now the trends are the  models are getting substantially better generation

697
01:14:02,440 --> 01:14:08,680
over generation. I don't see that slowing  down in the next few generations probably.

698
01:14:08,680 --> 01:14:13,200
So that means the models say two to three  generations from now are going to be capable

699
01:14:13,200 --> 01:14:18,760
of... Let's go back to the example of breaking  down a simple task into 10 sub pieces and doing

700
01:14:18,760 --> 01:14:25,080
it 80% of the time, to something that can  break down a task, a very high level task,

701
01:14:25,080 --> 01:14:29,920
into 100 or 1,000 pieces and get that  right 90% of the time. That's a major,

702
01:14:29,920 --> 01:14:35,880
major step up in what the models are capable of. So I think it's important for people to understand

703
01:14:37,400 --> 01:14:42,400
what is happening in the progress in the field.  And then those models are going to be applied

704
01:14:42,400 --> 01:14:49,040
in a bunch of different domains. I think it's  really good to make sure that we, as a society,

705
01:14:49,040 --> 01:14:56,040
get the maximal benefits from what these models  can do to improve things. I'm super excited

706
01:14:56,040 --> 01:15:03,080
about areas like education and healthcare,  making information accessible to all people.

707
01:15:03,720 --> 01:15:09,840
But we also realize that they could be used for  misinformation, they could be used for automated

708
01:15:09,840 --> 01:15:16,720
hacking of computer systems, and we want to put  as many safeguards and mitigations and understand

709
01:15:16,720 --> 01:15:24,960
the capabilities of the models in place as we  can. I think Google as a whole has a really

710
01:15:25,520 --> 01:15:31,200
good view to how we should approach this. Our  Responsible AI principles actually are a pretty

711
01:15:31,200 --> 01:15:37,720
nice framework for how to think about trade offs  of making better and better AI systems available

712
01:15:37,720 --> 01:15:43,320
in different contexts and settings, while also  sort of making sure that we're doing the right

713
01:15:43,320 --> 01:15:49,760
thing in terms of making sure they're safe and  not saying toxic things and things like that.

714
01:15:49,760 --> 01:15:54,000
I guess the thing that stands out to me, if  you were zooming out and looking at this period

715
01:15:54,000 --> 01:16:00,800
of human history, if we're in the world where,  look, if you do post-training on Gemini 3 badly,

716
01:16:00,800 --> 01:16:06,760
it can do some misinformation – but then you  fix the post training. It's a bad mistake,

717
01:16:06,760 --> 01:16:08,680
but it's a fixable mistake, right? Right.

718
01:16:08,680 --> 01:16:17,360
Whereas if you have this feedback loop dynamic,  which is a possibility, then the mistake of the

719
01:16:17,360 --> 01:16:26,640
thing that catapults this intelligence  explosion is misaligned, is not trying to

720
01:16:26,640 --> 01:16:31,680
write the code you think it's trying to write, and  [instead] optimizing for some other objective.

721
01:16:31,680 --> 01:16:37,160
And on the other end of this very rapid process  that lasts a couple of years, maybe less,

722
01:16:37,160 --> 01:16:42,760
you have things that are approaching Jeff Dean  or beyond level, or Noam Shazeer or beyond

723
01:16:42,760 --> 01:16:52,280
level. And then you have millions of copies  of Jeff Dean level programmers, and- anyways,

724
01:16:52,280 --> 01:17:01,200
that seems like a harder to recover mistake. As these systems do get more powerful,

725
01:17:01,200 --> 01:17:08,160
you have to be more and more careful. One thing I would say is, there are extreme

726
01:17:08,160 --> 01:17:13,800
views on either end. There's, "Oh my goodness,  these systems are going to be so much better

727
01:17:13,800 --> 01:17:18,040
than humans at all things, and we're going  to be kind of overwhelmed." And then there's,

728
01:17:18,680 --> 01:17:21,800
"These systems are going to be amazing, and  we don't have to worry about them at all."

729
01:17:21,800 --> 01:17:26,880
I think I'm somewhere in the middle. I've been  a co-author on a paper called "Shaping AI,"

730
01:17:26,880 --> 01:17:34,640
which is, you know, those two extreme views often  kind of view our role as kind of laissez-faire,

731
01:17:34,640 --> 01:17:39,240
like we're just going to have the AI  develop in the path that it takes.

732
01:17:39,240 --> 01:17:44,720
And I think there's actually a really good  argument to be made that what we're going to

733
01:17:44,720 --> 01:17:51,360
do is try to shape and steer the way in which  AI is deployed in the world so that it is,

734
01:17:51,360 --> 01:17:58,640
you know, maximally beneficial in the areas that  we want to capture and benefit from, in education,

735
01:17:58,640 --> 01:18:05,680
some of the areas I mentioned, healthcare. And steer it as much as we can away- maybe

736
01:18:05,680 --> 01:18:13,880
with policy-related things, maybe with technical  measures and safeguards- away from, you know,

737
01:18:13,880 --> 01:18:18,200
the computer will take over and  have unlimited control of what

738
01:18:18,200 --> 01:18:23,880
it can do. So I think that's an engineering  problem: how do you engineer safe systems?

739
01:18:23,880 --> 01:18:31,360
I think it's kind of the modern equivalent  of what we've done in older-style software

740
01:18:31,360 --> 01:18:36,400
development. Like if you look at, you know,  airplane software development, that has a pretty

741
01:18:36,400 --> 01:18:45,600
good record of how do you rigorously develop safe  and secure systems for doing a pretty risky task?

742
01:18:45,600 --> 01:18:50,160
The difficulty there is that there's not some  feedback loop where the 737, you put it in

743
01:18:50,160 --> 01:18:55,160
a box with a bunch of compute for a couple of  years, and it comes out with the version 1000.

744
01:18:55,160 --> 01:19:05,440
I think the good news is that analyzing text  seems to be easier than generating text. So

745
01:19:06,200 --> 01:19:19,800
I believe that the ability of language models to  actually analyze language model output and figure

746
01:19:19,800 --> 01:19:35,040
out what is problematic or dangerous will actually  be the solution to a lot of these control issues.

747
01:19:35,600 --> 01:19:39,920
We are definitely working on this stuff.  We've got a bunch of brilliant folks at

748
01:19:39,920 --> 01:19:45,120
Google working on this now. And I think it's  just going to be more and more important,

749
01:19:45,120 --> 01:19:55,600
both from a “do something good for people”  standpoint, but also from a business standpoint,

750
01:19:55,600 --> 01:20:09,320
that you are, a lot of the time, limited in what  you can deploy based on keeping things safe.

751
01:20:09,320 --> 01:20:15,760
And so it becomes very, very important  to be really, really good at that.

752
01:20:15,760 --> 01:20:21,160
Yeah, obviously, I know you guys take the  potential benefits and costs here seriously,

753
01:20:21,160 --> 01:20:23,744
and it's truly remarkable. I know you guys get  credit for it, but not enough. I think there's

754
01:20:23,744 --> 01:20:28,560
just, there are so many different applications  that you have put out for using these models to

755
01:20:28,560 --> 01:20:35,480
make the different areas you talked about better. Um, but I do think that… again, if you have a

756
01:20:35,480 --> 01:20:39,600
situation where plausibly there's some  feedback loop process, on the other end,

757
01:20:39,600 --> 01:20:45,200
you have a model that is as good as  Noam Shazeer, as good as Jeff Dean.

758
01:20:45,200 --> 01:20:49,600
If there's an evil version of you running  around, and suppose there's a million of them,

759
01:20:49,600 --> 01:20:55,680
I think that's really, really bad. That could be  much, much worse than any other risk, maybe short

760
01:20:55,680 --> 01:21:00,680
of nuclear war or something. Just think about  it, like a million evil Jeff Deans or something.

761
01:21:00,680 --> 01:21:06,000
Where do we get the training data? But, to the extent that you think that's

762
01:21:06,000 --> 01:21:13,200
a plausible output of some quick feedback  loop process, what is your plan of okay,

763
01:21:13,200 --> 01:21:20,320
we've got Gemini 3 or Gemini 4, and we think  it's helping us do a better job of training

764
01:21:20,320 --> 01:21:24,680
future versions, it's writing a bunch of the  training code for us. From this point forward,

765
01:21:24,680 --> 01:21:29,160
we just kind of look over it, verify it. Even the verifiers you talked about of looking

766
01:21:29,160 --> 01:21:33,280
at the output of these models will eventually  be trained by, or a lot of the code will be

767
01:21:33,280 --> 01:21:40,680
written by the AIs you make. What do you want  to know for sure before we have the Gemini 4

768
01:21:40,680 --> 01:21:45,200
help us with the AI research? We really want  to make sure, we want to run this test on it

769
01:21:45,200 --> 01:21:50,880
before we let it write our AI code for us. I mean, I think having the system explore

770
01:21:50,880 --> 01:21:56,760
algorithmic research ideas seems like something  where there's still a human in charge of that.

771
01:21:56,760 --> 01:22:01,200
Like, it's exploring the space, and then  it's going to, like, get a bunch of results,

772
01:22:01,200 --> 01:22:04,600
and we're going to make a decision, like,  are we going to incorporate this particular,

773
01:22:04,600 --> 01:22:11,240
you know, learning algorithm or change to  the system into kind of the core code base?

774
01:22:11,240 --> 01:22:19,560
And so I think you can put in safeguards like that  that enable us to get the benefits of the system

775
01:22:19,560 --> 01:22:26,000
that can sort of improve or kind of self-improve  with human oversight, uh, without necessarily

776
01:22:26,000 --> 01:22:32,360
letting the system go full-on self-improving  without any any notion of a person looking at what

777
01:22:32,360 --> 01:22:37,840
it's doing, right? That's the kind of engineering  safeguards I'm talking about, where you want to

778
01:22:37,840 --> 01:22:44,560
be kind of looking at the characteristics  of the systems you're deploying, not deploy

779
01:22:44,560 --> 01:22:50,640
ones that are harmful by some measures and some  ways, and you have an understanding of what its

780
01:22:50,640 --> 01:22:56,080
capabilities are and what it's likely to do in  certain scenarios. So, you know, I think it's

781
01:22:56,080 --> 01:23:02,640
not an easy problem by any means, but I do think  it is possible to make these these systems safe.

782
01:23:02,640 --> 01:23:08,880
Yeah. I mean, I think we are also going to  use these systems a lot to check themselves,

783
01:23:08,880 --> 01:23:18,400
check other systems. Even as a human, it is easier  to recognize something than to generate it.

784
01:23:20,680 --> 01:23:28,080
One thing I would say is if you expose the model's  capabilities through an API or through a user

785
01:23:28,080 --> 01:23:33,600
interface that people interact with, I think then  you have a level of control to understand how is

786
01:23:33,600 --> 01:23:41,640
it being used and put some boundaries on what it  can do. And that I think is one of the tools in

787
01:23:41,640 --> 01:23:47,240
the arsenal of how do you make sure that what  it's going to do is sort of acceptable by some

788
01:23:47,240 --> 01:23:52,200
set of standards you've set out in your mind? Yeah. I mean, I think the goal is to empower

789
01:23:52,200 --> 01:23:59,600
people, but for the most part we should be  mostly letting people do things with these

790
01:23:59,600 --> 01:24:06,280
systems that make sense and closing off as  few parts of the space as we can. But yeah,

791
01:24:06,280 --> 01:24:11,880
if you let somebody take your thing and create a  million evil software engineers, then that doesn't

792
01:24:11,880 --> 01:24:17,520
empower people because they're going to hurt  others with a million evil software engineers.

793
01:24:17,520 --> 01:24:22,280
So I'm against that. Me too. I'll go on.

794
01:24:22,280 --> 01:24:29,320
All right, let's talk about a few more fun topics.  Make it a little lighter. Over the last 25 years,

795
01:24:29,320 --> 01:24:34,440
what was the most fun time? What period of  time do you have the most nostalgia over?

796
01:24:34,440 --> 01:24:38,560
I think the early sort of four  or five years at Google when I

797
01:24:38,560 --> 01:24:44,800
was one of a handful of people working on  search and crawling and indexing systems,

798
01:24:44,800 --> 01:24:50,120
our traffic was growing tremendously fast. We  were trying to expand our index size and make

799
01:24:50,120 --> 01:24:57,040
it so we updated it every minute instead of every  month, or two months if something went wrong.

800
01:24:57,720 --> 01:25:04,600
Seeing the growth in usage of our systems was  really just personally satisfying. Building

801
01:25:04,600 --> 01:25:10,760
something that is used by two billion  people a day is pretty incredible.

802
01:25:10,760 --> 01:25:17,520
But I would also say equally exciting is working  with people on the Gemini team today. I think

803
01:25:17,520 --> 01:25:23,880
the progress we've been making in what these  models can do over the last year and a half is

804
01:25:23,880 --> 01:25:28,080
really fun. People are really dedicated,  really excited about what we're doing.

805
01:25:28,080 --> 01:25:33,320
I think the models are getting better and  better at pretty complex tasks. Like if

806
01:25:33,320 --> 01:25:38,520
you showed someone using a computer 20 years ago  what these models are capable of, they wouldn't

807
01:25:38,520 --> 01:25:45,000
believe it. And even five years ago, they might  not believe it. And that's pretty satisfying.

808
01:25:45,000 --> 01:25:49,840
I think we'll see a similar growth in usage  of these models and impact in the world.

809
01:25:49,840 --> 01:25:58,760
Yeah, I'm with you. Early days were super fun.  Part of that is just knowing everybody and the

810
01:25:58,760 --> 01:26:02,320
social aspect, and the fact that you're  just building something that millions

811
01:26:02,320 --> 01:26:07,920
and millions of people are using. Same thing today. We got that whole

812
01:26:07,920 --> 01:26:15,040
nice micro kitchen area where you get lots of  people hanging out. I love being in person,

813
01:26:16,040 --> 01:26:21,280
working with a bunch of great people, and building  something that's helping millions to billions of

814
01:26:21,280 --> 01:26:25,000
people. What could be better? What was this micro kitchen?

815
01:26:25,000 --> 01:26:30,880
Oh, we have a micro kitchen area in the building  we both sit in. It's the new, so-named Gradient

816
01:26:30,880 --> 01:26:35,720
Canopy. It used to be named Charleston East,  and we decided we needed a more exciting

817
01:26:35,720 --> 01:26:40,720
name because it's a lot of machine learning  researchers and AI research happening in there.

818
01:26:41,720 --> 01:26:49,440
There's a micro kitchen area that we've set up  with, normally it's just like an espresso machine

819
01:26:49,440 --> 01:26:55,000
and a bunch of snacks, but this particular one has  a bunch of space in it. So we've set up maybe 50

820
01:26:55,000 --> 01:26:59,480
desks in there, and so people are just hanging  out in there. It's a little noisy because people

821
01:26:59,480 --> 01:27:07,200
are always grinding beans and brewing espresso,  but you also get a lot of face-to-face ideas of

822
01:27:07,200 --> 01:27:12,760
connections, like, "Oh, I've tried that. Did  you think about trying this in your idea?" Or,

823
01:27:12,760 --> 01:27:17,120
"Oh, we're going to launch this thing  next week. How's the load test looking?"

824
01:27:17,120 --> 01:27:22,160
There's just lots of feedback that happens. And then we have our Gemini chat room for people

825
01:27:22,160 --> 01:27:29,440
who are not in that micro kitchen. We have a team  all over the world, and there's probably 120 chat

826
01:27:29,440 --> 01:27:37,120
rooms I'm in related to Gemini things. In this  particular very focused topic, we have seven

827
01:27:37,120 --> 01:27:41,560
people working on this, and there are exciting  results being shared by the London colleagues.

828
01:27:41,560 --> 01:27:46,040
When you wake up, you see what's happening  in there, or it's a big group of people

829
01:27:46,040 --> 01:27:51,720
focused on data, and there are all kinds of  issues happening in there. It's just fun.

830
01:27:51,720 --> 01:27:54,120
What I find remarkable about some  of the calls you guys have made

831
01:27:55,080 --> 01:28:01,280
is you're anticipating a level of demand for  compute, which at the time wasn't obvious or

832
01:28:01,280 --> 01:28:06,760
evident. TPUs being a famous example of this,  or the first TPU being an example of this.

833
01:28:08,040 --> 01:28:12,440
That thinking you had in, I guess, 2013  or earlier, if you think about it that way

834
01:28:12,440 --> 01:28:16,520
today and you do an estimate of, look, we're  going to have these models that are going to

835
01:28:16,520 --> 01:28:20,280
be a backbone of our services, and we're going  to be doing constant inference for them. We're

836
01:28:20,280 --> 01:28:25,360
going to be training future versions. And you  think about the amount of compute we'll need by

837
01:28:25,360 --> 01:28:31,600
2030 to accommodate all these use cases,  where does the Fermi estimate get you?

838
01:28:31,600 --> 01:28:39,720
Yeah, I mean, I think you're going to want a lot  of inference. Compute is the rough, highest-level

839
01:28:39,720 --> 01:28:46,240
view of these capable models because if one of the  techniques for improving their quality is scaling

840
01:28:46,240 --> 01:28:52,680
up the amount of inference compute you use, then  all of a sudden what's currently like one request

841
01:28:52,680 --> 01:28:59,920
to generate some tokens now becomes 50 or 100  or 1000 times as computationally intensive, even

842
01:28:59,920 --> 01:29:06,240
though it's producing the same amount of output.  And you're also going to then see tremendous

843
01:29:06,240 --> 01:29:11,880
scaling up of the uses of these services as  not everyone in the world has discovered these

844
01:29:11,880 --> 01:29:16,080
chat-based conversational interfaces where  you can get them to do all kinds of amazing

845
01:29:16,080 --> 01:29:23,320
things. Probably 10% of the computer users in  the world have discovered that today, or 20%. As

846
01:29:25,320 --> 01:29:31,000
that pushes towards 100% and people make  heavier use of it, that's going to be

847
01:29:31,000 --> 01:29:36,800
another order of magnitude or two of scaling. And so you're now going to have two orders of

848
01:29:36,800 --> 01:29:40,360
magnitude from that, two orders of magnitude from  that. The models are probably going to be bigger,

849
01:29:40,360 --> 01:29:44,200
you'll get another order of magnitude or two  from that. And there's a lot of inference

850
01:29:44,200 --> 01:29:50,160
compute you want. So you want extremely efficient  hardware for inference for models you care about.

851
01:29:50,160 --> 01:29:59,160
In flops, global total global inference in 2030? I think just more is always going to be better.

852
01:30:01,000 --> 01:30:14,960
If you just kind of think about, okay, what  fraction of world GDP will people decide to

853
01:30:14,960 --> 01:30:23,200
spend on AI at that point? And then, like,  okay, what do the AI systems look like?

854
01:30:23,200 --> 01:30:28,720
Well, maybe it's some sort of personal  assistant-like thing that is in your

855
01:30:28,720 --> 01:30:34,640
glasses and can see everything around you and  has access to all your digital information

856
01:30:34,640 --> 01:30:39,720
and the world's digital information.  And maybe it's like you're Joe Biden,

857
01:30:39,720 --> 01:30:45,640
and you have the earpiece in the cabinet that  can advise you about anything in real-time

858
01:30:45,640 --> 01:30:52,120
and solve problems for you and give you helpful  pointers. Or you could talk to it, and it wants

859
01:30:52,120 --> 01:31:00,120
to analyze anything that it sees around you for  any potential useful impact that it has on you.

860
01:31:00,120 --> 01:31:06,800
So I mean, I can imagine, okay, and then say  it's like your personal assistant or your

861
01:31:06,800 --> 01:31:12,960
personal cabinet or something, and that every  time you spend 2x as much money on compute,

862
01:31:12,960 --> 01:31:20,200
the thing gets like 5, 10 IQ points smarter or  something like that. And, okay, would you rather

863
01:31:20,200 --> 01:31:30,160
spend $10 a day and have an assistant or $20 a day  and have a smarter assistant? And not only is it

864
01:31:30,160 --> 01:31:35,120
an assistant in life but an assistant in getting  your job done better because now it makes you from

865
01:31:35,120 --> 01:31:47,560
a 10x engineer to a 100x or 10 millionx engineer? Okay, so let's see: from first principles,

866
01:31:47,560 --> 01:31:53,840
right? So people are going to want to spend  some fraction of world GDP on this thing.

867
01:31:54,680 --> 01:32:01,520
The world GDP is almost certainly going to go way,  way up, two orders of magnitude higher than it is

868
01:32:01,520 --> 01:32:07,280
today, due to the fact that we have all of these  artificial engineers working on improving things.

869
01:32:07,280 --> 01:32:16,280
Probably we'll have solved unlimited energy and  carbon issues by that point. So we should be able

870
01:32:16,280 --> 01:32:23,120
to have lots of energy. We should be able to  have millions to billions of robots building

871
01:32:23,120 --> 01:32:32,640
us data centers. Let's see, the sun is what,  10 to the 26 watts or something like that?

872
01:32:34,800 --> 01:32:47,040
I'm guessing that the amount of compute being used  for AI to help each person will be astronomical.

873
01:32:47,040 --> 01:32:51,320
I would add on to that. I'm not sure  I agree completely, but it's a pretty

874
01:32:51,320 --> 01:32:56,400
interesting thought experiment to go in that  direction. And even if you get partway there,

875
01:32:56,400 --> 01:33:01,800
it's definitely going to be a lot of compute. And this is why it's super important to have as

876
01:33:01,800 --> 01:33:09,680
cheap a hardware platform for using these  models and applying them to problems that

877
01:33:09,680 --> 01:33:15,640
Noam described, so that you can then  make it accessible to everyone in some

878
01:33:15,640 --> 01:33:21,760
form and have as low a cost for access to  these capabilities as you possibly can.

879
01:33:21,760 --> 01:33:28,920
And I think that's achievable by focusing on  hardware and model co-design kinds of things,

880
01:33:28,920 --> 01:33:33,440
we should be able to make these things much,  much more efficient than they are today.

881
01:33:33,440 --> 01:33:40,880
Is Google's data center build-out plan over  the next few years aggressive enough given

882
01:33:40,880 --> 01:33:46,200
this increase in demand you're expecting? I'm not going to comment on our future capital

883
01:33:46,200 --> 01:33:53,000
spending because our CEO and CFO would prefer  I probably not. But I will say, you can look at

884
01:33:53,000 --> 01:33:58,280
our past capital expenditures over the last few  years and see that we're definitely investing

885
01:33:58,280 --> 01:34:05,880
in this area because we think it's important. We are continuing to build new and interesting,

886
01:34:05,880 --> 01:34:12,320
innovative hardware that we think really helps us  have an edge in deploying these systems to more

887
01:34:12,320 --> 01:34:19,200
and more people, both training them and also, how  do we make them usable by people for inference?

888
01:34:19,200 --> 01:34:21,920
One thing I've heard you talk a  lot about is continual learning,

889
01:34:21,920 --> 01:34:26,600
the idea that you could just have a model  which improves over time rather than having to

890
01:34:26,600 --> 01:34:33,040
start from scratch. Is there any fundamental  impediment to that? Because theoretically,

891
01:34:33,040 --> 01:34:38,000
you should just be able to keep fine-tuning a  model. What does that future look like to you?

892
01:34:38,000 --> 01:34:44,040
Yeah, I've been thinking about this more and  more. I've been a big fan of models that are

893
01:34:44,040 --> 01:34:49,760
sparse because I think you want different parts  of the model to be good at different things. We

894
01:34:49,760 --> 01:34:56,840
have our Gemini 1.5 Pro model, and other  models are mixture-of-experts style models

895
01:34:56,840 --> 01:35:02,280
where you now have parts of the model that are  activated for some token and parts that are not

896
01:35:02,280 --> 01:35:08,360
activated at all because you've decided this is a  math-oriented thing, and this part's good at math,

897
01:35:08,360 --> 01:35:16,200
and this part's good at understanding cat images.  So, that gives you this ability to have a much

898
01:35:16,200 --> 01:35:21,280
more capable model that's still quite efficient at  inference time because it has very large capacity,

899
01:35:21,280 --> 01:35:26,760
but you activate a small part of it. But I think the current problem, well,

900
01:35:26,760 --> 01:35:31,280
one limitation of what we're doing today is  it's still a very regular structure where

901
01:35:31,280 --> 01:35:37,840
each of the experts is the same size. The  paths merge back together very fast. They

902
01:35:37,840 --> 01:35:43,400
don't go off and have lots of different  branches for mathy things that don't merge

903
01:35:43,400 --> 01:35:50,680
back together with the kind of cat-image thing. I think we should probably have a more organic

904
01:35:50,680 --> 01:35:55,680
structure in these things. I also would like  it if the pieces of those model of the model

905
01:35:55,680 --> 01:36:01,200
could be developed a little bit independently.  Like right now, I think we have this issue where

906
01:36:01,200 --> 01:36:05,600
we're going to train a model. So, we do a  bunch of preparation work on deciding the

907
01:36:05,600 --> 01:36:10,720
most awesome algorithms we can come up with and  the most awesome data mix we can come up with.

908
01:36:10,720 --> 01:36:15,960
But there's always trade-offs there, like we'd  love to include more multilingual data, but that

909
01:36:15,960 --> 01:36:21,040
might come at the expense of including less coding  data, and so, the model's less good at coding but

910
01:36:21,040 --> 01:36:27,880
better at multilingual, or vice versa. I think it  would be really great if we could have a small set

911
01:36:27,880 --> 01:36:34,760
of people who care about a particular subset of  languages go off and create really good training

912
01:36:34,760 --> 01:36:41,960
data, train a modular piece of a model that we  can then hook up to a larger model that improves

913
01:36:41,960 --> 01:36:52,480
its capability in, say, Southeast Asian languages  or in reasoning about Haskell code or something.

914
01:36:53,760 --> 01:36:58,480
Then, you also have a nice software engineering  benefit where you've decomposed the problem a

915
01:36:58,480 --> 01:37:03,560
bit compared to what we do today, which is we have  this kind of a whole bunch of people working. But

916
01:37:03,560 --> 01:37:09,080
then, we have this kind of monolithic process  of starting to do pre-training on this model.

917
01:37:09,080 --> 01:37:13,280
If we could do that, you could have 100 teams  around Google. You could have people all around

918
01:37:13,280 --> 01:37:18,760
the world working to improve languages they care  about or particular problems they care about and

919
01:37:18,760 --> 01:37:24,880
all collectively work on improving the model.  And that's kind of a form of continual learning.

920
01:37:24,880 --> 01:37:28,840
That would be so nice. You could just glue  models together or rip out pieces of models

921
01:37:28,840 --> 01:37:30,720
and shove them into other... Upgrade this piece without

922
01:37:30,720 --> 01:37:35,720
throwing out the thing... ...or you just attach a fire hose,

923
01:37:35,720 --> 01:37:41,720
and you suck all the information out of this  model, shove it into another model. There is,

924
01:37:41,720 --> 01:37:52,080
I mean, the countervailing interest there is sort  of science, in terms of, okay, we're still in the

925
01:37:52,080 --> 01:38:00,240
period of rapid progress, so, if you want to  do sort of controlled experiments, and okay,

926
01:38:00,240 --> 01:38:07,240
I want to compare this thing to that thing because  that then is helping us figure out what to build.

927
01:38:09,800 --> 01:38:14,600
In that interest, it's often best to just start  from scratch so you can compare one complete

928
01:38:14,600 --> 01:38:23,080
training run to another complete training run at  the practical level because it helps us figure out

929
01:38:24,000 --> 01:38:30,000
what to build in the future. It's less  exciting but does lead to rapid progress.

930
01:38:30,000 --> 01:38:33,440
Yeah, I think there may be ways to  get a lot of the benefits of that

931
01:38:33,440 --> 01:38:39,120
with a version system of modularity.  I have a frozen version of my model,

932
01:38:39,120 --> 01:38:43,520
and then I include a different variant of some  particular module, and I want to compare its

933
01:38:43,520 --> 01:38:49,320
performance or train it a bit more. Then,  I compare it to the baseline of this thing

934
01:38:49,320 --> 01:38:56,040
with now version N prime of this particular  module that does Haskell interpretation.

935
01:38:56,040 --> 01:38:59,960
Actually, that could lead to faster research  progress, right? You've got some system, and

936
01:38:59,960 --> 01:39:06,160
you do something to improve it. And if that thing  you're doing to improve it is relatively cheap

937
01:39:06,160 --> 01:39:12,880
compared to training the system from scratch,  then it could actually make research much,

938
01:39:12,880 --> 01:39:17,600
much cheaper and faster. Yeah, and also more

939
01:39:17,600 --> 01:39:23,200
parallelizable, I think, across people. Okay, let's figure it out and do that next.

940
01:39:26,280 --> 01:39:30,840
So, this idea that is sort of casually  laid out there would actually be a big

941
01:39:30,840 --> 01:39:33,920
regime shift compared to how things are done  today. If you think the way things are headed,

942
01:39:33,920 --> 01:39:41,520
this is a sort of very interesting prediction  about... You just have this blob where things

943
01:39:41,520 --> 01:39:45,080
are getting pipelined back and forth –  and if you want to make something better,

944
01:39:45,080 --> 01:39:48,320
you can do like a sort of  surgical incision almost.

945
01:39:48,320 --> 01:39:53,160
Right, or grow the model, add another little bit  of it here. Yeah, I've been sort of sketching out

946
01:39:53,160 --> 01:39:58,600
this vision for a while in Pathways... Yeah, you've been building the...

947
01:39:58,600 --> 01:40:03,400
...and we've been building the infrastructure  for it. So, a lot of what Pathways, the system,

948
01:40:03,400 --> 01:40:08,640
can support is this kind of twisty, weird  model with asynchronous updates to different

949
01:40:08,640 --> 01:40:14,280
pieces. And we're using Pathways to train our  Gemini models, but we're not making use of some

950
01:40:14,280 --> 01:40:22,000
of its capabilities yet. But maybe we should. Ooh maybe. There have been times, like the way the

951
01:40:22,000 --> 01:40:29,720
TPU pods were set up. I don't know who did that,  but they did a pretty brilliant job. The low-level

952
01:40:29,720 --> 01:40:37,280
software stack and the hardware stack, okay,  you've got your nice regular high-performance

953
01:40:37,280 --> 01:40:43,480
hardware, you've got these great torus-shaped  interconnects, and then you've got the right

954
01:40:43,480 --> 01:40:51,640
low-level collectives, the all-reduces, et cetera,  which I guess came from supercomputing, but it

955
01:40:51,640 --> 01:41:00,840
turned out to be kind of just the right thing  to build distributed deep learning on top of.

956
01:41:00,840 --> 01:41:07,520
Okay, so a couple of questions. One,  suppose Noam makes another breakthrough,

957
01:41:07,520 --> 01:41:13,680
and now we've got a better architecture.  Would you just take each compartment and

958
01:41:13,680 --> 01:41:17,960
distill it into this better architecture?  And that's how it keeps improving over time?

959
01:41:18,520 --> 01:41:23,640
I do think distillation is a really useful  tool because it enables you to transform

960
01:41:23,640 --> 01:41:29,200
a model in its current model architecture  form into a different form. Often,

961
01:41:29,200 --> 01:41:35,400
you use it to take a really capable but large  and unwieldy model and distill it into a smaller

962
01:41:35,400 --> 01:41:40,840
one that maybe you want to serve with really  good, fast latency inference characteristics.

963
01:41:41,440 --> 01:41:45,920
But I think you can also view this as  something that's happening at the module

964
01:41:45,920 --> 01:41:52,480
level. Maybe there'd be a continual process where  you have each module, and it has a few different

965
01:41:52,480 --> 01:41:57,400
representations of itself. It has a really  big one. It's got a much smaller one that is

966
01:41:57,400 --> 01:42:04,200
continually distilling into the small version. And then the small version, once that's finished,

967
01:42:04,200 --> 01:42:08,800
you sort of delete the big one and you add a  bunch more parameter capacity. Now, start to

968
01:42:08,800 --> 01:42:14,400
learn all the things that the distilled small  one doesn't know by training it on more data,

969
01:42:14,400 --> 01:42:20,040
and then you kind of repeat that process. If you  have that kind of running a thousand different

970
01:42:20,040 --> 01:42:25,720
places in your modular model in the background,  that seems like it would work reasonably well.

971
01:42:25,720 --> 01:42:27,640
This could be a way of doing  inference scaling, like the router

972
01:42:27,640 --> 01:42:32,360
decides how much do you want the big one. Yeah, you can have multiple versions. Oh,

973
01:42:32,360 --> 01:42:36,800
this is an easy math problem, so I'm going  to route it to the really tiny math distilled

974
01:42:36,800 --> 01:42:40,160
thing. Oh, this one's really hard, so... One, at least from public research,

975
01:42:40,160 --> 01:42:45,320
it seems like it's often hard to decode what  each expert is doing in mixture of expert type

976
01:42:45,320 --> 01:42:50,920
models. If you have something like this, how  would you enforce the kind of modularity that

977
01:42:50,920 --> 01:42:58,440
would be visible and understandable to us? Actually, in the past, I found experts to be

978
01:42:58,440 --> 01:43:03,560
relatively easy to understand. I mean,  the first Mixture of Experts paper,

979
01:43:03,560 --> 01:43:05,480
you could just look at the experts. “I don’t know, I'm only the

980
01:43:05,480 --> 01:43:11,440
inventor of Mixture of Experts.” Like, you could just see, okay, this expert,

981
01:43:11,440 --> 01:43:18,080
like we did, you know, a thousand, two thousand  experts. Okay, and this expert, was getting words

982
01:43:18,080 --> 01:43:22,520
referring to cylindrical objects. This one's super good at dates.

983
01:43:22,520 --> 01:43:26,240
Yeah.  Talking about times.

984
01:43:26,240 --> 01:43:30,920
Yeah, pretty easy to do. Not that you would need that

985
01:43:30,920 --> 01:43:38,440
human understanding to figure out how to work the  thing at runtime because you just have some sort

986
01:43:38,440 --> 01:43:45,320
of learned router that's looking at the example. One thing I would say is there is a bunch of

987
01:43:45,320 --> 01:43:50,960
work on interpretability of models and what  are they doing inside. Sort of expert-level

988
01:43:50,960 --> 01:43:56,440
interpretability is a sub-problem  of that broader area. I really like

989
01:43:56,440 --> 01:43:59,880
some of the work that my former intern,  Chris Olah, and others did at Anthropic,

990
01:43:59,880 --> 01:44:07,360
where they trained a very sparse autoencoder and  were able to deduce what characteristics some

991
01:44:07,360 --> 01:44:11,960
particular neuron in a large language model has,  so they found a Golden Gate Bridge neuron that's

992
01:44:11,960 --> 01:44:17,760
activated when you're talking about the Golden  Gate Bridge. And I think you could do that at

993
01:44:17,760 --> 01:44:23,040
the expert level, you could do that at a variety  of different levels and get pretty interpretable

994
01:44:24,480 --> 01:44:28,240
results, and it's a little unclear if you  necessarily need that. If the model is just

995
01:44:28,240 --> 01:44:35,360
really good at stuff, we don't necessarily care  what every neuron in the Gemini model is doing, as

996
01:44:35,360 --> 01:44:42,400
long as the collective output and characteristics  of the overall system are good. That's one of the

997
01:44:42,400 --> 01:44:48,000
beauties of deep learning, is you don't need to  understand or hand-engineer every last feature.

998
01:44:48,000 --> 01:44:51,960
Man, there are so many interesting implications  of this that I could just keep asking you about

999
01:44:51,960 --> 01:44:54,840
this- I would regret not asking you more about  this, so I'll keep going. One implication is,

1000
01:44:54,840 --> 01:44:59,520
currently, if you have a model that has some  tens or hundreds of billions of parameters,

1001
01:44:59,520 --> 01:45:09,680
you can serve it on a handful of GPUs. In this system, where any one query might

1002
01:45:09,680 --> 01:45:15,840
only make its way through a small fraction of  the total parameters, but you need the whole

1003
01:45:15,840 --> 01:45:22,400
thing loaded into memory, the specific kind of  infrastructure that Google has invested in with

1004
01:45:22,400 --> 01:45:28,320
these TPUs that exist in pods of hundreds or  thousands would be immensely valuable, right?

1005
01:45:28,840 --> 01:45:34,280
For any sort of even existing mixtures of  experts, you want the whole thing in-memory.

1006
01:45:36,560 --> 01:45:43,600
I guess there's kind of this misconception  running around with Mixture of Experts that,

1007
01:45:43,600 --> 01:45:51,400
okay, the benefit is that you don't even have  to go through those weights in the model.

1008
01:45:51,400 --> 01:45:57,520
If some expert is unused, it doesn't mean that  you don't have to retrieve that memory because,

1009
01:45:57,520 --> 01:46:03,200
really, in order to be efficient, you're  serving at very large batch sizes.

1010
01:46:03,200 --> 01:46:06,240
Of independent requests. Right, of independent requests.

1011
01:46:06,240 --> 01:46:13,560
So it's not really the case that, okay, at  this step, you're either looking at this

1012
01:46:13,560 --> 01:46:19,080
expert or you're not looking at this expert. Because if that were the case, then when you did

1013
01:46:19,080 --> 01:46:24,720
look at the expert, you would be running it at  batch size one, which is massively inefficient.

1014
01:46:24,720 --> 01:46:35,480
Like you've got modern hardware, the operational  intensities are whatever, hundreds. So that's

1015
01:46:35,480 --> 01:46:41,800
not what's happening. It's that you are looking  at all the experts, but you only have to send a

1016
01:46:41,800 --> 01:46:45,640
small fraction of the batch through each one. Right, but you still have a smaller batch at

1017
01:46:45,640 --> 01:46:51,560
each expert that then goes through. And in  order to get kind of reasonable balance,

1018
01:46:52,120 --> 01:46:55,600
one of the things that the current models  typically do is they have all the experts

1019
01:46:55,600 --> 01:47:01,520
be roughly the same compute cost, and then you  run roughly the same size batches through them

1020
01:47:01,520 --> 01:47:08,480
in order to propagate the very large batch you're  doing at inference time and have good efficiency.

1021
01:47:08,480 --> 01:47:14,440
But I think you often in the future might  want experts that vary in computational cost

1022
01:47:14,440 --> 01:47:22,840
by factors of 100 or 1000. Or maybe paths  that go for many layers on one case, and

1023
01:47:24,320 --> 01:47:30,160
a single layer or even a skip connection in  the other case. And there, I think you're going

1024
01:47:30,160 --> 01:47:35,520
to want very large batches still, but you're  going to want to push things through the model

1025
01:47:35,520 --> 01:47:41,040
a little bit asynchronously at inference time,  which is a little easier than training time.

1026
01:47:42,440 --> 01:47:48,440
That's part of one of the things that pathways was  designed to support. You have these components,

1027
01:47:48,440 --> 01:47:53,760
and the components can be variable cost and you  kind of can say, for this particular example,

1028
01:47:53,760 --> 01:47:57,200
I want to go through this subset  of the model, and for this example,

1029
01:47:57,200 --> 01:48:04,040
I want to go through this subset of the model  and have the system kind of orchestrate that.

1030
01:48:05,840 --> 01:48:13,600
It also would mean that it would take companies  of a certain size and sophistication to be able

1031
01:48:13,600 --> 01:48:19,240
to... Right now, anybody can train a  sufficiently small enough model. But

1032
01:48:19,880 --> 01:48:22,760
if it ends up being the case that this  is the best way to train future models,

1033
01:48:23,320 --> 01:48:29,640
then you would need a company that can basically  have a data center serving a single quote, unquote

1034
01:48:29,640 --> 01:48:36,320
“blob” or model. So it would be an interesting  change in paradigms in that way as well.

1035
01:48:36,320 --> 01:48:43,960
You definitely want to have at least enough  HBM to put your whole model. So depending

1036
01:48:43,960 --> 01:48:53,480
on the size of your model, most likely that's  how much HBM you'd want to have at a minimum.

1037
01:48:55,120 --> 01:49:00,160
It also means you don't necessarily need to  grow your entire model footprint to be the

1038
01:49:00,160 --> 01:49:06,240
size of a data center. You might  want it to be a bit below that.

1039
01:49:06,240 --> 01:49:12,880
And then have potentially many replicated copies  of one particular expert that is being used a lot,

1040
01:49:12,880 --> 01:49:17,960
so that you get better load balancing. This one's  being used a lot because we get a lot of math

1041
01:49:17,960 --> 01:49:25,760
questions, and this one is an expert on Tahitian  dance, and it is called on really rarely.

1042
01:49:25,760 --> 01:49:30,760
That one, maybe you even page out to  DRAM rather than putting it in HBM.

1043
01:49:30,760 --> 01:49:35,000
But you want the system to figure all this  stuff out based on load characteristics.

1044
01:49:36,080 --> 01:49:38,920
Right now, language models,  obviously, you put in language,

1045
01:49:38,920 --> 01:49:47,560
you get language out. Obviously, it's multimodal. But the Pathways blog post talks about so many

1046
01:49:47,560 --> 01:49:55,040
different use cases that are not obviously  of this kind of auto-regressive nature going

1047
01:49:55,040 --> 01:50:02,880
through the same model. Could you imagine,  basically, Google as a company, the product

1048
01:50:02,880 --> 01:50:06,520
is like Google Search goes through this, Google  Images goes through this, Gmail goes through it?

1049
01:50:06,520 --> 01:50:11,720
Just like the entire server is just this  huge mixture of experts, specialized?

1050
01:50:12,320 --> 01:50:17,600
You're starting to see some of this by having a  lot of uses of Gemini models across Google that

1051
01:50:17,600 --> 01:50:24,920
are not necessarily fine-tuned. They're just  given instructions for this particular use

1052
01:50:24,920 --> 01:50:31,840
case in this feature in this product setting. So, I definitely see a lot more sharing of what

1053
01:50:31,840 --> 01:50:38,240
the underlying models are capable of across  more and more services. I do think that's a

1054
01:50:38,240 --> 01:50:42,200
pretty interesting direction to go, for sure. Yeah, I feel like people listening might not

1055
01:50:44,040 --> 01:50:52,320
register how interesting a prediction this is  about where AI is going. It's like sort of getting

1056
01:50:52,320 --> 01:50:56,320
Noam on a podcast in 2018 and being like, "Yeah,  so I think language models will be a thing."

1057
01:50:56,320 --> 01:51:01,680
It's like, if this is where things go,  this is actually incredibly interesting.

1058
01:51:01,680 --> 01:51:07,120
Yeah, and I think you might see that might  be a big base model. And then you might want

1059
01:51:07,120 --> 01:51:11,240
customized versions of that model with different  modules that are added onto it for different

1060
01:51:11,240 --> 01:51:17,320
settings that maybe have access restrictions. Maybe we have an internal one for Google use,

1061
01:51:17,320 --> 01:51:22,240
for Google employees, that we've trained some  modules on internal data, and we don't allow

1062
01:51:22,240 --> 01:51:27,320
anyone else to use those modules, but we  can make use of it. Maybe other companies,

1063
01:51:27,320 --> 01:51:33,520
you add on other modules that are useful for that  company setting and serve it in our cloud APIs.

1064
01:51:33,520 --> 01:51:35,840
What is the bottleneck to  making this sort of system

1065
01:51:35,840 --> 01:51:45,080
viable? Is it systems engineering? Is it ML? It's a pretty different way of operating than

1066
01:51:45,080 --> 01:51:50,440
our current Gemini development. So,  I think we will explore these kinds

1067
01:51:50,440 --> 01:51:56,960
of areas and make some progress on them. But we need to really see evidence that it's

1068
01:51:56,960 --> 01:52:02,760
the right way, that it has a lot of benefits.  Some of those benefits may be improved quality,

1069
01:52:02,760 --> 01:52:09,160
some may be less concretely measurable,  like this ability to have lots of parallel

1070
01:52:09,160 --> 01:52:15,040
development of different modules. But that's  still a pretty exciting improvement because

1071
01:52:15,040 --> 01:52:21,880
I think that would enable us to make faster  progress on improving the model's capabilities

1072
01:52:21,880 --> 01:52:26,520
for lots of different distinct areas. Even the data control modularity stuff

1073
01:52:26,520 --> 01:52:30,480
seems really cool because then you could  have the piece of the model that's just

1074
01:52:30,480 --> 01:52:35,800
trained for me. It knows all my private data. Like a personal module for you would be useful.

1075
01:52:36,680 --> 01:52:42,000
Another thing might be you can use certain data  in some settings but not in other settings.

1076
01:52:43,080 --> 01:52:48,040
Maybe we have some YouTube data that's only usable  in a YouTube product surface but not in other

1077
01:52:48,040 --> 01:52:52,920
settings. So, we could have a module that is  trained on that data for that particular purpose.

1078
01:52:52,920 --> 01:53:00,240
We're going to need a million automated  researchers to invent all of this stuff.

1079
01:53:00,240 --> 01:53:04,840
It's going to be great. Yeah, well the thing itself, you build the blob,

1080
01:53:04,840 --> 01:53:12,240
and it tells you how to make the blob better. Blob 2.0. Or maybe they're not even versions,

1081
01:53:12,240 --> 01:53:20,280
it's just like an incrementally growing blob. Yeah. Okay, Jeff, motivate for me, big picture:

1082
01:53:20,280 --> 01:53:23,360
why is this a good idea? Why  is this the next direction?

1083
01:53:23,360 --> 01:53:33,480
Yeah, this notion of an organic, not quite so  carefully mathematically constructed machine

1084
01:53:33,480 --> 01:53:39,560
learning model is one that's been with me for a  little while. I feel like in the development of

1085
01:53:39,560 --> 01:53:49,000
neural nets, the artificial neurons, inspiration  from biological neurons is a good one and has

1086
01:53:49,000 --> 01:53:53,760
served us well in the deep learning field. We've been able to make a lot of progress with

1087
01:53:53,760 --> 01:53:58,960
that. But I feel like we're not necessarily  looking at other things that real brains do

1088
01:53:58,960 --> 01:54:04,600
as much as we perhaps could, and that's not to  say we should exactly mimic that because silicon

1089
01:54:04,600 --> 01:54:11,760
and wetware have very different characteristics  and strengths. But I do think one thing we could

1090
01:54:11,760 --> 01:54:18,440
draw more inspiration from is this notion  of having different specialized portions,

1091
01:54:19,480 --> 01:54:24,640
sort of areas of a model of a brain  that are good at different things.

1092
01:54:24,640 --> 01:54:27,480
We have a little bit of that  in Mixture of Experts models,

1093
01:54:27,480 --> 01:54:34,240
but it's still very structured. I feel like  this kind of more organic growth of expertise,

1094
01:54:34,240 --> 01:54:39,080
and when you want more expertise of that, you  add some more capacity to the model there and

1095
01:54:39,080 --> 01:54:46,080
let it learn a bit more on that kind of thing. Also this notion of adapting the connectivity

1096
01:54:46,080 --> 01:54:53,760
of the model to the connectivity of the hardware  is a good one. I think you want incredibly dense

1097
01:54:53,760 --> 01:55:01,680
connections between artificial neurons in the same  chip and the same HBM because that doesn't cost

1098
01:55:01,680 --> 01:55:11,120
you that much. But then you want a smaller number  of connections to nearby neurons. So, like a chip

1099
01:55:11,120 --> 01:55:16,160
away, you should have some amount of connections  and then, like many, many chips away, you should

1100
01:55:16,160 --> 01:55:21,920
have a smaller number of connections where you  send over a very limited kind of bottlenecky

1101
01:55:21,920 --> 01:55:28,520
thing: the most important things that this part  of the model is learning for other parts of the

1102
01:55:28,520 --> 01:55:34,160
model to make use of. And even across multiple TPU  pods, you'd like to send even less information but

1103
01:55:34,160 --> 01:55:39,800
the most salient kind of representations. And then  across metro areas, you'd like to send even less.

1104
01:55:39,800 --> 01:55:44,880
Yeah, and then that emerges organically. Yeah, I'd like that to emerge organically. You

1105
01:55:44,880 --> 01:55:49,960
could hand-specify these characteristics, but  I think you don't know exactly what the right

1106
01:55:49,960 --> 01:55:54,160
proportions of these kinds of connections are so  you should just let the hardware dictate things

1107
01:55:54,160 --> 01:55:59,440
a little bit. Like if you're communicating over  here and this data always shows up really early,

1108
01:55:59,440 --> 01:56:05,200
you should add some more connections, then it'll  take longer and show up at just the right time.

1109
01:56:05,200 --> 01:56:13,000
Oh here's another interesting implication: Right  now, we think about the growth in AI use as a

1110
01:56:13,000 --> 01:56:18,240
sort of horizontal- so, suppose you're like,  how many AI engineers will Google have working

1111
01:56:18,240 --> 01:56:24,560
for it? You think about how many instances  of Gemini 3 will be working at one time.

1112
01:56:24,560 --> 01:56:33,760
If you have this, whatever you want to call it,  this blob, and it can sort of organically decide

1113
01:56:33,760 --> 01:56:42,680
how much of itself to activate, then it's more  of, if you want 10 engineers worth of output,

1114
01:56:42,680 --> 01:56:47,200
it just activates a different pattern or a larger  pattern. If you want 100 engineers of output, it's

1115
01:56:47,200 --> 01:56:51,640
not like calling more agents or more instances,  it's just calling different sub-patterns.

1116
01:56:51,640 --> 01:56:56,800
I think there's a notion of how much compute do  you want to spend on this particular inference,

1117
01:56:56,800 --> 01:57:02,280
and that should vary by factors of 10,000 for  really easy things and really hard things,

1118
01:57:02,280 --> 01:57:07,120
maybe even a million. It might be iterative,  you might make a pass through the model,

1119
01:57:07,120 --> 01:57:11,160
get some stuff, and then decide you now need  to call on some other parts of the model.

1120
01:57:13,840 --> 01:57:20,400
The other thing I would say is this sounds super  complicated to deploy because it's this weird,

1121
01:57:21,120 --> 01:57:28,160
constantly evolving thing with maybe not super  optimized ways of communicating between pieces,

1122
01:57:28,160 --> 01:57:34,520
but you can always distill from that. If you say,  "This is the kind of task I really care about, let

1123
01:57:34,520 --> 01:57:40,240
me distill from this giant kind of organic thing  into something that I know can be served really

1124
01:57:40,240 --> 01:57:45,320
efficiently," you could do that distillation  process whenever you want, once a day, once an

1125
01:57:45,320 --> 01:57:50,920
hour. That seems like it'd be kind of good. Yeah, we need better distillation.

1126
01:57:50,920 --> 01:57:53,840
Yeah. Anyone out there who invents amazing distillation

1127
01:57:53,840 --> 01:57:59,680
techniques that instantly distill from a giant  blob onto your phone, that would be wonderful.

1128
01:57:59,680 --> 01:58:02,800
How would you characterize what's missing  from current distillation techniques?

1129
01:58:02,800 --> 01:58:07,800
Well, I just want it to work faster. A related thing is I feel like we

1130
01:58:07,800 --> 01:58:13,240
need interesting learning techniques during  pre-training. I'm not sure we're extracting

1131
01:58:13,240 --> 01:58:18,760
the maximal value from every token we look at  with the current training objective. Maybe we

1132
01:58:18,760 --> 01:58:24,280
should think a lot harder about some tokens. When you get to "the answer is," maybe the

1133
01:58:24,280 --> 01:58:32,440
model should, at training time, do a lot  more work than when it gets to "the".

1134
01:58:33,960 --> 01:58:38,360
Right. There's got to be some way  to get more from the same data,

1135
01:58:38,360 --> 01:58:44,240
make it learn it forwards and backwards. And every which way. Hide some stuff this way,

1136
01:58:44,240 --> 01:58:51,160
hide some stuff that way, make it infer from  partial information. I think people have been

1137
01:58:51,160 --> 01:58:56,920
doing this in vision models for a while. You  distort the model or you hide parts of it and

1138
01:58:56,920 --> 01:59:02,440
try to make it guess the bird from half, like  that it's a bird from this upper corner of the

1139
01:59:02,440 --> 01:59:07,680
image or the lower left corner of the image. That makes the task harder, and I feel like

1140
01:59:07,680 --> 01:59:13,760
there's an analog for more textual or  coding-related data where you want to

1141
01:59:13,760 --> 01:59:19,640
force the model to work harder. You'll get  more interesting observations from it.

1142
01:59:19,640 --> 01:59:24,080
Yeah, the image people didn't have enough labeled  data so they had to invent all this stuff.

1143
01:59:24,080 --> 01:59:28,840
And they invented -- I mean, dropout was invented  on images, but we're not really using it for text

1144
01:59:28,840 --> 01:59:34,640
mostly. That's one way you could get a lot  more learning in a more large-scale model

1145
01:59:34,640 --> 01:59:44,560
without overfitting is just make like 100 epochs  over the world's text data and use dropout.

1146
01:59:45,080 --> 01:59:49,960
But that's pretty computationally expensive,  but it does mean we won't run it. Even though

1147
01:59:49,960 --> 01:59:54,920
people are saying, "Oh no, we're almost out  of textual data," I don't really believe that

1148
01:59:54,920 --> 02:00:00,680
because I think we can get a lot more capable  models out of the text data that does exist.

1149
02:00:00,680 --> 02:00:06,360
I mean, a person has seen a billion tokens. Yeah, and they're pretty good at a lot of stuff.

1150
02:00:06,360 --> 02:00:11,920
So obviously human data efficiency  sets a lower bound on how, or I guess,

1151
02:00:11,920 --> 02:00:16,720
upper bound, one of them, maybe not. It's an interesting data point.

1152
02:00:16,720 --> 02:00:21,960
Yes. So there's a sort of modus  ponens, modus tollens thing here.

1153
02:00:22,720 --> 02:00:28,120
One way to look at it is, look, LLMs have so  much further to go, therefore we project orders

1154
02:00:28,120 --> 02:00:33,000
of magnitude improvement in sample efficiency  just if they could match humans. Another is,

1155
02:00:33,000 --> 02:00:37,720
maybe they're doing something clearly different  given the orders of magnitude difference. What's

1156
02:00:37,720 --> 02:00:44,520
your intuition of what it would take to make  these models as sample efficient as humans are?

1157
02:00:44,520 --> 02:00:50,280
Yeah, I think we should consider changing the  training objective a little bit. Just predicting

1158
02:00:50,280 --> 02:00:57,720
the next token from the previous ones you've seen  seems like not how people learn. It's a little bit

1159
02:00:57,720 --> 02:01:03,320
related to how people learn, I think, but not  entirely. A person might read a whole chapter

1160
02:01:03,320 --> 02:01:08,800
of a book and then try to answer questions at  the back, and that's a different kind of thing.

1161
02:01:08,800 --> 02:01:14,080
I also think we're not learning from visual  data very much. We're training a little bit on

1162
02:01:14,080 --> 02:01:19,560
video data, but we're definitely not anywhere  close to thinking about training on all the

1163
02:01:19,560 --> 02:01:25,760
visual inputs you could get. So you have visual  data that we haven't really begun to train on.

1164
02:01:25,760 --> 02:01:31,960
Then I think we could extract a lot more  information from every bit of data we do see.

1165
02:01:31,960 --> 02:01:36,880
I think one of the ways people are so sample  efficient is they explore the world and take

1166
02:01:36,880 --> 02:01:43,040
actions in the world and observe what happens. You  see it with very small infants picking things up

1167
02:01:43,040 --> 02:01:47,600
and dropping them; they learn about gravity  from that. And that's a much harder thing to

1168
02:01:47,600 --> 02:01:54,240
learn when you're not initiating the action. I think having a model that can take actions as

1169
02:01:54,240 --> 02:01:58,600
part of its learning process would be just  a lot better than just sort of passively

1170
02:01:58,600 --> 02:02:01,720
observing a giant dataset. Is Gato the future, then?

1171
02:02:01,720 --> 02:02:07,680
Something where the model can observe  and take actions and observe the

1172
02:02:07,680 --> 02:02:15,560
corresponding results seems pretty useful. I mean, people can learn a lot from thought

1173
02:02:15,560 --> 02:02:20,680
experiments that don't even involve extra input.  Einstein learned a lot of stuff from thought

1174
02:02:20,680 --> 02:02:27,560
experiments, or like Newton went into quarantine  and got an apple dropped on his head or something

1175
02:02:27,560 --> 02:02:34,040
and invented gravity. And like mathematicians  -- math didn't have any extra input.

1176
02:02:34,040 --> 02:02:39,400
Chess, okay, you have the thing play chess  against itself and it gets good at chess. That

1177
02:02:39,400 --> 02:02:48,200
was DeepMind, but also all it needs is the rules  of chess. So there's actually probably a lot of

1178
02:02:48,200 --> 02:02:58,240
learning that you can do even without external  data, and then you can make it in exactly the

1179
02:02:58,240 --> 02:03:04,440
fields that you care about. Of course, there  is learning that will require external data,

1180
02:03:04,440 --> 02:03:10,600
but maybe we can just have this thing  talk to itself and make itself smarter.

1181
02:03:10,600 --> 02:03:16,680
So here's the question I have. What you've just  laid out over the last hour is potentially just

1182
02:03:16,680 --> 02:03:22,880
like the big next paradigm shift in AI.  That's a tremendously valuable insight,

1183
02:03:22,880 --> 02:03:32,080
potentially. Noam, in 2017 you released  the Transformer paper on which tens,

1184
02:03:32,080 --> 02:03:36,240
if not hundreds, of billions of dollars of  market value is based in other companies,

1185
02:03:36,240 --> 02:03:40,560
not to mention all this other research  that Google has released over time,

1186
02:03:41,720 --> 02:03:47,840
which you've been relatively generous with. In retrospect, when you think about divulging

1187
02:03:47,840 --> 02:03:51,120
this information that has been helpful to  your competitors, in retrospect is it like,

1188
02:03:51,120 --> 02:03:54,240
"Yeah, we'd still do it," or would you  be like, "Ah, we didn't realize how big

1189
02:03:54,240 --> 02:03:58,840
a deal Transformer was. We should have kept  it indoors." How do you think about that?

1190
02:03:58,840 --> 02:04:09,880
It's a good question because I think probably  we did need to see the size of the opportunity,

1191
02:04:09,880 --> 02:04:16,720
often reflected in what other companies  are doing. And also it's not a fixed pie.

1192
02:04:18,800 --> 02:04:26,240
The current state of the world is pretty  much as far from fixed pie as you can get.

1193
02:04:26,240 --> 02:04:35,520
I think we're going to see orders of magnitude of  improvements in GDP, health, wealth, and anything

1194
02:04:35,520 --> 02:04:46,120
else you can think of. So I think it's definitely  been nice that Transformer has got around.

1195
02:04:46,120 --> 02:04:50,360
It’s transformative.  Woo. Thank God Google's

1196
02:04:50,360 --> 02:04:58,000
doing well as well. So these days we do  publish a little less of what we're doing.

1197
02:05:00,960 --> 02:05:08,240
There's always this trade-off: should we publish  exactly what we're doing right away? Should we put

1198
02:05:08,240 --> 02:05:17,280
it in the next stages of research and then roll it  out into production Gemini models and not publish

1199
02:05:17,280 --> 02:05:22,440
it at all? Or is there some intermediate point? And for example, in our computational photography

1200
02:05:22,440 --> 02:05:29,320
work in Pixel cameras, we've often taken the  decision to develop interesting new techniques,

1201
02:05:29,320 --> 02:05:38,800
like the ability to do super good night sight  vision for low-light situations or whatever,

1202
02:05:38,800 --> 02:05:44,240
put that into the product and then published a  real research paper about the system that does

1203
02:05:44,240 --> 02:05:52,040
that after the product is released. Different techniques and developments

1204
02:05:52,040 --> 02:05:57,520
have different treatments. Some things we think  are super critical we might not publish. Some

1205
02:05:57,520 --> 02:06:01,760
things we think are really interesting  but important for improving our products;

1206
02:06:01,760 --> 02:06:05,800
we'll get them out into our products and then  make a decision: did we publish this or do

1207
02:06:05,800 --> 02:06:11,160
we give kind of a lightweight discussion  of it, but maybe not every last detail?

1208
02:06:11,160 --> 02:06:15,960
Other things I think we publish openly and try  to advance the field and the community because

1209
02:06:15,960 --> 02:06:22,360
that's how we all benefit from participating.  I think it's great to go to conferences like

1210
02:06:22,360 --> 02:06:28,880
NeurIPS last week with 15,000 people all sharing  lots and lots of great ideas. We publish a lot

1211
02:06:28,880 --> 02:06:35,600
of papers there as we have in the past, and  see the field advance is super exciting.

1212
02:06:35,600 --> 02:06:44,400
How would you account for... so obviously Google  had all these insights internally rather early on,

1213
02:06:44,920 --> 02:06:56,360
including the top researchers. And now Gemini 2 is  out. We didn't get a chance much to talk about it,

1214
02:06:56,360 --> 02:07:01,000
but people know it's a really great model. Such a good model. As we say around the

1215
02:07:01,000 --> 02:07:03,760
micro-kitchen, “such a good  model, such a good model”.

1216
02:07:03,760 --> 02:07:10,360
So it's top in LMSYS Chatbot Arena. And so now  Google's on top. But how would you account for

1217
02:07:10,360 --> 02:07:15,120
basically coming up with all the great insights  for a couple of years? Other competitors had

1218
02:07:15,120 --> 02:07:28,120
models that were better for a while despite that. We've been working on language models for a long

1219
02:07:28,120 --> 02:07:36,400
time. Noam's early work on spelling correction in  2001, the work on translation, very large-scale

1220
02:07:36,400 --> 02:07:43,400
language models in 2007, and seq2seq and word2vec  and more recent Transformers and then BERT.

1221
02:07:44,800 --> 02:07:52,560
Things like the internal Meena system that was  actually a chatbot-based system designed to kind

1222
02:07:52,560 --> 02:07:59,560
of engage people in interesting conversations.  We actually had an internal chatbot system that

1223
02:07:59,560 --> 02:08:06,560
Googlers could play with even before ChatGPT  came out. And actually, during the pandemic,

1224
02:08:06,560 --> 02:08:10,280
a lot of Googlers would enjoy spending,  you know, everyone was locked down at home,

1225
02:08:10,280 --> 02:08:14,720
and so they enjoyed spending time chatting  with Meena during lunch because it was

1226
02:08:14,720 --> 02:08:21,200
like a nice, you know, lunch partner. I think one of the things we were a little,

1227
02:08:22,760 --> 02:08:27,720
our view of things from a search perspective  was these models hallucinate a lot,

1228
02:08:27,720 --> 02:08:34,080
they don't get things right a lot of the time- or  some of the time- and that means that they aren't

1229
02:08:34,080 --> 02:08:40,640
as useful as they could be and so we’d like to  make that better. From a search perspective,

1230
02:08:40,640 --> 02:08:47,240
you want to get the right answer 100% of the  time, ideally and be very high on factuality.

1231
02:08:47,240 --> 02:08:54,000
These models were not near that bar. I think what we were a little unsure

1232
02:08:54,000 --> 02:08:59,080
about is that they were incredibly useful. Oh  and they also had all kinds of safety issues,

1233
02:08:59,080 --> 02:09:04,480
like they might say offensive things and we had to  work on that aspect and get that to a point where

1234
02:09:04,480 --> 02:09:09,320
we were comfortable releasing the model. But I  think what we didn’t quite appreciate was how

1235
02:09:09,320 --> 02:09:16,320
useful they could be for things you wouldn't ask  a search engine, right? Like, help me write a note

1236
02:09:16,320 --> 02:09:22,360
to my veterinarian, or like, can you take this  text and give me a quick summary of it? I think

1237
02:09:22,360 --> 02:09:29,360
that's the kind of thing we've seen people really  flock to in terms of using chatbots as amazing new

1238
02:09:29,360 --> 02:09:37,480
capabilities rather than as a pure search engine. So I think we took our time and got to the point

1239
02:09:37,480 --> 02:09:43,040
where we actually released quite capable chatbots  and have been improving them through Gemini models

1240
02:09:43,720 --> 02:09:49,960
quite a bit. I think that's actually not  a bad path to have taken. Would we like

1241
02:09:49,960 --> 02:09:54,560
to have released the chatbot earlier? Maybe.  But I think we have a pretty awesome chatbot

1242
02:09:54,560 --> 02:09:58,680
with awesome Gemini models that are getting  better all the time. And that's pretty cool.

1243
02:09:58,680 --> 02:10:04,027
So we've discussed some of the things you guys  have worked on over the last 25 years, and there are so many different fields, right? You start off  with search and indexing to distributed systems,

1244
02:10:11,080 --> 02:10:19,800
to hardware, to AI algorithms. And genuinely,  there are a thousand more, just go on either of

1245
02:10:19,800 --> 02:10:27,280
their Google Scholar pages or something. What  is the trick to having this level of, not only

1246
02:10:27,280 --> 02:10:34,640
career longevity where you're having many decades  of making breakthroughs, but also the breadth of

1247
02:10:34,640 --> 02:10:41,120
different fields, both of you, in either order,  what’s the trick to career longevity and breadth?

1248
02:10:43,200 --> 02:10:49,720
One thing that I like to do is to find out about a  new and interesting area, and one of the best ways

1249
02:10:49,720 --> 02:10:55,840
to do that is to pay attention to what's going  on, talk to colleagues, pay attention to research

1250
02:10:55,840 --> 02:11:01,440
papers that are being published, and look at the  kind of research landscape as it's evolving.

1251
02:11:01,440 --> 02:11:08,080
Be willing to say, "Oh, chip design. I wonder  if we could use reinforcement learning for some

1252
02:11:08,080 --> 02:11:14,040
aspect of that." Be able to dive into a new area,  work with people who know a lot about a different

1253
02:11:14,040 --> 02:11:21,000
domain or AI for healthcare or something.  I've done a bit of working with clinicians

1254
02:11:21,000 --> 02:11:26,120
about what are the real problems, how could AI  help? It wouldn't be that useful for this thing,

1255
02:11:26,120 --> 02:11:30,200
but it would be super useful for this. Getting those insights, and often working

1256
02:11:30,200 --> 02:11:37,040
with a set of five or six colleagues who have  different expertise than you do. It enables you to

1257
02:11:37,040 --> 02:11:41,800
collectively do something that none of you could  do individually. Then some of their expertise

1258
02:11:41,800 --> 02:11:47,320
rubs off on you and some of your expertise rubs  off on them, and now you have this bigger set of

1259
02:11:47,320 --> 02:11:51,680
tools in your tool belt as an engineering  researcher to go tackle the next thing.

1260
02:11:51,680 --> 02:11:57,080
I think that's one of the beauties of  continuing to learn on the job. It's

1261
02:11:57,080 --> 02:12:02,800
something I treasure. I really enjoy diving  into new things and seeing what we can do.

1262
02:12:02,800 --> 02:12:15,360
I'd say probably a big thing is humility, like  I’d say I’m the most humble. But seriously,

1263
02:12:18,520 --> 02:12:29,280
to say what I just did is nothing compared to what  I can do or what can be done. And to be able to

1264
02:12:29,280 --> 02:12:37,160
drop an idea as soon as you see something better,  like you or somebody with some better idea,

1265
02:12:37,160 --> 02:12:41,640
and you see how maybe what you're thinking  about, what they're thinking about or something

1266
02:12:41,640 --> 02:12:52,880
totally different can conceivably work better. I think there is a drive in some sense to say,

1267
02:12:52,880 --> 02:13:02,200
"Hey, the thing I just invented is awesome, give  me more chips." Particularly if there's a lot of

1268
02:13:02,200 --> 02:13:13,640
top-down resource assignment. But I think we also  need to incentivize people to say, "Hey, this

1269
02:13:13,640 --> 02:13:21,760
thing I am doing is not working at all. Let me  just drop it completely and try something else."

1270
02:13:21,760 --> 02:13:29,080
Which I think Google Brain did quite well.  We had the very kind of bottoms-up UBI kind

1271
02:13:29,080 --> 02:13:33,720
of chip allocation. You had a UBI?

1272
02:13:33,720 --> 02:13:40,200
Yeah, it was like basically everyone  had one credit and you could pool them.

1273
02:13:40,200 --> 02:13:47,640
Gemini has been mostly top-down, which  has been very good in some sense because

1274
02:13:47,640 --> 02:13:54,720
it has led to a lot more collaboration and  people working together. You less often have

1275
02:13:54,720 --> 02:13:59,400
five groups of people all building the same  thing or building interchangeable things.

1276
02:14:01,600 --> 02:14:09,600
But on the other hand, it does lead to some  incentive to say, "Hey, what I'm doing is working

1277
02:14:09,600 --> 02:14:18,000
great." And then, as a lead, you hear hundreds  of groups, and everything is, "So you should give

1278
02:14:18,000 --> 02:14:24,720
them more chips." There's less of an incentive to  say, "Hey, what I'm doing is not actually working

1279
02:14:24,720 --> 02:14:29,520
that well. Let me try something different." So I think going forward, we're going to have

1280
02:14:29,520 --> 02:14:35,240
some amount of top-down, some amount of bottom-up,  so as to incentivize both of these behaviors:

1281
02:14:35,240 --> 02:14:42,600
collaboration and flexibility. I think both  those things lead to a lot of innovation.

1282
02:14:42,600 --> 02:14:48,840
I think it's also good to articulate  interesting directions you think we should go.

1283
02:14:51,320 --> 02:14:58,040
I have an internal slide deck called "Go,  Jeff, Wacky Ideas." I think those are a

1284
02:14:58,040 --> 02:15:02,800
little bit more product-oriented things,  like, "Hey, I think now that we have these

1285
02:15:02,800 --> 02:15:10,400
capabilities, we could do these 17 things." I think that's a good thing because sometimes

1286
02:15:10,400 --> 02:15:16,000
people get excited about that and want to start  working with you on one or more of them. And I

1287
02:15:16,000 --> 02:15:23,080
think that's a good way to bootstrap where we  should go without necessarily ordering people,

1288
02:15:23,080 --> 02:15:25,720
"We must go here." Alright, this was great.

1289
02:15:25,720 --> 02:15:27,320
Yeah. Thank you, guys.

1290
02:15:27,320 --> 02:15:36,408
Appreciate you taking the time, it was  great chatting. That was awesome.

