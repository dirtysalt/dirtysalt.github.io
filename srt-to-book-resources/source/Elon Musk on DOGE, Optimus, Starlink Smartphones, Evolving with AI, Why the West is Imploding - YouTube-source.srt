1
00:00:00,960 --> 00:00:05,900
I believe Optimus is gonna be the greatest product ever created by humanity.

2
00:00:05,900 --> 00:00:15,220
Elon Musk and his xAI startup have built the largest and most powerful artificial intelligence training super computer in the world.

3
00:00:15,220 --> 00:00:18,190
As far as I know, there's only one person in the world who could do that

4
00:00:18,190 --> 00:00:18,650
you know?

5
00:00:18,650 --> 00:00:21,580
Thi- this is a- an arms race of epic proportions.

6
00:00:21,580 --> 00:00:23,440
He's a big thinker!

7
00:00:23,440 --> 00:00:31,200
You guys went on Fox the other day with the Doge team- I saw Elon's face nodding while they were speaking with a grin ear to ear.

8
00:00:31,200 --> 00:00:31,230
Right.

9
00:00:31,230 --> 00:00:31,940
He was proud.

10
00:00:31,940 --> 00:00:32,680
He is proud.

11
00:00:32,680 --> 00:00:35,900
xAI has acquired X in an all-stock transaction.

12
00:00:35,900 --> 00:00:39,240
Tesla's first robo-taxis are officially on the road.

13
00:00:39,240 --> 00:00:45,720
The company's board proposed a new compensation package for the CEO that could pay him just about a trillion dollars in stock.

14
00:00:45,720 --> 00:00:49,160
He gets nothing if he doesn't hit the numbers.

15
00:00:49,160 --> 00:00:58,360
SpaceX will buy wireless spectrum licenses from Echo Star for its Starlink satellite network for about $17 billion.

16
00:00:58,360 --> 00:01:02,320
Three, two, one.

17
00:01:02,320 --> 00:01:07,350
 There's a  Oh my god.

18
00:01:07,350 --> 00:01:13,860
 There's a splash down.

19
00:01:13,860 --> 00:01:15,620
 That's so awesome.

20
00:01:15,620 --> 00:01:16,840
How do you have time?

21
00:01:16,840 --> 00:01:20,220
This, I- I- I never understand you.

22
00:01:20,220 --> 00:01:21,560
Yeah, well, I do work a lot.

23
00:01:21,560 --> 00:01:22,966
 I mean .

24
00:01:22,966 --> 00:01:23,020
..

25
00:01:23,020 --> 00:01:28,280
Ladies and gentlemen, please welcome Elon Musk.

26
00:01:28,280 --> 00:01:29,580
 All right.

27
00:01:29,580 --> 00:01:30,270
Yeah.

28
00:01:30,270 --> 00:01:32,300
All right.

29
00:01:32,300 --> 00:01:32,370
Hello.

30
00:01:32,370 --> 00:01:33,540
Where are you?

31
00:01:33,540 --> 00:01:34,580
Alto.

32
00:01:34,580 --> 00:01:35,860
You're in Palo Alto.

33
00:01:35,860 --> 00:01:36,490
 Yeah.

34
00:01:36,490 --> 00:01:40,560
And, um, not Washington, DC- I'm at- I'm at- I'm at- I'm at Tesla

35
00:01:40,560 --> 00:01:42,920
uh, global engineering headquarters in Palo Alto.

36
00:01:42,920 --> 00:01:43,680
Yeah.

37
00:01:43,680 --> 00:01:45,520
So, no more Washington, DC?

38
00:01:45,520 --> 00:01:48,490
You're back at work, you're focused, yeah?

39
00:01:48,490 --> 00:01:50,820
Uh, yeah, I haven't been to DC since May.

40
00:01:50,820 --> 00:01:52,340
Okay.

41
00:01:52,340 --> 00:01:55,220
Uh- That was a s- that was a hell of a side quest.

42
00:01:55,220 --> 00:01:59,680
  Um- That was a good s- any lessons from your time in Washington

43
00:01:59,680 --> 00:02:00,280
DC?

44
00:02:00,280 --> 00:02:05,860
Uh, the government is basically unfixable.

45
00:02:05,860 --> 00:02:14,240
   Elon- O- o- only-  O- only- I applaud David's

46
00:02:14,240 --> 00:02:17,100
uh, noble efforts, and let's uh, it- it's good to

47
00:02:17,100 --> 00:02:19,820
it's good to have talented people in the administration

48
00:02:19,820 --> 00:02:21,100
uh, but at the end of the day

49
00:02:21,100 --> 00:02:23,760
um, if you look at our national debt

50
00:02:23,760 --> 00:02:27,370
which is, uh, insanely high, uh

51
00:02:27,370 --> 00:02:32,360
the interest payments exceed the, uh, Defense Department

52
00:02:32,360 --> 00:02:35,070
I guess, sorry, War Department-  .

53
00:02:35,070 --> 00:02:35,070
..

54
00:02:35,070 --> 00:02:36,940
uh, budget.

55
00:02:36,940 --> 00:02:40,420
Um, and, um, they keep rising.

56
00:02:40,420 --> 00:02:44,880
So, if AI and robots don't solve our national debt

57
00:02:44,880 --> 00:02:46,500
we're- we're toast.

58
00:02:46,500 --> 00:02:46,890
Hm.

59
00:02:46,890 --> 00:02:49,140
Which is a great segue.

60
00:02:49,140 --> 00:02:54,570
Um, Optimus is, um, I think- That's right.

61
00:02:54,570 --> 00:02:54,603
.

62
00:02:54,603 --> 00:02:54,609
..

63
00:02:54,609 --> 00:02:56,740
gonna be the greatest- That's right.

64
00:02:56,740 --> 00:02:57,003
.

65
00:02:57,003 --> 00:02:57,010
..

66
00:02:57,010 --> 00:03:01,380
uh, product in the history of humanity.

67
00:03:01,380 --> 00:03:07,440
What's the progress like in how much of your- how many of your cycles are going specifically to Optimus?

68
00:03:07,440 --> 00:03:09,100
What's the timeline?

69
00:03:09,100 --> 00:03:11,180
I think you're on version three, maybe four.

70
00:03:11,180 --> 00:03:14,100
Uh, tell us everything.

71
00:03:14,100 --> 00:03:17,100
Uh, well, yeah.

72
00:03:17,100 --> 00:03:19,440
No, everything will take a long time.

73
00:03:19,440 --> 00:03:20,480
We've got time.

74
00:03:20,480 --> 00:03:30,920
  Um, we're- we're finalizing the design of Optimus version three.

75
00:03:30,920 --> 00:03:37,220
And, uh, that- that really is gonna be a very remarkable robot.

76
00:03:37,220 --> 00:03:43,120
Um, it will have the, essentially the manual dexterity of a human

77
00:03:43,120 --> 00:03:45,880
so meaning a very complex hand.

78
00:03:45,880 --> 00:03:53,180
Um, a- the- a- an AI mind that can navigate and comprehend reality.

79
00:03:53,180 --> 00:03:57,260
Um, and be made in very high volume.

80
00:03:57,260 --> 00:03:58,750
Uh, those are the three things that are missing.

81
00:03:58,750 --> 00:04:00,910
Like, if you see any other, um

82
00:04:00,910 --> 00:04:04,840
robotics, uh, company, they're missing those three things.

83
00:04:04,840 --> 00:04:07,620
Those are the three really hard things.

84
00:04:07,620 --> 00:04:14,920
Um, and, uh, I- I- I spend actually at this point

85
00:04:14,920 --> 00:04:19,255
um .

86
00:04:19,255 --> 00:04:19,490
..

87
00:04:19,490 --> 00:04:23,260
it- it might be more of my mental cycles than anyth- anything else

88
00:04:23,260 --> 00:04:25,880
any other single thing on Optimus.

89
00:04:25,880 --> 00:04:29,730
Uh, that's- that- that's solving for, uh

90
00:04:29,730 --> 00:04:36,260
real world AI, uh, all of the electromechanical issues of Optimus

91
00:04:36,260 --> 00:04:39,180
the- the supply chain and production challenges of it.

92
00:04:39,180 --> 00:04:42,320
Because we have- there- there is no supply chain that exists for humanoid robots

93
00:04:42,320 --> 00:04:43,270
so it has to be created.

94
00:04:43,270 --> 00:04:45,390
We have to recreate it from scratch, um

95
00:04:45,390 --> 00:04:48,980
and which requires doing a lot of vertical integration.

96
00:04:48,980 --> 00:04:53,120
Um, n- none of the actuators in Optimus

97
00:04:53,120 --> 00:04:57,740
um, are available from an existing supply chain.

98
00:04:57,740 --> 00:05:02,770
Um, so, but I- I- I think it is accurate to say that if successful

99
00:05:02,770 --> 00:05:06,350
Optimus will be the biggest product ever.

100
00:05:06,350 --> 00:05:09,360
And the cost of it at scale?

101
00:05:09,360 --> 00:05:11,860
20, 30, $40,000 a robot?

102
00:05:11,860 --> 00:05:17,660
What- what do you think the first wave of them will cost and

103
00:05:17,660 --> 00:05:23,240
uh, when will we be able to buy one to work on the ranch?

104
00:05:23,240 --> 00:05:27,680
I think the- the- the marginal cost of production

105
00:05:27,680 --> 00:05:30,160
once you hit a million units per year

106
00:05:30,160 --> 00:05:35,180
uh, is probably around the $20,000 range.

107
00:05:35,180 --> 00:05:44,260
Uh, it- it- it sort of depends on how much you spend on the AI chip in the- in the robot.

108
00:05:44,260 --> 00:05:49,860
Um, and you need to achieve a lot of efficiencies in the actuators.

109
00:05:49,860 --> 00:05:55,220
Uh, there are, um, 26 actuators per arm.

110
00:05:55,220 --> 00:05:58,040
Like, 26 electric, like motors, gear boxes

111
00:05:58,040 --> 00:05:59,960
and power electronics.

112
00:05:59,960 --> 00:06:02,485
Um-.

113
00:06:02,485 --> 00:06:03,698
..

114
00:06:03,698 --> 00:06:04,708
so, so it.

115
00:06:04,708 --> 00:06:05,068
..

116
00:06:05,068 --> 00:06:08,228
But- but the- the- the- the AI chip will be pretty expensive

117
00:06:08,228 --> 00:06:09,973
like that- that might be like five- $5

118
00:06:09,973 --> 00:06:13,228
000 or $6,000 of the- of the bill of materials

119
00:06:13,228 --> 00:06:14,348
maybe more.

120
00:06:14,348 --> 00:06:21,008
Um, and, um, but- but so I- but I think at volume

121
00:06:21,008 --> 00:06:25,983
at a million units a year, the- the production cost is probably in the order of $20

122
00:06:25,983 --> 00:06:28,808
000, maybe 25, something like that.

123
00:06:28,808 --> 00:06:32,308
And, um, the price will be as a function of demand.

124
00:06:32,308 --> 00:06:38,668
Elon, um, can you maybe explain to everybody why the hand is so important to get right

125
00:06:38,668 --> 00:06:42,208
and why, you know, the actuator design is so unique

126
00:06:42,208 --> 00:06:44,368
and you know, why it's so difficult

127
00:06:44,368 --> 00:06:52,048
why nobody makes it, and why you have to start there almost to build the rest of the- the robot properly?

128
00:06:52,048 --> 00:06:55,053
Well, it turns out that human hands are incredibly.

129
00:06:55,053 --> 00:06:55,178
..

130
00:06:55,178 --> 00:06:59,348
that they've evolved to this- to be this incredibly sophisticated machine.

131
00:06:59,348 --> 00:07:04,658
Like the- your hand is the- an inc- an- a- actually a remarkable thing.

132
00:07:04,658 --> 00:07:09,918
It's just, look- look closely at your hands  and- and-  .

133
00:07:09,918 --> 00:07:09,918
..

134
00:07:09,918 --> 00:07:11,528
think of all the things you can do with your hands

135
00:07:11,528 --> 00:07:12,488
which is a lot.

136
00:07:12,488 --> 00:07:14,038
 I can think of many things.

137
00:07:14,038 --> 00:07:17,168
   Yeah, I was just thinking about something.

138
00:07:17,168 --> 00:07:20,188
 You know, your- your hand's a very versatile instrument.

139
00:07:20,188 --> 00:07:20,868
Um- Yeah.

140
00:07:20,868 --> 00:07:22,888
You can give a high five.

141
00:07:22,888 --> 00:07:25,288
  Very versatile.

142
00:07:25,288 --> 00:07:29,268
Um, you know, you- you- you can swing a baseball bat.

143
00:07:29,268 --> 00:07:31,468
You can, uh, thread needles.

144
00:07:31,468 --> 00:07:33,728
You can- you can- you can put thread in a needle.

145
00:07:33,728 --> 00:07:37,388
Uh, you can play the piano, violin.

146
00:07:37,388 --> 00:07:41,308
Um, you know, you could disassemble or assemble a car.

147
00:07:41,308 --> 00:07:45,908
The- the hands are inc- incredibly versatile instruments.

148
00:07:45,908 --> 00:07:52,948
Um, and, um, most of the muscles of- of- of the hand are- are actually in the forearm.

149
00:07:52,948 --> 00:07:56,668
So your- your hand is kind of like a- like a- like it's like a puppet

150
00:07:56,668 --> 00:07:58,028
like it's mostly a puppet.

151
00:07:58,028 --> 00:08:01,528
The mus- the muscles are coming from the forearm and they're pulling the tendons

152
00:08:01,528 --> 00:08:08,888
uh, which are l- you know, it- also human tendon design's in- or human- human tendon evolution is incredibly good.

153
00:08:08,888 --> 00:08:12,757
Um, so you've- you've got this web of tendons.

154
00:08:12,757 --> 00:08:18,838
You've- you've- you've got, um, I think- I think ha- the ha- human hand has something like

155
00:08:18,838 --> 00:08:22,737
depending on how you count it, 27 or 28 degrees of freedom per ha- you know

156
00:08:22,737 --> 00:08:24,328
i- i- in- in the hand.

157
00:08:24,328 --> 00:08:27,228
It's, uh, it's amazing.

158
00:08:27,228 --> 00:08:31,708
So, i- in order to create a robot that can

159
00:08:31,708 --> 00:08:34,948
uh, be a generalized, uh, humanoid

160
00:08:34,948 --> 00:08:37,288
you- you must solve the hand- the hands problem.

161
00:08:37,288 --> 00:08:38,148
Yeah.

162
00:08:38,148 --> 00:08:40,948
We had, uh- we had already a- It's- it's- it's got hands

163
00:08:40,948 --> 00:08:41,908
these hands.

164
00:08:41,908 --> 00:08:42,278
 Yeah.

165
00:08:42,278 --> 00:08:45,128
And so is it like, uh, when you were first building Tesla

166
00:08:45,128 --> 00:08:49,428
where the supply chain doesn't exist and now you have to go out and find folks to work with

167
00:08:49,428 --> 00:08:51,908
and you know, build all this vertical integration

168
00:08:51,908 --> 00:08:52,848
get support?

169
00:08:52,848 --> 00:08:57,198
Is it- is it literally like it's just nowhere to be found and you're gonna have to build- Yes.

170
00:08:57,198 --> 00:08:57,198
.

171
00:08:57,198 --> 00:08:57,198
.

172
00:08:57,198 --> 00:08:59,068
all of this stuff up?

173
00:08:59,068 --> 00:08:59,328
Yes.

174
00:08:59,328 --> 00:09:02,488
We- we- we could not actually buy the actuators for any amount of money.

175
00:09:02,488 --> 00:09:03,628
They simply didn't exist.

176
00:09:03,628 --> 00:09:06,178
Even though there are, I don't know

177
00:09:06,178 --> 00:09:10,568
10,000, 20,000 electric motors out there of various sizes and shapes.

178
00:09:10,568 --> 00:09:15,328
Um, we've had to design, uh, every electric motor gearbox

179
00:09:15,328 --> 00:09:17,988
um, out of- and the controlling electronics from scratch

180
00:09:17,988 --> 00:09:19,948
basically from physics first principles.

181
00:09:19,948 --> 00:09:25,688
Um, and then- The good news is you've got a lot of experience with factories over the last couple of decades.

182
00:09:25,688 --> 00:09:26,608
 So- Yeah.

183
00:09:26,608 --> 00:09:26,618
.

184
00:09:26,618 --> 00:09:26,618
..

185
00:09:26,618 --> 00:09:32,188
how challenging is this versus Cybertruck, Model Y- Model X.

186
00:09:32,188 --> 00:09:32,211
.

187
00:09:32,211 --> 00:09:32,218
..

188
00:09:32,218 --> 00:09:34,608
Gigafactory, you know, the- yeah.

189
00:09:34,608 --> 00:09:37,688
The Faberge egg known as the Model X?

190
00:09:37,688 --> 00:09:37,928
 Yeah.

191
00:09:37,928 --> 00:09:39,468
Right.

192
00:09:39,468 --> 00:09:44,448
Um, it's harder than any- any of those things.

193
00:09:44,448 --> 00:09:45,588
Okay.

194
00:09:45,588 --> 00:09:45,968
Yeah.

195
00:09:45,968 --> 00:09:47,728
 Much harder?

196
00:09:47,728 --> 00:09:49,028
Significantly, yeah?

197
00:09:49,028 --> 00:09:49,228
Starship?

198
00:09:49,228 --> 00:09:49,638
Yes.

199
00:09:49,638 --> 00:09:51,628
Well, at least for- Harder than Starship?

200
00:09:51,628 --> 00:09:51,731
.

201
00:09:51,731 --> 00:09:51,918
..

202
00:09:51,918 --> 00:09:53,748
no, not ha-  Starship's harder.

203
00:09:53,748 --> 00:09:56,408
   Okay.

204
00:09:56,408 --> 00:09:56,418
Harder.

205
00:09:56,418 --> 00:09:59,088
So somewhere between a Model X and a Starship?

206
00:09:59,088 --> 00:09:59,118
.

207
00:09:59,118 --> 00:10:00,208
Yeah.

208
00:10:00,208 --> 00:10:00,778
Is it- Yeah.

209
00:10:00,778 --> 00:10:00,778
.

210
00:10:00,778 --> 00:10:00,778
.

211
00:10:00,778 --> 00:10:01,360
is the.

212
00:10:01,360 --> 00:10:01,577
..

213
00:10:01,577 --> 00:10:04,708
What's harder, the hardware or the software?

214
00:10:04,708 --> 00:10:12,258
Right now, we're struggling with the- the final design of the hardware.

215
00:10:12,258 --> 00:10:14,148
D- like I said, it's really primarily the hand.

216
00:10:14,148 --> 00:10:16,928
Uh, and not to dis- just dismiss the rest of the robot

217
00:10:16,928 --> 00:10:19,408
re- rest of it's also, uh, important

218
00:10:19,408 --> 00:10:25,948
but- but the hands are- the hands and inclusive of the forearm are a majority of the engineering difficulty of the entire robot.

219
00:10:25,948 --> 00:10:29,148
And then le- let's assume you get past the hardware challenges.

220
00:10:29,148 --> 00:10:31,428
How much do you sort of get for free

221
00:10:31,428 --> 00:10:34,948
um, based on all the progress that's happening with LLMs?

222
00:10:34,948 --> 00:10:38,508
Will, you know, will consumers just be able to interact with this

223
00:10:38,508 --> 00:10:39,478
talk to the robot- Oh, yeah.

224
00:10:39,478 --> 00:10:39,478
.

225
00:10:39,478 --> 00:10:39,478
.

226
00:10:39,478 --> 00:10:42,238
ask it to do things, it'll understand and sort of- Oh

227
00:10:42,238 --> 00:10:42,437
yeah.

228
00:10:42,437 --> 00:10:43,788
Yeah.

229
00:10:43,788 --> 00:10:43,838
Yeah.

230
00:10:43,838 --> 00:10:44,908
Yeah, no problem.

231
00:10:44,908 --> 00:10:46,788
You- you're spending a lot of time with Annie

232
00:10:46,788 --> 00:10:48,767
I noticed online.

233
00:10:48,767 --> 00:10:49,928
 Not- not that long.

234
00:10:49,928 --> 00:10:50,878
Um- Yeah.

235
00:10:50,878 --> 00:10:50,878
.

236
00:10:50,878 --> 00:10:50,878
.

237
00:10:50,878 --> 00:10:53,508
maybe I went a little over the top promoting Rock Imagine but

238
00:10:53,508 --> 00:10:56,648
uh-  Well, I- but in all seriousness

239
00:10:56,648 --> 00:11:02,248
those characters and these robots that seems to be

240
00:11:02,248 --> 00:11:04,598
you know, like maybe they- You could get the embodiments of Annie

241
00:11:04,598 --> 00:11:05,598
I suppose.

242
00:11:05,598 --> 00:11:06,158
Yeah.

243
00:11:06,158 --> 00:11:08,528
Why- why the human form factor, Elon?

244
00:11:08,528 --> 00:11:17,258
You could make something that's maybe better than a human or maybe simpler than a human to do specific tasks and maybe better than a human to do more things than a human can do.

245
00:11:17,258 --> 00:11:22,028
How'd you decide to make it just like a human?

246
00:11:22,028 --> 00:11:24,367
Well, if you wanted to do all the things that a human can do

247
00:11:24,367 --> 00:11:27,568
it turns out you need a humanoid robot.

248
00:11:27,568 --> 00:11:30,788
Um, so if you wanted to do a subset

249
00:11:30,788 --> 00:11:32,367
it- that's much easier.

250
00:11:32,367 --> 00:11:41,988
Um, but, uh, i- it turns out humans evolved to this- this shape and capabilities that we- we- we have

251
00:11:41,988 --> 00:11:46,548
um, i- it- it- for- for good reasons.

252
00:11:46,548 --> 00:11:52,888
Uh, it actually is- there- there is a- like there's value to having five- you know

253
00:11:52,888 --> 00:11:54,218
four fingers and a thumb.

254
00:11:54,218 --> 00:11:59,428
Um, and e- even the pinky actually is- is quite useful.

255
00:11:59,428 --> 00:12:02,648
Um, uh, toes are a much more question mark

256
00:12:02,648 --> 00:12:06,508
but- but- but the fingers-   Well

257
00:12:06,508 --> 00:12:09,778
also humans- I- Humans have designed the world as well

258
00:12:09,778 --> 00:12:10,928
so we designed it for us.

259
00:12:10,928 --> 00:12:11,208
For us.

260
00:12:11,208 --> 00:12:11,218
Right.

261
00:12:11,218 --> 00:12:12,028
So- Exactly.

262
00:12:12,028 --> 00:12:12,038
.

263
00:12:12,038 --> 00:12:12,038
..

264
00:12:12,038 --> 00:12:16,328
if you could make a humanoid robot, it'll be immediately backwards compatible with what we've built the world for.

265
00:12:16,328 --> 00:12:17,208
Precisely.

266
00:12:17,208 --> 00:12:18,538
M- m- Elon, there's another- Yeah.

267
00:12:18,538 --> 00:12:21,348
There's another part of, um, the robot.

268
00:12:21,348 --> 00:12:25,408
So there's the LLMs, there's the actuation in the hands

269
00:12:25,408 --> 00:12:31,548
but also there's the, um, the silicon that runs it and there was.

270
00:12:31,548 --> 00:12:31,818
..

271
00:12:31,818 --> 00:12:43,668
You know, Dojo, I think you, you posted on X AI5 and AI6 and it just seemed like you were incredibly excited about the direction in which the silicon layer was also going.

272
00:12:43,668 --> 00:12:47,516
Can you tell us about that and what that is and what

273
00:12:47,516 --> 00:12:49,588
what, what, what are we ac- what are we building here?

274
00:12:49,588 --> 00:12:50,288
What is being built?

275
00:12:50,288 --> 00:12:53,648
Is it a compliment to everything that exists in the world?

276
00:12:53,648 --> 00:12:55,348
Is it a potential long-term competitor?

277
00:12:55,348 --> 00:12:57,867
What is it?

278
00:12:57,867 --> 00:13:00,388
Um, yeah.

279
00:13:00,388 --> 00:13:06,988
So, at T- at Tesla we basically had two different chip programs

280
00:13:06,988 --> 00:13:09,016
one for Dojo and one, uh.

281
00:13:09,016 --> 00:13:09,288
..

282
00:13:09,288 --> 00:13:11,918
D- Dojo on the training side, and then what we call

283
00:13:11,918 --> 00:13:16,828
you know, AI4, which is our inference chip.

284
00:13:16,828 --> 00:13:21,468
Um, uh, the, the AI4 is what's currently shipping in all vehicles

285
00:13:21,468 --> 00:13:24,768
um, and we're finalize- finalizing the design of AI5

286
00:13:24,768 --> 00:13:28,568
which will be an immense jump from AI4.

287
00:13:28,568 --> 00:13:35,827
Um, by some metrics, the improvement in AI5 will be 40 times better than AI4.

288
00:13:35,827 --> 00:13:36,528
Wow.

289
00:13:36,528 --> 00:13:39,108
So 40%, 40 times.

290
00:13:39,108 --> 00:13:49,028
Um, and, and, uh, this is because we work so closely at a very fine grain level on the AI software and the AI hardware.

291
00:13:49,028 --> 00:13:53,158
So we know exactly where the limiting factors are and

292
00:13:53,158 --> 00:13:57,808
and, um, and so effectively the AI hardware and software teams are co-designing the chip.

293
00:13:57,808 --> 00:13:59,506
Um.

294
00:13:59,506 --> 00:13:59,958
..

295
00:13:59,958 --> 00:14:03,361
So a 40x improvement in the silicon, I think then as it.

296
00:14:03,361 --> 00:14:03,637
..

297
00:14:03,637 --> 00:14:05,928
As everybody here in the audience experiences it

298
00:14:05,928 --> 00:14:14,387
is that just an almost like an order of magnitude increase in the quality of FSD and the safety that you experience as a Tesla driver

299
00:14:14,387 --> 00:14:15,748
and then the quality of the robot?

300
00:14:15,748 --> 00:14:18,228
Like where does it all manifest when you

301
00:14:18,228 --> 00:14:22,228
when you, you know, bring it up and actually get it into production?

302
00:14:22,228 --> 00:14:25,229
Yeah, to be precise, the 40x is on.

303
00:14:25,229 --> 00:14:25,458
..

304
00:14:25,458 --> 00:14:30,908
If you said like compared to the worst limitation on AI4

305
00:14:30,908 --> 00:14:34,188
which is running the Softmax operation- Yeah.

306
00:14:34,188 --> 00:14:34,198
.

307
00:14:34,198 --> 00:14:34,198
..

308
00:14:34,198 --> 00:14:39,528
uh, we currently have to run Softmax in around 40 steps in emulation mode.

309
00:14:39,528 --> 00:14:42,148
Whereas that'll be, just be done in a few steps

310
00:14:42,148 --> 00:14:44,648
uh, natively in AI5.

311
00:14:44,648 --> 00:14:47,250
Um, AI5 should also be, uh.

312
00:14:47,250 --> 00:14:47,438
..

313
00:14:47,438 --> 00:14:51,528
Easily handle mixed precision, um, models.

314
00:14:51,528 --> 00:14:58,968
So you don't hav- i- i- it'll dynamically handle mixed preci- There's a bunch of sort of technical stuff that AI5 will do a lot better.

315
00:14:58,968 --> 00:15:02,867
Um, in terms of, uh, of nominal sort of r- uh

316
00:15:02,867 --> 00:15:06,548
raw compute, it's, it's eight times more compute.

317
00:15:06,548 --> 00:15:10,718
Um, about nine times more memory, uh

318
00:15:10,718 --> 00:15:13,088
roughly five times more memory bandwidth.

319
00:15:13,088 --> 00:15:18,568
Um, so, uh, but because we're addressing some core limitations in AI4

320
00:15:18,568 --> 00:15:20,193
you multiply that by.

321
00:15:20,193 --> 00:15:20,637
..

322
00:15:20,637 --> 00:15:24,648
That, that, that 8x compute improvement by another 5x improvement because of

323
00:15:24,648 --> 00:15:27,148
of, uh, optimization at a, at a

324
00:15:27,148 --> 00:15:31,808
at a very fine grain silicon level of things that are currently suboptimal in AI4

325
00:15:31,808 --> 00:15:33,668
that's where you get the 40x improvement.

326
00:15:33,668 --> 00:15:36,008
You had, um- Um- Oh, keep going.

327
00:15:36,008 --> 00:15:36,248
Keep going.

328
00:15:36,248 --> 00:15:38,748
Uh, so now, now that said, I th- I'm

329
00:15:38,748 --> 00:15:41,098
I, I am confident that the current chi- uh

330
00:15:41,098 --> 00:15:45,168
chips, uh, AI, AI4 chips that are in the cars will

331
00:15:45,168 --> 00:15:52,458
uh, achieve self-driving safety that is at least two to three times that of the c- of human and

332
00:15:52,458 --> 00:15:54,948
and maybe even 10x.

333
00:15:54,948 --> 00:15:54,958
Wow.

334
00:15:54,958 --> 00:15:58,318
Um, and the software that, uh, will be released for that is

335
00:15:58,318 --> 00:15:59,698
is coming out over the next, uh

336
00:15:59,698 --> 00:16:01,208
few months.

337
00:16:01,208 --> 00:16:01,238
Mm-hmm.

338
00:16:01,238 --> 00:16:04,718
So version 14 will be the biggest, uh

339
00:16:04,718 --> 00:16:07,668
upgrade in Tesla software since version 12.

340
00:16:07,668 --> 00:16:13,908
Um, we are increasing the, uh, parameter count by an order of magnitude.

341
00:16:13,908 --> 00:16:17,117
Um, the, th- th- th- th- there's

342
00:16:17,117 --> 00:16:20,478
there's, there's a lot of, uh, reinforcement learning that's been used.

343
00:16:20,478 --> 00:16:23,251
There's, um.

344
00:16:23,251 --> 00:16:24,587
..

345
00:16:24,587 --> 00:16:25,237
We, we, we.

346
00:16:25,237 --> 00:16:25,318
..

347
00:16:25,318 --> 00:16:26,292
There, there were, there was a.

348
00:16:26,292 --> 00:16:26,396
..

349
00:16:26,396 --> 00:16:27,588
Like you can think of AI sort of

350
00:16:27,588 --> 00:16:30,588
eh, as a way of compressing reality and

351
00:16:30,588 --> 00:16:32,988
and l- and some of those compression steps

352
00:16:32,988 --> 00:16:36,718
uh, we, uh, were, were too lossy and

353
00:16:36,718 --> 00:16:39,968
and we addressed the lossiness in the compression steps.

354
00:16:39,968 --> 00:16:42,397
Um, so the- these are all software updates that'll

355
00:16:42,397 --> 00:16:43,008
that'll go out.

356
00:16:43,008 --> 00:16:44,788
So just over-the-air updates.

357
00:16:44,788 --> 00:16:48,887
Um, your car is going to feel like it is sentient by the end of the year.

358
00:16:48,887 --> 00:16:50,238
Yeah.

359
00:16:50,238 --> 00:16:52,608
  It feels that way already, to be honest.

360
00:16:52,608 --> 00:17:01,098
 Um, I saw in the trades that you spent about $17 billion on some spectrum and that- Oh

361
00:17:01,098 --> 00:17:01,428
yeah.

362
00:17:01,428 --> 00:17:01,438
 Yeah.

363
00:17:01,438 --> 00:17:11,548
Um, so some cash change, um, to enable your satellites and the Starlink network to connect directly with phones.

364
00:17:11,548 --> 00:17:13,367
What will that look like in a year or two?

365
00:17:13,367 --> 00:17:19,248
Are we going to drop our Verizon account and just expand our Starlink account?

366
00:17:19,248 --> 00:17:20,417
Uh.

367
00:17:20,417 --> 00:17:20,558
..

368
00:17:20,558 --> 00:17:23,598
 We're kind of hoping 'cause-  .

369
00:17:23,598 --> 00:17:23,598
..

370
00:17:23,598 --> 00:17:25,127
Verizon kind of sucks.

371
00:17:25,127 --> 00:17:28,048
Um, how many, how many of you want a Starlink phone?

372
00:17:28,048 --> 00:17:30,628
Who wants a Starlink phone?

373
00:17:30,628 --> 00:17:31,708
  Oh.

374
00:17:31,708 --> 00:17:33,488
Is it, is it technically possible?

375
00:17:33,488 --> 00:17:33,965
Uh, Elon, I know you can't.

376
00:17:33,965 --> 00:17:33,998
..

377
00:17:33,998 --> 00:17:35,228
I know you can't see it, but it's everyone.

378
00:17:35,228 --> 00:17:35,838
Yeah, exactly.

379
00:17:35,838 --> 00:17:37,188
 All right, cool.

380
00:17:37,188 --> 00:17:41,808
Um, so this is a kind of a long term thing.

381
00:17:41,808 --> 00:17:48,548
Uh, it, it, it will allow SpaceX to ha- uh

382
00:17:48,548 --> 00:17:56,048
deliver high bandwidth connectivity directly from the satellites to the phones.

383
00:17:56,048 --> 00:18:00,247
Um, but, uh, there are hardware changes that need to happen in the phone so the.

384
00:18:00,247 --> 00:18:00,398
..

385
00:18:00,398 --> 00:18:03,967
And since these frequencies are not supported in current phones

386
00:18:03,967 --> 00:18:09,184
uh, they, the chipset has to be modified to in- in- add these frequencies.

387
00:18:09,184 --> 00:18:09,404
..

388
00:18:09,404 --> 00:18:11,204
. um, and that probably is a two-year timeframe

389
00:18:11,204 --> 00:18:18,764
so the phones that, um, are able to use the spectrum that was acquired probably start shipping in around two years.

390
00:18:18,764 --> 00:18:25,984
Um, and, um, a- and then we also need to build the satellites that are gonna communicate on those frequencies.

391
00:18:25,984 --> 00:18:33,004
So in parallel, we're building the satellites and working with the handset makers to add these frequencies to the phones.

392
00:18:33,004 --> 00:18:39,244
Um, and then the, the satellites and the phones will then handshake very well to achieve high bandwidth connectivity.

393
00:18:39,244 --> 00:18:42,043
But the, the net effect is that you should be able to watch

394
00:18:42,043 --> 00:18:45,924
uh, videos, uh, anywhere on your phone.

395
00:18:45,924 --> 00:18:47,303
Wow.

396
00:18:47,303 --> 00:18:47,774
And it's gonna- Phone is good.

397
00:18:47,774 --> 00:18:47,774
.

398
00:18:47,774 --> 00:18:47,774
.

399
00:18:47,774 --> 00:18:48,293
be crazy.

400
00:18:48,293 --> 00:18:48,786
And what .

401
00:18:48,786 --> 00:18:48,793
..

402
00:18:48,793 --> 00:18:51,083
A- and do these, do these frequencies

403
00:18:51,083 --> 00:18:53,244
would they work indoors, inside buildings?

404
00:18:53,244 --> 00:18:53,974
You know, like, like- Yes.

405
00:18:53,974 --> 00:18:53,974
.

406
00:18:53,974 --> 00:18:53,974
.

407
00:18:53,974 --> 00:18:54,884
your phone currently does?

408
00:18:54,884 --> 00:18:55,704
Okay.

409
00:18:55,704 --> 00:18:56,924
Oh.

410
00:18:56,924 --> 00:18:58,894
And so will you be able to have basically like- No

411
00:18:58,894 --> 00:19:00,364
no, if you, if you, if you're in a building with a

412
00:19:00,364 --> 00:19:01,924
with a, like, a, a thick metal roof

413
00:19:01,924 --> 00:19:05,043
then no, but-  Um- No, the sa- the same types of

414
00:19:05,043 --> 00:19:05,684
of, um- Yeah, yeah.

415
00:19:05,684 --> 00:19:06,434
Normal- Yeah.

416
00:19:06,434 --> 00:19:06,566
.

417
00:19:06,566 --> 00:19:06,573
..

418
00:19:06,573 --> 00:19:07,764
normal homes, yes, yeah.

419
00:19:07,764 --> 00:19:08,283
Yeah.

420
00:19:08,283 --> 00:19:09,086
Elon, is your vision- Um- .

421
00:19:09,086 --> 00:19:09,094
..

422
00:19:09,094 --> 00:19:10,573
for this that instead of, you know

423
00:19:10,573 --> 00:19:12,354
having an AT&T account or .

424
00:19:12,354 --> 00:19:12,354
..

425
00:19:12,354 --> 00:19:15,474
and then roaming when you're in the UK or you're in India

426
00:19:15,474 --> 00:19:18,244
it's just, we could have one direct deal with Starlink

427
00:19:18,244 --> 00:19:21,563
it works all over the world eventually, not today

428
00:19:21,563 --> 00:19:22,904
but at some point?

429
00:19:22,904 --> 00:19:26,063
Is that the end goal, that basically we don't need a- Yeah.

430
00:19:26,063 --> 00:19:26,086
.

431
00:19:26,086 --> 00:19:26,094
..

432
00:19:26,094 --> 00:19:28,504
regional carrier, we have a global carrier

433
00:19:28,504 --> 00:19:30,524
and that would be you?

434
00:19:30,524 --> 00:19:32,004
Uh, that, that would be one of the options.

435
00:19:32,004 --> 00:19:34,424
To be clear, we're not, we're not gonna put the other carriers out of business.

436
00:19:34,424 --> 00:19:37,844
They're still gonna be around 'cause they, they own a lot of spectrum.

437
00:19:37,844 --> 00:19:40,667
So, uh, there's, uh .

438
00:19:40,667 --> 00:19:40,674
..

439
00:19:40,674 --> 00:19:43,543
but, but yes, you'll, you should be able to have a Starlink

440
00:19:43,543 --> 00:19:46,744
uh, like you have, like you have an AT&T or T-Mobile

441
00:19:46,744 --> 00:19:47,704
Verizon or whatever.

442
00:19:47,704 --> 00:19:49,033
You should be a- you, you could have a

443
00:19:49,033 --> 00:19:51,944
a, a, you know, account with Starlink that

444
00:19:51,944 --> 00:19:54,384
uh, works with your, you know, Starlink

445
00:19:54,384 --> 00:20:00,364
uh, antenna at home wi- uh, with your wifi as well as on your phone.

446
00:20:00,364 --> 00:20:08,323
And, um, yeah, it would be a comprehensive solution for high bandwidth at home and for high bandwidth direct to cell.

447
00:20:08,323 --> 00:20:09,894
Could you buy some carriers- Um- .

448
00:20:09,894 --> 00:20:09,894
..

449
00:20:09,894 --> 00:20:12,083
to have more spectrum?

450
00:20:12,083 --> 00:20:13,467
 Maybe even- Um- .

451
00:20:13,467 --> 00:20:13,474
..

452
00:20:13,474 --> 00:20:14,224
buy Verizon?

453
00:20:14,224 --> 00:20:15,014
Not out of the question.

454
00:20:15,014 --> 00:20:15,793
I suppose it, it, it .

455
00:20:15,793 --> 00:20:15,793
..

456
00:20:15,793 --> 00:20:17,503
that may happen.

457
00:20:17,503 --> 00:20:19,347
 Let's talk about, um .

458
00:20:19,347 --> 00:20:19,353
..

459
00:20:19,353 --> 00:20:20,624
Let's talk about Starship.

460
00:20:20,624 --> 00:20:25,293
You just had a really what appeared to be a phenomenal

461
00:20:25,293 --> 00:20:27,824
um, launch.

462
00:20:27,824 --> 00:20:30,714
Ho- how close is it to, you know

463
00:20:30,714 --> 00:20:35,303
being predictable and ready to go in a commercial setting?

464
00:20:35,303 --> 00:20:41,324
I, I th- I think we will recover the ship next year.

465
00:20:41,324 --> 00:20:44,644
Um, we've got one more launch of the

466
00:20:44,644 --> 00:20:48,964
um, Starlink version two, uh, s- uh

467
00:20:48,964 --> 00:20:52,353
stack, but there's only one, one, uh

468
00:20:52,353 --> 00:20:55,103
booster and ship left that's in the version two

469
00:20:55,103 --> 00:20:57,603
uh, design.

470
00:20:57,603 --> 00:20:59,824
Uh, and then thereafter it's, it's version three

471
00:20:59,824 --> 00:21:02,624
which is a gigantic upgrade 'cause that's got Raptor 3

472
00:21:02,624 --> 00:21:07,224
um, and pretty much everything changes on the rocket with version three.

473
00:21:07,224 --> 00:21:12,464
Um, so version three, you know, might have some initial teething pains

474
00:21:12,464 --> 00:21:15,164
uh, 'cause it's such a radical redesign.

475
00:21:15,164 --> 00:21:21,364
Uh, but, uh, it's, it's capable of over 100 tons to orbit

476
00:21:21,364 --> 00:21:23,184
fully reusable.

477
00:21:23,184 --> 00:21:24,504
Um, and I think it's, I think

478
00:21:24,504 --> 00:21:29,144
I think, um, unless we have s- unless we have some very s- major setbacks

479
00:21:29,144 --> 00:21:34,364
uh, SpaceX will demonstrate a full reusability next year

480
00:21:34,364 --> 00:21:37,124
uh, catching both the booster and the ship

481
00:21:37,124 --> 00:21:42,094
um, and being able to deliver over 100 tons to a useful orbit.

482
00:21:42,094 --> 00:21:49,103
Um- How- What is the best rocket in the world do now in terms of tonnage to space?

483
00:21:49,103 --> 00:21:53,344
Uh, well, in terms of, uh

484
00:21:53,344 --> 00:21:56,594
sort of commercial rockets, there's, there's Falcon Heavy- Yeah.

485
00:21:56,594 --> 00:21:56,594
.

486
00:21:56,594 --> 00:21:56,594
.

487
00:21:56,594 --> 00:21:59,444
uh, which will do, uh, in

488
00:21:59,444 --> 00:22:00,066
um .

489
00:22:00,066 --> 00:22:00,072
..

490
00:22:00,072 --> 00:22:04,274
wi- with, with side booster reuse, uh

491
00:22:04,274 --> 00:22:06,783
will do about 40 tons.

492
00:22:06,783 --> 00:22:07,524
Hmm.

493
00:22:07,524 --> 00:22:08,543
So this is five times- Um- .

494
00:22:08,543 --> 00:22:08,583
..

495
00:22:08,583 --> 00:22:10,303
bigger, yeah.

496
00:22:10,303 --> 00:22:13,144
Well, two and a half times bigger in- Interesting.

497
00:22:13,144 --> 00:22:15,283
But, but Starship will be full reuse

498
00:22:15,283 --> 00:22:16,184
full reusability.

499
00:22:16,184 --> 00:22:16,603
Got it.

500
00:22:16,603 --> 00:22:17,333
Okay.

501
00:22:17,333 --> 00:22:18,944
Hey, uh- So everything comes back.

502
00:22:18,944 --> 00:22:19,033
Yeah.

503
00:22:19,033 --> 00:22:22,204
Elon, after, after the explosion that happened

504
00:22:22,204 --> 00:22:25,644
um, with the, the, the, the failed launch

505
00:22:25,644 --> 00:22:27,603
um, there was a lot of- Which one?

506
00:22:27,603 --> 00:22:28,444
Sorry?

507
00:22:28,444 --> 00:22:30,134
 Which, which failed launch?

508
00:22:30,134 --> 00:22:31,224
Oh, oh, the more recent one.

509
00:22:31,224 --> 00:22:31,813
The more recent- Okay.

510
00:22:31,813 --> 00:22:31,813
.

511
00:22:31,813 --> 00:22:31,813
.

512
00:22:31,813 --> 00:22:33,444
one with the Starship he- with the he- The big boom.

513
00:22:33,444 --> 00:22:34,174
Yeah.

514
00:22:34,174 --> 00:22:35,524
And so with- The big boom on the base.

515
00:22:35,524 --> 00:22:35,619
.

516
00:22:35,619 --> 00:22:35,793
..

517
00:22:35,793 --> 00:22:36,293
with the Starship, and, and, and- Oh

518
00:22:36,293 --> 00:22:36,313
yeah, yeah.

519
00:22:36,313 --> 00:22:36,313
.

520
00:22:36,313 --> 00:22:36,313
.

521
00:22:36,313 --> 00:22:42,504
there was a lot of, there was a lot of proclamations that there's gonna be environmental and FAA and all these other sorts.

522
00:22:42,504 --> 00:22:42,514
Oh, yeah.

523
00:22:42,514 --> 00:22:48,024
The recovery back to the launch pad again was incredibly fast.

524
00:22:48,024 --> 00:22:52,144
How did you get back so fast, not just technically and work-wise

525
00:22:52,144 --> 00:22:55,083
but just, like, regulatory clearance wise?

526
00:22:55,083 --> 00:22:58,424
Because they said there were gonna be all these questions and reviews and so on.

527
00:22:58,424 --> 00:23:00,124
How did, how did you guys manage that?

528
00:23:00,124 --> 00:23:03,024
Uh, well, there were a lot of questions and reviews.

529
00:23:03,024 --> 00:23:04,364
We got through them all.

530
00:23:04,364 --> 00:23:06,164
Um, and credit to the SpaceX team.

531
00:23:06,164 --> 00:23:09,543
They worked incredibly hard, and they, uh

532
00:23:09,543 --> 00:23:14,154
got the next ship and booster tested and on the pad and

533
00:23:14,154 --> 00:23:14,684
and flown.

534
00:23:14,684 --> 00:23:18,144
And, um, yeah, hu- huge credit to the SpaceX team.

535
00:23:18,144 --> 00:23:18,174
Wow.

536
00:23:18,174 --> 00:23:21,083
Very proud of them for doing, doing such a job

537
00:23:21,083 --> 00:23:23,063
a, a great job recovering.

538
00:23:23,063 --> 00:23:31,324
Um, I, I mean, it's just creating a fully reusable orbital rocket is one of the hardest engineering problems ever.

539
00:23:31,324 --> 00:23:37,454
Certainly, uh, you know, a candidate for most difficult engineering  project ever.

540
00:23:37,454 --> 00:23:40,063
Uh, uh, you know, it's on the podium at least.

541
00:23:40,063 --> 00:23:43,187
Um, so it's a .

542
00:23:43,187 --> 00:23:43,194
..

543
00:23:43,194 --> 00:23:46,344
tha- that, that's been the goal of SpaceX from the beginning

544
00:23:46,344 --> 00:23:48,103
from 2002.

545
00:23:48,103 --> 00:23:50,004
Um, and here we are 23 years later.

546
00:23:50,004 --> 00:23:52,744
So it's, it's a long journey.

547
00:23:52,744 --> 00:23:55,734
A- and, um, with, with a

548
00:23:55,734 --> 00:23:57,324
a super tal- like, by far the

549
00:23:57,324 --> 00:24:01,563
I think the most talented group of rocket engineers that's ever been assembled.

550
00:24:01,563 --> 00:24:06,664
Um, and, uh, you know, finally next year

551
00:24:06,664 --> 00:24:10,018
I think we'll be able to achieve full reusability.

552
00:24:10,018 --> 00:24:16,496
Elon, what are the big, um, technical blockers that you're focused on there between now and that full reusability?

553
00:24:16,496 --> 00:24:23,636
Are there some showstoppers where you're just kind of literally just obsessing over trying to figure out still?

554
00:24:23,636 --> 00:24:32,176
Or is it more about getting through a sort of a laundry list of your learnings and just integrating it into the next launch?

555
00:24:32,176 --> 00:24:37,446
Well, the- the.

556
00:24:37,446 --> 00:24:38,016
..

557
00:24:38,016 --> 00:24:44,796
For- for- for reusability of the ship, there's still a lot of work that remains on the heat shield.

558
00:24:44,796 --> 00:24:47,696
So no one's ever made a fully reusable orbital heat shield.

559
00:24:47,696 --> 00:24:52,956
Like, the shuttle heat shield, uh, had to go through nine months of repair after every flight.

560
00:24:52,956 --> 00:24:53,636
Right.

561
00:24:53,636 --> 00:24:58,716
Um, so n- no one has ever made a fully reusable orbital heat shield.

562
00:24:58,716 --> 00:25:01,196
Um- And is that a material science problem

563
00:25:01,196 --> 00:25:03,356
or is that an engineering problem, or both?

564
00:25:03,356 --> 00:25:07,196
Uh, yeah, I mean, it's a ma- material science eng- engineering problem.

565
00:25:07,196 --> 00:25:08,123
So it's.

566
00:25:08,123 --> 00:25:08,406
..

567
00:25:08,406 --> 00:25:13,616
But we really are, uh, looking at the fundamental physics here.

568
00:25:13,616 --> 00:25:20,076
Um, again, physics-first principles and trying to figure out how do we make something that

569
00:25:20,076 --> 00:25:24,040
um, is, uh, it, you know.

570
00:25:24,040 --> 00:25:24,266
..

571
00:25:24,266 --> 00:25:26,996
it can withstand the heat, it's very light

572
00:25:26,996 --> 00:25:31,556
doesn't transmit the heat to the- the primary sh- Yeah.

573
00:25:31,556 --> 00:25:31,566
.

574
00:25:31,566 --> 00:25:31,566
..

575
00:25:31,566 --> 00:25:33,256
primary structure.

576
00:25:33,256 --> 00:25:38,526
Um, and, uh, a- a- where- But then whose integrity is intact?

577
00:25:38,526 --> 00:25:38,539
.

578
00:25:38,539 --> 00:25:38,546
..

579
00:25:38,546 --> 00:25:41,236
all the tiles stay on and they don't crack.

580
00:25:41,236 --> 00:25:41,756
Yeah.

581
00:25:41,756 --> 00:25:45,176
Um, uh, and then as you ascend

582
00:25:45,176 --> 00:25:47,716
if you hit some rain, you know

583
00:25:47,716 --> 00:25:50,316
the tiles don't dissolve in rain.

584
00:25:50,316 --> 00:25:52,976
There's- there's a lot of different issues.

585
00:25:52,976 --> 00:25:56,396
And- and then you really need to know that these tiles are working.

586
00:25:56,396 --> 00:26:01,236
You- you can't, uh, you know, go through this laborious in- inspection.

587
00:26:01,236 --> 00:26:03,136
So it really needs to be where, you know

588
00:26:03,136 --> 00:26:13,246
these- these tens of thousands of tiles all work and don't need to be refurbished or checked one by one.

589
00:26:13,246 --> 00:26:14,396
That was the case with the shuttle.

590
00:26:14,396 --> 00:26:17,916
Can we maybe, um, switch now?

591
00:26:17,916 --> 00:26:21,416
It's in- I mean, who- who else were you talk- about Tesla then you go to SpaceX.

592
00:26:21,416 --> 00:26:21,876
Yeah.

593
00:26:21,876 --> 00:26:25,386
Now, I'd- I'd like to ask you some questions about Grok and

594
00:26:25,386 --> 00:26:27,356
um, xAI.

595
00:26:27,356 --> 00:26:29,496
Um, you want to just give us an update?

596
00:26:29,496 --> 00:26:32,356
I think you- you kind of talked about where the next gen model is

597
00:26:32,356 --> 00:26:36,796
and you said something incredible, I still don't think people really understand it which is

598
00:26:36,796 --> 00:26:40,476
you know, there's gonna be a next training run where you expect

599
00:26:40,476 --> 00:26:43,776
you know, not to start from the

600
00:26:43,776 --> 00:26:45,876
you know, common web and common crawl

601
00:26:45,876 --> 00:26:49,205
where you expected an enormous amount of synthetic data.

602
00:26:49,205 --> 00:26:57,276
Just tell us about how, uh, the evolution of Grok is going and this innovation and why it's so important.

603
00:26:57,276 --> 00:26:59,596
Yeah.

604
00:26:59,596 --> 00:27:01,586
So we're- we're running a lot of.

605
00:27:01,586 --> 00:27:01,726
..

606
00:27:01,726 --> 00:27:04,156
using a lot of an- of inference compute and

607
00:27:04,156 --> 00:27:12,156
um, and reasoning to look at all of the source data which is really like the corpus of human knowledge.

608
00:27:12,156 --> 00:27:19,996
And then, uh, thinking about each piece of information and then adding mod- a- a- adding what's missing

609
00:27:19,996 --> 00:27:27,356
um, and correcting- correcting mistakes and removing falsehoods from this- from that training data.

610
00:27:27,356 --> 00:27:28,565
So it's- it's- it's.

611
00:27:28,565 --> 00:27:28,705
..

612
00:27:28,705 --> 00:27:31,216
Like if you take, say, Wikipedia as an example

613
00:27:31,216 --> 00:27:33,916
but this really applies to- to books, PDFs

614
00:27:33,916 --> 00:27:39,416
uh, the websites, uh, every form of information.

615
00:27:39,416 --> 00:27:41,516
Um, the.

616
00:27:41,516 --> 00:27:42,006
..

617
00:27:42,006 --> 00:27:47,925
Grok is using, um, heavy amounts of inference compute to

618
00:27:47,925 --> 00:27:49,996
say, to look at, as an example

619
00:27:49,996 --> 00:27:54,116
a Wikipedia page and say, uh, "What is true

620
00:27:54,116 --> 00:27:57,946
partially true or false, or missing, uh

621
00:27:57,946 --> 00:27:59,216
in this page?

622
00:27:59,216 --> 00:28:03,962
Now rewrite the page to inc- to correct the.

623
00:28:03,962 --> 00:28:04,246
..

624
00:28:04,246 --> 00:28:12,913
remove the falsehoods, uh- uh, correct the half-truths and add the missing context.

625
00:28:12,913 --> 00:28:15,196
" Well, Elon, by the way, could you just publish that?

626
00:28:15,196 --> 00:28:17,056
Could we create like a Grokopedia?

627
00:28:17,056 --> 00:28:20,676
I mean that would be- Yeah, especially for our bio pages which are a disaster.

628
00:28:20,676 --> 00:28:21,419
 .

629
00:28:21,419 --> 00:28:21,425
..

630
00:28:21,425 --> 00:28:21,425
very valuable.

631
00:28:21,425 --> 00:28:21,677
Because Wikipedia.

632
00:28:21,677 --> 00:28:21,726
..

633
00:28:21,726 --> 00:28:21,726
Yeah.

634
00:28:21,726 --> 00:28:25,735
Wikipedia is so biased and it's- it's a constant war.

635
00:28:25,735 --> 00:28:29,710
You know, if something gets corrected, five minutes later there'll be an army of people trying to.

636
00:28:29,710 --> 00:28:29,946
..

637
00:28:29,946 --> 00:28:34,446
I mean it's become hyper partisan and there's activists all over it.

638
00:28:34,446 --> 00:28:35,106
Hyper political.

639
00:28:35,106 --> 00:28:35,106
True.

640
00:28:35,106 --> 00:28:36,996
So if you do fix, for example

641
00:28:36,996 --> 00:28:44,616
Wikipedia as a source of truth, it'd be great to publish that just so the world has it.

642
00:28:44,616 --> 00:28:44,946
All right.

643
00:28:44,946 --> 00:28:46,936
I'll talk- talk with you about that.

644
00:28:46,936 --> 00:28:50,576
So talk to the team about that like a Grokipedia or whatever.

645
00:28:50,576 --> 00:28:52,256
This- here's the Grokipedia version.

646
00:28:52,256 --> 00:28:54,026
Um-  It'd be interesting, yeah.

647
00:28:54,026 --> 00:28:55,309
And then just have it out there for.

648
00:28:55,309 --> 00:28:55,386
..

649
00:28:55,386 --> 00:28:57,053
 Where.

650
00:28:57,053 --> 00:28:57,266
..

651
00:28:57,266 --> 00:28:58,858
In- in terms of, um.

652
00:28:58,858 --> 00:28:59,286
..

653
00:28:59,286 --> 00:29:00,216
People here like it.

654
00:29:00,216 --> 00:29:03,266
Um, in terms of training Grok-5, um

655
00:29:03,266 --> 00:29:08,876
you're- you're scaling up your supercluster in Colossus in- in Memphis.

656
00:29:08,876 --> 00:29:09,186
Can- Whoo!

657
00:29:09,186 --> 00:29:10,196
Colossus 2 now, yeah.

658
00:29:10,196 --> 00:29:11,596
But they're- they have a second one.

659
00:29:11,596 --> 00:29:11,766
Yeah.

660
00:29:11,766 --> 00:29:11,766
Yeah.

661
00:29:11,766 --> 00:29:13,286
Could you- could you give us an update on that?

662
00:29:13,286 --> 00:29:15,336
And then also, as part of that

663
00:29:15,336 --> 00:29:18,216
um, whe- where are we in the scaling laws?

664
00:29:18,216 --> 00:29:20,436
Um, if you scale a bigger cluster

665
00:29:20,436 --> 00:29:22,266
do you get a more powerful AI model?

666
00:29:22,266 --> 00:29:25,282
Is there a point of diminishing returns or.

667
00:29:25,282 --> 00:29:25,606
..

668
00:29:25,606 --> 00:29:28,496
Like how much more compu- if you throw twice as much compute at it

669
00:29:28,496 --> 00:29:31,596
do you get a 10% better model?

670
00:29:31,596 --> 00:29:32,996
Do you get a 100% better model?

671
00:29:32,996 --> 00:29:34,916
Like is it log linear?

672
00:29:34,916 --> 00:29:35,617
What- what.

673
00:29:35,617 --> 00:29:35,826
..

674
00:29:35,826 --> 00:29:41,656
I guess how much more juice is there left in scaling hardware do you think?

675
00:29:41,656 --> 00:29:47,816
I think- I think there's a natural logarithmic function associated with the amount of compute.

676
00:29:47,816 --> 00:29:55,356
So, uh, then like say for argument's sake like 10X more compute will double the intelligence.

677
00:29:55,356 --> 00:29:58,856
Maybe that's- that might be a rough rule of thumb.

678
00:29:58,856 --> 00:30:00,466
Uh, but, you know, that still means that

679
00:30:00,466 --> 00:30:02,576
you know, you go from 100 IQ to 200 IQ.

680
00:30:02,576 --> 00:30:04,876
Still a pretty- pretty big deal.

681
00:30:04,876 --> 00:30:07,616
Um, so I.

682
00:30:07,616 --> 00:30:08,016
..

683
00:30:08,016 --> 00:30:15,183
And- and I think- I think we'll see intelligence continue to scale all the way up to where-.

684
00:30:15,183 --> 00:30:15,494
..

685
00:30:15,494 --> 00:30:19,544
you know, most of the power of the sun is harnessed for compute

686
00:30:19,544 --> 00:30:22,324
and then ultimately, most of the power of the galaxy.

687
00:30:22,324 --> 00:30:25,344
You know, sort of, Kadachev-IIP, Kadachev-III scale

688
00:30:25,344 --> 00:30:27,464
uh, compute.

689
00:30:27,464 --> 00:30:34,304
Um, so I guess, once you think about artificial intelligence not as sort of this

690
00:30:34,304 --> 00:30:35,554
you know, a destination that you reach

691
00:30:35,554 --> 00:30:41,024
but really, uh, as part of the overall escalation of intelligence

692
00:30:41,024 --> 00:30:44,824
um, that, that, that we are aware of.

693
00:30:44,824 --> 00:30:49,184
Um, you know, human intelligence has also scaled as you've ha- have

694
00:30:49,184 --> 00:30:52,544
uh, as the population has increased, um

695
00:30:52,544 --> 00:30:55,944
and we've been able to store more and more information

696
00:30:55,944 --> 00:30:57,464
uh, human intelligence has scaled.

697
00:30:57,464 --> 00:30:58,241
Now, human.

698
00:30:58,241 --> 00:30:58,434
..

699
00:30:58,434 --> 00:31:03,454
Because of population declines and low birthrate, human intelligence is

700
00:31:03,454 --> 00:31:06,384
is somewhat plateauing, um, and will actually decline.

701
00:31:06,384 --> 00:31:09,624
And, uh, m- my guess is that

702
00:31:09,624 --> 00:31:20,044
eh, eh, I, I, I think that we might have AI smarter than any single human at anything as soon as next year.

703
00:31:20,044 --> 00:31:20,343
Wow.

704
00:31:20,343 --> 00:31:21,384
Okay.

705
00:31:21,384 --> 00:31:21,874
Yeah.

706
00:31:21,874 --> 00:31:24,344
Um, and, and, and then, and then probably within five

707
00:31:24,344 --> 00:31:28,634
like say 2030, probably AI is smarter than the sum of all humans.

708
00:31:28,634 --> 00:31:34,514
Do you think, do you think humans are on the decline because the AI is evolving?

709
00:31:34,514 --> 00:31:41,304
Do you think there's this evolution of the ecosystem on Earth that's underway  that we don't really understand the structure of what's going on

710
00:31:41,304 --> 00:31:43,263
but.

711
00:31:43,263 --> 00:31:44,223
..

712
00:31:44,223 --> 00:31:51,244
May- yeah, maybe we implicitly know that it's coming.

713
00:31:51,244 --> 00:31:53,404
Um.

714
00:31:53,404 --> 00:31:54,764
..

715
00:31:54,764 --> 00:32:00,204
Yeah.

716
00:32:00,204 --> 00:32:02,764
I, I, I mean, I hope the birthrates turn around.

717
00:32:02,764 --> 00:32:04,404
I'm a, I'm a big proponent of

718
00:32:04,404 --> 00:32:08,364
uh, increased birthrate, uh, obviously.

719
00:32:08,364 --> 00:32:11,984
  Well, are you doing anything about it or no?

720
00:32:11,984 --> 00:32:14,704
  Yeah, I'm trying to set a good example.

721
00:32:14,704 --> 00:32:19,494
 You know, we had a big conversation at this conference we didn't expect

722
00:32:19,494 --> 00:32:24,504
which is suicidal empathy, the West, this

723
00:32:24,504 --> 00:32:27,624
um, declining birthrate.

724
00:32:27,624 --> 00:32:27,634
Yeah.

725
00:32:27,634 --> 00:32:29,624
Uh, I noticed you've been pretty active about it.

726
00:32:29,624 --> 00:32:30,784
And, and open borders.

727
00:32:30,784 --> 00:32:31,614
And open borders.

728
00:32:31,614 --> 00:32:32,714
Which is like Let the Invaders In.

729
00:32:32,714 --> 00:32:33,394
Yeah, Tucker talked about it.

730
00:32:33,394 --> 00:32:35,164
Could, could all three of those be the same thing?

731
00:32:35,164 --> 00:32:39,734
It seems like there's a number of symptoms of the West being suicidal.

732
00:32:39,734 --> 00:32:42,644
The most obvious one being the birthrate is not at replacement level

733
00:32:42,644 --> 00:32:49,574
so obviously, if that continues indefinitely, then the West will literally not reproduce enough to replace itself.

734
00:32:49,574 --> 00:32:49,574
Yeah.

735
00:32:49,574 --> 00:32:50,834
But there's other things too.

736
00:32:50,834 --> 00:32:55,854
There's the fact that the borders were totally opened to the point where Western culture

737
00:32:55,854 --> 00:32:58,204
w- the social fabric started to come apart.

738
00:32:58,204 --> 00:33:01,084
And, uh, you see this especially in Europe where their

739
00:33:01,084 --> 00:33:06,914
um, you know, the indigenous cultures of the UK or France or Germany are starting to

740
00:33:06,914 --> 00:33:11,904
um, potentially be taken over by, by cultures of people who were brought in and aren't assimilating.

741
00:33:11,904 --> 00:33:16,664
You have crime where, you know, we have this case on social media right now

742
00:33:16,664 --> 00:33:19,364
this young woman, Irina, who was just- Yeah

743
00:33:19,364 --> 00:33:19,504
yeah.

744
00:33:19,504 --> 00:33:19,514
.

745
00:33:19,514 --> 00:33:19,514
..

746
00:33:19,514 --> 00:33:22,324
killed in a senseless way on a subway

747
00:33:22,324 --> 00:33:24,444
uh, which is horrific enough in and of itself

748
00:33:24,444 --> 00:33:27,433
but then in addition to that, the elite media just

749
00:33:27,433 --> 00:33:29,104
for whatever reason, just refused to cover it

750
00:33:29,104 --> 00:33:30,184
like it didn't exist.

751
00:33:30,184 --> 00:33:30,864
Yeah.

752
00:33:30,864 --> 00:33:35,054
Um, so you have this issue of crime that's not being addressed or even- And no truth.

753
00:33:35,054 --> 00:33:35,054
.

754
00:33:35,054 --> 00:33:35,054
.

755
00:33:35,054 --> 00:33:35,593
acknowledged.

756
00:33:35,593 --> 00:33:37,544
And no acknowledgement of this.

757
00:33:37,544 --> 00:33:38,134
Like, it's- Right.

758
00:33:38,134 --> 00:33:38,134
.

759
00:33:38,134 --> 00:33:38,134
.

760
00:33:38,134 --> 00:33:43,904
almost like we're trying to deny the reality of the spiral and this

761
00:33:43,904 --> 00:33:44,144
yeah.

762
00:33:44,144 --> 00:33:44,794
The death spiral.

763
00:33:44,794 --> 00:33:45,394
So you have, so you have the

764
00:33:45,394 --> 00:33:47,494
you have all these data points, um

765
00:33:47,494 --> 00:33:51,524
that seem to suggest that, um, the West

766
00:33:51,524 --> 00:33:53,324
uh, is suicidal or doesn't, you know

767
00:33:53,324 --> 00:33:57,684
doesn't seem to want to defend itself or propagate itself.

768
00:33:57,684 --> 00:33:59,944
Um, look, I think everyone in this room thinks that

769
00:33:59,944 --> 00:34:01,524
um, life is awesome, right?

770
00:34:01,524 --> 00:34:03,004
I mean- It's pretty great.

771
00:34:03,004 --> 00:34:04,524
 And I think we're- It's worth living.

772
00:34:04,524 --> 00:34:05,324
  Yeah.

773
00:34:05,324 --> 00:34:09,074
And when Al- when Alex Karp was here earlier today defending the West

774
00:34:09,074 --> 00:34:11,152
that got some of the loudest applause of the conference.

775
00:34:11,152 --> 00:34:14,944
So, uh, I guess we probably don't really understand what's going on.

776
00:34:14,944 --> 00:34:16,393
We don't really, you know- Yeah, what's your take

777
00:34:16,393 --> 00:34:16,784
Elon?

778
00:34:16,784 --> 00:34:19,724
'Cause y- you know- What's your take on the suicide of the West?

779
00:34:19,724 --> 00:34:21,444
Yeah.

780
00:34:21,444 --> 00:34:21,994
 What's, what's going on?

781
00:34:21,994 --> 00:34:23,184
I'm very worried about it.

782
00:34:23,184 --> 00:34:24,164
Yeah.

783
00:34:24,164 --> 00:34:25,164
I'm very worried about it.

784
00:34:25,164 --> 00:34:27,454
Um, you know, I think there's a

785
00:34:27,454 --> 00:34:28,580
there's.

786
00:34:28,580 --> 00:34:28,853
..

787
00:34:28,853 --> 00:34:32,524
Let's just say that the actions of the West are indistinguishable from suicide.

788
00:34:32,524 --> 00:34:36,549
  So,  what, it's.

789
00:34:36,549 --> 00:34:37,143
..

790
00:34:37,143 --> 00:34:39,103
At, at, look, at least i- in America

791
00:34:39,103 --> 00:34:43,103
there's y- there's, there's generally a sense of optimism

792
00:34:43,103 --> 00:34:46,583
but when's the last time you m- you talked to someone from Europe

793
00:34:46,583 --> 00:34:49,143
who lives in Europe, who's optimistic?

794
00:34:49,143 --> 00:34:50,924
Not for a while.

795
00:34:50,924 --> 00:34:51,924
Yeah.

796
00:34:51,924 --> 00:34:52,284
Decades.

797
00:34:52,284 --> 00:34:53,344
Like, even one?

798
00:34:53,344 --> 00:34:56,123
 It's rare.

799
00:34:56,123 --> 00:34:57,814
And, uh, so I, I think uh

800
00:34:57,814 --> 00:35:02,644
unless people have a sense of optimism and purpose about the future

801
00:35:02,644 --> 00:35:04,261
they.

802
00:35:04,261 --> 00:35:04,884
..

803
00:35:04,884 --> 00:35:07,184
S- Suicide might be just what happens.

804
00:35:07,184 --> 00:35:12,624
Um, like, like, like having a child is an act of optimism about the future.

805
00:35:12,624 --> 00:35:18,724
So, uh, if you're not optimistic, there's hope-  Yeah.

806
00:35:18,724 --> 00:35:27,704
S- so, so, I think we need to maybe give people a sense of optimism and excitement about the future

807
00:35:27,704 --> 00:35:30,384
and a, and a belief that the future will be better than the past.

808
00:35:30,384 --> 00:35:33,704
Um, and they'll be more interested in having kids.

809
00:35:33,704 --> 00:35:36,264
Um- Did reli- did religion play a role in the past

810
00:35:36,264 --> 00:35:37,104
Elon, to- Yeah.

811
00:35:37,104 --> 00:35:37,114
.

812
00:35:37,114 --> 00:35:37,114
..

813
00:35:37,114 --> 00:35:40,444
kind of placate and make folks feel that way when they won?

814
00:35:40,444 --> 00:35:41,504
Yeah, I think so.

815
00:35:41,504 --> 00:35:46,784
Uh, uh, uh, d- nature abhors a vacuum.

816
00:35:46,784 --> 00:35:50,946
And if you take away religion, then I think you're actually.

817
00:35:50,946 --> 00:35:51,174
..

818
00:35:51,174 --> 00:35:57,264
You, you, you get something in its place which is actually worse than what was there before.

819
00:35:57,264 --> 00:35:58,924
I mean, it's, uh, like destructive basically.

820
00:35:58,924 --> 00:36:00,904
You get, you get like the white wok mind virus

821
00:36:00,904 --> 00:36:04,314
filling th- filling the hole that religion used to ha- like

822
00:36:04,314 --> 00:36:07,064
like pla- taking the place of, of

823
00:36:07,064 --> 00:36:08,164
of religion.

824
00:36:08,164 --> 00:36:11,324
You, you get these this dystopian de facto religions

825
00:36:11,324 --> 00:36:17,404
um, that, uh, that are, that are very s- very self destructive.

826
00:36:17,404 --> 00:36:19,892
Um, so-.

827
00:36:19,892 --> 00:36:20,486
..

828
00:36:20,486 --> 00:36:24,785
I, I think perhaps some- some sort of re- revival of religion

829
00:36:24,785 --> 00:36:27,245
or at least w- what we need is- is

830
00:36:27,245 --> 00:36:32,796
um, some coherent philosophy that people can get excited about.

831
00:36:32,796 --> 00:36:36,836
Um, you know, I mean, for me it's a philosophy of curiosity.

832
00:36:36,836 --> 00:36:43,935
I'm curious about the nature of the universe and I wanna go out there and I- I want humanity to be out there exploring the stars

833
00:36:43,935 --> 00:36:47,946
um, maybe meeting alien civilizations.

834
00:36:47,946 --> 00:36:52,616
Uh, maybe in some cases we- we see the ruins of a long-dead alien civilization

835
00:36:52,616 --> 00:36:56,276
but they were- they were very strong for 10 million years.

836
00:36:56,276 --> 00:36:58,995
Um, you know, the kind of stuff that you see in Star Trek

837
00:36:58,995 --> 00:37:03,576
in a- in a non-dystopian, sci-fi book or- or movie or show.

838
00:37:03,576 --> 00:37:07,506
Um, and so I- I'm just, I have a- I have a philosophy of curiosity of- of like

839
00:37:07,506 --> 00:37:09,276
I just want to know what's going on.

840
00:37:09,276 --> 00:37:11,276
And- and- and in order to know what's going on

841
00:37:11,276 --> 00:37:17,736
we- we must have, uh, an- an increase in the s- in the scope and scale of consciousness.

842
00:37:17,736 --> 00:37:20,756
We must- we must expand as a- a consciousness.

843
00:37:20,756 --> 00:37:22,515
We must grow, we must grow humanity

844
00:37:22,515 --> 00:37:36,576
and we must extend humanity in order to comprehend the- and un- to- to understand the universe or even what- what questions should we should ask about the answer that is the universe.

845
00:37:36,576 --> 00:37:39,895
Um, you know, Dou- Douglas Adams' book

846
00:37:39,895 --> 00:37:45,875
The Hi- Hitchhiker's Guide To The Galaxy is actually a bo- a- a deep book on philosophy disguised as humor.

847
00:37:45,875 --> 00:37:46,883
Um, and what.

848
00:37:46,883 --> 00:37:46,926
..

849
00:37:46,926 --> 00:37:50,006
The point he was trying to make in that book was that

850
00:37:50,006 --> 00:37:53,356
um, the questions are the really- the- the hard part.

851
00:37:53,356 --> 00:37:55,395
The- the answer is the universe, like

852
00:37:55,395 --> 00:37:57,395
the answer is everything you see around you.

853
00:37:57,395 --> 00:37:59,995
But- but- but one of the questions that we don't know to ask.

854
00:37:59,995 --> 00:38:01,116
Yeah.

855
00:38:01,116 --> 00:38:02,895
Um, now- now some of the questions

856
00:38:02,895 --> 00:38:04,356
I guess, I- we- we- I do know

857
00:38:04,356 --> 00:38:08,395
I'd like to know is the standard model of physics correct about the origins of the universe?

858
00:38:08,395 --> 00:38:09,390
Are we actually 13.

859
00:38:09,390 --> 00:38:10,895
8 billion years old?

860
00:38:10,895 --> 00:38:12,265
Um, how does the universe end?

861
00:38:12,265 --> 00:38:14,656
Does it end in heat, death or in some other way?

862
00:38:14,656 --> 00:38:19,076
 Um, you know- Or in a black hole.

863
00:38:19,076 --> 00:38:19,756
We might be.

864
00:38:19,756 --> 00:38:20,475
All right.

865
00:38:20,475 --> 00:38:22,375
Um .

866
00:38:22,375 --> 00:38:22,415
..

867
00:38:22,415 --> 00:38:25,276
Elon, can you talk about, uh- Because the wh- the whole sort of simulation question

868
00:38:25,276 --> 00:38:26,076
are we a simulation?

869
00:38:26,076 --> 00:38:26,435
Maybe.

870
00:38:26,435 --> 00:38:26,466
I don't know.

871
00:38:26,466 --> 00:38:27,719
Where does the, uh .

872
00:38:27,719 --> 00:38:27,725
..

873
00:38:27,725 --> 00:38:30,236
Where do you think we find the answer first?

874
00:38:30,236 --> 00:38:32,856
In AI or in the stars?

875
00:38:32,856 --> 00:38:37,895
'Cause you're pursuing both, obviously.

876
00:38:37,895 --> 00:38:39,236
Yeah.

877
00:38:39,236 --> 00:38:42,352
I- I don't- I don't know if- if .

878
00:38:42,352 --> 00:38:42,506
..

879
00:38:42,506 --> 00:38:49,236
I- I ho- I hope more people can get behind a philosophy of curiosity.

880
00:38:49,236 --> 00:38:50,536
Yeah.

881
00:38:50,536 --> 00:38:53,736
Because I think it's very exciting.

882
00:38:53,736 --> 00:38:54,136
Yeah.

883
00:38:54,136 --> 00:38:58,616
Um, and- and- and- and inherently optimistic.

884
00:38:58,616 --> 00:39:00,088
Um, you- the.

885
00:39:00,088 --> 00:39:00,165
..

886
00:39:00,165 --> 00:39:00,628
L- like you.

887
00:39:00,628 --> 00:39:00,705
..

888
00:39:00,705 --> 00:39:06,915
Because there's- there's this amaz- amazing sense of wonder about the nature of the universe.

889
00:39:06,915 --> 00:39:07,116
Yeah.

890
00:39:07,116 --> 00:39:10,836
And when you just, when you uncover some secret to the universe

891
00:39:10,836 --> 00:39:17,415
that's amazing, y- and you're like, a whole world of understanding has opened up.

892
00:39:17,415 --> 00:39:20,856
I mean, w- w- we used to not even know where all the continents were

893
00:39:20,856 --> 00:39:22,796
um, you know, it used to be like

894
00:39:22,796 --> 00:39:24,806
just the map would be, "There be dragons.

895
00:39:24,806 --> 00:39:27,404
"  And like, all we know is that when they sailed in that direction

896
00:39:27,404 --> 00:39:28,296
they didn't come back.

897
00:39:28,296 --> 00:39:31,911
  I mean, the moon base.

898
00:39:31,911 --> 00:39:32,006
..

899
00:39:32,006 --> 00:39:33,756
That's- that's all- that's all they knew.

900
00:39:33,756 --> 00:39:34,701
 Um.

901
00:39:34,701 --> 00:39:34,705
..

902
00:39:34,705 --> 00:39:41,466
I kind of feel like the moon base or just going to the moon for real this time would be a big step in the right direction.

903
00:39:41,466 --> 00:39:42,606
Y- you still have the moon, uh

904
00:39:42,606 --> 00:39:43,555
planned?

905
00:39:43,555 --> 00:39:44,725
What's the status of that?

906
00:39:44,725 --> 00:39:46,926
Is- is that still on the agenda?

907
00:39:46,926 --> 00:39:49,099
Yeah, I- I think it- I think having.

908
00:39:49,099 --> 00:39:49,386
..

909
00:39:49,386 --> 00:39:52,935
I- I think we want to try to reach new heights as a civilization.

910
00:39:52,935 --> 00:39:53,356
Yeah.

911
00:39:53,356 --> 00:40:01,676
So, I- I think that it's fine to go to the moon but- but we should go to the moon in order to establish a lunar base like a re- a lunar research base.

912
00:40:01,676 --> 00:40:01,995
Yeah.

913
00:40:01,995 --> 00:40:07,776
Um, I mean, there are parts of the moon that are p- perhaps older than parts of- of Earth

914
00:40:07,776 --> 00:40:12,756
um, and we- we- we might understand more about the nature of the universe if we had a science base on the moon.

915
00:40:12,756 --> 00:40:13,395
Yeah.

916
00:40:13,395 --> 00:40:15,136
Um, that would be very cool.

917
00:40:15,136 --> 00:40:17,096
And then we- we obviously want to go beyond

918
00:40:17,096 --> 00:40:19,536
uh, the moon, uh, to Mars and

919
00:40:19,536 --> 00:40:22,596
uh, build a self-sustaining city on Mars.

920
00:40:22,596 --> 00:40:23,428
The.

921
00:40:23,428 --> 00:40:23,805
..

922
00:40:23,805 --> 00:40:26,265
I- I- I do think that, uh

923
00:40:26,265 --> 00:40:29,765
that- that there is a fork in the road of human destiny where

924
00:40:29,765 --> 00:40:34,116
um, if we can establish a self-sustaining city on Mars

925
00:40:34,116 --> 00:40:34,606
uh, the.

926
00:40:34,606 --> 00:40:34,606
..

927
00:40:34,606 --> 00:40:40,116
With the- the key test being if the resupply ships from Earth stop coming for any reason

928
00:40:40,116 --> 00:40:43,236
does Mars continue to- to prosper or does it die out?

929
00:40:43,236 --> 00:40:43,935
Mm.

930
00:40:43,935 --> 00:40:46,606
At the point at which Mars is able to

931
00:40:46,606 --> 00:40:49,395
uh, prosper and grow on its own

932
00:40:49,395 --> 00:40:52,676
the probable lifespan of consciousness is dramatically greater

933
00:40:52,676 --> 00:40:58,015
because we are no longer dependent on everything going right on Earth.

934
00:40:58,015 --> 00:41:01,975
You know, there's- there's always some possibility of self-annihilation on Earth with World War III

935
00:41:01,975 --> 00:41:02,343
some.

936
00:41:02,343 --> 00:41:02,486
..

937
00:41:02,486 --> 00:41:06,296
Or- or a super virus or, um

938
00:41:06,296 --> 00:41:08,026
or- or a meteor, like, extinct- you know

939
00:41:08,026 --> 00:41:09,435
that destroyed the dinosaurs.

940
00:41:09,435 --> 00:41:13,555
We know from the fossil record that there have been many mass ex- mass extinction events.

941
00:41:13,555 --> 00:41:18,616
So, uh, the question, uh, that I sort of am- was wondering about is

942
00:41:18,616 --> 00:41:20,062
will civilization can.

943
00:41:20,062 --> 00:41:20,185
..

944
00:41:20,185 --> 00:41:29,975
Will- will the civilizational arc continue to ascend such that we can make Mars self-sustaining before the civil- civilizational arc descends?

945
00:41:29,975 --> 00:41:30,716
Mm.

946
00:41:30,716 --> 00:41:40,455
Um, because the- the- the window of opportunity to make life multi-planetary exists now for the first time in the four and a half billion year history of Earth.

947
00:41:40,455 --> 00:41:41,915
Yeah.

948
00:41:41,915 --> 00:41:43,796
Elon, let's assume that we get there

949
00:41:43,796 --> 00:41:46,036
and you're there.

950
00:41:46,036 --> 00:41:49,356
Um, you know, you'd be the elder statesman

951
00:41:49,356 --> 00:41:52,435
you'd have the moral authority of Mars.

952
00:41:52,435 --> 00:41:55,895
How do you run Mars?

953
00:41:55,895 --> 00:41:57,576
 But I just.

954
00:41:57,576 --> 00:41:57,665
..

955
00:41:57,665 --> 00:42:10,046
Uh, there's- there's a point that I- I think I- I wanna just emphasize again that- that's more important than what the form of governance on Mars or who's there in the early days.

956
00:42:10,046 --> 00:42:13,826
What really matters is that Mars, um

957
00:42:13,826 --> 00:42:16,296
is- is self-sustaining.

958
00:42:16,296 --> 00:42:22,708
That we are truly a multi-planet species and such- such that we would achieve planetary redundancy.

959
00:42:22,708 --> 00:42:22,906
..

960
00:42:22,906 --> 00:42:24,826
. so that, that if, if something ch- .

961
00:42:24,826 --> 00:42:24,926
..

962
00:42:24,926 --> 00:42:28,515
And I, and I obviously we sh- we should do everything possible to make sure life on Earth is great

963
00:42:28,515 --> 00:42:32,296
but there's always some risk that, of an annihilation event on Earth.

964
00:42:32,296 --> 00:42:32,935
Yeah.

965
00:42:32,935 --> 00:42:36,955
Um, like I said, self-annihilation or some natural disaster.

966
00:42:36,955 --> 00:42:40,015
Um, and, uh, and so the

967
00:42:40,015 --> 00:42:45,535
the probable lifespan of consciousness increases dramatically as soon as

968
00:42:45,535 --> 00:42:48,455
uh, as soon as we are a multi-planet species

969
00:42:48,455 --> 00:42:53,116
with the key test being can Mars survive if the resupply ships stop coming?

970
00:42:53,116 --> 00:42:54,236
So it's not, it's, so getting, like

971
00:42:54,236 --> 00:42:56,935
the first missions to Mars are not that important.

972
00:42:56,935 --> 00:42:59,656
The, what matters is can you get sufficient tonnage

973
00:42:59,656 --> 00:43:02,995
tonnage to Mars such that Mars can, uh

974
00:43:02,995 --> 00:43:04,676
prosper on its own?

975
00:43:04,676 --> 00:43:07,975
Um, and that means it has to have all of the ingredients of civilization.

976
00:43:07,975 --> 00:43:09,776
It, it, it's not just that you need to build

977
00:43:09,776 --> 00:43:13,216
for example, a chip factory on Mars or a ship fab on Mars

978
00:43:13,216 --> 00:43:15,518
but you, you need the ability- Ability to- .

979
00:43:15,518 --> 00:43:15,526
..

980
00:43:15,526 --> 00:43:16,466
to build ship fabs.

981
00:43:16,466 --> 00:43:18,276
Do you, do you have a sense of the time scale?

982
00:43:18,276 --> 00:43:21,966
Like, let's assume Starship is at a state starting in

983
00:43:21,966 --> 00:43:24,966
you know, 2026, then there's going to be a bunch of testing obviously

984
00:43:24,966 --> 00:43:27,156
there's going to be a bunch of early testing.

985
00:43:27,156 --> 00:43:30,606
We only have certain launch windows, so there's a bunch of time constraints.

986
00:43:30,606 --> 00:43:33,616
Is that a, is this a 50-year thing in your mind?

987
00:43:33,616 --> 00:43:35,035
Is it a 150-year thing?

988
00:43:35,035 --> 00:43:39,825
Is it something that is for our generation or is it our children's generation?

989
00:43:39,825 --> 00:43:43,296
Where do you see that point if it's optimally possible

990
00:43:43,296 --> 00:43:46,575
you know, if things go and break our way?

991
00:43:46,575 --> 00:43:49,086
Um, I think it can be done in

992
00:43:49,086 --> 00:43:51,125
in 30 years.

993
00:43:51,125 --> 00:43:51,926
Wow.

994
00:43:51,926 --> 00:43:55,296
Um, so if- if, provided there's an exponential increase in the

995
00:43:55,296 --> 00:43:59,435
in the tonnage to Mars with each successive Mars transfer window

996
00:43:59,435 --> 00:44:01,236
which is every two years.

997
00:44:01,236 --> 00:44:05,816
So every two years the, the planets align and you can tr- you can transfer to Mars.

998
00:44:05,816 --> 00:44:12,856
Um, so, uh, I think in roughly 15

999
00:44:12,856 --> 00:44:16,935
but maybe as few as 10, but as so- 10 to 15-ish

1000
00:44:16,935 --> 00:44:19,986
uh, Mars transfer windows if you're, um

1001
00:44:19,986 --> 00:44:26,856
seeing exponential increases in the tonnage to Mars with each Mars transfer window

1002
00:44:26,856 --> 00:44:29,676
then it should be possible to make Mars self-sustaining

1003
00:44:29,676 --> 00:44:33,196
um, in, in about roughly 25 years.

1004
00:44:33,196 --> 00:44:33,955
Amazing.

1005
00:44:33,955 --> 00:44:35,256
That's incredible.

1006
00:44:35,256 --> 00:44:38,035
All right, ladies and gentlemen,  Elon Musk.

1007
00:44:38,035 --> 00:44:40,595
We'll see you when we're back in town.

1008
00:44:40,595 --> 00:44:41,356
We miss you.

1009
00:44:41,356 --> 00:44:41,895
All right.

1010
00:44:41,895 --> 00:44:44,216
We'll see you in person next time.

1011
00:44:44,216 --> 00:44:44,506
Later, guys.

1012
00:44:44,506 --> 00:44:45,015
Thank you, brother.

1013
00:44:45,015 --> 00:44:46,156
Thank you, Elon.

1014
00:44:46,156 --> 00:44:46,875
All right.

1015
00:44:46,875 --> 00:44:47,336
Thanks, guys.

